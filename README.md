# ğŸ·ï¸ å¤§æ•°æ®æ ‡ç­¾ç³»ç»Ÿ

ä¼ä¸šçº§çš„å¤§æ•°æ®æ ‡ç­¾è®¡ç®—ç³»ç»Ÿï¼Œæ”¯æŒå¤šç¯å¢ƒéƒ¨ç½²ï¼ˆæœ¬åœ°ã€AWS Glueå¼€å‘ã€AWS Glueç”Ÿäº§ï¼‰ï¼Œé€šè¿‡PySparkä»S3è¯»å–æ•°æ®ï¼Œç»“åˆMySQLä¸­çš„è§„åˆ™è¿›è¡Œæ ‡ç­¾è®¡ç®—ï¼Œå¹¶å°†ç»“æœå­˜å‚¨å›MySQLã€‚

## ğŸ¯ ç³»ç»ŸåŠŸèƒ½

- âœ… ä»S3è¯»å–Hiveè¡¨æ•°æ®
- âœ… ä»MySQLè¯»å–æ ‡ç­¾è§„åˆ™é…ç½®
- âœ… åŸºäºè§„åˆ™å¼•æ“è®¡ç®—ç”¨æˆ·æ ‡ç­¾
- âœ… æ”¯æŒæ ‡ç­¾åˆå¹¶å’Œå»é‡
- âœ… å°†æ ‡ç­¾ç»“æœå†™å…¥MySQL
- âœ… æ”¯æŒå…¨é‡å’Œå¢é‡è®¡ç®—
- âœ… æ”¯æŒæŒ‡å®šæ ‡ç­¾è®¡ç®—
- âœ… å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
bigdata_tag_system/
â”œâ”€â”€ src/                          # ğŸ”§ æ ¸å¿ƒæºç 
â”‚   â”œâ”€â”€ config/                   # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ readers/                  # æ•°æ®è¯»å–å™¨
â”‚   â”œâ”€â”€ engine/                   # æ ‡ç­¾è®¡ç®—å¼•æ“
â”‚   â”œâ”€â”€ merger/                   # æ•°æ®åˆå¹¶å™¨
â”‚   â”œâ”€â”€ writers/                  # ç»“æœå†™å…¥å™¨
â”‚   â””â”€â”€ scheduler/                # ä¸»è°ƒåº¦å™¨
â”œâ”€â”€ environments/                 # ğŸŒ ç¯å¢ƒé…ç½®
â”‚   â”œâ”€â”€ local/                    # æœ¬åœ°Dockerç¯å¢ƒ
â”‚   â”œâ”€â”€ glue-dev/                 # AWS Glueå¼€å‘ç¯å¢ƒ
â”‚   â””â”€â”€ glue-prod/                # AWS Glueç”Ÿäº§ç¯å¢ƒ
â”œâ”€â”€ tests/                        # ğŸ§ª æµ‹è¯•ä»£ç 
â”œâ”€â”€ docs/                         # ğŸ“š æ–‡æ¡£
â””â”€â”€ main.py                       # ğŸ“ ç»Ÿä¸€å…¥å£
```

## âš¡ å¿«é€Ÿå¼€å§‹

### ğŸ”§ ç¯å¢ƒè¦æ±‚

- Python 3.8+
- Docker & Docker Compose (æœ¬åœ°ç¯å¢ƒ)
- AWS CLI (Glueç¯å¢ƒ)

### ğŸš€ æœ¬åœ°ç¯å¢ƒ

```bash
# 1. è®¾ç½®æœ¬åœ°ç¯å¢ƒ
cd environments/local
./setup.sh

# 2. è¿è¡Œæ ‡ç­¾è®¡ç®—
python ../../main.py --env local --mode health    # å¥åº·æ£€æŸ¥
python ../../main.py --env local --mode full      # å…¨é‡è®¡ç®—
```

### â˜ï¸ AWS Glueå¼€å‘ç¯å¢ƒ

```bash
# 1. éƒ¨ç½²åˆ°Glue
cd environments/glue-dev
python deploy.py

# 2. è¿è¡Œä½œä¸š
aws glue start-job-run --job-name tag-compute-dev \
  --arguments='--mode=full'
```

### ğŸ­ AWS Glueç”Ÿäº§ç¯å¢ƒ

```bash
# 1. éƒ¨ç½²åˆ°Glue
cd environments/glue-prod  
python deploy.py

# 2. è¿è¡Œä½œä¸š
aws glue start-job-run --job-name tag-compute-prod \
  --arguments='--mode=full'
```

## âš™ï¸ é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# Sparké…ç½®
export SPARK_APP_NAME=TagComputeSystem
export SPARK_MASTER=local[4]
export SPARK_EXECUTOR_MEMORY=4g
export SPARK_DRIVER_MEMORY=2g

# S3é…ç½®
export S3_BUCKET=your-data-bucket
export S3_ACCESS_KEY=your-access-key
export S3_SECRET_KEY=your-secret-key
export S3_ENDPOINT=http://localhost:9000  # å¯é€‰ï¼Œç”¨äºminio
export S3_REGION=us-east-1

# MySQLé…ç½®
export MYSQL_HOST=localhost
export MYSQL_PORT=3306
export MYSQL_DATABASE=tag_system
export MYSQL_USERNAME=root
export MYSQL_PASSWORD=your-password

# ç³»ç»Ÿé…ç½®
export BATCH_SIZE=10000
export MAX_RETRIES=3
export ENABLE_CACHE=true
export LOG_LEVEL=INFO
```

### ä»£ç é…ç½®

```python
from config.base_config import TagSystemConfig, SparkConfig, S3Config, MySQLConfig

config = TagSystemConfig(
    spark=SparkConfig(
        app_name="TagComputeSystem",
        master="local[4]",
        executor_memory="4g",
        driver_memory="2g"
    ),
    s3=S3Config(
        bucket="your-data-bucket",
        access_key="your-access-key",
        secret_key="your-secret-key"
    ),
    mysql=MySQLConfig(
        host="localhost",
        database="tag_system",
        username="root",
        password="password"
    )
)
```

## ğŸ“Š æ•°æ®è¡¨ç»“æ„

### MySQLæ ‡ç­¾è§„åˆ™è¡¨

```sql
-- æ ‡ç­¾åˆ†ç±»è¡¨
CREATE TABLE tag_category (
    id INT PRIMARY KEY AUTO_INCREMENT,
    category_name VARCHAR(50) NOT NULL,
    category_code VARCHAR(50) NOT NULL UNIQUE,
    status TINYINT DEFAULT 1,
    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- æ ‡ç­¾å®šä¹‰è¡¨
CREATE TABLE tag_definition (
    id INT PRIMARY KEY AUTO_INCREMENT,
    tag_name VARCHAR(100) NOT NULL,
    tag_code VARCHAR(100) NOT NULL UNIQUE,
    category_id INT NOT NULL,
    tag_type ENUM('AUTO', 'MANUAL') NOT NULL,
    status TINYINT DEFAULT 1,
    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (category_id) REFERENCES tag_category(id)
);

-- æ ‡ç­¾è§„åˆ™è¡¨
CREATE TABLE tag_rules (
    id INT PRIMARY KEY AUTO_INCREMENT,
    tag_id INT NOT NULL,
    rule_name VARCHAR(100),
    rule_description TEXT,
    condition_logic ENUM('AND', 'OR', 'NOT') DEFAULT 'AND',
    rule_conditions JSON,
    target_table VARCHAR(100),
    target_fields TEXT,
    status TINYINT DEFAULT 1,
    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (tag_id) REFERENCES tag_definition(id)
);

-- ç”¨æˆ·æ ‡ç­¾ç»“æœè¡¨
CREATE TABLE user_tags (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    user_id VARCHAR(50) NOT NULL UNIQUE,
    tag_ids JSON,  -- æˆ– TEXT (å…¼å®¹è€ç‰ˆæœ¬MySQL)
    tag_details JSON,  -- æˆ– TEXT
    updated_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_user_id (user_id)
);
```

### S3 Hiveè¡¨ç»“æ„ç¤ºä¾‹

```sql
-- ç”¨æˆ·åŸºç¡€ä¿¡æ¯è¡¨ (S3: s3://bucket/warehouse/user_basic_info/)
user_basic_info:
- user_id: string
- register_time: timestamp  
- register_country: string
- kyc_status: string
- user_level: string
- updated_time: timestamp

-- ç”¨æˆ·èµ„äº§æ±‡æ€»è¡¨ (S3: s3://bucket/warehouse/user_asset_summary/) 
user_asset_summary:
- user_id: string
- total_asset_value: decimal
- total_deposit_amount: decimal
- total_withdraw_amount: decimal
- updated_time: timestamp

-- ç”¨æˆ·æ´»åŠ¨æ±‡æ€»è¡¨ (S3: s3://bucket/warehouse/user_activity_summary/)
user_activity_summary:
- user_id: string
- last_login_time: timestamp
- login_count_7d: int
- trading_count_30d: int
- last_trading_time: timestamp
- updated_time: timestamp
```

## ğŸ”§ æ ‡ç­¾è§„åˆ™é…ç½®

### è§„åˆ™JSONæ ¼å¼

```json
{
  "logic": "AND",
  "conditions": [
    {
      "field": "total_asset_value",
      "operator": ">=",
      "value": 100000,
      "type": "number"
    },
    {
      "field": "kyc_status", 
      "operator": "=",
      "value": "verified",
      "type": "string"
    }
  ]
}
```

### æ”¯æŒçš„æ“ä½œç¬¦

| æ“ä½œç¬¦ | è¯´æ˜ | ç¤ºä¾‹ |
|-------|------|------|
| `=` | ç­‰äº | `{"field": "status", "operator": "=", "value": "active"}` |
| `!=` | ä¸ç­‰äº | `{"field": "status", "operator": "!=", "value": "inactive"}` |
| `>`, `<`, `>=`, `<=` | æ•°å€¼æ¯”è¾ƒ | `{"field": "amount", "operator": ">=", "value": 1000}` |
| `in` | åŒ…å« | `{"field": "level", "operator": "in", "value": ["VIP1", "VIP2"]}` |
| `not_in` | ä¸åŒ…å« | `{"field": "country", "operator": "not_in", "value": ["US", "UK"]}` |
| `in_range` | èŒƒå›´å†… | `{"field": "age", "operator": "in_range", "value": [18, 65]}` |
| `contains` | å­—ç¬¦ä¸²åŒ…å« | `{"field": "email", "operator": "contains", "value": "@gmail"}` |
| `recent_days` | æœ€è¿‘Nå¤© | `{"field": "login_time", "operator": "recent_days", "value": 7}` |
| `is_null` | ä¸ºç©º | `{"field": "phone", "operator": "is_null"}` |
| `is_not_null` | ä¸ä¸ºç©º | `{"field": "phone", "operator": "is_not_null"}` |

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šé«˜å‡€å€¼ç”¨æˆ·æ ‡ç­¾

```python
# æ’å…¥æ ‡ç­¾è§„åˆ™
INSERT INTO tag_rules (tag_id, rule_name, rule_conditions, target_table, target_fields) VALUES (
    1,
    'é«˜å‡€å€¼ç”¨æˆ·è§„åˆ™',
    '{"logic": "AND", "conditions": [{"field": "total_asset_value", "operator": ">=", "value": 100000, "type": "number"}]}',
    'user_asset_summary',
    'user_id,total_asset_value'
);

# è¿è¡Œè®¡ç®—
python main.py --mode tags --tag-ids 1
```

### ç¤ºä¾‹2ï¼šæ´»è·ƒç”¨æˆ·æ ‡ç­¾

```python
# æ’å…¥æ ‡ç­¾è§„åˆ™
INSERT INTO tag_rules (tag_id, rule_name, rule_conditions, target_table, target_fields) VALUES (
    2,
    'æ´»è·ƒç”¨æˆ·è§„åˆ™',
    '{"logic": "AND", "conditions": [{"field": "login_count_7d", "operator": ">=", "value": 5, "type": "number"}]}',
    'user_activity_summary', 
    'user_id,login_count_7d,last_login_time'
);

# è¿è¡Œè®¡ç®—
python main.py --mode tags --tag-ids 2
```

## ğŸ§ª æµ‹è¯•

```bash
# è¿è¡Œå•å…ƒæµ‹è¯•
python -m pytest tests/test_basic.py -v

# è¿è¡Œç¤ºä¾‹æµ‹è¯•
python run_examples.py validation

# è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
python run_examples.py all
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### Sparkä¼˜åŒ–

```python
# è°ƒæ•´Sparkå‚æ•°
spark_config = SparkConfig(
    executor_memory="8g",
    driver_memory="4g", 
    shuffle_partitions=200,
    max_result_size="4g"
)
```

### æ•°æ®è¯»å–ä¼˜åŒ–

```python
# ä½¿ç”¨åˆ†åŒºè¿‡æ»¤
partition_filter = "updated_time >= '2024-01-01'"
data = hive_reader.read_table_data('user_asset_summary', partition_filter=partition_filter)

# å­—æ®µè£å‰ª
required_fields = "user_id,total_asset_value,updated_time"
data = hive_reader.read_table_data('user_asset_summary', required_fields=required_fields)
```

### ç¼“å­˜ç­–ç•¥

```python
# ç¼“å­˜çƒ­ç‚¹æ•°æ®
if table_name in ['user_basic_info', 'user_asset_summary']:
    df = df.cache()
```

## ğŸ” ç›‘æ§å’Œæ—¥å¿—

### æ—¥å¿—é…ç½®

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('tag_system.log')
    ]
)
```

### å…³é”®æŒ‡æ ‡

- æ ‡ç­¾è®¡ç®—æ‰§è¡Œæ—¶é—´
- æ•°æ®è¯»å–é‡å’Œå¤„ç†é€Ÿåº¦
- æ ‡ç­¾å‘½ä¸­ç‡å’Œè¦†ç›–ç‡
- ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ•°æ®ä¸€è‡´æ€§**ï¼šç¡®ä¿S3æ•°æ®å’ŒMySQLè§„åˆ™çš„ä¸€è‡´æ€§
2. **èµ„æºç®¡ç†**ï¼šåˆç†è®¾ç½®Sparkèµ„æºå‚æ•°ï¼Œé¿å…OOM
3. **é”™è¯¯å¤„ç†**ï¼šé‡è¦æ“ä½œéƒ½æœ‰é‡è¯•æœºåˆ¶å’Œé”™è¯¯æ¢å¤
4. **æ•°æ®å¤‡ä»½**ï¼šå†™å…¥å‰è‡ªåŠ¨å¤‡ä»½ç°æœ‰æ•°æ®
5. **æƒé™æ§åˆ¶**ï¼šç¡®ä¿å¯¹S3å’ŒMySQLæœ‰è¶³å¤Ÿçš„è®¿é—®æƒé™

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªé¡¹ç›®ã€‚

## ğŸ“„ è®¸å¯è¯

MIT License