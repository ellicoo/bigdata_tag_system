# å¤§æ•°æ®æ ‡ç­¾ç³»ç»Ÿ

åŸºäºPySparkçš„åˆ†å¸ƒå¼æ ‡ç­¾è®¡ç®—ç³»ç»Ÿï¼Œä½¿ç”¨DSLå’ŒUDFä»S3 Hiveè¡¨è¯»å–ç”¨æˆ·æ•°æ®ï¼Œç»“åˆMySQLè§„åˆ™è¿›è¡Œæ ‡ç­¾è®¡ç®—ï¼Œä¸“ä¸ºæµ·è±šè°ƒåº¦å™¨éƒ¨ç½²è®¾è®¡ã€‚

## é¡¹ç›®æ¶æ„

### æ ¸å¿ƒç‰¹æ€§
- PySpark DSL + UDFï¼šå……åˆ†åˆ©ç”¨Spark DataFrame APIå’Œè‡ªå®šä¹‰ç”¨æˆ·å‡½æ•°
- æ™ºèƒ½å¹¶è¡Œå¤„ç†ï¼šåŸºäºè¡¨ä¾èµ–å…³ç³»çš„æ™ºèƒ½åˆ†ç»„å’Œå¹¶å‘è®¡ç®—  
- æ ‡ç­¾åˆå¹¶æœºåˆ¶ï¼šæ”¯æŒæ–°è€æ ‡ç­¾æ™ºèƒ½åˆå¹¶ï¼Œé¿å…æ•°æ®ä¸¢å¤±
- æµ·è±šè°ƒåº¦å™¨é›†æˆï¼šåŸç”Ÿæ”¯æŒDolphinScheduleréƒ¨ç½²å’Œè°ƒåº¦
- å¤šç¯å¢ƒæ”¯æŒï¼šæœ¬åœ°å¼€å‘ã€æµ‹è¯•ã€ç”Ÿäº§ç¯å¢ƒæ— ç¼åˆ‡æ¢

### æŠ€æœ¯æ ˆ
- è®¡ç®—å¼•æ“ï¼šPySpark 3.5+ (Spark SQL + DataFrame API)
- æ•°æ®æºï¼šS3 Hive Tables (Parquetæ ¼å¼)  
- è§„åˆ™å­˜å‚¨ï¼šMySQL (JSONæ ¼å¼è§„åˆ™)
- è°ƒåº¦ç³»ç»Ÿï¼šDolphinScheduler
- éƒ¨ç½²æ–¹å¼ï¼šYARN Clusteræ¨¡å¼

## é¡¹ç›®ç»“æ„

```
src/tag_engine/
â”œâ”€â”€ main.py                 # å‘½ä»¤è¡Œå…¥å£ï¼Œæ”¯æŒå¤šç§æ‰§è¡Œæ¨¡å¼
â”œâ”€â”€ engine/                 # æ ¸å¿ƒè®¡ç®—å¼•æ“
â”‚   â”œâ”€â”€ TagEngine.py       # ä¸»ç¼–æ’å¼•æ“ï¼Œå·¥ä½œæµåè°ƒ
â”‚   â””â”€â”€ TagGroup.py        # æ™ºèƒ½åˆ†ç»„ï¼ŒåŸºäºè¡¨ä¾èµ–çš„å¹¶è¡Œå¤„ç†
â”œâ”€â”€ meta/                  # æ•°æ®æºç®¡ç†
â”‚   â”œâ”€â”€ HiveMeta.py        # Hiveè¡¨æ“ä½œï¼Œæ™ºèƒ½ç¼“å­˜ä¸ä¼˜åŒ–
â”‚   â””â”€â”€ MysqlMeta.py       # MySQLè§„åˆ™å’Œç»“æœç®¡ç†
â”œâ”€â”€ parser/                # è§„åˆ™è§£æä¸SQLç”Ÿæˆ
â”‚   â””â”€â”€ TagRuleParser.py   # JSONè§„åˆ™è½¬SQLæ¡ä»¶
â””â”€â”€ utils/                 # ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°
    â””â”€â”€ SparkUdfs.py       # PySpark UDFå‡½æ•°é›†åˆï¼ˆæ¨¡å—çº§å‡½æ•°ï¼‰
```

## å¿«é€Ÿå¼€å§‹

### æœ¬åœ°å¼€å‘ç¯å¢ƒ

```bash
# 1. å¯åŠ¨æœ¬åœ°ç¯å¢ƒ (MySQL + MinIO)
cd environments/local
./setup.sh

# 2. åˆå§‹åŒ–æ•°æ®åº“å’Œæµ‹è¯•æ•°æ®
./init_data.sh

# 3. è¿è¡Œå¥åº·æ£€æŸ¥
cd ../../
python src/tag_engine/main.py --mode health

# 4. æ‰§è¡Œå…¨é‡æ ‡ç­¾è®¡ç®—
python src/tag_engine/main.py --mode task-all

# 5. è®¡ç®—æŒ‡å®šæ ‡ç­¾
python src/tag_engine/main.py --mode task-tags --tag-ids 1,2,3
```

### æµ·è±šè°ƒåº¦å™¨éƒ¨ç½²

```bash
# 1. ç”Ÿæˆéƒ¨ç½²åŒ…
python dolphin_deploy_package.py

# 2. ä¸Šä¼ åˆ°DolphinSchedulerèµ„æºä¸­å¿ƒ
# å°†ç”Ÿæˆçš„ZIPåŒ…ä¸Šä¼ åˆ°æµ·è±šè°ƒåº¦å™¨èµ„æºç®¡ç†

# 3. åˆ›å»ºSparkä»»åŠ¡
# ä¸»ç±»: src.tag_engine.main
# ç¨‹åºå‚æ•°: --mode task-all
# èµ„æºæ–‡ä»¶: bigdata_tag_system.zip
```

## æ ¸å¿ƒåŠŸèƒ½

### 1. æ™ºèƒ½æ ‡ç­¾åˆ†ç»„ (TagGroup.py)

ç³»ç»Ÿæ ¹æ®æ ‡ç­¾è§„åˆ™çš„è¡¨ä¾èµ–å…³ç³»è¿›è¡Œæ™ºèƒ½åˆ†ç»„ï¼Œå®ç°æœ€ä¼˜å¹¶è¡Œè®¡ç®—ï¼š

```python
# ç¤ºä¾‹åˆ†ç»„ç­–ç•¥:
# ç»„1: æ ‡ç­¾[1,2,3] â†’ ä¾èµ–è¡¨[user_basic_info, user_asset_summary]  
# ç»„2: æ ‡ç­¾[4,5] â†’ ä¾èµ–è¡¨[user_activity_summary]
# ç»„3: æ ‡ç­¾[6] â†’ ä¾èµ–è¡¨[user_basic_info, user_activity_summary]

def computeTagGroup(self, tagGroup):
    # 1. è·å–ç»„å†…æ‰€æœ‰æ ‡ç­¾ä¾èµ–çš„è¡¨
    # 2. æ‰§è¡Œä¸€æ¬¡æ€§JOINæ“ä½œ
    # 3. å¹¶è¡Œè®¡ç®—ç»„å†…æ‰€æœ‰æ ‡ç­¾
    # 4. è¿”å›ç”¨æˆ·æ ‡ç­¾ç»“æœ
```

### 2. PySpark DSLåº”ç”¨

å……åˆ†åˆ©ç”¨Spark DataFrame APIè¿›è¡Œåˆ†å¸ƒå¼è®¡ç®—ï¼š

```python
# Hiveè¡¨æ™ºèƒ½ç¼“å­˜å’ŒJOIN
cachedDF = spark.sql(f"SELECT * FROM {table_name}") \
               .persist(StorageLevel.MEMORY_AND_DISK)

# æ ‡ç­¾æ¡ä»¶è¿‡æ»¤
tagDF = joinedDF.filter(expr(sqlCondition)) \
               .select("user_id") \
               .withColumn("tag_id", lit(tagId))

# ç”¨æˆ·æ ‡ç­¾èšåˆ
userTagsDF = tagResultsDF.groupBy("user_id").agg(
    tagUdfs.mergeUserTags(collect_list("tag_id")).alias("tag_ids")
)
```

### 3. æ™ºèƒ½æ ‡ç­¾åˆå¹¶æœºåˆ¶

ç³»ç»Ÿé‡‡ç”¨**Sparkå†…ç½®å‡½æ•° + UDF**çš„æ··åˆç­–ç•¥ï¼Œç¡®ä¿ç±»å‹å®‰å…¨å’Œé«˜æ€§èƒ½ï¼š

#### å†…å­˜æ ‡ç­¾åˆå¹¶ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
```python
# ä½¿ç”¨Sparkå†…ç½®å‡½æ•°è¿›è¡Œæ ‡ç­¾åˆå¹¶ï¼Œé¿å…UDFåºåˆ—åŒ–å¼€é”€
finalDF = mergedDF.groupBy("user_id").agg(
    array_distinct(
        array_sort(
            flatten(collect_list("tag_ids_array"))
        )
    ).alias("merged_tag_ids")
)
```

#### è‡ªå®šä¹‰UDFå‡½æ•°ï¼ˆç±»å‹å®‰å…¨ï¼‰
```python
@udf(returnType=ArrayType(IntegerType()))
def mergeUserTags(tagList):
    """åˆå¹¶å•ä¸ªç”¨æˆ·çš„å¤šä¸ªæ ‡ç­¾ï¼šå»é‡+æ’åº
    æ”¯æŒå¤šç§è¾“å…¥ç±»å‹ï¼šList[int]ã€Array[int]ã€åµŒå¥—æ•°ç»„
    """
    if not tagList:
        return []
    
    # å¤„ç†ä¸åŒçš„è¾“å…¥ç±»å‹å’ŒåµŒå¥—æ•°ç»„
    if isinstance(tagList, list):
        flatTags = tagList
    else:
        flatTags = []
        for item in tagList:
            if isinstance(item, (list, tuple)):
                flatTags.extend(item)
            else:
                flatTags.append(item)
    
    # è¿‡æ»¤Noneå€¼ï¼Œå»é‡å¹¶æ’åº
    validTags = [tag for tag in flatTags if tag is not None]
    uniqueTags = list(set(validTags))
    uniqueTags.sort()
    return uniqueTags

@udf(returnType=ArrayType(IntegerType()))
def mergeWithExistingTags(newTags, existingTags):
    """æ–°è€æ ‡ç­¾æ™ºèƒ½åˆå¹¶"""
    if not newTags:
        newTags = []
    if not existingTags:
        existingTags = []
    
    # åˆå¹¶ã€å»é‡ã€æ’åº
    allTags = list(set(newTags + existingTags))
    allTags.sort()
    return allTags
```

### 4. JSONè§„åˆ™ç³»ç»Ÿ

æ”¯æŒå¤æ‚çš„ä¸šåŠ¡è§„åˆ™å®šä¹‰ï¼š

```json
{
  "logic": "AND",
  "conditions": [
    {
      "fields": [
        {
          "table": "user_basic_info",
          "field": "age",
          "operator": ">=", 
          "value": 30,
          "type": "number"
        },
        {
          "table": "user_asset_summary", 
          "field": "total_assets",
          "operator": ">=",
          "value": 100000,
          "type": "number"
        }
      ]
    }
  ]
}
```

## æ€§èƒ½ä¼˜åŒ–

### 1. æ™ºèƒ½ç¼“å­˜ç­–ç•¥
- **è¡¨çº§ç¼“å­˜**ï¼šé¢‘ç¹è®¿é—®çš„Hiveè¡¨ç¼“å­˜åˆ°å†…å­˜ï¼Œä½¿ç”¨`persist(StorageLevel.MEMORY_AND_DISK)`
- **åˆ†åŒºä¼˜åŒ–**ï¼šåŠ¨æ€è°ƒæ•´åˆ†åŒºæ•°æå‡å¹¶è¡Œåº¦ï¼Œé¿å…å°æ–‡ä»¶é—®é¢˜
- **å­—æ®µæŠ•å½±**ï¼šåªåŠ è½½å¿…è¦å­—æ®µå‡å°‘I/Oï¼Œä½¿ç”¨`select()`ç²¾ç¡®é€‰æ‹©å­—æ®µ

### 2. ç±»å‹å®‰å…¨ä¸æ€§èƒ½å¹¶é‡
- **Sparkå†…ç½®å‡½æ•°ä¼˜å…ˆ**ï¼šä½¿ç”¨`array_distinct`ã€`array_sort`ã€`flatten`ç­‰åŸç”Ÿå‡½æ•°
- **UDFå¤‡ç”¨ç­–ç•¥**ï¼šå¤æ‚é€»è¾‘ä½¿ç”¨ç±»å‹å®‰å…¨çš„UDFï¼Œæ”¯æŒå¤šç§è¾“å…¥ç±»å‹
- **åºåˆ—åŒ–ä¼˜åŒ–**ï¼šå‡å°‘UDFè°ƒç”¨ï¼Œé¿å…Python-JVMåºåˆ—åŒ–å¼€é”€

### 3. å¹¶è¡Œå¤„ç†ä¼˜åŒ–  
- **ä¾èµ–åˆ†æ**ï¼šåŸºäºè¡¨ä¾èµ–å…³ç³»çš„æ™ºèƒ½åˆ†ç»„ï¼Œæœ€å°åŒ–JOINæ“ä½œ
- **æ‰¹é‡è®¡ç®—**ï¼šåŒç»„æ ‡ç­¾å¹¶è¡Œè®¡ç®—ï¼Œå…±äº«è¡¨è¯»å–å’ŒJOINç»“æœ
- **èµ„æºç®¡ç†**ï¼šåˆç†çš„Sparkèµ„æºé…ç½®å’Œå†…å­˜ç®¡ç†

### 4. æ•°æ®å†™å…¥ä¼˜åŒ–
- **æ‰¹é‡UPSERT**ï¼šé«˜æ•ˆçš„MySQLæ‰¹é‡æ›´æ–°ï¼Œæ”¯æŒæ ‡ç­¾åˆå¹¶
- **æ ‡ç­¾åˆå¹¶ç®—æ³•**ï¼š
  ```
  å†…å­˜åˆå¹¶: Array[Array[Int]] â†’ flatten â†’ Array[Int] â†’ distinct â†’ sort
  MySQLåˆå¹¶: newTags + existingTags â†’ merge â†’ deduplicate â†’ JSON
  ```
- **æ—¶é—´æˆ³ç®¡ç†**ï¼šç²¾ç¡®çš„åˆ›å»ºå’Œæ›´æ–°æ—¶é—´è¿½è¸ªï¼Œæ”¯æŒå¹‚ç­‰æ“ä½œ

### 5. æ¶æ„ä¼˜åŒ–äº®ç‚¹
- **é›¶æ•°æ®ä¸¢å¤±**ï¼šæ™ºèƒ½æ ‡ç­¾åˆå¹¶ï¼Œæ–°è€æ ‡ç­¾å®Œç¾èåˆ
- **ç±»å‹å…¼å®¹**ï¼šæ”¯æŒ`List[int]`ã€`Array[int]`ã€åµŒå¥—æ•°ç»„ç­‰å¤šç§ç±»å‹
- **é”™è¯¯æ¢å¤**ï¼šå•ä¸ªæ ‡ç­¾ç»„å¤±è´¥ä¸å½±å“å…¶ä»–ç»„è®¡ç®—
- **èµ„æºæ¸…ç†**ï¼šè‡ªåŠ¨ç¼“å­˜æ¸…ç†å’ŒSparkä¼šè¯ç®¡ç†

## æ‰§è¡Œæ¨¡å¼

ç³»ç»Ÿæ”¯æŒå¤šç§æ‰§è¡Œæ¨¡å¼ï¼Œé€‚é…ä¸åŒä¸šåŠ¡åœºæ™¯ï¼š

| æ¨¡å¼ | å‘½ä»¤ | è¯´æ˜ |
|------|------|------|
| å¥åº·æ£€æŸ¥ | --mode health | æ£€æŸ¥Hiveå’ŒMySQLè¿æ¥çŠ¶æ€ |
| å…¨é‡è®¡ç®— | --mode task-all | è®¡ç®—æ‰€æœ‰æ¿€æ´»æ ‡ç­¾ |
| æŒ‡å®šæ ‡ç­¾ | --mode task-tags --tag-ids 1,2,3 | è®¡ç®—æŒ‡å®šæ ‡ç­¾ID |
| æµ‹è¯•æ•°æ®ç”Ÿæˆ | --mode generate-test-data --dt 2025-01-20 | ç”Ÿæˆæµ‹è¯•æ•°æ® |
| ä»»åŠ¡åˆ—è¡¨ | --mode list-tasks | åˆ—å‡ºå¯ç”¨æ ‡ç­¾ä»»åŠ¡ |

## æµ·è±šè°ƒåº¦å™¨é›†æˆ

### éƒ¨ç½²é…ç½®

```python
# environments/dolphinscheduler/config.py
class DolphinschedulerConfig(BaseConfig):
    def __init__(self):
        super().__init__(
            environment='dolphinscheduler',
            spark=SparkConfig(
                app_name="BigDataTagSystem-Dolphin",
                master="yarn",
                executor_memory="4g",
                driver_memory="2g",
                executor_cores=2,
                num_executors=10
            )
        )
```

### ä»»åŠ¡è°ƒåº¦ç¤ºä¾‹

```bash
# æµ·è±šè°ƒåº¦å™¨Sparkä»»åŠ¡é…ç½®
ä¸»ç±»: src.tag_engine.main
ç¨‹åºå‚æ•°: --mode task-all
éƒ¨ç½²æ¨¡å¼: cluster  
é©±åŠ¨ç¨‹åºå†…å­˜: 2g
æ‰§è¡Œå™¨å†…å­˜: 4g
æ‰§è¡Œå™¨æ•°é‡: 10
```

## æ•°æ®æµæ¶æ„

```
S3 Hive Tables â†’ TagEngine â†’ Smart Grouping â†’ Parallel Computation â†’ Tag Merging â†’ MySQL Results
     â†“              â†“            â†“                   â†“                â†“             â†“
  ç”¨æˆ·æ•°æ®        è§„åˆ™åŠ è½½      ä¾èµ–åˆ†æ           å¹¶è¡Œæ ‡ç­¾è®¡ç®—         ç»“æœåˆå¹¶      æŒä¹…åŒ–å­˜å‚¨
```

### æ•°æ®æ¨¡å‹

è¾“å…¥æ•°æ®:
- user_basic_info: ç”¨æˆ·åŸºç¡€ä¿¡æ¯ (å¹´é¾„ã€æ€§åˆ«ã€æ³¨å†Œæ—¶é—´ç­‰)
- user_asset_summary: ç”¨æˆ·èµ„äº§æ±‡æ€» (æ€»èµ„äº§ã€ç°é‡‘ä½™é¢ç­‰)  
- user_activity_summary: ç”¨æˆ·æ´»åŠ¨æ±‡æ€» (äº¤æ˜“æ¬¡æ•°ã€ç™»å½•æ—¶é—´ç­‰)

è¾“å‡ºç»“æœ:
- user_tagsè¡¨: user_id â†’ tag_ids (JSONæ•°ç»„æ ¼å¼: [1,2,3,5])

## ç›‘æ§å’Œè¿ç»´

### ç³»ç»Ÿå¥åº·æ£€æŸ¥
```bash
python src/tag_engine/main.py --mode health
```

è¾“å‡ºç¤ºä¾‹:
```
[INFO] Hiveè¿æ¥çŠ¶æ€: âœ“ æ­£å¸¸ (3ä¸ªè¡¨å¯ç”¨)
[INFO] MySQLè¿æ¥çŠ¶æ€: âœ“ æ­£å¸¸ (5ä¸ªæ¿€æ´»æ ‡ç­¾)
[INFO] ç³»ç»Ÿå†…å­˜ä½¿ç”¨: 2.1GB / 8GB  
[INFO] Sparkæ‰§è¡Œå™¨çŠ¶æ€: 10ä¸ªæ‰§è¡Œå™¨æ­£å¸¸è¿è¡Œ
```

### æ€§èƒ½æŒ‡æ ‡
- è®¡ç®—é€Ÿåº¦ï¼š100ä¸‡ç”¨æˆ· Ã— 10ä¸ªæ ‡ç­¾ â‰ˆ 5-8åˆ†é’Ÿ
- å†…å­˜æ•ˆç‡ï¼šæ™ºèƒ½ç¼“å­˜ + åˆ†åŒºä¼˜åŒ–ï¼Œå†…å­˜ä½¿ç”¨ç‡ < 70%
- å‡†ç¡®æ€§ï¼šæ ‡ç­¾åˆå¹¶é›¶ä¸¢å¤±ï¼Œæ”¯æŒå¹‚ç­‰æ“ä½œ

## æµ‹è¯•æ¡†æ¶

### æµ‹è¯•æ¶æ„
é¡¹ç›®é‡‡ç”¨ **pytest + PySpark** æµ‹è¯•æ¡†æ¶ï¼Œæä¾›å®Œæ•´çš„å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•èƒ½åŠ›ã€‚

### æµ‹è¯•è¿è¡Œ

#### 1. è¿è¡Œå…¨éƒ¨æµ‹è¯•
```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•ï¼ˆæ¨èï¼‰
python -m pytest tests/ -v

# å¸¦è¦†ç›–ç‡æŠ¥å‘Š
python -m pytest tests/ -v --cov=src/tag_engine --cov-report=html
```

#### 2. è¿è¡Œç‰¹å®šæ¨¡å—æµ‹è¯•
```bash
# è§„åˆ™è§£æå™¨æµ‹è¯•
python -m pytest tests/test_rule_parser.py -v

# æ ‡ç­¾åˆ†ç»„æµ‹è¯•  
python -m pytest tests/test_tag_grouping.py -v

# è¿è¡Œç‰¹å®šæµ‹è¯•ç”¨ä¾‹
python -m pytest tests/test_rule_parser.py::TestTagRuleParser::test_not_logic -v
```

#### 3. æµ‹è¯•ç»“æœç¤ºä¾‹
```
======================== test session starts ========================
tests/test_rule_parser.py::TestTagRuleParser::test_init PASSED [  4%]
tests/test_rule_parser.py::TestTagRuleParser::test_simple_number_condition_sql_generation PASSED [  9%]
tests/test_rule_parser.py::TestTagRuleParser::test_complex_multi_condition_and_logic PASSED [ 36%]
tests/test_rule_parser.py::TestTagRuleParser::test_not_logic PASSED [ 45%]
tests/test_tag_grouping.py::TestTagGrouping::test_analyze_dependencies_single_table PASSED [ 68%]
tests/test_tag_grouping.py::TestTagGrouping::test_group_tags_complex_scenario PASSED [ 90%]
======================== 22 passed, 1 warning in 13.07s ========================
```

### æµ‹è¯•è¦†ç›–èŒƒå›´

#### **è§„åˆ™è§£æå™¨æµ‹è¯•** (14ä¸ªæµ‹è¯•ç”¨ä¾‹)
- âœ… **åŸºç¡€åŠŸèƒ½**: åˆå§‹åŒ–ã€SQLç”Ÿæˆã€æ¡ä»¶è§£æ
- âœ… **æ•°æ®ç±»å‹**: æ•°å€¼ã€å­—ç¬¦ä¸²ã€æšä¸¾ã€æ—¥æœŸã€å¸ƒå°”æ¡ä»¶
- âœ… **æ“ä½œç¬¦æ”¯æŒ**: `=`, `!=`, `>=`, `<=`, `LIKE`, `IN`, `BETWEEN`, `IS NULL` ç­‰
- âœ… **å¤æ‚é€»è¾‘**: AND/OR/NOT åµŒå¥—æ¡ä»¶ã€å­—æ®µé€»è¾‘ä¼˜å…ˆçº§
- âœ… **å­—ç¬¦ä¸²åŒ¹é…**: `contains`, `starts_with`, `ends_with` æ¨¡å¼
- âœ… **åˆ—è¡¨æ“ä½œ**: `contains_any`, `contains_all`, `array_contains`
- âœ… **å¼‚å¸¸å¤„ç†**: æ— æ•ˆJSONã€ç©ºè§„åˆ™ã€è¾¹ç•Œæƒ…å†µ

#### **æ ‡ç­¾åˆ†ç»„æµ‹è¯•** (8ä¸ªæµ‹è¯•ç”¨ä¾‹)  
- âœ… **ä¾èµ–åˆ†æ**: å•è¡¨ä¾èµ–ã€å¤šè¡¨ä¾èµ–ã€å­—æ®µä¾èµ–åˆ†æ
- âœ… **æ™ºèƒ½åˆ†ç»„**: ç›¸åŒè¡¨åˆ†ç»„ã€ä¸åŒè¡¨åˆ†ç»„ã€å¤æ‚åœºæ™¯ç»„åˆ
- âœ… **åˆ†ç»„ä¼˜åŒ–**: ä¾èµ–è¡¨ç»„åˆçš„å‡†ç¡®æ€§å’Œæ•ˆç‡éªŒè¯
- âœ… **è¾¹ç•Œå¤„ç†**: ç©ºè§„åˆ™ã€æ— æ•ˆè§„åˆ™çš„å¥å£®æ€§æµ‹è¯•

### æµ‹è¯•æ•°æ®æ¨¡å‹

#### **æµ‹è¯•ç¯å¢ƒé…ç½®** (`tests/conftest.py`)
```python
@pytest.fixture(scope="session")
def spark():
    """æœ¬åœ°Sparkä¼šè¯ - æµ‹è¯•ä¼˜åŒ–é…ç½®"""
    return SparkSession.builder \
        .appName("TagSystem_Test") \
        .master("local[2]") \
        .config("spark.sql.adaptive.enabled", "false") \
        .getOrCreate()

@pytest.fixture  
def sample_user_data():
    """çœŸå®ä¸šåŠ¡åœºæ™¯æµ‹è¯•æ•°æ®"""
    return {
        "user_basic_info": [
            ("user001", 30, "VIP2", "verified", True),
            ("user002", 25, "VIP1", "verified", False),
            # ... æ›´å¤šæµ‹è¯•ç”¨æˆ·
        ],
        "user_asset_summary": [...],
        "user_activity_summary": [...]
    }
```

#### **å¤æ‚è§„åˆ™æµ‹è¯•ç”¨ä¾‹**
```python
# æµ‹è¯•é«˜å‡€å€¼ç”¨æˆ·æ ‡ç­¾ï¼ˆå¤šæ¡ä»¶ANDï¼‰
{
    "logic": "AND",
    "conditions": [
        {
            "condition": {
                "logic": "OR", 
                "fields": [
                    {"table": "user_basic_info", "field": "user_level", "operator": "belongs_to", "value": ["VIP2", "VIP3"]},
                    {"table": "user_asset_summary", "field": "total_asset_value", "operator": ">=", "value": "100000"}
                ]
            }
        },
        {
            "condition": {
                "logic": "AND",
                "fields": [
                    {"table": "user_basic_info", "field": "kyc_status", "operator": "=", "value": "verified"},
                    {"table": "user_activity_summary", "field": "trade_count_30d", "operator": ">", "value": "5"}
                ]
            }
        }
    ]
}
```

### æµ‹è¯•æœ€ä½³å®è·µ

#### **å•å…ƒæµ‹è¯•åŸåˆ™**
```python
def test_simple_number_condition_sql_generation(self):
    """æµ‹è¯•æ•°å€¼æ¡ä»¶SQLç”Ÿæˆ - è¦†ç›–å•è¡¨å’Œå¤šè¡¨åœºæ™¯"""
    parser = TagRuleParser()
    
    # æµ‹è¯•å¤šè¡¨åœºæ™¯
    sql = parser.parseRuleToSql(rule_json, ["user_asset_summary", "user_basic_info"])
    expected = "`tag_system.user_asset_summary`.`total_asset_value` >= 100000"
    assert expected in sql
    
    # æµ‹è¯•å•è¡¨åœºæ™¯  
    sql_single = parser.parseRuleToSql(rule_json, ["user_asset_summary"])
    expected_single = "`user_asset_summary`.`total_asset_value` >= 100000"
    assert expected_single in sql_single
```

#### **é›†æˆæµ‹è¯•ç­–ç•¥**
- **ä¾èµ–éš”ç¦»**: ä½¿ç”¨å†…å­˜DataFrameæ¨¡æ‹ŸHiveè¡¨ï¼Œé¿å…å¤–éƒ¨ä¾èµ–
- **æ•°æ®é©±åŠ¨**: å‚æ•°åŒ–æµ‹è¯•è¦†ç›–å¤šç§ä¸šåŠ¡åœºæ™¯
- **æ–­è¨€å®Œæ•´**: éªŒè¯SQLè¯­æ³•ã€é€»è¾‘ç»“æ„ã€è¾¹ç•Œæƒ…å†µ

### æŒç»­é›†æˆæ”¯æŒ

#### **GitHub Actionsé…ç½®**
```yaml
# .github/workflows/test.yml
name: Test Suite
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.12
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run tests
        run: python -m pytest tests/ -v --cov=src/tag_engine
```

#### **æµ‹è¯•æ€§èƒ½åŸºå‡†**
- **æµ‹è¯•é€Ÿåº¦**: 22ä¸ªæµ‹è¯•ç”¨ä¾‹ â‰ˆ 13ç§’
- **è¦†ç›–ç‡ç›®æ ‡**: > 85% ä»£ç è¦†ç›–
- **æµ‹è¯•ç¨³å®šæ€§**: 100% é€šè¿‡ç‡ï¼Œæ— éšæœºå¤±è´¥

## å¼€å‘æŒ‡å—

### æ–°å¢æ ‡ç­¾æ­¥éª¤

1. MySQLè§„åˆ™é…ç½®:
```sql
INSERT INTO tag_rules (tag_id, rule_content, status) VALUES 
(æ–°æ ‡ç­¾ID, JSONè§„åˆ™, 'active');
```

2. æµ‹è¯•éªŒè¯:
```bash
# å…ˆè¿è¡Œç›¸å…³æµ‹è¯•éªŒè¯è§„åˆ™è§£æ
python -m pytest tests/test_rule_parser.py -v

# å†æµ‹è¯•æ ‡ç­¾è®¡ç®—
python src/tag_engine/main.py --mode task-tags --tag-ids æ–°æ ‡ç­¾ID
```

3. æµ·è±šè°ƒåº¦å™¨éƒ¨ç½²:
```bash
python dolphin_deploy_package.py
# ä¸Šä¼ æ–°çš„éƒ¨ç½²åŒ…åˆ°èµ„æºä¸­å¿ƒ
```

### è‡ªå®šä¹‰UDFå¼€å‘

```python
# åœ¨SparkUdfs.pyä¸­æ·»åŠ æ–°çš„æ¨¡å—çº§å‡½æ•°
def custom_tag_logic(input_column):
    """è‡ªå®šä¹‰æ ‡ç­¾é€»è¾‘ - ä½¿ç”¨SparkåŸç”Ÿå‡½æ•°
    é¿å…UDFåºåˆ—åŒ–å¼€é”€ï¼Œä¼˜å…ˆä½¿ç”¨Columnè¡¨è¾¾å¼
    """
    return when(input_column.isNotNull() & (input_column > 0), lit(True)).otherwise(lit(False))

# å¤æ‚é€»è¾‘æ‰ä½¿ç”¨UDF
@udf(returnType=ArrayType(IntegerType()))  
def complex_tag_udf(input_data):
    """ä»…åœ¨å¿…è¦æ—¶ä½¿ç”¨UDFï¼Œç¡®ä¿ç±»å‹å®‰å…¨"""
    if not input_data:
        return []
    return process_complex_logic(input_data)
```

### æµ‹è¯•é©±åŠ¨å¼€å‘æµç¨‹

1. **ç¼–å†™æµ‹è¯•ç”¨ä¾‹**:
```python
def test_new_feature_logic(self):
    """æ–°åŠŸèƒ½æµ‹è¯• - å…ˆå†™æµ‹è¯•ï¼Œå†å†™å®ç°"""
    parser = TagRuleParser()
    result = parser.new_feature_method(test_input)
    assert result == expected_output
```

2. **è¿è¡Œæµ‹è¯•éªŒè¯**:
```bash
python -m pytest tests/test_new_feature.py::test_new_feature_logic -v
```

3. **å®ç°åŠŸèƒ½ä»£ç **:
```python
def new_feature_method(self, input_data):
    """å®ç°æ–°åŠŸèƒ½ï¼Œç¡®ä¿æµ‹è¯•é€šè¿‡"""
    return processed_result
```

4. **å®Œæ•´æµ‹è¯•éªŒè¯**:
```bash
python -m pytest tests/ -v  # ç¡®ä¿ä¸ç ´åç°æœ‰åŠŸèƒ½
```

## ğŸ”§ æŠ€æœ¯äº®ç‚¹æ€»ç»“

### ç±»å‹å®‰å…¨ä¿éšœ
- âœ… **å®Œæ•´ç±»å‹æµ**ï¼š`Array[Array[Int]] â†’ flatten â†’ Array[Int] â†’ distinct â†’ sort`
- âœ… **UDFç±»å‹å…¼å®¹**ï¼šæ”¯æŒ`List[int]`ã€`Array[int]`ã€åµŒå¥—æ•°ç»„ç­‰å¤šç§è¾“å…¥
- âœ… **è¾¹ç•Œæƒ…å†µå¤„ç†**ï¼šNoneå€¼è¿‡æ»¤ã€ç©ºæ•°ç»„å¤„ç†ã€å¼‚å¸¸æ¢å¤

### æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
- âš¡ **Sparkå†…ç½®å‡½æ•°ä¼˜å…ˆ**ï¼šé¿å…UDFåºåˆ—åŒ–å¼€é”€ï¼Œæå‡è®¡ç®—æ€§èƒ½
- âš¡ **æ™ºèƒ½ç¼“å­˜æœºåˆ¶**ï¼šè¡¨çº§ç¼“å­˜ + å­—æ®µæŠ•å½±ï¼Œå‡å°‘é‡å¤I/O
- âš¡ **å¹¶è¡Œè®¡ç®—ä¼˜åŒ–**ï¼šè¡¨ä¾èµ–åˆ†ç»„ + æ‰¹é‡è®¡ç®—ï¼Œæœ€å¤§åŒ–èµ„æºåˆ©ç”¨

### æ¶æ„è®¾è®¡äº®ç‚¹
- ğŸ—ï¸ **æ¨¡å—åŒ–è®¾è®¡**ï¼šTagEngineã€TagGroupã€UDFsèŒè´£æ¸…æ™°åˆ†ç¦»
- ğŸ—ï¸ **ç»Ÿä¸€å…¥å£ç®¡ç†**ï¼š`src/tag_engine/main.py`ä½œä¸ºå”¯ä¸€çœŸå®æ¥æº
- ğŸ—ï¸ **å¤šç¯å¢ƒæ”¯æŒ**ï¼šæœ¬åœ°å¼€å‘ã€æµ·è±šè°ƒåº¦å™¨éƒ¨ç½²æ— ç¼åˆ‡æ¢

### ç”Ÿäº§å°±ç»ªç‰¹æ€§
- ğŸš€ **æµ·è±šè°ƒåº¦å™¨é›†æˆ**ï¼šåŸç”Ÿæ”¯æŒYARNé›†ç¾¤éƒ¨ç½²
- ğŸš€ **å¥åº·æ£€æŸ¥æœºåˆ¶**ï¼šå®Œæ•´çš„ç³»ç»ŸçŠ¶æ€ç›‘æ§
- ğŸš€ **é”™è¯¯æ¢å¤èƒ½åŠ›**ï¼šå•ç‚¹å¤±è´¥ä¸å½±å“å…¨å±€è®¡ç®—

---

## ğŸ¯ è®©æ•°æ®é©±åŠ¨ä¸šåŠ¡ï¼Œè®©æ ‡ç­¾åˆ›é€ ä»·å€¼ï¼

**åŸºäºPySpark DSL + UDFçš„ä¼ä¸šçº§æ ‡ç­¾è®¡ç®—ç³»ç»Ÿï¼ŒåŠ©åŠ›ç²¾å‡†è¥é”€å’Œç”¨æˆ·æ´å¯Ÿ**