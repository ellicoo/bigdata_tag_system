#!/usr/bin/env python3
"""
æµ·è±šè°ƒåº¦å™¨ä¸»ç¨‹åºå…¥å£
æ”¯æŒé€šè¿‡æµ·è±šè°ƒåº¦å™¨å›¾å½¢ç•Œé¢çš„ä¸»ç¨‹åºå‚æ•°æ‰§è¡Œ
ä½¿ç”¨ç»Ÿä¸€çš„MySQLé…ç½®ï¼ˆsrc.config.base.MySQLConfigï¼‰
"""

import sys
import os
import argparse
from pyspark.sql import SparkSession

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)


def create_spark_session():
    """åˆ›å»ºSparkä¼šè¯ - åŸºäºç°æœ‰HiveToKafka.pyæ¨¡å¼"""
    spark = SparkSession.builder \
        .appName("BigDataTagSystem-Dolphin") \
        .enableHiveSupport() \
        .getOrCreate()
    return spark


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = argparse.ArgumentParser(description="æµ·è±šè°ƒåº¦å™¨æ ‡ç­¾ç³»ç»Ÿ")
    parser.add_argument("--mode", required=True, choices=[
        "health", "task-all", "task-tags", "task-users", "list-tasks", "generate-test-data"
    ], help="æ‰§è¡Œæ¨¡å¼")
    parser.add_argument("--tag-ids", help="æ ‡ç­¾IDåˆ—è¡¨ï¼Œé€—å·åˆ†éš”")
    parser.add_argument("--user-ids", help="ç”¨æˆ·IDåˆ—è¡¨ï¼Œé€—å·åˆ†éš”")
    parser.add_argument("--dt", default="2025-01-20", help="æ•°æ®æ—¥æœŸ")

    args = parser.parse_args()

    print(f"ğŸš€ æµ·è±šè°ƒåº¦å™¨æ ‡ç­¾ç³»ç»Ÿå¯åŠ¨")
    print(f"ğŸ“‹ æ‰§è¡Œæ¨¡å¼: {args.mode}")

    # åˆ›å»ºSparkä¼šè¯
    spark = create_spark_session()

    try:
        if args.mode == "generate-test-data":
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            print("ğŸš€ å¼€å§‹ç”Ÿæˆæµ‹è¯•æ•°æ®...")

            # å…ˆåˆ›å»ºæ•°æ®åº“
            spark.sql("CREATE DATABASE IF NOT EXISTS tag_test")
            print("âœ… æ•°æ®åº“ tag_test åˆ›å»ºæˆåŠŸ")

            from generate_test_data import generate_test_data
            generate_test_data(spark, args.dt)

            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            # from generate_test_data import generate_test_data
            # generate_test_data(spark, args.dt)

        elif args.mode == "health":
            # å¥åº·æ£€æŸ¥
            print("ğŸ” æ‰§è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥...")

            # æ£€æŸ¥Hiveè¡¨è®¿é—®
            try:
                spark.sql("SHOW DATABASES").show()
                print("âœ… Hiveè®¿é—®æ­£å¸¸")
            except Exception as e:
                print(f"âŒ Hiveè®¿é—®å¤±è´¥: {e}")
                return 1

            # æ£€æŸ¥MySQLè¿æ¥ï¼ˆä½¿ç”¨ç»Ÿä¸€é…ç½®ï¼‰
            try:
                from src.config.base import MySQLConfig
                config = MySQLConfig()

                # æµ‹è¯•MySQLè¿æ¥
                mysql_df = spark.read \
                    .format("jdbc") \
                    .option("url", config.jdbc_url) \
                    .option("driver", "com.mysql.cj.jdbc.Driver") \
                    .option("user", config.username) \
                    .option("password", config.password) \
                    .option("query", "SELECT 1 as test") \
                    .load()

                mysql_df.show()
                print("âœ… MySQLè¿æ¥æ­£å¸¸")

            except Exception as e:
                print(f"âŒ MySQLè¿æ¥å¤±è´¥: {e}")
                return 1

            print("ğŸ‰ ç³»ç»Ÿå¥åº·æ£€æŸ¥é€šè¿‡")

        elif args.mode == "task-all":
            # å…¨é‡æ ‡ç­¾è®¡ç®—
            from src.entry.tag_system_api import TagSystemAPI

            with TagSystemAPI(environment='dolphinscheduler') as api:
                success = api.run_task_all_users_all_tags()
                if not success:
                    return 1

        elif args.mode == "task-tags":
            # æŒ‡å®šæ ‡ç­¾è®¡ç®—
            if not args.tag_ids:
                print("âŒ æŒ‡å®šæ ‡ç­¾æ¨¡å¼éœ€è¦æä¾› --tag-ids å‚æ•°")
                return 1

            tag_ids = [int(x.strip()) for x in args.tag_ids.split(',')]

            from src.entry.tag_system_api import TagSystemAPI
            with TagSystemAPI(environment='dolphinscheduler') as api:
                success = api.run_task_specific_tags(tag_ids)
                if not success:
                    return 1

        elif args.mode == "list-tasks":
            # åˆ—å‡ºå¯ç”¨ä»»åŠ¡
            from src.tasks.task_registry import TagTaskFactory
            TagTaskFactory.register_all_tasks()
            tasks = TagTaskFactory.get_all_available_tasks()

            print("ğŸ“‹ å¯ç”¨æ ‡ç­¾ä»»åŠ¡:")
            for task_id, task_class in tasks.items():
                print(f"  {task_id}: {task_class.__name__}")

        else:
            print(f"âŒ ä¸æ”¯æŒçš„æ¨¡å¼: {args.mode}")
            return 1

        print("âœ… ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ")
        return 0

    except Exception as e:
        print(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1

    finally:
        spark.stop()


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)