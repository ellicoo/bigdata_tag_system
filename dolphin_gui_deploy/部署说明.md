# 🐬 海豚调度器图形界面部署说明

## 📦 部署包内容（统一架构版）
- `main.py` - 主程序入口（统一wrapper，调用src/tag_engine/main.py）
- `src/` - 项目源码（PySpark DSL + UDF架构）  
- `generate_test_data.py` - 测试数据生成器
- `create_test_tables.sql` - 测试表创建SQL
- `requirements.txt` - Python依赖

## 🏗️ 架构说明
- **核心入口**: `src/tag_engine/main.py` - 完整的标签计算引擎
- **部署入口**: `main.py` - 简单wrapper，统一调用核心入口
- **消除重复**: 所有入口都统一调用核心main.py，避免代码重复

## 🚀 UI界面部署步骤（推荐）

### 1. 上传ZIP包到资源中心
1. 登录海豚调度器Web界面
2. 进入 **资源中心** → **文件管理**
3. 上传 `tag_system_dolphin.zip`

### 2. 创建Shell任务 - 解压部署包
创建Shell任务来解压部署包：

**任务名称**: `unzip_deploy_package`
**任务类型**: Shell
**脚本内容**:
```bash
#!/bin/bash
cd /dolphinscheduler/default/resources/
unzip -o tag_system_dolphin.zip -d bigdata_tag_system/
echo "✅ 部署包解压完成到: /dolphinscheduler/default/resources/bigdata_tag_system/"
ls -la bigdata_tag_system/main.py bigdata_tag_system/src/ bigdata_tag_system/requirements.txt

```

### 3. 创建Shell任务 - 文件权限修复
创建第二个Shell任务来修复文件权限：

**任务名称**: `fix_permissions`
**任务类型**: Shell
**依赖任务**: `unzip_deploy_package`
**脚本内容**:
```bash
#!/bin/bash
chmod +x /dolphinscheduler/default/resources/bigdata_tag_system/main.py
chmod -R 755 /dolphinscheduler/default/resources/bigdata_tag_system/
echo "✅ 文件权限修复完成"
```

### 4. 创建Shell任务 - 安装依赖（可选）
创建第三个Shell任务来安装Python依赖：

**任务名称**: `install_dependencies`
**任务类型**: Shell
**依赖任务**: `fix_permissions`
**脚本内容**:
```bash
#!/bin/bash
cd /dolphinscheduler/default/resources/bigdata_tag_system
pip3 install -r requirements.txt --break-system-packages
echo "✅ Python依赖安装完成"
```

### 5. 创建Spark任务 - 系统健康检查
创建第四个Spark任务进行健康检查：

**任务名称**: `health_check`
**任务类型**: Spark
**依赖任务**: `install_dependencies`（或者`fix_permissions`如果跳过依赖安装）
**主程序**: `/dolphinscheduler/default/resources/bigdata_tag_system/main.py`
**主程序参数**: `--mode health`
**Driver内存**: 1g
**Executor数量**: 1

### 6. 创建完整工作流 - 标签系统部署测试

创建一个完整的工作流来验证部署，包含按顺序执行的任务：

#### 📋 工作流名称：`tag_system_deploy_test`

**任务A: 解压部署包** 📦
- **任务名称**: `unzip_deploy_package`
- **任务类型**: Shell
- **依赖**: 无（起始任务）
- **脚本**: 
```bash
#!/bin/bash
cd /dolphinscheduler/default/resources/
unzip -o tag_system_dolphin.zip -d bigdata_tag_system/
echo "✅ 部署包解压完成到: /dolphinscheduler/default/resources/bigdata_tag_system/"
ls -la bigdata_tag_system/main.py bigdata_tag_system/src/ bigdata_tag_system/requirements.txt
```

**任务B: 安装依赖** 🔧 (可选)
- **任务名称**: `install_dependencies`
- **任务类型**: Shell
- **依赖**: `unzip_deploy_package`
- **脚本**: 
```bash
#!/bin/bash
cd /dolphinscheduler/default/resources/bigdata_tag_system
pip3 install -r requirements.txt --break-system-packages
echo "✅ Python依赖安装完成"
```

**任务C: 系统健康检查** ⚡
- **任务名称**: `health_check`
- **任务类型**: Spark
- **依赖**: `install_dependencies`（或`unzip_deploy_package`如果跳过依赖安装）
- **主程序**: `/dolphinscheduler/default/resources/bigdata_tag_system/main.py`
- **主程序参数**: `--mode health`
- **Driver内存**: 1g, **Executor数量**: 1
- **作用**: 验证Hive和MySQL连接

**任务D: 生成测试数据** 📊
- **任务名称**: `generate_test_data`
- **任务类型**: Spark
- **依赖**: `health_check`
- **主程序**: `/dolphinscheduler/default/resources/bigdata_tag_system/main.py`
- **主程序参数**: `--mode generate-test-data --dt 2025-01-20`
- **选项参数**: `--jars /dolphinscheduler/default/resources/mysql-connector-j-8.0.33.jar`
- **Driver内存**: 2g, **Executor数量**: 3, **Executor内存**: 4g
- **作用**: 生成1000个用户的测试数据到Hive表

**任务E: 指定标签计算** 🏷️
- **任务名称**: `compute_specific_tags`
- **任务类型**: Spark
- **依赖**: `generate_test_data`
- **主程序**: `/dolphinscheduler/default/resources/bigdata_tag_system/main.py`
- **主程序参数**: `--mode task-tags --tag-ids 1,2,3`
- **Driver内存**: 4g, **Executor数量**: 5, **Executor内存**: 8g
- **作用**: 计算指定标签并写入MySQL

### 7. 通用Spark配置
**所有任务的基础配置**:
- Driver核心数: 2
- Executor核心数: 2
- YARN队列: default
- 失败重试次数: 2
- 任务超时: 30分钟

## 🎯 支持的执行模式

### 健康检查模式
```bash
--mode health
```
验证Hive和MySQL连接

### 测试数据生成
```bash
--mode generate-test-data --dt 2025-01-20
```
生成测试数据到Hive表

### 全量标签计算
```bash
--mode task-all
```
计算所有用户的所有标签

### 指定标签计算
```bash
--mode task-tags --tag-ids 1,2,3
```
计算指定标签ID的标签

### 任务列表查看
```bash
--mode list-tasks
```
查看所有可用的标签任务

## 💡 最佳实践

### 1. 参数化工作流
在工作流中使用全局参数:
- `tag_ids`: 标签ID列表
- `data_date`: 数据日期
- `mode`: 执行模式

### 2. 依赖管理
**通常情况**：无需额外安装Python依赖，使用集群预装的PySpark环境

**特殊情况**：如果集群环境缺少pymysql，可创建Shell任务：
```bash
#!/bin/bash
pip3 install pymysql
echo "✅ 安装MySQL连接器完成"
```

**验证依赖**：运行健康检查任务验证所有依赖是否正常

### 3. 错误处理
- 设置任务失败重试次数: 2
- 设置任务超时时间: 30分钟
- 配置告警通知

### 4. 监控建议
- 定期执行健康检查任务
- 监控MySQL标签数据增长
- 查看Spark UI资源使用情况

## 🔧 故障排除

### MySQL连接问题
- 检查网络连通性
- 验证用户名密码
- 确认数据库权限

### Hive表访问问题
- 验证表是否存在
- 检查分区数据
- 确认权限配置

### 性能优化
- 根据数据量调整Executor配置
- 考虑增加并行度
- 优化SQL查询逻辑

## 🏷️ 关键配置

### MySQL连接（统一配置）
系统使用src.config.base.MySQLConfig统一配置：
- **主机**: cex-mysql-test.c5mgk4qm8m2z.ap-southeast-1.rds.amazonaws.com
- **端口**: 3358
- **数据库**: biz_statistics

### Hive表直接读取
基于DirectHiveReader实现，支持：
- 使用 `MSCK REPAIR TABLE` 自动发现分区
- 直接通过 `spark.table()` 读取表数据
- 支持分区过滤：`spark.table("table").where("dt = 'xxx'")`

### 标签数据模型
- **一个用户一条记录**设计
- 标签ID存储为JSON数组: `[1,2,3,5]`
- 支持标签合并：新标签与MySQL现有标签自动合并
- 时间戳追踪：created_time（永不变），updated_time（标签变化时更新）