#!/usr/bin/env python3
"""
DWSå±‚ç”¨æˆ·æŒ‡æ ‡æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨ - æ”¯æŒ49ä¸ªç”¨æˆ·æŒ‡æ ‡
åŸºäº model/dws_user_index.sql ä¸­çš„7ä¸ªDWSè¡¨ç»“æ„ï¼Œç”Ÿæˆç¬¦åˆæ‰€æœ‰æŒ‡æ ‡ç±»å‹çš„å¤šæ ·åŒ–æµ‹è¯•æ•°æ®
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import random
from datetime import datetime, timedelta

def create_spark_session():
    """åˆ›å»ºSparkä¼šè¯ - DWSæ•°æ®ç”Ÿæˆä¼˜åŒ–é…ç½®"""
    spark = SparkSession.builder \
        .appName("DWSUserIndexTestDataGenerator") \
        .enableHiveSupport() \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .getOrCreate()
    return spark

def generate_dws_test_data(spark, dt='2025-01-20', user_count=1000):
    """ç”Ÿæˆå®Œæ•´çš„DWSå±‚ç”¨æˆ·æŒ‡æ ‡æµ‹è¯•æ•°æ®ï¼Œè¦†ç›–æ‰€æœ‰49ä¸ªæŒ‡æ ‡"""
    
    print(f"ğŸš€ ç”ŸæˆDWSå±‚ç”¨æˆ·æŒ‡æ ‡æµ‹è¯•æ•°æ®ï¼Œæ—¥æœŸ: {dt}, ç”¨æˆ·æ•°: {user_count}")
    
    # åˆ›å»ºæ•°æ®åº“
    spark.sql("CREATE DATABASE IF NOT EXISTS dws_user")
    print("âœ… æ•°æ®åº“ dws_user åˆ›å»ºæˆåŠŸ")
    
    # 1. ç”Ÿæˆç”¨æˆ·åŸºç¡€ç”»åƒè¡¨ dws_user_profile_df
    profile_data = []
    countries = ["CN", "US", "JP", "KR", "SG", "HK", "TW"]
    channels = ["organic", "advertisement", "referral", "social_media"]
    methods = ["email", "mobile", "google", "apple"]
    levels = ["VIP0", "VIP1", "VIP2", "VIP3", "VIP4", "VIP5"]
    
    for i in range(user_count):
        user_id = f"user_{i:06d}"
        
        # æ³¨å†ŒåŸºç¡€ä¿¡æ¯
        register_time = (datetime(2020, 1, 1) + timedelta(days=random.randint(0, 1800))).strftime('%Y-%m-%d %H:%M:%S')
        register_source_channel = random.choice(channels)
        register_method = random.choice(methods)
        register_country = random.choice(countries)
        
        # èº«ä»½è®¤è¯ä¿¡æ¯  
        is_kyc_completed = random.choice(["true", "false"])
        kyc_country = random.choice(countries)
        is_2fa_enabled = random.choice(["true", "false"])
        user_level = random.choice(levels)
        is_agent = random.choice(["true", "false"])
        
        profile_data.append((user_id, register_time, register_source_channel, register_method, register_country,
                           is_kyc_completed, kyc_country, is_2fa_enabled, user_level, is_agent))
    
    profile_schema = StructType([
        StructField("user_id", StringType(), True),
        StructField("register_time", StringType(), True),
        StructField("register_source_channel", StringType(), True),
        StructField("register_method", StringType(), True),
        StructField("register_country", StringType(), True),
        StructField("is_kyc_completed", StringType(), True),
        StructField("kyc_country", StringType(), True),
        StructField("is_2fa_enabled", StringType(), True),
        StructField("user_level", StringType(), True),
        StructField("is_agent", StringType(), True)
    ])
    
    profile_df = spark.createDataFrame(profile_data, profile_schema)
    profile_df = profile_df.withColumn("dt", lit(dt))
    
    profile_df.write \
        .mode("overwrite") \
        .partitionBy("dt") \
        .saveAsTable("dws_user.dws_user_profile_df")
    
    print("âœ… ç”¨æˆ·åŸºç¡€ç”»åƒæ•°æ®ç”Ÿæˆå®Œæˆ")
    
    # 2. ç”Ÿæˆç”¨æˆ·èµ„äº§è´¢åŠ¡è¡¨ dws_user_asset_df
    asset_data = []
    for i in range(user_count):
        user_id = f"user_{i:06d}"
        
        # å……å€¼æç°ç›¸å…³ - æ•°å€¼ç±»å‹æŒ‡æ ‡
        total_deposit_amount = str(random.choice([0, 50000, 100000, 200000, 500000, 1000000]))
        total_withdraw_amount = str(random.choice([0, 25000, 75000, 150000, 300000]))
        net_deposit_amount = str(float(total_deposit_amount) - float(total_withdraw_amount))
        withdraw_count_30d = str(random.randint(0, 20))
        deposit_fail_count = str(random.randint(0, 5))
        
        # å½“å‰èµ„äº§çŠ¶å†µ - æŒ‰è´¦æˆ·ç±»å‹ç»†åˆ†
        spot_position_value = str(random.choice([0, 5000, 25000, 50000, 100000]))
        contract_position_value = str(random.choice([0, 10000, 50000, 100000, 200000]))
        finance_position_value = str(random.choice([0, 20000, 50000, 100000, 300000]))
        onchain_position_value = str(random.choice([0, 5000, 10000, 25000, 50000]))
        current_total_position_value = str(float(spot_position_value) + float(contract_position_value) + float(finance_position_value) + float(onchain_position_value))
        
        # å¯ç”¨ä½™é¢ - æŒ‰è´¦æˆ·ç±»å‹ç»†åˆ†
        spot_available_balance = str(random.choice([0, 2000, 10000, 20000, 50000]))
        contract_available_balance = str(random.choice([0, 3000, 15000, 30000, 60000]))
        onchain_available_balance = str(random.choice([0, 1000, 5000, 10000, 20000]))
        available_balance = str(float(spot_available_balance) + float(contract_available_balance) + float(onchain_available_balance))
        
        # é”ä»“é‡‘é¢ - æŒ‰è´¦æˆ·ç±»å‹ç»†åˆ†
        spot_locked_amount = str(random.choice([0, 500, 2000, 5000, 10000]))
        contract_locked_amount = str(random.choice([0, 1000, 5000, 10000, 20000]))
        finance_locked_amount = str(random.choice([0, 2000, 10000, 20000, 50000]))
        onchain_locked_amount = str(random.choice([0, 200, 1000, 2000, 5000]))
        locked_amount = str(float(spot_locked_amount) + float(contract_locked_amount) + float(finance_locked_amount) + float(onchain_locked_amount))
        
        # æ—¶é—´ç›¸å…³ - æ—¥æœŸç±»å‹æŒ‡æ ‡
        base_time = datetime(2024, 1, 1) + timedelta(days=random.randint(0, 400))
        last_deposit_time = base_time.strftime('%Y-%m-%d %H:%M:%S')
        last_withdraw_time = (base_time - timedelta(days=random.randint(1, 30))).strftime('%Y-%m-%d %H:%M:%S')
        
        asset_data.append((user_id, total_deposit_amount, total_withdraw_amount, net_deposit_amount,
                         last_deposit_time, last_withdraw_time, withdraw_count_30d, deposit_fail_count,
                         spot_position_value, contract_position_value, finance_position_value, onchain_position_value, current_total_position_value,
                         spot_available_balance, contract_available_balance, onchain_available_balance, available_balance,
                         spot_locked_amount, contract_locked_amount, finance_locked_amount, onchain_locked_amount, locked_amount))
    
    asset_schema = StructType([
        StructField("user_id", StringType(), True),
        StructField("total_deposit_amount", StringType(), True),
        StructField("total_withdraw_amount", StringType(), True),
        StructField("net_deposit_amount", StringType(), True),
        StructField("last_deposit_time", StringType(), True),
        StructField("last_withdraw_time", StringType(), True),
        StructField("withdraw_count_30d", StringType(), True),
        StructField("deposit_fail_count", StringType(), True),
        # å½“å‰èµ„äº§çŠ¶å†µ - æŒ‰è´¦æˆ·ç±»å‹ç»†åˆ†
        StructField("spot_position_value", StringType(), True),
        StructField("contract_position_value", StringType(), True),
        StructField("finance_position_value", StringType(), True),
        StructField("onchain_position_value", StringType(), True),
        StructField("current_total_position_value", StringType(), True),
        # å¯ç”¨ä½™é¢ - æŒ‰è´¦æˆ·ç±»å‹ç»†åˆ†
        StructField("spot_available_balance", StringType(), True),
        StructField("contract_available_balance", StringType(), True),
        StructField("onchain_available_balance", StringType(), True),
        StructField("available_balance", StringType(), True),
        # é”ä»“é‡‘é¢ - æŒ‰è´¦æˆ·ç±»å‹ç»†åˆ†
        StructField("spot_locked_amount", StringType(), True),
        StructField("contract_locked_amount", StringType(), True),
        StructField("finance_locked_amount", StringType(), True),
        StructField("onchain_locked_amount", StringType(), True),
        StructField("locked_amount", StringType(), True)
    ])
    
    asset_df = spark.createDataFrame(asset_data, asset_schema)
    asset_df = asset_df.withColumn("dt", lit(dt))
    
    asset_df.write \
        .mode("overwrite") \
        .partitionBy("dt") \
        .saveAsTable("dws_user.dws_user_asset_df")
    
    print("âœ… ç”¨æˆ·èµ„äº§è´¢åŠ¡æ•°æ®ç”Ÿæˆå®Œæˆ")
    
    # 3. ç”Ÿæˆç”¨æˆ·äº¤æ˜“è¡Œä¸ºè¡¨ dws_user_trading_df
    trading_data = []
    trading_styles = ["é«˜æ æ†", "ä½é¢‘", "å¥—ä¿", "æ¿€è¿›", "ä¿å®ˆ"]
    
    for i in range(user_count):
        user_id = f"user_{i:06d}"
        
        # äº¤æ˜“é‡‘é¢ç»Ÿè®¡ - æŒ‰ä¸šåŠ¡ç±»å‹ç»†åˆ†
        spot_trading_volume = str(random.choice([0, 50000, 200000, 500000, 1000000]))
        contract_trading_volume = str(random.choice([0, 100000, 300000, 800000, 2000000]))
        total_trading_volume = str(float(spot_trading_volume) + float(contract_trading_volume))
        
        # æœ€è¿‘30æ—¥äº¤æ˜“é¢ - æŒ‰ä¸šåŠ¡ç±»å‹ç»†åˆ†
        spot_recent_30d_volume = str(random.choice([0, 20000, 80000, 200000, 400000]))
        contract_recent_30d_volume = str(random.choice([0, 30000, 120000, 300000, 600000]))
        recent_30d_trading_volume = str(float(spot_recent_30d_volume) + float(contract_recent_30d_volume))
        
        # äº¤æ˜“æ¬¡æ•°ç»Ÿè®¡ - æŒ‰ä¸šåŠ¡ç±»å‹ç»†åˆ†
        spot_trade_count = str(random.randint(0, 100))
        contract_trade_count = str(random.randint(0, 50))
        finance_trade_count = str(random.randint(0, 20))
        onchain_trade_count = str(random.randint(0, 10))
        
        # æ—¶é—´ç›¸å…³ - æ—¥æœŸç±»å‹æŒ‡æ ‡
        base_time = datetime(2024, 1, 1) + timedelta(days=random.randint(0, 400))
        first_trade_time = base_time.strftime('%Y-%m-%d %H:%M:%S')
        last_trade_time = (base_time + timedelta(days=random.randint(1, 100))).strftime('%Y-%m-%d %H:%M:%S')
        
        # äº¤æ˜“è¡Œä¸ºç‰¹å¾ - å¸ƒå°”ç±»å‹æŒ‡æ ‡
        has_contract_trading = random.choice(["true", "false"])
        has_finance_management = random.choice(["true", "false"])
        has_pending_orders = random.choice(["true", "false"])
        
        # æšä¸¾ç±»å‹æŒ‡æ ‡
        contract_trading_style = random.choice(trading_styles)
        
        trading_data.append((user_id, spot_trading_volume, contract_trading_volume, total_trading_volume,
                           spot_recent_30d_volume, contract_recent_30d_volume, recent_30d_trading_volume,
                           spot_trade_count, contract_trade_count, finance_trade_count, onchain_trade_count,
                           first_trade_time, last_trade_time, has_contract_trading, contract_trading_style, has_finance_management, has_pending_orders))
    
    trading_schema = StructType([
        StructField("user_id", StringType(), True),
        # äº¤æ˜“é‡‘é¢ç»Ÿè®¡ - æŒ‰ä¸šåŠ¡ç±»å‹ç»†åˆ†
        StructField("spot_trading_volume", StringType(), True),
        StructField("contract_trading_volume", StringType(), True),
        StructField("total_trading_volume", StringType(), True),
        # æœ€è¿‘30æ—¥äº¤æ˜“é¢ - æŒ‰ä¸šåŠ¡ç±»å‹ç»†åˆ†
        StructField("spot_recent_30d_volume", StringType(), True),
        StructField("contract_recent_30d_volume", StringType(), True),
        StructField("recent_30d_trading_volume", StringType(), True),
        # äº¤æ˜“æ¬¡æ•°ç»Ÿè®¡ - æŒ‰ä¸šåŠ¡ç±»å‹ç»†åˆ†
        StructField("spot_trade_count", StringType(), True),
        StructField("contract_trade_count", StringType(), True),
        StructField("finance_trade_count", StringType(), True),
        StructField("onchain_trade_count", StringType(), True),
        # äº¤æ˜“æ—¶é—´ç›¸å…³
        StructField("first_trade_time", StringType(), True),
        StructField("last_trade_time", StringType(), True),
        # äº¤æ˜“è¡Œä¸ºç‰¹å¾
        StructField("has_contract_trading", StringType(), True),
        StructField("contract_trading_style", StringType(), True),
        StructField("has_finance_management", StringType(), True),
        StructField("has_pending_orders", StringType(), True)
    ])
    
    trading_df = spark.createDataFrame(trading_data, trading_schema)
    trading_df = trading_df.withColumn("dt", lit(dt))
    
    trading_df.write \
        .mode("overwrite") \
        .partitionBy("dt") \
        .saveAsTable("dws_user.dws_user_trading_df")
    
    print("âœ… ç”¨æˆ·äº¤æ˜“è¡Œä¸ºæ•°æ®ç”Ÿæˆå®Œæˆ")
    
    # 4. ç”Ÿæˆç”¨æˆ·æ´»è·ƒè¡Œä¸ºè¡¨ dws_user_activity_df
    activity_data = []
    os_list = ["iOS", "Android", "Windows", "macOS", "Linux"]
    email_domains = ["gmail.com", "yahoo.com", "hotmail.com", "163.com", "qq.com"]
    
    for i in range(user_count):
        user_id = f"user_{i:06d}"
        
        # æ—¶é—´ç»´åº¦æŒ‡æ ‡ - æ•°å€¼ç±»å‹
        days_since_register = str(random.randint(30, 1800))
        days_since_last_login = str(random.randint(0, 90))
        login_count_7d = str(random.randint(0, 50))
        
        # æ—¶é—´ç›¸å…³ - æ—¥æœŸç±»å‹æŒ‡æ ‡
        base_time = datetime.now() - timedelta(days=int(days_since_last_login))
        last_login_time = base_time.strftime('%Y-%m-%d %H:%M:%S')
        last_activity_time = (base_time + timedelta(hours=random.randint(1, 24))).strftime('%Y-%m-%d %H:%M:%S')
        
        # è®¾å¤‡å’Œåœ°ç†ä¿¡æ¯ - å­—ç¬¦ä¸²ç±»å‹æŒ‡æ ‡
        login_ip_address = f"{random.randint(1, 255)}.{random.randint(1, 255)}.{random.randint(1, 255)}.{random.randint(1, 255)}"
        country_region_code = f"+{random.choice([86, 1, 44, 81, 82, 65])}"
        email_suffix = random.choice(email_domains)
        operating_system = random.choice(os_list)
        
        activity_data.append((user_id, days_since_register, days_since_last_login, last_login_time, last_activity_time,
                            login_count_7d, login_ip_address, country_region_code, email_suffix, operating_system))
    
    activity_schema = StructType([
        StructField("user_id", StringType(), True),
        StructField("days_since_register", StringType(), True),
        StructField("days_since_last_login", StringType(), True),
        StructField("last_login_time", StringType(), True),
        StructField("last_activity_time", StringType(), True),
        StructField("login_count_7d", StringType(), True),
        StructField("login_ip_address", StringType(), True),
        StructField("country_region_code", StringType(), True),
        StructField("email_suffix", StringType(), True),
        StructField("operating_system", StringType(), True)
    ])
    
    activity_df = spark.createDataFrame(activity_data, activity_schema)
    activity_df = activity_df.withColumn("dt", lit(dt))
    
    activity_df.write \
        .mode("overwrite") \
        .partitionBy("dt") \
        .saveAsTable("dws_user.dws_user_activity_df")
    
    print("âœ… ç”¨æˆ·æ´»è·ƒè¡Œä¸ºæ•°æ®ç”Ÿæˆå®Œæˆ")
    
    # 5. ç”Ÿæˆç”¨æˆ·é£é™©é£æ§è¡¨ dws_user_risk_df
    risk_data = []
    channel_sources = ["è‡ªç„¶æµé‡", "å¹¿å‘ŠæŠ•æ”¾", "é‚€è¯·å¥½å‹", "åˆä½œä¼™ä¼´", "æœç´¢å¼•æ“"]
    
    for i in range(user_count):
        user_id = f"user_{i:06d}"
        
        # é£é™©æ ‡è¯† - å¸ƒå°”ç±»å‹æŒ‡æ ‡
        is_blacklist_user = random.choice(["true", "false"])
        is_high_risk_ip = random.choice(["true", "false"])
        
        # æ¸ é“é£é™© - æšä¸¾ç±»å‹æŒ‡æ ‡
        channel_source = random.choice(channel_sources)
        
        risk_data.append((user_id, is_blacklist_user, is_high_risk_ip, channel_source))
    
    risk_schema = StructType([
        StructField("user_id", StringType(), True),
        StructField("is_blacklist_user", StringType(), True),
        StructField("is_high_risk_ip", StringType(), True),
        StructField("channel_source", StringType(), True)
    ])
    
    risk_df = spark.createDataFrame(risk_data, risk_schema)
    risk_df = risk_df.withColumn("dt", lit(dt))
    
    risk_df.write \
        .mode("overwrite") \
        .partitionBy("dt") \
        .saveAsTable("dws_user.dws_user_risk_df")
    
    print("âœ… ç”¨æˆ·é£é™©é£æ§æ•°æ®ç”Ÿæˆå®Œæˆ")
    
    # 6. ç”Ÿæˆç”¨æˆ·è¥é”€æ¿€åŠ±è¡¨ dws_user_marketing_df
    marketing_data = []
    
    for i in range(user_count):
        user_id = f"user_{i:06d}"
        
        # æ¿€åŠ±ç»Ÿè®¡ - æ•°å€¼ç±»å‹æŒ‡æ ‡
        red_packet_count = str(random.randint(0, 50))
        successful_invites_count = str(random.randint(0, 20))
        commission_rate = str(round(random.uniform(0, 5), 2))
        total_commission_amount = str(random.choice([0, 100, 500, 1000, 2000, 5000]))
        
        marketing_data.append((user_id, red_packet_count, successful_invites_count, commission_rate, total_commission_amount))
    
    marketing_schema = StructType([
        StructField("user_id", StringType(), True),
        StructField("red_packet_count", StringType(), True),
        StructField("successful_invites_count", StringType(), True),
        StructField("commission_rate", StringType(), True),
        StructField("total_commission_amount", StringType(), True)
    ])
    
    marketing_df = spark.createDataFrame(marketing_data, marketing_schema)
    marketing_df = marketing_df.withColumn("dt", lit(dt))
    
    marketing_df.write \
        .mode("overwrite") \
        .partitionBy("dt") \
        .saveAsTable("dws_user.dws_user_marketing_df")
    
    print("âœ… ç”¨æˆ·è¥é”€æ¿€åŠ±æ•°æ®ç”Ÿæˆå®Œæˆ")
    
    # 7. ç”Ÿæˆç”¨æˆ·è¡Œä¸ºåå¥½è¡¨ dws_user_behavior_df - åˆ—è¡¨ç±»å‹æŒ‡æ ‡
    behavior_data = []
    coins = ["BTC", "ETH", "USDT", "BNB", "SOL", "XRP", "ADA", "DOT", "MATIC", "AVAX"]
    devices = ["Chrome_Win10", "Safari_macOS", "Chrome_Android", "Safari_iOS", "Firefox_Linux"]
    activities = ["signup_bonus", "trading_contest", "staking_event", "referral_program", "vip_upgrade"]
    rewards = ["ä½“éªŒé‡‘", "ç©ºæŠ•", "è¿”ä½£", "æ‰‹ç»­è´¹å‡å…", "VIPæƒç›Š"]
    coupons = ["èµ é‡‘åˆ¸", "æ‰‹ç»­è´¹æŠµæ‰£åˆ¸", "äº¤æ˜“åˆ¸", "ä½“éªŒåˆ¸", "å‡çº§åˆ¸"]
    
    for i in range(user_count):
        user_id = f"user_{i:06d}"
        
        # åˆ—è¡¨ç±»å‹æŒ‡æ ‡ - ä½¿ç”¨array<string>ç±»å‹
        current_holding_coins = random.sample(coins, random.randint(1, 5))
        traded_coins_list = random.sample(coins, random.randint(2, 8))
        device_fingerprint_list = random.sample(devices, random.randint(1, 3))
        participated_activity_ids = random.sample(activities, random.randint(0, 4))
        reward_claim_history = random.sample(rewards, random.randint(0, 3))
        used_coupon_types = random.sample(coupons, random.randint(0, 3))
        
        behavior_data.append((user_id, current_holding_coins, traded_coins_list, device_fingerprint_list,
                            participated_activity_ids, reward_claim_history, used_coupon_types))
    
    behavior_schema = StructType([
        StructField("user_id", StringType(), True),
        StructField("current_holding_coins", ArrayType(StringType()), True),
        StructField("traded_coins_list", ArrayType(StringType()), True),
        StructField("device_fingerprint_list", ArrayType(StringType()), True),
        StructField("participated_activity_ids", ArrayType(StringType()), True),
        StructField("reward_claim_history", ArrayType(StringType()), True),
        StructField("used_coupon_types", ArrayType(StringType()), True)
    ])
    
    behavior_df = spark.createDataFrame(behavior_data, behavior_schema)
    behavior_df = behavior_df.withColumn("dt", lit(dt))
    
    behavior_df.write \
        .mode("overwrite") \
        .partitionBy("dt") \
        .saveAsTable("dws_user.dws_user_behavior_df")
    
    print("âœ… ç”¨æˆ·è¡Œä¸ºåå¥½æ•°æ®ç”Ÿæˆå®Œæˆ")
    
    # éªŒè¯æ•°æ®
    print("\nğŸ“Š DWSå±‚æ•°æ®éªŒè¯:")
    tables = [
        "dws_user_profile_df", 
        "dws_user_asset_df", 
        "dws_user_trading_df", 
        "dws_user_activity_df", 
        "dws_user_risk_df", 
        "dws_user_marketing_df", 
        "dws_user_behavior_df"
    ]
    
    for table in tables:
        count = spark.table(f'dws_user.{table}').count()
        print(f"   ğŸ“Š dws_user.{table}: {count} æ¡è®°å½•")
    
    print("ğŸ¯ å®Œæ•´çš„DWSå±‚7ä¸ªè¡¨æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆï¼Œå·²è¦†ç›–æ‰€æœ‰49ä¸ªç”¨æˆ·æŒ‡æ ‡")

if __name__ == "__main__":
    spark = create_spark_session()
    
    try:
        # ç”ŸæˆDWSå±‚æµ‹è¯•æ•°æ®
        generate_dws_test_data(spark)
        
        print("ğŸ‰ DWSå±‚æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆï¼")
        
    finally:
        spark.stop()
