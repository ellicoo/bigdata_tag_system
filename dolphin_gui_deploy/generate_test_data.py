#!/usr/bin/env python3
"""
å»ºè¡¨å‚è€ƒè€Œå·²ï¼Œæµ‹è¯•æ•°æ®ä¼šé€šè¿‡generate_test_data.pyè„šæœ¬äº§ç”Ÿ
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import random
from datetime import datetime, timedelta

def create_spark_session():
    """åˆ›å»ºSparkä¼šè¯ - åŸºäºç°æœ‰HiveToKafka.pyæ¨¡å¼"""
    spark = SparkSession.builder \
        .appName("TagSystemTestDataGenerator") \
        .enableHiveSupport() \
        .getOrCreate()
    return spark

def generate_test_data(spark, dt='2025-01-20'):
    """ç”Ÿæˆæµ‹è¯•æ•°æ®å¹¶å†™å…¥Hiveè¡¨"""
    
    print(f"ğŸš€ ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼Œæ—¥æœŸ: {dt}")
    
    # ç”Ÿæˆç”¨æˆ·åŸºæœ¬ä¿¡æ¯æµ‹è¯•æ•°æ®
    user_basic_data = []
    for i in range(1000):
        user_id = f"user_{i:06d}"
        age = random.randint(18, 65)
        user_level = random.choice(['VIP1', 'VIP2', 'VIP3', 'NORMAL'])
        kyc_status = random.choice(['verified', 'pending', 'rejected'])
        registration_date = (datetime.now() - timedelta(days=random.randint(1, 1000))).strftime('%Y-%m-%d')
        risk_score = random.uniform(0, 100)
        
        user_basic_data.append((user_id, age, user_level, kyc_status, registration_date, risk_score))
    
    # åˆ›å»ºDataFrameå¹¶å†™å…¥Hive
    user_basic_schema = StructType([
        StructField("user_id", StringType(), True),
        StructField("age", IntegerType(), True), 
        StructField("user_level", StringType(), True),
        StructField("kyc_status", StringType(), True),
        StructField("registration_date", StringType(), True),
        StructField("risk_score", DoubleType(), True)
    ])
    
    user_basic_df = spark.createDataFrame(user_basic_data, user_basic_schema)
    user_basic_df = user_basic_df.withColumn("dt", lit(dt))
    
    # å†™å…¥Hiveè¡¨
    user_basic_df.write \
        .mode("overwrite") \
        .partitionBy("dt") \
        .saveAsTable("tag_test.user_basic_info")
    
    print("âœ… ç”¨æˆ·åŸºæœ¬ä¿¡æ¯æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆ")
    
    # ç”Ÿæˆç”¨æˆ·èµ„äº§æ•°æ®
    user_asset_data = []
    for i in range(1000):
        user_id = f"user_{i:06d}"
        total_asset = random.uniform(1000, 1000000)
        cash_balance = random.uniform(100, total_asset * 0.5)
        
        user_asset_data.append((user_id, total_asset, cash_balance))
    
    user_asset_schema = StructType([
        StructField("user_id", StringType(), True),
        StructField("total_asset_value", DoubleType(), True),
        StructField("cash_balance", DoubleType(), True)
    ])
    
    user_asset_df = spark.createDataFrame(user_asset_data, user_asset_schema)
    user_asset_df = user_asset_df.withColumn("dt", lit(dt))
    
    user_asset_df.write \
        .mode("overwrite") \
        .partitionBy("dt") \
        .saveAsTable("tag_test.user_asset_summary")
    
    print("âœ… ç”¨æˆ·èµ„äº§æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆ")
    
    # ç”Ÿæˆç”¨æˆ·æ´»åŠ¨æ•°æ®
    user_activity_data = []
    for i in range(1000):
        user_id = f"user_{i:06d}"
        trade_count = random.randint(0, 100)
        last_login = (datetime.now() - timedelta(days=random.randint(0, 30))).strftime('%Y-%m-%d')
        
        user_activity_data.append((user_id, trade_count, last_login))
    
    user_activity_schema = StructType([
        StructField("user_id", StringType(), True),
        StructField("trade_count_30d", IntegerType(), True),
        StructField("last_login_date", StringType(), True)
    ])
    
    user_activity_df = spark.createDataFrame(user_activity_data, user_activity_schema)
    user_activity_df = user_activity_df.withColumn("dt", lit(dt))
    
    user_activity_df.write \
        .mode("overwrite") \
        .partitionBy("dt") \
        .saveAsTable("tag_test.user_activity_summary")
    
    print("âœ… ç”¨æˆ·æ´»åŠ¨æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆ")
    
    # éªŒè¯æ•°æ®
    print("\nğŸ“Š æ•°æ®éªŒè¯:")
    print(f"ç”¨æˆ·åŸºæœ¬ä¿¡æ¯è¡¨è®°å½•æ•°: {spark.table('tag_test.user_basic_info').count()}")
    print(f"ç”¨æˆ·èµ„äº§è¡¨è®°å½•æ•°: {spark.table('tag_test.user_asset_summary').count()}")
    print(f"ç”¨æˆ·æ´»åŠ¨è¡¨è®°å½•æ•°: {spark.table('tag_test.user_activity_summary').count()}")

if __name__ == "__main__":
    spark = create_spark_session()
    
    try:
        # åˆ›å»ºæ•°æ®åº“
        spark.sql("CREATE DATABASE IF NOT EXISTS tag_test")
        print("âœ… æ•°æ®åº“ tag_test åˆ›å»ºæˆåŠŸ")
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        generate_test_data(spark)
        
        print("ğŸ‰ æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆï¼")
        
    finally:
        spark.stop()
