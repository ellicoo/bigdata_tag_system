#!/usr/bin/env python3
"""
Hiveè¡¨é›†æˆæµ‹è¯•è„šæœ¬
éªŒè¯çœŸå®çš„S3/Hiveæ•°æ®è¯»å–å’Œæ ‡ç­¾è®¡ç®—åŠŸèƒ½
"""

import sys
import os
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class HiveIntegrationTester:
    """Hiveé›†æˆæµ‹è¯•å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        logger.info("ğŸ§ª åˆå§‹åŒ–Hiveé›†æˆæµ‹è¯•å™¨")
        
        from src.common.config.manager import ConfigManager
        self.config = ConfigManager.load_config('local')
        self.spark = None
        
    def create_spark_session(self):
        """åˆ›å»ºSparkä¼šè¯"""
        from pyspark.sql import SparkSession
        
        # è·å–JARæ–‡ä»¶è·¯å¾„
        jars_dir = os.path.join(os.path.dirname(__file__), "jars")
        jar_files = [
            os.path.join(jars_dir, "mysql-connector-j-8.0.33.jar"),
            os.path.join(jars_dir, "hadoop-aws-3.3.4.jar"),
            os.path.join(jars_dir, "aws-java-sdk-bundle-1.12.262.jar")
        ]
        existing_jars = [jar for jar in jar_files if os.path.exists(jar)]
        
        self.spark = SparkSession.builder \
            .appName("HiveIntegrationTest") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000") \
            .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
            .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
            .config("spark.hadoop.fs.s3a.path.style.access", "true") \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .config("spark.jars", ",".join(existing_jars) if existing_jars else "") \
            .getOrCreate()
        
        logger.info("âœ… Sparkä¼šè¯åˆ›å»ºæˆåŠŸ")
        return self.spark
    
    def test_s3_connectivity(self):
        """æµ‹è¯•S3è¿æ¥"""
        logger.info("ğŸ” æµ‹è¯•S3è¿æ¥...")
        
        try:
            import boto3
            from botocore.exceptions import ClientError
            
            s3_client = boto3.client(
                's3',
                endpoint_url='http://localhost:9000',
                aws_access_key_id='minioadmin',
                aws_secret_access_key='minioadmin',
                region_name='us-east-1'
            )
            
            # åˆ—å‡ºå­˜å‚¨æ¡¶
            buckets = s3_client.list_buckets()
            logger.info(f"âœ… S3è¿æ¥æˆåŠŸï¼Œå­˜å‚¨æ¡¶: {[b['Name'] for b in buckets.get('Buckets', [])]}")
            
            # åˆ—å‡ºtest-data-lakeä¸­çš„å¯¹è±¡
            try:
                objects = s3_client.list_objects_v2(Bucket='test-data-lake', Prefix='warehouse/')
                if 'Contents' in objects:
                    logger.info(f"ğŸ“Š warehouseç›®å½•åŒ…å« {len(objects['Contents'])} ä¸ªå¯¹è±¡")
                    for obj in objects['Contents'][:5]:  # æ˜¾ç¤ºå‰5ä¸ªå¯¹è±¡
                        logger.info(f"   - {obj['Key']} ({obj['Size']} bytes)")
                else:
                    logger.warning("âš ï¸ warehouseç›®å½•ä¸ºç©º")
                    return False
                    
            except ClientError as e:
                logger.error(f"âŒ å­˜å‚¨æ¡¶test-data-lakeè®¿é—®å¤±è´¥: {e}")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ S3è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def test_hive_table_reading(self):
        """æµ‹è¯•Hiveè¡¨è¯»å–"""
        logger.info("ğŸ“– æµ‹è¯•Hiveè¡¨è¯»å–...")
        
        if not self.spark:
            self.create_spark_session()
        
        tables_to_test = [
            ('user_basic_info', ['user_id', 'age', 'user_level', 'kyc_status']),
            ('user_asset_summary', ['user_id', 'total_asset_value', 'cash_balance']),
            ('user_activity_summary', ['user_id', 'trade_count_30d', 'risk_score'])
        ]
        
        test_results = {}
        
        for table_name, columns in tables_to_test:
            try:
                logger.info(f"ğŸ“‹ æµ‹è¯•è¡¨: {table_name}")
                
                # è¯»å–å®Œæ•´è¡¨
                full_path = f"s3a://test-data-lake/warehouse/{table_name}"
                df = self.spark.read.parquet(full_path)
                
                total_count = df.count()
                logger.info(f"   æ€»è®°å½•æ•°: {total_count:,}")
                
                # è¯»å–æŒ‡å®šåˆ—
                selected_df = df.select(*columns)
                selected_count = selected_df.count()
                logger.info(f"   é€‰æ‹©åˆ—åè®°å½•æ•°: {selected_count:,}")
                
                # æ˜¾ç¤ºschema
                logger.info(f"   è¡¨ç»“æ„: {[f.name for f in df.schema.fields]}")
                
                # æ˜¾ç¤ºå‰å‡ æ¡è®°å½•
                sample_data = selected_df.limit(3).collect()
                logger.info(f"   æ ·æœ¬æ•°æ®: {len(sample_data)} æ¡")
                for i, row in enumerate(sample_data):
                    logger.info(f"     [{i+1}] {row.asDict()}")
                
                test_results[table_name] = {
                    'success': True,
                    'count': total_count,
                    'columns': len(df.columns)
                }
                
                logger.info(f"âœ… è¡¨ {table_name} è¯»å–æµ‹è¯•é€šè¿‡")
                
            except Exception as e:
                logger.error(f"âŒ è¡¨ {table_name} è¯»å–æµ‹è¯•å¤±è´¥: {e}")
                test_results[table_name] = {
                    'success': False,
                    'error': str(e)
                }
        
        return test_results
    
    def test_tag_task_data_loading(self):
        """æµ‹è¯•æ ‡ç­¾ä»»åŠ¡æ•°æ®åŠ è½½"""
        logger.info("ğŸ·ï¸ æµ‹è¯•æ ‡ç­¾ä»»åŠ¡æ•°æ®åŠ è½½...")
        
        # æµ‹è¯•ä¸åŒæ ‡ç­¾ä»»åŠ¡çš„æ•°æ®åŠ è½½
        tag_tasks_config = [
            {
                'tag_id': 1,
                'tag_name': 'é«˜å‡€å€¼ç”¨æˆ·',
                'data_source': 'user_asset_summary',
                'required_fields': ['user_id', 'total_asset_value', 'cash_balance']
            },
            {
                'tag_id': 2,
                'tag_name': 'VIPå®¢æˆ·',
                'data_source': 'user_basic_info',
                'required_fields': ['user_id', 'user_level', 'kyc_status']
            },
            {
                'tag_id': 4,
                'tag_name': 'æ´»è·ƒäº¤æ˜“è€…',
                'data_source': 'user_activity_summary',
                'required_fields': ['user_id', 'trade_count_30d']
            }
        ]
        
        from src.batch.core.data_loader import BatchDataLoader
        data_loader = BatchDataLoader(self.spark, self.config)
        
        task_results = {}
        
        for task_config in tag_tasks_config:
            try:
                logger.info(f"ğŸ“Š æµ‹è¯•ä»»åŠ¡: {task_config['tag_name']}")
                
                # åŠ è½½ä»»åŠ¡æ•°æ®
                df = data_loader.load_user_data(
                    task_config['data_source'],
                    task_config['required_fields']
                )
                
                if df is not None:
                    count = df.count()
                    logger.info(f"   âœ… æ•°æ®åŠ è½½æˆåŠŸ: {count:,} æ¡è®°å½•")
                    
                    # éªŒè¯å­—æ®µ
                    actual_columns = set(df.columns)
                    required_columns = set(task_config['required_fields'])
                    
                    if required_columns.issubset(actual_columns):
                        logger.info(f"   âœ… å­—æ®µéªŒè¯é€šè¿‡: {task_config['required_fields']}")
                    else:
                        missing = required_columns - actual_columns
                        logger.warning(f"   âš ï¸ ç¼ºå°‘å­—æ®µ: {missing}")
                    
                    task_results[task_config['tag_name']] = {
                        'success': True,
                        'count': count,
                        'fields_ok': required_columns.issubset(actual_columns)
                    }
                else:
                    logger.error(f"   âŒ æ•°æ®åŠ è½½å¤±è´¥: è¿”å›None")
                    task_results[task_config['tag_name']] = {
                        'success': False,
                        'error': 'DataFrame is None'
                    }
                    
            except Exception as e:
                logger.error(f"   âŒ ä»»åŠ¡ {task_config['tag_name']} æµ‹è¯•å¤±è´¥: {e}")
                task_results[task_config['tag_name']] = {
                    'success': False,
                    'error': str(e)
                }
        
        return task_results
    
    def test_rule_application(self):
        """æµ‹è¯•è§„åˆ™åº”ç”¨"""
        logger.info("âš–ï¸ æµ‹è¯•è§„åˆ™åº”ç”¨...")
        
        try:
            # åŠ è½½èµ„äº§æ•°æ®æµ‹è¯•é«˜å‡€å€¼ç”¨æˆ·è§„åˆ™
            from src.batch.core.data_loader import BatchDataLoader
            data_loader = BatchDataLoader(self.spark, self.config)
            
            asset_df = data_loader.load_user_data('user_asset_summary', ['user_id', 'total_asset_value'])
            
            if asset_df is None:
                logger.error("âŒ æ— æ³•åŠ è½½èµ„äº§æ•°æ®")
                return False
            
            # åº”ç”¨é«˜å‡€å€¼ç”¨æˆ·è§„åˆ™ (total_asset_value >= 150000)
            from pyspark.sql.functions import col
            high_net_worth_users = asset_df.filter(col('total_asset_value') >= 150000)
            
            total_users = asset_df.count()
            high_net_worth_count = high_net_worth_users.count()
            
            logger.info(f"ğŸ“Š è§„åˆ™åº”ç”¨ç»“æœ:")
            logger.info(f"   æ€»ç”¨æˆ·æ•°: {total_users:,}")
            logger.info(f"   é«˜å‡€å€¼ç”¨æˆ·æ•°: {high_net_worth_count:,}")
            logger.info(f"   åŒ¹é…ç‡: {high_net_worth_count/total_users*100:.1f}%")
            
            if high_net_worth_count > 0:
                # æ˜¾ç¤ºæ ·æœ¬ç”¨æˆ·
                sample_users = high_net_worth_users.limit(5).collect()
                logger.info(f"   æ ·æœ¬é«˜å‡€å€¼ç”¨æˆ·:")
                for user in sample_users:
                    logger.info(f"     {user.user_id}: {user.total_asset_value:,.2f}")
                
                logger.info("âœ… è§„åˆ™åº”ç”¨æµ‹è¯•é€šè¿‡")
                return True
            else:
                logger.warning("âš ï¸ æ²¡æœ‰ç”¨æˆ·åŒ¹é…é«˜å‡€å€¼è§„åˆ™")
                return False
                
        except Exception as e:
            logger.error(f"âŒ è§„åˆ™åº”ç”¨æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def test_full_tag_calculation(self):
        """æµ‹è¯•å®Œæ•´æ ‡ç­¾è®¡ç®—æµç¨‹"""
        logger.info("ğŸ¯ æµ‹è¯•å®Œæ•´æ ‡ç­¾è®¡ç®—æµç¨‹...")
        
        try:
            # å¯¼å…¥å¹¶è¿è¡Œæ ‡ç­¾è®¡ç®—
            from src.batch.orchestrator.batch_orchestrator import BatchOrchestrator
            
            orchestrator = BatchOrchestrator(self.config)
            
            # æµ‹è¯•æŒ‡å®šç”¨æˆ·çš„æ ‡ç­¾è®¡ç®—
            test_user_ids = [f"user_{i:06d}" for i in range(1, 11)]  # å‰10ä¸ªç”¨æˆ·
            
            logger.info(f"ğŸ”„ ä¸º {len(test_user_ids)} ä¸ªç”¨æˆ·è®¡ç®—æ ‡ç­¾...")
            
            # æ‰§è¡Œæ ‡ç­¾è®¡ç®—
            result = orchestrator.execute_user_tags(
                user_ids=test_user_ids,
                tag_ids=[1, 2, 3],  # æµ‹è¯•å‰3ä¸ªæ ‡ç­¾
                mode='user-tags'
            )
            
            if result and result.get('success'):
                logger.info("âœ… å®Œæ•´æ ‡ç­¾è®¡ç®—æµ‹è¯•é€šè¿‡")
                logger.info(f"   å¤„ç†ç”¨æˆ·æ•°: {result.get('total_users', 0)}")
                logger.info(f"   ç”Ÿæˆæ ‡ç­¾æ•°: {result.get('total_tags', 0)}")
                return True
            else:
                logger.error(f"âŒ æ ‡ç­¾è®¡ç®—å¤±è´¥: {result}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ å®Œæ•´æ ‡ç­¾è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹Hiveé›†æˆæµ‹è¯•")
        logger.info("=" * 50)
        
        test_results = {}
        
        try:
            # 1. æµ‹è¯•S3è¿æ¥
            test_results['s3_connectivity'] = self.test_s3_connectivity()
            
            # 2. åˆ›å»ºSparkä¼šè¯
            self.create_spark_session()
            
            # 3. æµ‹è¯•Hiveè¡¨è¯»å–
            test_results['hive_table_reading'] = self.test_hive_table_reading()
            
            # 4. æµ‹è¯•æ ‡ç­¾ä»»åŠ¡æ•°æ®åŠ è½½
            test_results['tag_task_loading'] = self.test_tag_task_data_loading()
            
            # 5. æµ‹è¯•è§„åˆ™åº”ç”¨
            test_results['rule_application'] = self.test_rule_application()
            
            # 6. æµ‹è¯•å®Œæ•´æ ‡ç­¾è®¡ç®—
            test_results['full_tag_calculation'] = self.test_full_tag_calculation()
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
            test_results['error'] = str(e)
        
        finally:
            if self.spark:
                self.spark.stop()
                logger.info("ğŸ›‘ Sparkä¼šè¯å·²å…³é—­")
        
        # æ˜¾ç¤ºæµ‹è¯•æ€»ç»“
        self.show_test_summary(test_results)
        
        return test_results
    
    def show_test_summary(self, results):
        """æ˜¾ç¤ºæµ‹è¯•æ€»ç»“"""
        logger.info("\n" + "=" * 50)
        logger.info("ğŸ“Š Hiveé›†æˆæµ‹è¯•æ€»ç»“")
        logger.info("=" * 50)
        
        total_tests = 0
        passed_tests = 0
        
        # S3è¿æ¥æµ‹è¯•
        if results.get('s3_connectivity'):
            logger.info("âœ… S3è¿æ¥æµ‹è¯•: é€šè¿‡")
            passed_tests += 1
        else:
            logger.info("âŒ S3è¿æ¥æµ‹è¯•: å¤±è´¥")
        total_tests += 1
        
        # Hiveè¡¨è¯»å–æµ‹è¯•
        hive_results = results.get('hive_table_reading', {})
        if hive_results:
            successful_tables = sum(1 for r in hive_results.values() if r.get('success'))
            total_tables = len(hive_results)
            if successful_tables == total_tables:
                logger.info(f"âœ… Hiveè¡¨è¯»å–æµ‹è¯•: é€šè¿‡ ({successful_tables}/{total_tables})")
                passed_tests += 1
            else:
                logger.info(f"âš ï¸ Hiveè¡¨è¯»å–æµ‹è¯•: éƒ¨åˆ†é€šè¿‡ ({successful_tables}/{total_tables})")
        else:
            logger.info("âŒ Hiveè¡¨è¯»å–æµ‹è¯•: å¤±è´¥")
        total_tests += 1
        
        # æ ‡ç­¾ä»»åŠ¡æ•°æ®åŠ è½½æµ‹è¯•
        task_results = results.get('tag_task_loading', {})
        if task_results:
            successful_tasks = sum(1 for r in task_results.values() if r.get('success'))
            total_tasks = len(task_results)
            if successful_tasks == total_tasks:
                logger.info(f"âœ… æ ‡ç­¾ä»»åŠ¡æ•°æ®åŠ è½½æµ‹è¯•: é€šè¿‡ ({successful_tasks}/{total_tasks})")
                passed_tests += 1
            else:
                logger.info(f"âš ï¸ æ ‡ç­¾ä»»åŠ¡æ•°æ®åŠ è½½æµ‹è¯•: éƒ¨åˆ†é€šè¿‡ ({successful_tasks}/{total_tasks})")
        else:
            logger.info("âŒ æ ‡ç­¾ä»»åŠ¡æ•°æ®åŠ è½½æµ‹è¯•: å¤±è´¥")
        total_tests += 1
        
        # è§„åˆ™åº”ç”¨æµ‹è¯•
        if results.get('rule_application'):
            logger.info("âœ… è§„åˆ™åº”ç”¨æµ‹è¯•: é€šè¿‡")
            passed_tests += 1
        else:
            logger.info("âŒ è§„åˆ™åº”ç”¨æµ‹è¯•: å¤±è´¥")
        total_tests += 1
        
        # å®Œæ•´æ ‡ç­¾è®¡ç®—æµ‹è¯•
        if results.get('full_tag_calculation'):
            logger.info("âœ… å®Œæ•´æ ‡ç­¾è®¡ç®—æµ‹è¯•: é€šè¿‡")
            passed_tests += 1
        else:
            logger.info("âŒ å®Œæ•´æ ‡ç­¾è®¡ç®—æµ‹è¯•: å¤±è´¥")
        total_tests += 1
        
        # æ€»ç»“
        success_rate = passed_tests / total_tests * 100
        logger.info(f"\nğŸ¯ æµ‹è¯•æ€»ä½“ç»“æœ: {passed_tests}/{total_tests} é€šè¿‡ ({success_rate:.1f}%)")
        
        if success_rate >= 80:
            logger.info("ğŸ‰ Hiveé›†æˆæµ‹è¯•æ•´ä½“é€šè¿‡ï¼ç³»ç»Ÿå¯ä»¥æ­£å¸¸ä½¿ç”¨çœŸå®çš„S3/Hiveæ•°æ®")
        elif success_rate >= 60:
            logger.info("âš ï¸ Hiveé›†æˆæµ‹è¯•éƒ¨åˆ†é€šè¿‡ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½éœ€è¦æ£€æŸ¥")
        else:
            logger.info("âŒ Hiveé›†æˆæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œæ•°æ®")


def main():
    """ä¸»å‡½æ•°"""
    tester = HiveIntegrationTester()
    tester.run_all_tests()


if __name__ == "__main__":
    main()