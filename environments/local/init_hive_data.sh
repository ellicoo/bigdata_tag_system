#!/bin/bash

# Hiveè¡¨æ•°æ®åˆå§‹åŒ–è„šæœ¬

set -e

echo "ğŸ—„ï¸ åˆå§‹åŒ–Hiveè¡¨æ•°æ®"
echo "================================="

# æ£€æŸ¥MinIOæœåŠ¡çŠ¶æ€
wait_for_minio() {
    echo "â³ ç­‰å¾…MinIOæœåŠ¡å¯åŠ¨..."
    local max_attempts=30
    local attempt=1
    
    while [ $attempt -le $max_attempts ]; do
        if curl -f http://localhost:9000/minio/health/live > /dev/null 2>&1; then
            echo "âœ… MinIOæœåŠ¡å·²å°±ç»ª"
            return 0
        fi
        
        echo "   å°è¯• $attempt/$max_attempts ..."
        sleep 2
        ((attempt++))
    done
    
    echo "âŒ MinIOæœåŠ¡å¯åŠ¨è¶…æ—¶"
    exit 1
}

# æ£€æŸ¥Sparkä¾èµ–
check_spark_dependencies() {
    echo "ğŸ” æ£€æŸ¥Sparkä¾èµ–..."
    
    local jars_dir="./jars"
    local required_jars=(
        "hadoop-aws-3.3.4.jar"
        "aws-java-sdk-bundle-1.12.262.jar"
        "mysql-connector-j-8.0.33.jar"
    )
    
    for jar in "${required_jars[@]}"; do
        if [ ! -f "$jars_dir/$jar" ]; then
            echo "âŒ ç¼ºå°‘å¿…éœ€çš„JARæ–‡ä»¶: $jar"
            echo "è¯·ç¡®ä¿ $jars_dir ç›®å½•åŒ…å«æ‰€æœ‰å¿…éœ€çš„JARæ–‡ä»¶"
            exit 1
        fi
    done
    
    echo "âœ… Sparkä¾èµ–æ£€æŸ¥é€šè¿‡"
}

# åˆå§‹åŒ–Hiveè¡¨
init_hive_tables() {
    echo "ğŸ“Š å¼€å§‹åˆå§‹åŒ–Hiveè¡¨ç»“æ„å’Œæ•°æ®..."
    
    # è¿”å›é¡¹ç›®æ ¹ç›®å½•
    cd ../..
    
    # è®¾ç½®Pythonè·¯å¾„
    export PYTHONPATH="$PWD:$PYTHONPATH"
    
    # è¿è¡ŒHiveè¡¨åˆå§‹åŒ–
    if python environments/local/init_hive_tables.py; then
        echo "âœ… Hiveè¡¨åˆå§‹åŒ–å®Œæˆ"
        return 0
    else
        echo "âŒ Hiveè¡¨åˆå§‹åŒ–å¤±è´¥"
        return 1
    fi
}

# éªŒè¯Hiveè¡¨æ•°æ®
verify_hive_tables() {
    echo "ğŸ” éªŒè¯Hiveè¡¨æ•°æ®..."
    
    # è¿”å›é¡¹ç›®æ ¹ç›®å½•ï¼ˆå¦‚æœè¿˜æ²¡æœ‰çš„è¯ï¼‰
    cd ../.. 2>/dev/null || true
    
    echo "ğŸ“‹ è¿è¡Œå¥åº·æ£€æŸ¥éªŒè¯æ•°æ®è¯»å–..."
    if python main.py --env local --mode health --log-level INFO; then
        echo "âœ… Hiveè¡¨æ•°æ®éªŒè¯é€šè¿‡"
        return 0
    else
        echo "âŒ Hiveè¡¨æ•°æ®éªŒè¯å¤±è´¥"
        return 1
    fi
}

# æ˜¾ç¤ºè¡¨ç»Ÿè®¡ä¿¡æ¯
show_table_stats() {
    echo ""
    echo "ğŸ“Š Hiveè¡¨ç»Ÿè®¡ä¿¡æ¯:"
    echo "================================="
    
    # ä½¿ç”¨Pythonè„šæœ¬å¿«é€Ÿç»Ÿè®¡
    cd ../.. 2>/dev/null || true
    
    python -c "
import sys
sys.path.append('.')
try:
    from src.common.config.manager import ConfigManager
    from pyspark.sql import SparkSession
    import os
    
    # åŠ è½½é…ç½®
    config = ConfigManager.load_config('local')
    
    # ç®€åŒ–çš„Sparké…ç½®ç”¨äºå¿«é€ŸæŸ¥è¯¢
    spark = SparkSession.builder \
        .appName('TableStats') \
        .config('spark.hadoop.fs.s3a.endpoint', 'http://localhost:9000') \
        .config('spark.hadoop.fs.s3a.access.key', 'minioadmin') \
        .config('spark.hadoop.fs.s3a.secret.key', 'minioadmin') \
        .config('spark.hadoop.fs.s3a.path.style.access', 'true') \
        .config('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') \
        .getOrCreate()
    
    bucket_name = 'test-data-lake'
    tables = ['user_basic_info', 'user_asset_summary', 'user_activity_summary']
    
    for table in tables:
        try:
            df = spark.read.parquet(f's3a://{bucket_name}/warehouse/{table}')
            count = df.count()
            print(f'  âœ… {table}: {count:,} æ¡è®°å½•')
        except Exception as e:
            print(f'  âŒ {table}: è¯»å–å¤±è´¥ - {str(e)[:50]}...')
    
    spark.stop()
    
except Exception as e:
    print(f'  âš ï¸ ç»Ÿè®¡ä¿¡æ¯è·å–å¤±è´¥: {e}')
" 2>/dev/null || echo "  âš ï¸ æ— æ³•è·å–ç»Ÿè®¡ä¿¡æ¯ï¼Œä½†è¡¨å·²åˆ›å»º"
}

# æ¸…ç†Hiveæ•°æ®
clean_hive_data() {
    echo "ğŸ§¹ æ¸…ç†Hiveæ•°æ®..."
    
    # æ¸…ç†MinIOä¸­çš„æ•°æ®
    cd ../.. 2>/dev/null || true
    python -c "
import boto3
from botocore.exceptions import ClientError
try:
    s3_client = boto3.client(
        's3',
        endpoint_url='http://localhost:9000',
        aws_access_key_id='minioadmin',
        aws_secret_access_key='minioadmin'
    )
    
    bucket_name = 'test-data-lake'
    
    # åˆ é™¤warehouseç›®å½•ä¸‹çš„æ‰€æœ‰å¯¹è±¡
    try:
        response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix='warehouse/')
        if 'Contents' in response:
            objects = [{'Key': obj['Key']} for obj in response['Contents']]
            s3_client.delete_objects(Bucket=bucket_name, Delete={'Objects': objects})
            print('âœ… Hiveæ•°æ®æ¸…ç†å®Œæˆ')
        else:
            print('â„¹ï¸ æ²¡æœ‰éœ€è¦æ¸…ç†çš„Hiveæ•°æ®')
    except ClientError:
        print('â„¹ï¸ å­˜å‚¨æ¡¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æ¸…ç†')
        
except Exception as e:
    print(f'âš ï¸ Hiveæ•°æ®æ¸…ç†å¤±è´¥: {e}')
" 2>/dev/null || echo "âš ï¸ Hiveæ•°æ®æ¸…ç†è·³è¿‡"
    
    cd environments/local
}

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
check_services() {
    echo "ğŸ” æ£€æŸ¥æœåŠ¡çŠ¶æ€..."
    
    # æ£€æŸ¥Dockerå®¹å™¨
    if ! docker-compose ps | grep -q "Up"; then
        echo "âŒ DockeræœåŠ¡æœªå¯åŠ¨ï¼Œè¯·å…ˆè¿è¡Œ: ./setup.sh start"
        exit 1
    fi
    
    echo "âœ… DockeræœåŠ¡è¿è¡Œæ­£å¸¸"
}

# æ˜¾ç¤ºå®Œæˆä¿¡æ¯
show_completion_info() {
    echo ""
    echo "ğŸ‰ Hiveè¡¨åˆå§‹åŒ–å®Œæˆï¼"
    echo "================================="
    echo ""
    echo "ğŸ“Š å·²åˆ›å»ºçš„Hiveè¡¨:"
    echo "  âœ… user_basic_info - ç”¨æˆ·åŸºç¡€ä¿¡æ¯è¡¨"
    echo "  âœ… user_asset_summary - ç”¨æˆ·èµ„äº§æ±‡æ€»è¡¨"  
    echo "  âœ… user_activity_summary - ç”¨æˆ·æ´»åŠ¨æ±‡æ€»è¡¨"
    echo "  âœ… user_transaction_detail - ç”¨æˆ·äº¤æ˜“æ˜ç»†è¡¨"
    echo ""
    echo "ğŸ¯ æ”¯æŒçš„æ ‡ç­¾ä»»åŠ¡æ•°æ®æº:"
    echo "  - é«˜å‡€å€¼ç”¨æˆ·: user_asset_summary"
    echo "  - VIPå®¢æˆ·: user_basic_info"
    echo "  - å¹´è½»ç”¨æˆ·: user_basic_info" 
    echo "  - æ´»è·ƒäº¤æ˜“è€…: user_activity_summary"
    echo "  - ä½é£é™©ç”¨æˆ·: user_activity_summary"
    echo "  - æ–°ç”¨æˆ·: user_basic_info"
    echo "  - ç°é‡‘å¯Œè£•ç”¨æˆ·: user_asset_summary"
    echo ""
    echo "ğŸš€ ç°åœ¨å¯ä»¥è¿è¡ŒçœŸå®çš„S3/Hiveæ•°æ®æµ‹è¯•:"
    echo "  cd ../../"
    echo "  python main.py --env local --mode health           # å¥åº·æ£€æŸ¥"
    echo "  python main.py --env local --mode full-parallel   # å…¨é‡å¹¶è¡Œè®¡ç®—"
    echo "  python main.py --env local --mode tags-parallel --tag-ids 1,2,3  # æŒ‡å®šæ ‡ç­¾æµ‹è¯•"
    echo ""
}

# ä¸»æµç¨‹
main() {
    case "${1:-init}" in
        "init")
            check_services
            wait_for_minio
            check_spark_dependencies
            init_hive_tables
            verify_hive_tables
            show_table_stats
            show_completion_info
            ;;
        "clean")
            check_services
            clean_hive_data
            echo "âœ… Hiveæ•°æ®æ¸…ç†å®Œæˆï¼Œè¿è¡Œ ./init_hive_data.sh é‡æ–°åˆå§‹åŒ–"
            ;;
        "reset")
            check_services
            clean_hive_data
            wait_for_minio
            check_spark_dependencies
            init_hive_tables
            verify_hive_tables
            show_table_stats
            show_completion_info
            ;;
        "verify")
            check_services
            verify_hive_tables
            show_table_stats
            ;;
        "stats")
            check_services
            show_table_stats
            ;;
        *)
            echo "ç”¨æ³•: $0 {init|clean|reset|verify|stats}"
            echo ""
            echo "  init     - åˆå§‹åŒ–Hiveè¡¨å’Œæ•°æ®ï¼ˆé»˜è®¤ï¼‰"
            echo "  clean    - æ¸…ç†Hiveæ•°æ®"
            echo "  reset    - æ¸…ç†å¹¶é‡æ–°åˆå§‹åŒ–Hiveæ•°æ®"
            echo "  verify   - éªŒè¯Hiveè¡¨æ•°æ®"
            echo "  stats    - æ˜¾ç¤ºè¡¨ç»Ÿè®¡ä¿¡æ¯"
            exit 1
            ;;
    esac
}

main "$@"