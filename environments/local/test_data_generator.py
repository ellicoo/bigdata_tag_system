"""
æœ¬åœ°ç¯å¢ƒæµ‹è¯•æ•°æ®ç”Ÿæˆå™¨
"""

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from src.config.manager import ConfigManager


def _upload_parquet_to_s3(s3_client, bucket_name, local_path, s3_prefix):
    """å°†æœ¬åœ°Parquetæ–‡ä»¶ä¸Šä¼ åˆ°S3"""
    import os
    
    for root, dirs, files in os.walk(local_path):
        for file in files:
            if file.endswith('.parquet'):
                local_file = os.path.join(root, file)
                # ä¿æŒParquetçš„åˆ†åŒºç»“æ„
                relative_path = os.path.relpath(local_file, local_path)
                s3_key = f"{s3_prefix}/{relative_path}"
                
                with open(local_file, 'rb') as f:
                    s3_client.put_object(
                        Bucket=bucket_name,
                        Key=s3_key,
                        Body=f.read()
                    )


def generate_test_data():
    """ç”Ÿæˆç”Ÿäº§ç¯å¢ƒæ¨¡æ‹Ÿçš„æµ‹è¯•æ•°æ®"""
    print("ğŸ—„ï¸ ç”Ÿæˆç”Ÿäº§ç¯å¢ƒæ¨¡æ‹Ÿæµ‹è¯•æ•°æ®...")
    
    try:
        from pyspark.sql import SparkSession
        import pandas as pd
        import random
        from datetime import datetime, timedelta
        import boto3
        from botocore.exceptions import ClientError
        
        # åŠ è½½æœ¬åœ°é…ç½®
        config = ConfigManager.load_config('local')
        print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ: {config.environment}")
        
        # åˆå§‹åŒ–Spark (ç®€åŒ–é…ç½®ï¼Œå…ˆåœ¨æœ¬åœ°ç”Ÿæˆå†ä¸Šä¼ )
        spark = SparkSession.builder \
            .appName("TestDataGenerator") \
            .config("spark.sql.adaptive.enabled", "true") \
            .getOrCreate()
        
        # é…ç½®MinIOå®¢æˆ·ç«¯
        s3_client = boto3.client(
            's3',
            endpoint_url='http://localhost:9000',
            aws_access_key_id='minioadmin',
            aws_secret_access_key='minioadmin',
            region_name='us-east-1'
        )
        
        # åˆ›å»ºå­˜å‚¨æ¡¶
        bucket_name = 'test-data-lake'
        try:
            s3_client.create_bucket(Bucket=bucket_name)
            print(f"âœ… åˆ›å»ºå­˜å‚¨æ¡¶: {bucket_name}")
        except ClientError as e:
            if e.response['Error']['Code'] == 'BucketAlreadyOwnedByYou':
                print(f"ğŸ“ å­˜å‚¨æ¡¶å·²å­˜åœ¨: {bucket_name}")
            else:
                print(f"âš ï¸ åˆ›å»ºå­˜å‚¨æ¡¶å¤±è´¥: {e}")
        
        # 1. ç”Ÿæˆç”¨æˆ·åŸºç¡€ä¿¡æ¯è¡¨ (user_basic_info)
        print("ğŸ“Š ç”Ÿæˆç”¨æˆ·åŸºç¡€ä¿¡æ¯è¡¨...")
        users = []
        for i in range(1000):  # ç”Ÿæˆ1000ä¸ªç”¨æˆ·ï¼Œæ›´çœŸå®
            user_id = f"user_{i+1:06d}"
            
            users.append({
                'user_id': user_id,
                'age': random.randint(18, 65),
                'registration_date': (datetime.now() - timedelta(days=random.randint(1, 365))).strftime('%Y-%m-%d'),
                'user_level': random.choice(['BRONZE', 'SILVER', 'GOLD', 'VIP1', 'VIP2', 'VIP3']),
                'kyc_status': random.choice(['verified', 'pending', 'rejected']),
                'last_login_date': (datetime.now() - timedelta(days=random.randint(0, 30))).strftime('%Y-%m-%d'),
                'created_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })
        
        # è½¬æ¢ä¸ºSpark DataFrameå¹¶ä¿å­˜ä¸ºæœ¬åœ°Parquetï¼Œç„¶åä¸Šä¼ 
        import tempfile
        import os
        
        user_basic_df = spark.createDataFrame(users)
        
        # å…ˆä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
        temp_dir = tempfile.mkdtemp()
        local_basic_path = os.path.join(temp_dir, "user_basic_info")
        user_basic_df.write.mode("overwrite").parquet(local_basic_path)
        
        # ä¸Šä¼ åˆ°MinIO
        _upload_parquet_to_s3(s3_client, bucket_name, local_basic_path, "warehouse/user_basic_info")
        print(f"âœ… ç”¨æˆ·åŸºç¡€ä¿¡æ¯è¡¨å·²ä¿å­˜åˆ°: s3a://{bucket_name}/warehouse/user_basic_info")
        
        # 2. ç”Ÿæˆç”¨æˆ·èµ„äº§æ±‡æ€»è¡¨ (user_asset_summary)  
        print("ğŸ’° ç”Ÿæˆç”¨æˆ·èµ„äº§æ±‡æ€»è¡¨...")
        assets = []
        for user in users:
            # ä¸ºéƒ¨åˆ†ç”¨æˆ·ç”Ÿæˆé«˜å‡€å€¼ï¼Œæ¨¡æ‹ŸçœŸå®åˆ†å¸ƒ
            if random.random() < 0.3:  # 30%çš„ç”¨æˆ·æ˜¯é«˜å‡€å€¼
                total_asset = random.randint(100000, 2000000)
                cash_balance = random.randint(10000, 200000)
            else:
                total_asset = random.randint(1000, 99999)
                cash_balance = random.randint(100, 50000)
                
            assets.append({
                'user_id': user['user_id'],
                'total_asset_value': float(total_asset),
                'cash_balance': float(cash_balance),
                'stock_value': float(random.randint(0, total_asset // 2)),
                'bond_value': float(random.randint(0, total_asset // 4)),
                'fund_value': float(random.randint(0, total_asset // 3)),
                'computed_date': datetime.now().strftime('%Y-%m-%d'),
                'created_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })
        
        user_asset_df = spark.createDataFrame(assets)
        local_asset_path = os.path.join(temp_dir, "user_asset_summary")
        user_asset_df.write.mode("overwrite").parquet(local_asset_path)
        
        _upload_parquet_to_s3(s3_client, bucket_name, local_asset_path, "warehouse/user_asset_summary")
        print(f"âœ… ç”¨æˆ·èµ„äº§æ±‡æ€»è¡¨å·²ä¿å­˜åˆ°: s3a://{bucket_name}/warehouse/user_asset_summary")
        
        # 3. ç”Ÿæˆç”¨æˆ·æ´»åŠ¨æ±‡æ€»è¡¨ (user_activity_summary)
        print("ğŸ“ˆ ç”Ÿæˆç”¨æˆ·æ´»åŠ¨æ±‡æ€»è¡¨...")
        activities = []
        for user in users:
            activities.append({
                'user_id': user['user_id'],
                'trade_count_30d': random.randint(0, 100),
                'trade_amount_30d': float(random.randint(0, 1000000)),
                'login_count_30d': random.randint(0, 30),
                'risk_score': random.randint(10, 90),
                'computed_date': datetime.now().strftime('%Y-%m-%d'),
                'created_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })
        
        user_activity_df = spark.createDataFrame(activities)
        local_activity_path = os.path.join(temp_dir, "user_activity_summary")
        user_activity_df.write.mode("overwrite").parquet(local_activity_path)
        
        _upload_parquet_to_s3(s3_client, bucket_name, local_activity_path, "warehouse/user_activity_summary")
        print(f"âœ… ç”¨æˆ·æ´»åŠ¨æ±‡æ€»è¡¨å·²ä¿å­˜åˆ°: s3a://{bucket_name}/warehouse/user_activity_summary")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        shutil.rmtree(temp_dir)
        print("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å®Œæˆ")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_users = len(users)
        high_value_users = len([a for a in assets if a['total_asset_value'] >= 100000])
        active_traders = len([a for a in activities if a['trade_count_30d'] > 10])
        low_risk_users = len([a for a in activities if a['risk_score'] <= 30])
        
        print("\nğŸ“Š ç”Ÿäº§ç¯å¢ƒæ¨¡æ‹Ÿæ•°æ®ç»Ÿè®¡:")
        print(f"   - æ€»ç”¨æˆ·æ•°: {total_users:,}")
        print(f"   - é«˜å‡€å€¼ç”¨æˆ·: {high_value_users:,} ({high_value_users/total_users*100:.1f}%)")
        print(f"   - æ´»è·ƒäº¤æ˜“è€…: {active_traders:,} ({active_traders/total_users*100:.1f}%)")
        print(f"   - ä½é£é™©ç”¨æˆ·: {low_risk_users:,} ({low_risk_users/total_users*100:.1f}%)")
        
        # VIPç”¨æˆ·ç»Ÿè®¡
        vip_users = len([u for u in users if u['user_level'] in ['VIP2', 'VIP3'] and u['kyc_status'] == 'verified'])
        print(f"   - VIPå®¢æˆ·: {vip_users:,} ({vip_users/total_users*100:.1f}%)")
        
        print("\nğŸ¯ æ•°æ®å·²æŒ‰ç”Ÿäº§ç¯å¢ƒæ ‡å‡†ç”Ÿæˆ:")
        print("   - æ ¼å¼: Parquet (åˆ—å¼å­˜å‚¨)")
        print("   - å­˜å‚¨: S3å…¼å®¹çš„MinIO")
        print("   - ç»“æ„: åˆ†è¡¨å­˜å‚¨ï¼Œæ¨¡æ‹ŸçœŸå®æ•°ä»“")
        print("   - è§„æ¨¡: 1000ç”¨æˆ·ï¼Œæ”¯æŒå®Œæ•´æ ‡ç­¾è®¡ç®—æµ‹è¯•")
        
        spark.stop()
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ•°æ®ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    generate_test_data()