#!/usr/bin/env python3
"""
æœ¬åœ°ç¯å¢ƒHiveè¡¨åˆå§‹åŒ–è„šæœ¬
åˆ›å»ºçœŸå®çš„Hiveè¡¨ç»“æ„å’Œä¸šåŠ¡æ•°æ®
"""

import sys
import os
import logging
from datetime import datetime, timedelta
import random
from typing import Dict, List, Any
import tempfile
import shutil

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class HiveTableInitializer:
    """Hiveè¡¨åˆå§‹åŒ–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–é…ç½®"""
        from src.common.config.manager import ConfigManager
        import boto3
        from botocore.exceptions import ClientError
        
        self.config = ConfigManager.load_config('local')
        logger.info(f"âœ… é…ç½®åŠ è½½æˆåŠŸ: {self.config.environment}")
        
        # é…ç½®MinIOå®¢æˆ·ç«¯
        self.s3_client = boto3.client(
            's3',
            endpoint_url='http://localhost:9000',
            aws_access_key_id='minioadmin',
            aws_secret_access_key='minioadmin',
            region_name='us-east-1'
        )
        
        # å­˜å‚¨æ¡¶åç§°
        self.bucket_name = 'test-data-lake'
        self.warehouse_path = 'warehouse'
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        self.temp_dir = tempfile.mkdtemp()
        logger.info(f"ğŸ“ ä¸´æ—¶ç›®å½•: {self.temp_dir}")
    
    def ensure_bucket_exists(self):
        """ç¡®ä¿S3å­˜å‚¨æ¡¶å­˜åœ¨"""
        try:
            self.s3_client.create_bucket(Bucket=self.bucket_name)
            logger.info(f"âœ… åˆ›å»ºå­˜å‚¨æ¡¶: {self.bucket_name}")
        except Exception as e:
            if 'BucketAlreadyOwnedByYou' in str(e):
                logger.info(f"ğŸ“ å­˜å‚¨æ¡¶å·²å­˜åœ¨: {self.bucket_name}")
            else:
                logger.warning(f"âš ï¸ åˆ›å»ºå­˜å‚¨æ¡¶å¼‚å¸¸: {e}")
    
    def create_spark_session(self):
        """åˆ›å»ºSparkä¼šè¯"""
        from pyspark.sql import SparkSession
        
        # è·å–JARæ–‡ä»¶è·¯å¾„
        jars_dir = os.path.join(os.path.dirname(__file__), "jars")
        jar_files = [
            os.path.join(jars_dir, "mysql-connector-j-8.0.33.jar"),
            os.path.join(jars_dir, "hadoop-aws-3.3.4.jar"),
            os.path.join(jars_dir, "aws-java-sdk-bundle-1.11.1034.jar")
        ]
        existing_jars = [jar for jar in jar_files if os.path.exists(jar)]
        
        spark = SparkSession.builder \
            .appName("HiveTableInitializer") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000") \
            .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
            .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
            .config("spark.hadoop.fs.s3a.path.style.access", "true") \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .config("spark.jars", ",".join(existing_jars) if existing_jars else "") \
            .getOrCreate()
        
        logger.info("âœ… Sparkä¼šè¯åˆ›å»ºæˆåŠŸ")
        return spark
    
    def generate_user_basic_info(self, spark, num_users: int = 2000) -> None:
        """ç”Ÿæˆç”¨æˆ·åŸºç¡€ä¿¡æ¯è¡¨ - user_basic_info"""
        logger.info(f"ğŸ“Š ç”Ÿæˆç”¨æˆ·åŸºç¡€ä¿¡æ¯è¡¨ ({num_users} ç”¨æˆ·)...")
        
        # ç”Ÿæˆå¤šæ ·åŒ–çš„ç”¨æˆ·æ•°æ®ï¼Œç¡®ä¿æ ‡ç­¾è§„åˆ™èƒ½å¤ŸåŒ¹é…
        users = []
        
        for i in range(num_users):
            user_id = f"user_{i+1:06d}"
            
            # å¹´é¾„åˆ†å¸ƒï¼šç¡®ä¿æœ‰è¶³å¤Ÿçš„å¹´è½»ç”¨æˆ·ï¼ˆ18-30å²ï¼‰
            if i < 400:  # 20%å¹´è½»ç”¨æˆ·
                age = random.randint(18, 30)
            elif i < 1200:  # 40%ä¸­å¹´ç”¨æˆ·
                age = random.randint(31, 50)
            else:  # 40%è€å¹´ç”¨æˆ·
                age = random.randint(51, 65)
            
            # ç”¨æˆ·ç­‰çº§åˆ†å¸ƒï¼šç¡®ä¿æœ‰VIPç”¨æˆ·
            if i < 100:  # 5% VIP3
                user_level = 'VIP3'
            elif i < 250:  # 7.5% VIP2
                user_level = 'VIP2'
            elif i < 450:  # 10% VIP1
                user_level = 'VIP1'
            elif i < 750:  # 15% GOLD
                user_level = 'GOLD'
            elif i < 1250:  # 25% SILVER
                user_level = 'SILVER'
            else:  # 37.5% BRONZE
                user_level = 'BRONZE'
            
            # KYCçŠ¶æ€ï¼šVIPç”¨æˆ·ä¼˜å…ˆverified
            if user_level in ['VIP2', 'VIP3']:
                kyc_status = 'verified'
            else:
                kyc_status = random.choices(
                    ['verified', 'pending', 'rejected'],
                    weights=[0.7, 0.2, 0.1]
                )[0]
            
            # æ³¨å†Œæ—¥æœŸï¼šç¡®ä¿æœ‰æ–°ç”¨æˆ·ï¼ˆæœ€è¿‘30å¤©ï¼‰
            if i < 300:  # 15%æ–°ç”¨æˆ·
                reg_days_ago = random.randint(1, 30)
            else:
                reg_days_ago = random.randint(31, 365)
            
            registration_date = (datetime.now() - timedelta(days=reg_days_ago)).strftime('%Y-%m-%d')
            
            # æœ€åç™»å½•ï¼šç¡®ä¿æœ‰æœ€è¿‘æ´»è·ƒç”¨æˆ·
            if i < 400:  # 20%æœ€è¿‘æ´»è·ƒ
                login_days_ago = random.randint(0, 7)
            else:
                login_days_ago = random.randint(8, 30)
            
            last_login_date = (datetime.now() - timedelta(days=login_days_ago)).strftime('%Y-%m-%d')
            
            users.append({
                'user_id': user_id,
                'age': age,
                'gender': random.choice(['M', 'F']),
                'registration_date': registration_date,
                'user_level': user_level,
                'kyc_status': kyc_status,
                'last_login_date': last_login_date,
                'country': random.choice(['CN', 'US', 'SG', 'HK']),
                'city': random.choice(['Beijing', 'Shanghai', 'Shenzhen', 'Singapore', 'HongKong']),
                'created_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'updated_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })
        
        # åˆ›å»ºDataFrameå¹¶ä¿å­˜
        user_basic_df = spark.createDataFrame(users)
        local_path = os.path.join(self.temp_dir, "user_basic_info")
        user_basic_df.write.mode("overwrite").parquet(local_path)
        
        # ä¸Šä¼ åˆ°S3
        self._upload_to_s3(local_path, f"{self.warehouse_path}/user_basic_info")
        
        # ç»Ÿè®¡ä¿¡æ¯
        young_users = len([u for u in users if u['age'] <= 30])
        vip_users = len([u for u in users if u['user_level'] in ['VIP2', 'VIP3'] and u['kyc_status'] == 'verified'])
        new_users = len([u for u in users if (datetime.now() - datetime.strptime(u['registration_date'], '%Y-%m-%d')).days <= 30])
        recent_active = len([u for u in users if (datetime.now() - datetime.strptime(u['last_login_date'], '%Y-%m-%d')).days <= 7])
        
        logger.info(f"âœ… user_basic_info åˆ›å»ºå®Œæˆ:")
        logger.info(f"   - æ€»ç”¨æˆ·æ•°: {num_users:,}")
        logger.info(f"   - å¹´è½»ç”¨æˆ· (â‰¤30å²): {young_users:,} ({young_users/num_users*100:.1f}%)")
        logger.info(f"   - VIPå®¢æˆ· (VIP2/3+éªŒè¯): {vip_users:,} ({vip_users/num_users*100:.1f}%)")
        logger.info(f"   - æ–°ç”¨æˆ· (30å¤©å†…): {new_users:,} ({new_users/num_users*100:.1f}%)")
        logger.info(f"   - æœ€è¿‘æ´»è·ƒ (7å¤©å†…): {recent_active:,} ({recent_active/num_users*100:.1f}%)")
    
    def generate_user_asset_summary(self, spark, users_data: List[Dict]) -> None:
        """ç”Ÿæˆç”¨æˆ·èµ„äº§æ±‡æ€»è¡¨ - user_asset_summary"""
        logger.info("ğŸ’° ç”Ÿæˆç”¨æˆ·èµ„äº§æ±‡æ€»è¡¨...")
        
        assets = []
        for user in users_data:
            user_id = user['user_id']
            user_level = user['user_level']
            
            # æ ¹æ®ç”¨æˆ·ç­‰çº§ç”Ÿæˆä¸åŒçš„èµ„äº§åˆ†å¸ƒ
            if user_level == 'VIP3':
                # VIP3ï¼šé«˜å‡€å€¼ç”¨æˆ·
                total_asset = random.randint(500000, 2000000)
                cash_balance = random.randint(100000, 500000)
            elif user_level == 'VIP2':
                # VIP2ï¼šä¸­é«˜å‡€å€¼ç”¨æˆ·
                total_asset = random.randint(200000, 800000)
                cash_balance = random.randint(50000, 200000)
            elif user_level == 'VIP1':
                # VIP1ï¼šæœ‰ä¸€å®šèµ„äº§
                total_asset = random.randint(100000, 300000)
                cash_balance = random.randint(30000, 100000)
            elif user_level == 'GOLD':
                # GOLDï¼šä¸­ç­‰èµ„äº§
                total_asset = random.randint(50000, 150000)
                cash_balance = random.randint(15000, 60000)
            elif user_level == 'SILVER':
                # SILVERï¼šå°é¢èµ„äº§
                total_asset = random.randint(10000, 80000)
                cash_balance = random.randint(3000, 30000)
            else:  # BRONZE
                # BRONZEï¼šæ–°æ‰‹ç”¨æˆ·
                total_asset = random.randint(1000, 50000)
                cash_balance = random.randint(500, 15000)
            
            # å…¶ä»–èµ„äº§ç±»å‹åˆ†é…
            stock_value = random.randint(0, int(total_asset * 0.6))
            bond_value = random.randint(0, int(total_asset * 0.3))
            fund_value = max(0, total_asset - cash_balance - stock_value - bond_value)
            
            assets.append({
                'user_id': user_id,
                'total_asset_value': float(total_asset),
                'cash_balance': float(cash_balance),
                'stock_value': float(stock_value),
                'bond_value': float(bond_value),
                'fund_value': float(fund_value),
                'crypto_value': float(random.randint(0, int(total_asset * 0.1))),
                'computed_date': datetime.now().strftime('%Y-%m-%d'),
                'created_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'updated_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })
        
        # åˆ›å»ºDataFrameå¹¶ä¿å­˜
        user_asset_df = spark.createDataFrame(assets)
        local_path = os.path.join(self.temp_dir, "user_asset_summary")
        user_asset_df.write.mode("overwrite").parquet(local_path)
        
        # ä¸Šä¼ åˆ°S3
        self._upload_to_s3(local_path, f"{self.warehouse_path}/user_asset_summary")
        
        # ç»Ÿè®¡ä¿¡æ¯
        high_net_worth = len([a for a in assets if a['total_asset_value'] >= 150000])
        cash_rich = len([a for a in assets if a['cash_balance'] >= 60000])
        
        logger.info(f"âœ… user_asset_summary åˆ›å»ºå®Œæˆ:")
        logger.info(f"   - é«˜å‡€å€¼ç”¨æˆ· (â‰¥150K): {high_net_worth:,} ({high_net_worth/len(assets)*100:.1f}%)")
        logger.info(f"   - ç°é‡‘å¯Œè£•ç”¨æˆ· (â‰¥60K): {cash_rich:,} ({cash_rich/len(assets)*100:.1f}%)")
    
    def generate_user_activity_summary(self, spark, users_data: List[Dict]) -> None:
        """ç”Ÿæˆç”¨æˆ·æ´»åŠ¨æ±‡æ€»è¡¨ - user_activity_summary"""
        logger.info("ğŸ“ˆ ç”Ÿæˆç”¨æˆ·æ´»åŠ¨æ±‡æ€»è¡¨...")
        
        activities = []
        for i, user in enumerate(users_data):
            user_id = user['user_id']
            user_level = user['user_level']
            
            # æ ¹æ®ç”¨æˆ·ç­‰çº§ç”Ÿæˆä¸åŒçš„æ´»åŠ¨æ¨¡å¼
            if user_level in ['VIP3', 'VIP2']:
                # VIPç”¨æˆ·ï¼šæ›´æ´»è·ƒçš„äº¤æ˜“
                trade_count_30d = random.randint(15, 100)
                trade_amount_30d = random.randint(50000, 1000000)
                login_count_30d = random.randint(20, 30)
                # VIPç”¨æˆ·é£é™©è¯„åˆ†ç›¸å¯¹è¾ƒä½
                risk_score = random.randint(10, 40)
            elif user_level in ['VIP1', 'GOLD']:
                # ä¸­çº§ç”¨æˆ·ï¼šä¸­ç­‰æ´»è·ƒåº¦
                trade_count_30d = random.randint(8, 50)
                trade_amount_30d = random.randint(10000, 200000)
                login_count_30d = random.randint(10, 25)
                risk_score = random.randint(15, 60)
            else:
                # æ™®é€šç”¨æˆ·ï¼šè¾ƒä½æ´»è·ƒåº¦
                trade_count_30d = random.randint(0, 20)
                trade_amount_30d = random.randint(0, 50000)
                login_count_30d = random.randint(1, 15)
                risk_score = random.randint(20, 80)
            
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ´»è·ƒäº¤æ˜“è€…ï¼ˆtrade_count_30d > 15ï¼‰
            if i < 300:  # å‰300ä¸ªç”¨æˆ·å¼ºåˆ¶ä¸ºæ´»è·ƒäº¤æ˜“è€…
                trade_count_30d = max(trade_count_30d, 16)
            
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ä½é£é™©ç”¨æˆ·ï¼ˆrisk_score <= 30ï¼‰
            if i < 400:  # å‰400ä¸ªç”¨æˆ·æœ‰è¾ƒä½é£é™©
                risk_score = min(risk_score, 30)
            
            # ç”Ÿæˆæœ€è¿‘ç™»å½•æ—¶é—´
            base_date = datetime.now()
            
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æœ€è¿‘æ´»è·ƒç”¨æˆ·ï¼ˆæœ€è¿‘7å¤©å†…ç™»å½•ï¼‰
            if i < 200:  # å‰200ä¸ªç”¨æˆ·ä¸ºæœ€è¿‘æ´»è·ƒç”¨æˆ·
                last_login_date = base_date - timedelta(days=random.randint(1, 7))
            elif user_level in ['VIP3', 'VIP2']:
                # VIPç”¨æˆ·ç›¸å¯¹æ›´æ´»è·ƒ
                last_login_date = base_date - timedelta(days=random.randint(1, 15))
            else:
                # æ™®é€šç”¨æˆ·ç™»å½•é¢‘ç‡è¾ƒä½
                last_login_date = base_date - timedelta(days=random.randint(1, 90))
            
            activities.append({
                'user_id': user_id,
                'trade_count_30d': trade_count_30d,
                'trade_amount_30d': float(trade_amount_30d),
                'login_count_30d': login_count_30d,
                'last_login_date': last_login_date.strftime('%Y-%m-%d'),  # æ·»åŠ æœ€è¿‘ç™»å½•æ—¥æœŸ
                'page_view_count_30d': random.randint(10, 500),
                'app_duration_minutes_30d': random.randint(60, 3000),
                'risk_score': float(risk_score),
                'credit_score': float(random.randint(300, 850)),
                'computed_date': datetime.now().strftime('%Y-%m-%d'),
                'created_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'updated_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })
        
        # åˆ›å»ºDataFrameå¹¶ä¿å­˜
        user_activity_df = spark.createDataFrame(activities)
        local_path = os.path.join(self.temp_dir, "user_activity_summary")
        user_activity_df.write.mode("overwrite").parquet(local_path)
        
        # ä¸Šä¼ åˆ°S3
        self._upload_to_s3(local_path, f"{self.warehouse_path}/user_activity_summary")
        
        # ç»Ÿè®¡ä¿¡æ¯
        active_traders = len([a for a in activities if a['trade_count_30d'] > 15])
        low_risk_users = len([a for a in activities if a['risk_score'] <= 30])
        # ç»Ÿè®¡æœ€è¿‘æ´»è·ƒç”¨æˆ·ï¼ˆæœ€è¿‘7å¤©å†…ç™»å½•ï¼‰
        recent_active_users = len([a for a in activities if (datetime.now() - datetime.strptime(a['last_login_date'], '%Y-%m-%d')).days <= 7])
        
        logger.info(f"âœ… user_activity_summary åˆ›å»ºå®Œæˆ:")
        logger.info(f"   - æ´»è·ƒäº¤æ˜“è€… (>15æ¬¡/æœˆ): {active_traders:,} ({active_traders/len(activities)*100:.1f}%)")
        logger.info(f"   - ä½é£é™©ç”¨æˆ· (â‰¤30åˆ†): {low_risk_users:,} ({low_risk_users/len(activities)*100:.1f}%)")
        logger.info(f"   - æœ€è¿‘æ´»è·ƒç”¨æˆ· (7å¤©å†…ç™»å½•): {recent_active_users:,} ({recent_active_users/len(activities)*100:.1f}%)")
    
    def generate_user_transaction_detail(self, spark, users_data: List[Dict]) -> None:
        """ç”Ÿæˆç”¨æˆ·äº¤æ˜“æ˜ç»†è¡¨ - user_transaction_detailï¼ˆå¯é€‰ï¼‰"""
        logger.info("ğŸ’³ ç”Ÿæˆç”¨æˆ·äº¤æ˜“æ˜ç»†è¡¨...")
        
        transactions = []
        
        # ä¸ºå‰500ä¸ªç”¨æˆ·ç”Ÿæˆè¯¦ç»†äº¤æ˜“è®°å½•
        for user in users_data[:500]:
            user_id = user['user_id']
            
            # æ¯ä¸ªç”¨æˆ·ç”Ÿæˆ5-20ç¬”äº¤æ˜“
            tx_count = random.randint(5, 20)
            
            for j in range(tx_count):
                tx_date = datetime.now() - timedelta(days=random.randint(1, 90))
                
                transactions.append({
                    'user_id': user_id,
                    'transaction_id': f"TX_{user_id}_{j+1:03d}",
                    'transaction_type': random.choice(['BUY', 'SELL', 'DEPOSIT', 'WITHDRAW']),
                    'product_type': random.choice(['STOCK', 'BOND', 'FUND', 'CRYPTO']),
                    'amount': float(random.randint(100, 50000)),
                    'transaction_date': tx_date.strftime('%Y-%m-%d'),
                    'transaction_time': tx_date.strftime('%H:%M:%S'),
                    'status': random.choice(['SUCCESS', 'PENDING', 'FAILED']),
                    'created_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                })
        
        # åˆ›å»ºDataFrameå¹¶ä¿å­˜
        if transactions:
            user_tx_df = spark.createDataFrame(transactions)
            local_path = os.path.join(self.temp_dir, "user_transaction_detail")
            user_tx_df.write.mode("overwrite").parquet(local_path)
            
            # ä¸Šä¼ åˆ°S3
            self._upload_to_s3(local_path, f"{self.warehouse_path}/user_transaction_detail")
            
            logger.info(f"âœ… user_transaction_detail åˆ›å»ºå®Œæˆ: {len(transactions):,} ç¬”äº¤æ˜“")
    
    def _upload_to_s3(self, local_path: str, s3_prefix: str):
        """å°†æœ¬åœ°Parquetæ–‡ä»¶ä¸Šä¼ åˆ°S3"""
        for root, dirs, files in os.walk(local_path):
            for file in files:
                if file.endswith('.parquet'):
                    local_file = os.path.join(root, file)
                    # ä¿æŒParquetçš„åˆ†åŒºç»“æ„
                    relative_path = os.path.relpath(local_file, local_path)
                    s3_key = f"{s3_prefix}/{relative_path}"
                    
                    with open(local_file, 'rb') as f:
                        self.s3_client.put_object(
                            Bucket=self.bucket_name,
                            Key=s3_key,
                            Body=f.read()
                        )
        
        logger.info(f"ğŸ“¤ å·²ä¸Šä¼ åˆ°: s3a://{self.bucket_name}/{s3_prefix}")
    
    def create_hive_metadata(self, spark):
        """åˆ›å»ºHiveè¡¨å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼Œç”¨äºæ”¯æŒSQLæŸ¥è¯¢ï¼‰"""
        logger.info("ğŸ“‹ åˆ›å»ºHiveè¡¨å…ƒæ•°æ®...")
        
        try:
            # æ³¨å†Œä¸´æ—¶è§†å›¾ï¼Œæ¨¡æ‹ŸHiveè¡¨
            
            # 1. user_basic_info
            basic_df = spark.read.parquet(f"s3a://{self.bucket_name}/{self.warehouse_path}/user_basic_info")
            basic_df.createOrReplaceTempView("user_basic_info")
            
            # 2. user_asset_summary
            asset_df = spark.read.parquet(f"s3a://{self.bucket_name}/{self.warehouse_path}/user_asset_summary")
            asset_df.createOrReplaceTempView("user_asset_summary")
            
            # 3. user_activity_summary
            activity_df = spark.read.parquet(f"s3a://{self.bucket_name}/{self.warehouse_path}/user_activity_summary")
            activity_df.createOrReplaceTempView("user_activity_summary")
            
            # éªŒè¯è¡¨åˆ›å»º
            tables = spark.catalog.listTables()
            table_names = [t.name for t in tables]
            
            logger.info(f"âœ… Hiveè¡¨å…ƒæ•°æ®åˆ›å»ºå®Œæˆ:")
            for table in table_names:
                count = spark.table(table).count()
                logger.info(f"   - {table}: {count:,} æ¡è®°å½•")
                
        except Exception as e:
            logger.warning(f"âš ï¸ Hiveè¡¨å…ƒæ•°æ®åˆ›å»ºå¤±è´¥: {e}")
    
    def validate_data_quality(self, spark):
        """éªŒè¯æ•°æ®è´¨é‡å’Œæ ‡ç­¾è§„åˆ™åŒ¹é…åº¦"""
        logger.info("ğŸ” éªŒè¯æ•°æ®è´¨é‡...")
        
        try:
            # éªŒè¯æ¯ä¸ªæ ‡ç­¾çš„æ•°æ®åŒ¹é…æƒ…å†µ
            validations = []
            
            # 1. é«˜å‡€å€¼ç”¨æˆ· (total_asset_value >= 150000)
            asset_df = spark.read.parquet(f"s3a://{self.bucket_name}/{self.warehouse_path}/user_asset_summary")
            high_net_worth = asset_df.filter(asset_df.total_asset_value >= 150000).count()
            validations.append(("é«˜å‡€å€¼ç”¨æˆ·", high_net_worth))
            
            # 2. VIPç”¨æˆ· (user_level in ['VIP2', 'VIP3'] AND kyc_status = 'verified')
            basic_df = spark.read.parquet(f"s3a://{self.bucket_name}/{self.warehouse_path}/user_basic_info")
            vip_users = basic_df.filter(
                (basic_df.user_level.isin(['VIP2', 'VIP3'])) & 
                (basic_df.kyc_status == 'verified')
            ).count()
            validations.append(("VIPå®¢æˆ·", vip_users))
            
            # 3. å¹´è½»ç”¨æˆ· (age <= 30)
            young_users = basic_df.filter(basic_df.age <= 30).count()
            validations.append(("å¹´è½»ç”¨æˆ·", young_users))
            
            # 4. æ´»è·ƒäº¤æ˜“è€… (trade_count_30d > 15)
            activity_df = spark.read.parquet(f"s3a://{self.bucket_name}/{self.warehouse_path}/user_activity_summary")
            active_traders = activity_df.filter(activity_df.trade_count_30d > 15).count()
            validations.append(("æ´»è·ƒäº¤æ˜“è€…", active_traders))
            
            # 5. ä½é£é™©ç”¨æˆ· (risk_score <= 30)
            low_risk = activity_df.filter(activity_df.risk_score <= 30).count()
            validations.append(("ä½é£é™©ç”¨æˆ·", low_risk))
            
            # 6. æ–°ç”¨æˆ· (registration_date >= 30å¤©å‰)
            from pyspark.sql.functions import col, date_sub, current_date
            new_users = basic_df.filter(
                col('registration_date') >= date_sub(current_date(), 30)
            ).count()
            validations.append(("æ–°ç”¨æˆ·", new_users))
            
            # 7. ç°é‡‘å¯Œè£•ç”¨æˆ· (cash_balance >= 60000)
            cash_rich = asset_df.filter(asset_df.cash_balance >= 60000).count()
            validations.append(("ç°é‡‘å¯Œè£•ç”¨æˆ·", cash_rich))
            
            # 8. æœ€è¿‘æ´»è·ƒç”¨æˆ· (last_login_date >= 7å¤©å‰)
            recent_active = activity_df.filter(
                col('last_login_date') >= date_sub(current_date(), 7)
            ).count()
            validations.append(("æœ€è¿‘æ´»è·ƒç”¨æˆ·", recent_active))
            
            logger.info("âœ… æ•°æ®è´¨é‡éªŒè¯ç»“æœ:")
            for label, count in validations:
                logger.info(f"   - {label}: {count:,} ç”¨æˆ·ç¬¦åˆæ¡ä»¶")
                
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ ‡ç­¾éƒ½æœ‰è¶³å¤Ÿçš„ç”¨æˆ·
            min_users = min(count for _, count in validations)
            if min_users < 10:
                logger.warning(f"âš ï¸ æŸäº›æ ‡ç­¾åŒ¹é…ç”¨æˆ·æ•°è¾ƒå°‘ï¼Œæœ€å°‘: {min_users}")
            else:
                logger.info(f"âœ… æ‰€æœ‰æ ‡ç­¾éƒ½æœ‰è¶³å¤Ÿçš„åŒ¹é…ç”¨æˆ·ï¼Œæœ€å°‘: {min_users}")
                
        except Exception as e:
            logger.error(f"âŒ æ•°æ®è´¨é‡éªŒè¯å¤±è´¥: {e}")
    
    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            shutil.rmtree(self.temp_dir)
            logger.info("ğŸ§¹ ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.warning(f"âš ï¸ ä¸´æ—¶æ–‡ä»¶æ¸…ç†å¤±è´¥: {e}")
    
    def initialize_all_tables(self):
        """åˆå§‹åŒ–æ‰€æœ‰Hiveè¡¨"""
        try:
            logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–Hiveè¡¨ç»“æ„...")
            
            # 1. ç¡®ä¿å­˜å‚¨æ¡¶å­˜åœ¨
            self.ensure_bucket_exists()
            
            # 2. åˆ›å»ºSparkä¼šè¯
            spark = self.create_spark_session()
            
            # 3. ç”Ÿæˆç”¨æˆ·åŸºç¡€ä¿¡æ¯è¡¨
            self.generate_user_basic_info(spark, num_users=2000)
            
            # 4. è·å–ç”¨æˆ·æ•°æ®ç”¨äºå…³è”è¡¨ç”Ÿæˆ
            # æ³¨æ„ï¼šç”±äºSpark S3é›†æˆé—®é¢˜ï¼Œè¿™é‡Œä½¿ç”¨ç”Ÿæˆçš„æ•°æ®è€Œä¸æ˜¯ä»S3è¯»å–
            logger.info("ğŸ“Š ç”Ÿæˆå…³è”è¡¨æ•°æ®...")
            users_data = []
            for i in range(2000):
                user_id = f"user_{i+1:06d}"
                if i < 400:  # 20%å¹´è½»ç”¨æˆ·
                    age = random.randint(18, 30)
                elif i < 1200:  # 40%ä¸­å¹´ç”¨æˆ·
                    age = random.randint(31, 50)
                else:  # 40%è€å¹´ç”¨æˆ·
                    age = random.randint(51, 65)
                
                if i < 100:  # 5% VIP3
                    user_level = 'VIP3'
                elif i < 250:  # 7.5% VIP2
                    user_level = 'VIP2'
                elif i < 450:  # 10% VIP1
                    user_level = 'VIP1'
                elif i < 750:  # 15% GOLD
                    user_level = 'GOLD'
                elif i < 1250:  # 25% SILVER
                    user_level = 'SILVER'
                else:  # 37.5% BRONZE
                    user_level = 'BRONZE'
                
                users_data.append({
                    'user_id': user_id,
                    'user_level': user_level,
                    'age': age
                })
            
            # 5. ç”Ÿæˆèµ„äº§æ±‡æ€»è¡¨
            self.generate_user_asset_summary(spark, users_data)
            
            # 6. ç”Ÿæˆæ´»åŠ¨æ±‡æ€»è¡¨
            self.generate_user_activity_summary(spark, users_data)
            
            # 7. ç”Ÿæˆäº¤æ˜“æ˜ç»†è¡¨ï¼ˆå¯é€‰ï¼‰
            self.generate_user_transaction_detail(spark, users_data)
            
            # 8. åˆ›å»ºHiveè¡¨å…ƒæ•°æ®
            self.create_hive_metadata(spark)
            
            # 9. éªŒè¯æ•°æ®è´¨é‡
            self.validate_data_quality(spark)
            
            logger.info("ğŸ‰ Hiveè¡¨åˆå§‹åŒ–å®Œæˆï¼")
            logger.info("================================")
            logger.info("ğŸ“Š å·²åˆ›å»ºçš„è¡¨:")
            logger.info("  âœ… user_basic_info - ç”¨æˆ·åŸºç¡€ä¿¡æ¯")
            logger.info("  âœ… user_asset_summary - ç”¨æˆ·èµ„äº§æ±‡æ€»")
            logger.info("  âœ… user_activity_summary - ç”¨æˆ·æ´»åŠ¨æ±‡æ€»")
            logger.info("  âœ… user_transaction_detail - ç”¨æˆ·äº¤æ˜“æ˜ç»†")
            logger.info("")
            logger.info("ğŸ¯ ç°åœ¨å¯ä»¥è¿è¡ŒçœŸå®çš„S3/Hiveæ•°æ®è¯»å–æµ‹è¯•:")
            logger.info("  cd ../../")
            logger.info("  python main.py --env local --mode health")
            logger.info("  python main.py --env local --mode full-parallel")
            
            spark.stop()
            
        except Exception as e:
            logger.error(f"âŒ Hiveè¡¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        finally:
            self.cleanup()


def main():
    """ä¸»å‡½æ•°"""
    initializer = HiveTableInitializer()
    initializer.initialize_all_tables()


if __name__ == "__main__":
    main()