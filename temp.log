platform... using builtin-java classes where applicable
	🔧 TagNativeFunctions初始化: 使用Spark原生函数，避免Python版本不匹配
	============================================================
	🏷️  大数据标签计算系统
	============================================================
	执行模式: task-all
	当前工作目录: /tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_16/904/3459
	脚本目录: /dolphinscheduler/default/resources/bigdata_tag_system
	项目根目录: /dolphinscheduler/default
	Python路径前3项: ['/dolphinscheduler/default', '/dolphinscheduler/default/resources/bigdata_tag_system', '/mnt/spark/python/lib/pyspark.zip']
	🚀 创建Spark会话: TagComputeEngine
[INFO] 2025-07-29 14:01:17.185 +0000 -  ->
	25/07/29 14:01:16 INFO EMRParamSideChannel: Setting FGAC mode to false
	25/07/29 14:01:16 INFO SparkContext: Running Spark version 3.5.2-amzn-1
	25/07/29 14:01:16 INFO SparkContext: OS info Linux, 6.8.0-1029-aws, amd64
	25/07/29 14:01:16 INFO SparkContext: Java version 17.0.15
	25/07/29 14:01:16 INFO ResourceUtils: ==============================================================
	25/07/29 14:01:16 INFO ResourceUtils: No custom resources configured for spark.driver.
	25/07/29 14:01:16 INFO ResourceUtils: ==============================================================
	25/07/29 14:01:16 INFO SparkContext: Submitted application: TagComputeEngine
	25/07/29 14:01:16 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(executorType -> name: executorType, amount: 1, script: , vendor: , cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	25/07/29 14:01:16 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor
	25/07/29 14:01:16 INFO ResourceProfileManager: Added ResourceProfile id: 0
	25/07/29 14:01:16 INFO ResourceProfile: User executor ResourceProfile created, executor resources: Map(executorType -> name: executorType, amount: 1, script: , vendor: , cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	25/07/29 14:01:16 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor
	25/07/29 14:01:16 INFO ResourceProfileManager: Added ResourceProfile id: 1
	25/07/29 14:01:16 INFO SecurityManager: Changing view acls to: root
	25/07/29 14:01:16 INFO SecurityManager: Changing modify acls to: root
	25/07/29 14:01:16 INFO SecurityManager: Changing view acls groups to:
	25/07/29 14:01:16 INFO SecurityManager: Changing modify acls groups to:
	25/07/29 14:01:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
	25/07/29 14:01:16 INFO Utils: Successfully started service 'sparkDriver' on port 14245.
	25/07/29 14:01:16 INFO SparkEnv: Registering MapOutputTracker
	25/07/29 14:01:16 INFO SparkEnv: Registering BlockManagerMaster
	25/07/29 14:01:16 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	25/07/29 14:01:16 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	25/07/29 14:01:16 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	25/07/29 14:01:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c262d28e-444b-425e-a737-4a03589bb610
	25/07/29 14:01:16 INFO MemoryStore: MemoryStore started with capacity 127.2 MiB
	25/07/29 14:01:16 INFO SparkEnv: Registering OutputCommitCoordinator
	25/07/29 14:01:16 INFO SubResultCacheManager: Sub-result caches are disabled.
	25/07/29 14:01:16 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	25/07/29 14:01:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	25/07/29 14:01:16 INFO Utils: Using 100 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
[INFO] 2025-07-29 14:01:18.186 +0000 -  ->
	25/07/29 14:01:17 INFO Configuration: resource-types.xml not found
	25/07/29 14:01:17 INFO ResourceUtils: Unable to find 'resource-types.xml'.
	25/07/29 14:01:17 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (23424 MB per container)
	25/07/29 14:01:17 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
	25/07/29 14:01:17 INFO Client: Setting up container launch context for our AM
	25/07/29 14:01:17 INFO Client: Setting up the launch environment for our AM container
	25/07/29 14:01:17 INFO Client: Preparing resources for our AM container
	25/07/29 14:01:17 INFO PlatformInfo: Unable to read clusterId from http://localhost:8321/configuration, trying extra instance data file: /var/lib/instance-controller/extraInstanceData.json
	25/07/29 14:01:17 INFO PlatformInfo: Unable to read clusterId from /var/lib/instance-controller/extraInstanceData.json, trying EMR job-flow data file: /var/lib/info/job-flow.json
	25/07/29 14:01:17 INFO PlatformInfo: Unable to read clusterId from /var/lib/info/job-flow.json, out of places to look
	25/07/29 14:01:17 INFO ClientConfigurationFactory: Set initial getObject socket timeout to 2000 ms.
[INFO] 2025-07-29 14:01:19.186 +0000 -  ->
	25/07/29 14:01:18 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/HikariCP-2.5.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/HikariCP-2.5.1.jar
	25/07/29 14:01:18 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/HikariCP-2.5.1.jar' for reading
[INFO] 2025-07-29 14:01:20.187 +0000 -  ->
	25/07/29 14:01:20 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/JLargeArrays-1.5.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/JLargeArrays-1.5.jar
	25/07/29 14:01:20 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/JLargeArrays-1.5.jar' for reading
	25/07/29 14:01:20 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/JTransforms-3.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/JTransforms-3.1.jar
	25/07/29 14:01:20 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/JTransforms-3.1.jar' for reading
[INFO] 2025-07-29 14:01:21.187 +0000 -  ->
	25/07/29 14:01:20 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/RoaringBitmap-0.9.45.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/RoaringBitmap-0.9.45.jar
	25/07/29 14:01:20 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/RoaringBitmap-0.9.45.jar' for reading
	25/07/29 14:01:20 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/ST4-4.0.4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/ST4-4.0.4.jar
	25/07/29 14:01:20 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/ST4-4.0.4.jar' for reading
	25/07/29 14:01:20 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/activation-1.1.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/activation-1.1.1.jar
	25/07/29 14:01:20 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/activation-1.1.1.jar' for reading
	25/07/29 14:01:20 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/aggdesigner-algorithm-6.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/aggdesigner-algorithm-6.0.jar
	25/07/29 14:01:20 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/aggdesigner-algorithm-6.0.jar' for reading
	25/07/29 14:01:20 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/aircompressor-0.27.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/aircompressor-0.27.jar
	25/07/29 14:01:20 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/aircompressor-0.27.jar' for reading
	25/07/29 14:01:20 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/algebra_2.12-2.0.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/algebra_2.12-2.0.1.jar
	25/07/29 14:01:20 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/algebra_2.12-2.0.1.jar' for reading
	25/07/29 14:01:20 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/animal-sniffer-annotations-1.14.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/animal-sniffer-annotations-1.14.jar
	25/07/29 14:01:20 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/animal-sniffer-annotations-1.14.jar' for reading
	25/07/29 14:01:21 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/animal-sniffer-annotations-1.23.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/animal-sniffer-annotations-1.23.jar
	25/07/29 14:01:21 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/animal-sniffer-annotations-1.23.jar' for reading
	25/07/29 14:01:21 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/annotations-16.0.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/annotations-16.0.2.jar
	25/07/29 14:01:21 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/annotations-16.0.2.jar' for reading
[INFO] 2025-07-29 14:01:22.188 +0000 -  ->
	25/07/29 14:01:21 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/annotations-17.0.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/annotations-17.0.0.jar
	25/07/29 14:01:21 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/annotations-17.0.0.jar' for reading
	25/07/29 14:01:21 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/annotations-4.1.1.4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/annotations-4.1.1.4.jar
	25/07/29 14:01:21 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/annotations-4.1.1.4.jar' for reading
	25/07/29 14:01:21 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/antlr-runtime-3.5.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/antlr-runtime-3.5.2.jar
	25/07/29 14:01:21 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/antlr-runtime-3.5.2.jar' for reading
	25/07/29 14:01:21 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/antlr4-runtime-4.9.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/antlr4-runtime-4.9.3.jar
	25/07/29 14:01:21 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/antlr4-runtime-4.9.3.jar' for reading
	25/07/29 14:01:21 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/aopalliance-1.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/aopalliance-1.0.jar
	25/07/29 14:01:21 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/aopalliance-1.0.jar' for reading
	25/07/29 14:01:21 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/aopalliance-repackaged-2.6.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/aopalliance-repackaged-2.6.1.jar
	25/07/29 14:01:21 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/aopalliance-repackaged-2.6.1.jar' for reading
	25/07/29 14:01:21 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/arpack-3.0.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/arpack-3.0.3.jar
	25/07/29 14:01:21 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/arpack-3.0.3.jar' for reading
	25/07/29 14:01:21 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/arpack_combined_all-0.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/arpack_combined_all-0.1.jar
	25/07/29 14:01:21 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/arpack_combined_all-0.1.jar' for reading
	25/07/29 14:01:22 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/arrow-format-12.0.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/arrow-format-12.0.1.jar
	25/07/29 14:01:22 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/arrow-format-12.0.1.jar' for reading
	25/07/29 14:01:22 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/arrow-memory-core-12.0.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/arrow-memory-core-12.0.1.jar
	25/07/29 14:01:22 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/arrow-memory-core-12.0.1.jar' for reading
[INFO] 2025-07-29 14:01:23.189 +0000 -  ->
	25/07/29 14:01:22 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/arrow-memory-netty-12.0.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/arrow-memory-netty-12.0.1.jar
	25/07/29 14:01:22 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/arrow-memory-netty-12.0.1.jar' for reading
	25/07/29 14:01:22 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/arrow-vector-12.0.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/arrow-vector-12.0.1.jar
	25/07/29 14:01:22 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/arrow-vector-12.0.1.jar' for reading
	25/07/29 14:01:22 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/audience-annotations-0.12.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/audience-annotations-0.12.0.jar
	25/07/29 14:01:22 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/audience-annotations-0.12.0.jar' for reading
	25/07/29 14:01:22 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/avro-1.11.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/avro-1.11.2.jar
	25/07/29 14:01:22 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/avro-1.11.2.jar' for reading
	25/07/29 14:01:22 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/avro-ipc-1.11.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/avro-ipc-1.11.2.jar
	25/07/29 14:01:22 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/avro-ipc-1.11.2.jar' for reading
	25/07/29 14:01:22 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/avro-mapred-1.11.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/avro-mapred-1.11.2.jar
	25/07/29 14:01:22 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/avro-mapred-1.11.2.jar' for reading
	25/07/29 14:01:22 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/bcprov-ext-jdk15on-1.66.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/bcprov-ext-jdk15on-1.66.jar
	25/07/29 14:01:22 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/bcprov-ext-jdk15on-1.66.jar' for reading
	25/07/29 14:01:22 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/blas-3.0.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/blas-3.0.3.jar
	25/07/29 14:01:23 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/blas-3.0.3.jar' for reading
	25/07/29 14:01:23 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/bonecp-0.8.0.RELEASE.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/bonecp-0.8.0.RELEASE.jar
	25/07/29 14:01:23 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/bonecp-0.8.0.RELEASE.jar' for reading
[INFO] 2025-07-29 14:01:24.190 +0000 -  ->
	25/07/29 14:01:23 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/breeze-macros_2.12-2.1.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/breeze-macros_2.12-2.1.0.jar
	25/07/29 14:01:23 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/breeze-macros_2.12-2.1.0.jar' for reading
	25/07/29 14:01:23 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/breeze_2.12-2.1.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/breeze_2.12-2.1.0.jar
	25/07/29 14:01:23 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/breeze_2.12-2.1.0.jar' for reading
	25/07/29 14:01:23 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/cats-kernel_2.12-2.1.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/cats-kernel_2.12-2.1.1.jar
	25/07/29 14:01:23 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/cats-kernel_2.12-2.1.1.jar' for reading
	25/07/29 14:01:23 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/checker-qual-2.5.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/checker-qual-2.5.2.jar
	25/07/29 14:01:23 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/checker-qual-2.5.2.jar' for reading
	25/07/29 14:01:23 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/chill-java-0.10.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/chill-java-0.10.0.jar
	25/07/29 14:01:23 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/chill-java-0.10.0.jar' for reading
	25/07/29 14:01:23 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/chill_2.12-0.10.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/chill_2.12-0.10.0.jar
	25/07/29 14:01:23 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/chill_2.12-0.10.0.jar' for reading
	25/07/29 14:01:23 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/commons-cli-1.5.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/commons-cli-1.5.0.jar
	25/07/29 14:01:23 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/commons-cli-1.5.0.jar' for reading
	25/07/29 14:01:24 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/commons-codec-1.16.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/commons-codec-1.16.1.jar
	25/07/29 14:01:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/commons-codec-1.16.1.jar' for reading
	25/07/29 14:01:24 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/commons-collections-3.2.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/commons-collections-3.2.2.jar
	25/07/29 14:01:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/commons-collections-3.2.2.jar' for reading
	25/07/29 14:01:24 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/commons-collections4-4.4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/commons-collections4-4.4.jar
[INFO] 2025-07-29 14:01:25.190 +0000 -  ->
	25/07/29 14:01:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/commons-collections4-4.4.jar' for reading
	25/07/29 14:01:24 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/commons-compiler-3.1.9.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/commons-compiler-3.1.9.jar
	25/07/29 14:01:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/commons-compiler-3.1.9.jar' for reading
	25/07/29 14:01:24 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/commons-compress-1.23.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/commons-compress-1.23.0.jar
	25/07/29 14:01:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/commons-compress-1.23.0.jar' for reading
	25/07/29 14:01:24 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/commons-crypto-1.1.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/commons-crypto-1.1.0.jar
	25/07/29 14:01:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/commons-crypto-1.1.0.jar' for reading
	25/07/29 14:01:24 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/commons-dbcp-1.4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/commons-dbcp-1.4.jar
	25/07/29 14:01:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/commons-dbcp-1.4.jar' for reading
	25/07/29 14:01:24 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/commons-io-2.16.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/commons-io-2.16.1.jar
	25/07/29 14:01:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/commons-io-2.16.1.jar' for reading
	25/07/29 14:01:24 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/commons-lang-2.6.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/commons-lang-2.6.jar
	25/07/29 14:01:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/commons-lang-2.6.jar' for reading
	25/07/29 14:01:24 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/commons-lang3-3.12.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/commons-lang3-3.12.0.jar
	25/07/29 14:01:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/commons-lang3-3.12.0.jar' for reading
	25/07/29 14:01:24 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/commons-logging-1.1.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/commons-logging-1.1.3.jar
	25/07/29 14:01:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/commons-logging-1.1.3.jar' for reading
	25/07/29 14:01:25 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/commons-math3-3.6.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/commons-math3-3.6.1.jar
	25/07/29 14:01:25 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/commons-math3-3.6.1.jar' for reading
	25/07/29 14:01:25 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/commons-pool-1.5.4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/commons-pool-1.5.4.jar
	25/07/29 14:01:25 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/commons-pool-1.5.4.jar' for reading
[INFO] 2025-07-29 14:01:26.191 +0000 -  ->
	25/07/29 14:01:25 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/commons-text-1.10.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/commons-text-1.10.0.jar
	25/07/29 14:01:25 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/commons-text-1.10.0.jar' for reading
	25/07/29 14:01:25 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/compress-lzf-1.1.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/compress-lzf-1.1.2.jar
	25/07/29 14:01:25 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/compress-lzf-1.1.2.jar' for reading
	25/07/29 14:01:25 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/curator-client-2.13.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/curator-client-2.13.0.jar
	25/07/29 14:01:25 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/curator-client-2.13.0.jar' for reading
	25/07/29 14:01:25 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/curator-framework-2.13.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/curator-framework-2.13.0.jar
	25/07/29 14:01:25 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/curator-framework-2.13.0.jar' for reading
	25/07/29 14:01:25 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/curator-recipes-2.13.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/curator-recipes-2.13.0.jar
	25/07/29 14:01:25 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/curator-recipes-2.13.0.jar' for reading
	25/07/29 14:01:25 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/datanucleus-api-jdo-4.2.4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/datanucleus-api-jdo-4.2.4.jar
	25/07/29 14:01:25 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/datanucleus-api-jdo-4.2.4.jar' for reading
	25/07/29 14:01:25 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/datanucleus-core-4.1.17.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/datanucleus-core-4.1.17.jar
	25/07/29 14:01:25 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/datanucleus-core-4.1.17.jar' for reading
	25/07/29 14:01:25 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/datanucleus-rdbms-4.1.19.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/datanucleus-rdbms-4.1.19.jar
	25/07/29 14:01:25 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/datanucleus-rdbms-4.1.19.jar' for reading
	25/07/29 14:01:25 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/datasketches-java-3.3.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/datasketches-java-3.3.0.jar
	25/07/29 14:01:26 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/datasketches-java-3.3.0.jar' for reading
	25/07/29 14:01:26 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/datasketches-memory-2.1.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/datasketches-memory-2.1.0.jar
	25/07/29 14:01:26 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/datasketches-memory-2.1.0.jar' for reading
	25/07/29 14:01:26 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/derby-10.14.2.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/derby-10.14.2.0.jar
	25/07/29 14:01:26 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/derby-10.14.2.0.jar' for reading
[INFO] 2025-07-29 14:01:27.192 +0000 -  ->
	25/07/29 14:01:26 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/disruptor-3.3.7.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/disruptor-3.3.7.jar
	25/07/29 14:01:26 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/disruptor-3.3.7.jar' for reading
	25/07/29 14:01:26 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar
	25/07/29 14:01:26 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar' for reading
	25/07/29 14:01:26 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/emr-spark-goodies.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/emr-spark-goodies.jar
	25/07/29 14:01:26 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/emr-spark-goodies.jar' for reading
	25/07/29 14:01:26 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/emrfs-hadoop-assembly-2.66.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/emrfs-hadoop-assembly-2.66.0.jar
	25/07/29 14:01:26 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/emrfs-hadoop-assembly-2.66.0.jar' for reading
	25/07/29 14:01:26 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/error_prone_annotations-2.1.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/error_prone_annotations-2.1.3.jar
	25/07/29 14:01:26 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/error_prone_annotations-2.1.3.jar' for reading
	25/07/29 14:01:26 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/error_prone_annotations-2.18.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/error_prone_annotations-2.18.0.jar
	25/07/29 14:01:26 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/error_prone_annotations-2.18.0.jar' for reading
	25/07/29 14:01:26 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/findbugs-annotations-3.0.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/findbugs-annotations-3.0.1.jar
	25/07/29 14:01:26 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/findbugs-annotations-3.0.1.jar' for reading
	25/07/29 14:01:27 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/flatbuffers-java-1.12.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/flatbuffers-java-1.12.0.jar
	25/07/29 14:01:27 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/flatbuffers-java-1.12.0.jar' for reading
	25/07/29 14:01:27 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/gmetric4j-1.0.10.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/gmetric4j-1.0.10.jar
	25/07/29 14:01:27 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/gmetric4j-1.0.10.jar' for reading
[INFO] 2025-07-29 14:01:28.193 +0000 -  ->
	25/07/29 14:01:27 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/grpc-api-1.56.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/grpc-api-1.56.0.jar
	25/07/29 14:01:27 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/grpc-api-1.56.0.jar' for reading
	25/07/29 14:01:27 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/grpc-context-1.56.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/grpc-context-1.56.0.jar
	25/07/29 14:01:27 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/grpc-context-1.56.0.jar' for reading
	25/07/29 14:01:27 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/grpc-core-1.56.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/grpc-core-1.56.0.jar
	25/07/29 14:01:27 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/grpc-core-1.56.0.jar' for reading
	25/07/29 14:01:27 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/grpc-netty-1.56.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/grpc-netty-1.56.0.jar
	25/07/29 14:01:27 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/grpc-netty-1.56.0.jar' for reading
	25/07/29 14:01:27 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/grpc-protobuf-1.56.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/grpc-protobuf-1.56.0.jar
	25/07/29 14:01:27 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/grpc-protobuf-1.56.0.jar' for reading
	25/07/29 14:01:27 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/grpc-protobuf-lite-1.56.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/grpc-protobuf-lite-1.56.0.jar
	25/07/29 14:01:27 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/grpc-protobuf-lite-1.56.0.jar' for reading
	25/07/29 14:01:27 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/grpc-services-1.56.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/grpc-services-1.56.0.jar
	25/07/29 14:01:27 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/grpc-services-1.56.0.jar' for reading
	25/07/29 14:01:27 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/grpc-stub-1.56.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/grpc-stub-1.56.0.jar
	25/07/29 14:01:27 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/grpc-stub-1.56.0.jar' for reading
	25/07/29 14:01:27 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/gson-2.10.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/gson-2.10.1.jar
	25/07/29 14:01:27 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/gson-2.10.1.jar' for reading
	25/07/29 14:01:28 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/guava-14.0.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/guava-14.0.1.jar
	25/07/29 14:01:28 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/guava-14.0.1.jar' for reading
	25/07/29 14:01:28 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/hadoop-client-api-3.4.0-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hadoop-client-api-3.4.0-amzn-1.jar
	25/07/29 14:01:28 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/hadoop-client-api-3.4.0-amzn-1.jar' for reading
[INFO] 2025-07-29 14:01:29.194 +0000 -  ->
	25/07/29 14:01:28 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/hadoop-client-runtime-3.4.0-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hadoop-client-runtime-3.4.0-amzn-1.jar
	25/07/29 14:01:28 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/hadoop-client-runtime-3.4.0-amzn-1.jar' for reading
	25/07/29 14:01:29 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/hadoop-shaded-guava-1.2.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hadoop-shaded-guava-1.2.0.jar
	25/07/29 14:01:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/hadoop-shaded-guava-1.2.0.jar' for reading
	25/07/29 14:01:29 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/hadoop-yarn-server-web-proxy-3.4.0-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hadoop-yarn-server-web-proxy-3.4.0-amzn-1.jar
	25/07/29 14:01:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/hadoop-yarn-server-web-proxy-3.4.0-amzn-1.jar' for reading
[INFO] 2025-07-29 14:01:30.194 +0000 -  ->
	25/07/29 14:01:29 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/hive-beeline-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hive-beeline-2.3.9-amzn-4.jar
	25/07/29 14:01:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/hive-beeline-2.3.9-amzn-4.jar' for reading
	25/07/29 14:01:29 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/hive-cli-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hive-cli-2.3.9-amzn-4.jar
	25/07/29 14:01:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/hive-cli-2.3.9-amzn-4.jar' for reading
	25/07/29 14:01:29 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/hive-common-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hive-common-2.3.9-amzn-4.jar
	25/07/29 14:01:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/hive-common-2.3.9-amzn-4.jar' for reading
	25/07/29 14:01:29 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/hive-exec-2.3.9-amzn-4-core.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hive-exec-2.3.9-amzn-4-core.jar
	25/07/29 14:01:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/hive-exec-2.3.9-amzn-4-core.jar' for reading
	25/07/29 14:01:29 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/hive-jdbc-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hive-jdbc-2.3.9-amzn-4.jar
	25/07/29 14:01:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/hive-jdbc-2.3.9-amzn-4.jar' for reading
	25/07/29 14:01:29 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/hive-llap-common-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hive-llap-common-2.3.9-amzn-4.jar
	25/07/29 14:01:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/hive-llap-common-2.3.9-amzn-4.jar' for reading
	25/07/29 14:01:29 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/hive-metastore-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hive-metastore-2.3.9-amzn-4.jar
	25/07/29 14:01:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/hive-metastore-2.3.9-amzn-4.jar' for reading
	25/07/29 14:01:30 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/hive-serde-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hive-serde-2.3.9-amzn-4.jar
	25/07/29 14:01:30 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/hive-serde-2.3.9-amzn-4.jar' for reading
	25/07/29 14:01:30 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/hive-service-rpc-3.1.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hive-service-rpc-3.1.3.jar
	25/07/29 14:01:30 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/hive-service-rpc-3.1.3.jar' for reading
[INFO] 2025-07-29 14:01:31.195 +0000 -  ->
	25/07/29 14:01:30 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/hive-shims-0.23-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hive-shims-0.23-2.3.9-amzn-4.jar
	25/07/29 14:01:30 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/hive-shims-0.23-2.3.9-amzn-4.jar' for reading
	25/07/29 14:01:30 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/hive-shims-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hive-shims-2.3.9-amzn-4.jar
	25/07/29 14:01:30 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/hive-shims-2.3.9-amzn-4.jar' for reading
	25/07/29 14:01:30 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/hive-shims-common-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hive-shims-common-2.3.9-amzn-4.jar
	25/07/29 14:01:30 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/hive-shims-common-2.3.9-amzn-4.jar' for reading
	25/07/29 14:01:30 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/hive-shims-scheduler-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hive-shims-scheduler-2.3.9-amzn-4.jar
	25/07/29 14:01:30 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/hive-shims-scheduler-2.3.9-amzn-4.jar' for reading
	25/07/29 14:01:30 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/hive-storage-api-2.8.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hive-storage-api-2.8.1.jar
	25/07/29 14:01:30 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/hive-storage-api-2.8.1.jar' for reading
	25/07/29 14:01:30 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/hk2-api-2.6.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hk2-api-2.6.1.jar
	25/07/29 14:01:30 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/hk2-api-2.6.1.jar' for reading
	25/07/29 14:01:30 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/hk2-locator-2.6.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hk2-locator-2.6.1.jar
	25/07/29 14:01:30 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/hk2-locator-2.6.1.jar' for reading
	25/07/29 14:01:30 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/hk2-utils-2.6.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hk2-utils-2.6.1.jar
	25/07/29 14:01:30 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/hk2-utils-2.6.1.jar' for reading
	25/07/29 14:01:30 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/httpclient-4.5.13.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/httpclient-4.5.13.jar
	25/07/29 14:01:30 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/httpclient-4.5.13.jar' for reading
	25/07/29 14:01:31 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/httpcore-4.4.13.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/httpcore-4.4.13.jar
	25/07/29 14:01:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/httpcore-4.4.13.jar' for reading
	25/07/29 14:01:31 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/istack-commons-runtime-3.0.8.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/istack-commons-runtime-3.0.8.jar
	25/07/29 14:01:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/istack-commons-runtime-3.0.8.jar' for reading
[INFO] 2025-07-29 14:01:32.195 +0000 -  ->
	25/07/29 14:01:31 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/ivy-2.5.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/ivy-2.5.1.jar
	25/07/29 14:01:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/ivy-2.5.1.jar' for reading
	25/07/29 14:01:31 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/j2objc-annotations-1.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/j2objc-annotations-1.1.jar
	25/07/29 14:01:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/j2objc-annotations-1.1.jar' for reading
	25/07/29 14:01:31 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jackson-annotations-2.15.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jackson-annotations-2.15.2.jar
	25/07/29 14:01:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jackson-annotations-2.15.2.jar' for reading
	25/07/29 14:01:31 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jackson-core-2.15.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jackson-core-2.15.2.jar
	25/07/29 14:01:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jackson-core-2.15.2.jar' for reading
	25/07/29 14:01:31 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jackson-core-asl-1.9.13.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jackson-core-asl-1.9.13.jar
	25/07/29 14:01:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jackson-core-asl-1.9.13.jar' for reading
	25/07/29 14:01:31 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jackson-databind-2.15.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jackson-databind-2.15.2.jar
	25/07/29 14:01:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jackson-databind-2.15.2.jar' for reading
	25/07/29 14:01:31 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jackson-dataformat-yaml-2.15.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jackson-dataformat-yaml-2.15.2.jar
	25/07/29 14:01:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jackson-dataformat-yaml-2.15.2.jar' for reading
	25/07/29 14:01:31 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jackson-datatype-jsr310-2.15.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jackson-datatype-jsr310-2.15.2.jar
	25/07/29 14:01:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jackson-datatype-jsr310-2.15.2.jar' for reading
	25/07/29 14:01:31 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jackson-mapper-asl-1.9.13.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jackson-mapper-asl-1.9.13.jar
	25/07/29 14:01:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jackson-mapper-asl-1.9.13.jar' for reading
	25/07/29 14:01:32 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jackson-module-scala_2.12-2.15.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jackson-module-scala_2.12-2.15.2.jar
	25/07/29 14:01:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jackson-module-scala_2.12-2.15.2.jar' for reading
	25/07/29 14:01:32 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jakarta.annotation-api-1.3.5.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jakarta.annotation-api-1.3.5.jar
	25/07/29 14:01:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jakarta.annotation-api-1.3.5.jar' for reading
[INFO] 2025-07-29 14:01:33.196 +0000 -  ->
	25/07/29 14:01:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jakarta.inject-2.6.1.jar' for reading
	25/07/29 14:01:32 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jakarta.servlet-api-4.0.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jakarta.servlet-api-4.0.3.jar
	25/07/29 14:01:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jakarta.servlet-api-4.0.3.jar' for reading
	25/07/29 14:01:32 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jakarta.validation-api-2.0.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jakarta.validation-api-2.0.2.jar
	25/07/29 14:01:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jakarta.validation-api-2.0.2.jar' for reading
	25/07/29 14:01:32 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jakarta.ws.rs-api-2.1.6.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jakarta.ws.rs-api-2.1.6.jar
	25/07/29 14:01:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jakarta.ws.rs-api-2.1.6.jar' for reading
	25/07/29 14:01:32 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jakarta.xml.bind-api-2.3.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jakarta.xml.bind-api-2.3.2.jar
	25/07/29 14:01:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jakarta.xml.bind-api-2.3.2.jar' for reading
	25/07/29 14:01:32 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/janino-3.1.9.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/janino-3.1.9.jar
	25/07/29 14:01:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/janino-3.1.9.jar' for reading
	25/07/29 14:01:32 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/javassist-3.29.2-GA.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/javassist-3.29.2-GA.jar
	25/07/29 14:01:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/javassist-3.29.2-GA.jar' for reading
	25/07/29 14:01:32 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/javax.inject-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/javax.inject-1.jar
	25/07/29 14:01:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/javax.inject-1.jar' for reading
	25/07/29 14:01:32 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/javax.jdo-3.2.0-m3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/javax.jdo-3.2.0-m3.jar
	25/07/29 14:01:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/javax.jdo-3.2.0-m3.jar' for reading
	25/07/29 14:01:32 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/javax.servlet-api-3.1.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/javax.servlet-api-3.1.0.jar
	25/07/29 14:01:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/javax.servlet-api-3.1.0.jar' for reading
	25/07/29 14:01:33 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/javolution-5.5.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/javolution-5.5.1.jar
	25/07/29 14:01:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/javolution-5.5.1.jar' for reading
	25/07/29 14:01:33 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jaxb-runtime-2.3.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jaxb-runtime-2.3.2.jar
	25/07/29 14:01:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jaxb-runtime-2.3.2.jar' for reading
[INFO] 2025-07-29 14:01:34.197 +0000 -  ->
	25/07/29 14:01:33 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jcl-over-slf4j-2.0.7.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jcl-over-slf4j-2.0.7.jar
	25/07/29 14:01:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jcl-over-slf4j-2.0.7.jar' for reading
	25/07/29 14:01:33 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jdo-api-3.0.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jdo-api-3.0.1.jar
	25/07/29 14:01:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jdo-api-3.0.1.jar' for reading
	25/07/29 14:01:33 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jersey-client-2.40.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jersey-client-2.40.jar
	25/07/29 14:01:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jersey-client-2.40.jar' for reading
	25/07/29 14:01:33 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jersey-common-2.40.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jersey-common-2.40.jar
	25/07/29 14:01:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jersey-common-2.40.jar' for reading
	25/07/29 14:01:33 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jersey-container-servlet-2.40.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jersey-container-servlet-2.40.jar
	25/07/29 14:01:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jersey-container-servlet-2.40.jar' for reading
	25/07/29 14:01:33 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jersey-container-servlet-core-2.40.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jersey-container-servlet-core-2.40.jar
	25/07/29 14:01:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jersey-container-servlet-core-2.40.jar' for reading
	25/07/29 14:01:33 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jersey-hk2-2.40.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jersey-hk2-2.40.jar
	25/07/29 14:01:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jersey-hk2-2.40.jar' for reading
	25/07/29 14:01:33 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jersey-server-2.40.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jersey-server-2.40.jar
	25/07/29 14:01:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jersey-server-2.40.jar' for reading
	25/07/29 14:01:33 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jetty-rewrite-9.3.27.v20190418.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jetty-rewrite-9.3.27.v20190418.jar
	25/07/29 14:01:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jetty-rewrite-9.3.27.v20190418.jar' for reading
	25/07/29 14:01:33 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jline-2.14.6.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jline-2.14.6.jar
	25/07/29 14:01:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jline-2.14.6.jar' for reading
	25/07/29 14:01:34 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jmespath-java-1.12.705.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jmespath-java-1.12.705.jar
	25/07/29 14:01:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jmespath-java-1.12.705.jar' for reading
	25/07/29 14:01:34 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/joda-time-2.12.5.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/joda-time-2.12.5.jar
	25/07/29 14:01:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/joda-time-2.12.5.jar' for reading
[INFO] 2025-07-29 14:01:35.198 +0000 -  ->
	25/07/29 14:01:34 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jodd-core-3.5.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jodd-core-3.5.2.jar
	25/07/29 14:01:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jodd-core-3.5.2.jar' for reading
	25/07/29 14:01:34 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jpam-1.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jpam-1.1.jar
	25/07/29 14:01:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jpam-1.1.jar' for reading
	25/07/29 14:01:34 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/json-1.8.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/json-1.8.jar
	25/07/29 14:01:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/json-1.8.jar' for reading
	25/07/29 14:01:34 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/json4s-ast_2.12-3.7.0-M11.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/json4s-ast_2.12-3.7.0-M11.jar
	25/07/29 14:01:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/json4s-ast_2.12-3.7.0-M11.jar' for reading
	25/07/29 14:01:34 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/json4s-core_2.12-3.7.0-M11.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/json4s-core_2.12-3.7.0-M11.jar
	25/07/29 14:01:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/json4s-core_2.12-3.7.0-M11.jar' for reading
	25/07/29 14:01:34 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/json4s-jackson_2.12-3.7.0-M11.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/json4s-jackson_2.12-3.7.0-M11.jar
	25/07/29 14:01:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/json4s-jackson_2.12-3.7.0-M11.jar' for reading
	25/07/29 14:01:34 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/json4s-scalap_2.12-3.7.0-M11.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/json4s-scalap_2.12-3.7.0-M11.jar
	25/07/29 14:01:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/json4s-scalap_2.12-3.7.0-M11.jar' for reading
	25/07/29 14:01:34 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jsr305-3.0.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jsr305-3.0.0.jar
	25/07/29 14:01:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jsr305-3.0.0.jar' for reading
	25/07/29 14:01:34 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jsr305-3.0.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jsr305-3.0.2.jar
	25/07/29 14:01:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jsr305-3.0.2.jar' for reading
	25/07/29 14:01:34 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jta-1.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jta-1.1.jar
	25/07/29 14:01:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jta-1.1.jar' for reading
	25/07/29 14:01:34 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/jul-to-slf4j-2.0.7.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/jul-to-slf4j-2.0.7.jar
	25/07/29 14:01:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/jul-to-slf4j-2.0.7.jar' for reading
	25/07/29 14:01:35 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/kryo-shaded-4.0.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/kryo-shaded-4.0.2.jar
	25/07/29 14:01:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/kryo-shaded-4.0.2.jar' for reading
	25/07/29 14:01:35 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/lapack-3.0.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/lapack-3.0.3.jar
	25/07/29 14:01:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/lapack-3.0.3.jar' for reading
[INFO] 2025-07-29 14:01:36.199 +0000 -  ->
	25/07/29 14:01:35 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/leveldbjni-all-1.8.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/leveldbjni-all-1.8.jar
	25/07/29 14:01:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/leveldbjni-all-1.8.jar' for reading
	25/07/29 14:01:35 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/libfb303-0.9.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/libfb303-0.9.3.jar
	25/07/29 14:01:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/libfb303-0.9.3.jar' for reading
	25/07/29 14:01:35 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/libthrift-0.12.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/libthrift-0.12.0.jar
	25/07/29 14:01:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/libthrift-0.12.0.jar' for reading
	25/07/29 14:01:35 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/log4j-1.2-api-2.20.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/log4j-1.2-api-2.20.0.jar
	25/07/29 14:01:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/log4j-1.2-api-2.20.0.jar' for reading
	25/07/29 14:01:35 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/log4j-api-2.20.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/log4j-api-2.20.0.jar
	25/07/29 14:01:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/log4j-api-2.20.0.jar' for reading
	25/07/29 14:01:35 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/log4j-core-2.20.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/log4j-core-2.20.0.jar
	25/07/29 14:01:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/log4j-core-2.20.0.jar' for reading
	25/07/29 14:01:35 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/log4j-slf4j2-impl-2.20.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/log4j-slf4j2-impl-2.20.0.jar
	25/07/29 14:01:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/log4j-slf4j2-impl-2.20.0.jar' for reading
	25/07/29 14:01:35 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/logging-interceptor-3.12.12.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/logging-interceptor-3.12.12.jar
	25/07/29 14:01:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/logging-interceptor-3.12.12.jar' for reading
	25/07/29 14:01:36 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/lz4-java-1.8.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/lz4-java-1.8.0.jar
	25/07/29 14:01:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/lz4-java-1.8.0.jar' for reading
	25/07/29 14:01:36 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/mariadb-connector-java.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/mariadb-connector-java.jar
	25/07/29 14:01:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/mariadb-connector-java.jar' for reading
	25/07/29 14:01:36 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/metrics-core-4.2.19.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/metrics-core-4.2.19.jar
[INFO] 2025-07-29 14:01:37.199 +0000 -  ->
	25/07/29 14:01:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/metrics-core-4.2.19.jar' for reading
	25/07/29 14:01:36 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/metrics-graphite-4.2.19.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/metrics-graphite-4.2.19.jar
	25/07/29 14:01:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/metrics-graphite-4.2.19.jar' for reading
	25/07/29 14:01:36 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/metrics-jmx-4.2.19.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/metrics-jmx-4.2.19.jar
	25/07/29 14:01:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/metrics-jmx-4.2.19.jar' for reading
	25/07/29 14:01:36 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/metrics-json-4.2.19.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/metrics-json-4.2.19.jar
	25/07/29 14:01:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/metrics-json-4.2.19.jar' for reading
	25/07/29 14:01:36 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/metrics-jvm-4.2.19.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/metrics-jvm-4.2.19.jar
	25/07/29 14:01:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/metrics-jvm-4.2.19.jar' for reading
	25/07/29 14:01:36 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/minlog-1.3.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/minlog-1.3.0.jar
	25/07/29 14:01:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/minlog-1.3.0.jar' for reading
	25/07/29 14:01:36 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/netty-all-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/netty-all-4.1.100.Final.jar
	25/07/29 14:01:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/netty-all-4.1.100.Final.jar' for reading
	25/07/29 14:01:36 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/netty-buffer-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/netty-buffer-4.1.100.Final.jar
	25/07/29 14:01:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/netty-buffer-4.1.100.Final.jar' for reading
	25/07/29 14:01:36 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/netty-codec-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/netty-codec-4.1.100.Final.jar
	25/07/29 14:01:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/netty-codec-4.1.100.Final.jar' for reading
	25/07/29 14:01:36 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/netty-codec-http-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/netty-codec-http-4.1.100.Final.jar
	25/07/29 14:01:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/netty-codec-http-4.1.100.Final.jar' for reading
	25/07/29 14:01:36 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/netty-codec-http2-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/netty-codec-http2-4.1.100.Final.jar
	25/07/29 14:01:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/netty-codec-http2-4.1.100.Final.jar' for reading
	25/07/29 14:01:37 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/netty-codec-socks-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/netty-codec-socks-4.1.100.Final.jar
	25/07/29 14:01:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/netty-codec-socks-4.1.100.Final.jar' for reading
	25/07/29 14:01:37 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/netty-common-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/netty-common-4.1.100.Final.jar
	25/07/29 14:01:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/netty-common-4.1.100.Final.jar' for reading
[INFO] 2025-07-29 14:01:38.200 +0000 -  ->
	25/07/29 14:01:37 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/netty-handler-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/netty-handler-4.1.100.Final.jar
	25/07/29 14:01:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/netty-handler-4.1.100.Final.jar' for reading
	25/07/29 14:01:37 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/netty-handler-proxy-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/netty-handler-proxy-4.1.100.Final.jar
	25/07/29 14:01:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/netty-handler-proxy-4.1.100.Final.jar' for reading
	25/07/29 14:01:37 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/netty-resolver-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/netty-resolver-4.1.100.Final.jar
	25/07/29 14:01:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/netty-resolver-4.1.100.Final.jar' for reading
	25/07/29 14:01:37 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/netty-transport-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/netty-transport-4.1.100.Final.jar
	25/07/29 14:01:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/netty-transport-4.1.100.Final.jar' for reading
	25/07/29 14:01:37 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/netty-transport-classes-epoll-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/netty-transport-classes-epoll-4.1.100.Final.jar
	25/07/29 14:01:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/netty-transport-classes-epoll-4.1.100.Final.jar' for reading
	25/07/29 14:01:37 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/netty-transport-classes-kqueue-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/netty-transport-classes-kqueue-4.1.100.Final.jar
	25/07/29 14:01:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/netty-transport-classes-kqueue-4.1.100.Final.jar' for reading
	25/07/29 14:01:37 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar
	25/07/29 14:01:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar' for reading
	25/07/29 14:01:37 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar
	25/07/29 14:01:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar' for reading
	25/07/29 14:01:37 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar
	25/07/29 14:01:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar' for reading
	25/07/29 14:01:37 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar
	25/07/29 14:01:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar' for reading
	25/07/29 14:01:38 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/netty-transport-native-unix-common-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/netty-transport-native-unix-common-4.1.100.Final.jar
	25/07/29 14:01:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/netty-transport-native-unix-common-4.1.100.Final.jar' for reading
	25/07/29 14:01:38 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/objenesis-2.5.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/objenesis-2.5.1.jar
	25/07/29 14:01:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/objenesis-2.5.1.jar' for reading
	25/07/29 14:01:38 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/objenesis-3.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/objenesis-3.3.jar
	25/07/29 14:01:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/objenesis-3.3.jar' for reading
	25/07/29 14:01:38 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/okhttp-3.12.12.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/okhttp-3.12.12.jar
	25/07/29 14:01:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/okhttp-3.12.12.jar' for reading
[INFO] 2025-07-29 14:01:39.201 +0000 -  ->
	25/07/29 14:01:38 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/okio-1.15.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/okio-1.15.0.jar
	25/07/29 14:01:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/okio-1.15.0.jar' for reading
	25/07/29 14:01:38 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/opencsv-2.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/opencsv-2.3.jar
	25/07/29 14:01:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/opencsv-2.3.jar' for reading
	25/07/29 14:01:38 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/orc-core-1.9.4-shaded-protobuf.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/orc-core-1.9.4-shaded-protobuf.jar
	25/07/29 14:01:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/orc-core-1.9.4-shaded-protobuf.jar' for reading
	25/07/29 14:01:38 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/orc-mapreduce-1.9.4-shaded-protobuf.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/orc-mapreduce-1.9.4-shaded-protobuf.jar
	25/07/29 14:01:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/orc-mapreduce-1.9.4-shaded-protobuf.jar' for reading
	25/07/29 14:01:38 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/orc-shims-1.9.4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/orc-shims-1.9.4.jar
	25/07/29 14:01:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/orc-shims-1.9.4.jar' for reading
	25/07/29 14:01:38 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/oro-2.0.8.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/oro-2.0.8.jar
	25/07/29 14:01:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/oro-2.0.8.jar' for reading
	25/07/29 14:01:38 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/osgi-resource-locator-1.0.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/osgi-resource-locator-1.0.3.jar
	25/07/29 14:01:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/osgi-resource-locator-1.0.3.jar' for reading
	25/07/29 14:01:38 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/paranamer-2.8.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/paranamer-2.8.jar
	25/07/29 14:01:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/paranamer-2.8.jar' for reading
	25/07/29 14:01:38 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/parquet-column-1.13.1-amzn-3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/parquet-column-1.13.1-amzn-3.jar
	25/07/29 14:01:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/parquet-column-1.13.1-amzn-3.jar' for reading
	25/07/29 14:01:39 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/parquet-common-1.13.1-amzn-3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/parquet-common-1.13.1-amzn-3.jar
	25/07/29 14:01:39 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/parquet-common-1.13.1-amzn-3.jar' for reading
	25/07/29 14:01:39 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/parquet-encoding-1.13.1-amzn-3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/parquet-encoding-1.13.1-amzn-3.jar
	25/07/29 14:01:39 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/parquet-encoding-1.13.1-amzn-3.jar' for reading
[INFO] 2025-07-29 14:01:40.202 +0000 -  ->
	25/07/29 14:01:39 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/parquet-format-structures-1.13.1-amzn-3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/parquet-format-structures-1.13.1-amzn-3.jar
	25/07/29 14:01:39 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/parquet-format-structures-1.13.1-amzn-3.jar' for reading
	25/07/29 14:01:39 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/parquet-hadoop-1.13.1-amzn-3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/parquet-hadoop-1.13.1-amzn-3.jar
	25/07/29 14:01:39 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/parquet-hadoop-1.13.1-amzn-3.jar' for reading
	25/07/29 14:01:39 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/parquet-jackson-1.13.1-amzn-3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/parquet-jackson-1.13.1-amzn-3.jar
	25/07/29 14:01:39 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/parquet-jackson-1.13.1-amzn-3.jar' for reading
	25/07/29 14:01:39 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/perfmark-api-0.26.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/perfmark-api-0.26.0.jar
	25/07/29 14:01:39 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/perfmark-api-0.26.0.jar' for reading
	25/07/29 14:01:40 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/pickle-1.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/pickle-1.3.jar
	25/07/29 14:01:40 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/pickle-1.3.jar' for reading
	25/07/29 14:01:40 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/proto-google-common-protos-2.17.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/proto-google-common-protos-2.17.0.jar
	25/07/29 14:01:40 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/proto-google-common-protos-2.17.0.jar' for reading
	25/07/29 14:01:40 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/py4j-0.10.9.7.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/py4j-0.10.9.7.jar
	25/07/29 14:01:40 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/py4j-0.10.9.7.jar' for reading
[INFO] 2025-07-29 14:01:41.202 +0000 -  ->
	25/07/29 14:01:40 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/remotetea-oncrpc-1.1.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/remotetea-oncrpc-1.1.2.jar
	25/07/29 14:01:40 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/remotetea-oncrpc-1.1.2.jar' for reading
	25/07/29 14:01:40 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/rocksdbjni-8.3.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/rocksdbjni-8.3.2.jar
	25/07/29 14:01:40 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/rocksdbjni-8.3.2.jar' for reading
[INFO] 2025-07-29 14:01:42.203 +0000 -  ->
	25/07/29 14:01:41 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/scala-collection-compat_2.12-2.7.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/scala-collection-compat_2.12-2.7.0.jar
	25/07/29 14:01:41 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/scala-collection-compat_2.12-2.7.0.jar' for reading
	25/07/29 14:01:41 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/scala-compiler-2.12.18.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/scala-compiler-2.12.18.jar
	25/07/29 14:01:41 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/scala-compiler-2.12.18.jar' for reading
	25/07/29 14:01:41 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/scala-library-2.12.18.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/scala-library-2.12.18.jar
	25/07/29 14:01:41 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/scala-library-2.12.18.jar' for reading
	25/07/29 14:01:41 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/scala-parser-combinators_2.12-2.3.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/scala-parser-combinators_2.12-2.3.0.jar
	25/07/29 14:01:41 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/scala-parser-combinators_2.12-2.3.0.jar' for reading
	25/07/29 14:01:41 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/scala-reflect-2.12.18.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/scala-reflect-2.12.18.jar
	25/07/29 14:01:41 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/scala-reflect-2.12.18.jar' for reading
	25/07/29 14:01:41 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/scala-xml_2.12-2.1.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/scala-xml_2.12-2.1.0.jar
	25/07/29 14:01:41 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/scala-xml_2.12-2.1.0.jar' for reading
	25/07/29 14:01:41 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/shims-0.9.45.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/shims-0.9.45.jar
	25/07/29 14:01:41 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/shims-0.9.45.jar' for reading
	25/07/29 14:01:42 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/slf4j-api-2.0.7.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/slf4j-api-2.0.7.jar
	25/07/29 14:01:42 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/slf4j-api-2.0.7.jar' for reading
	25/07/29 14:01:42 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/snakeyaml-2.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/snakeyaml-2.1.jar
	25/07/29 14:01:42 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/snakeyaml-2.1.jar' for reading
	25/07/29 14:01:42 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/snakeyaml-engine-2.6.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/snakeyaml-engine-2.6.jar
[INFO] 2025-07-29 14:01:43.203 +0000 -  ->
	25/07/29 14:01:42 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/snakeyaml-engine-2.6.jar' for reading
	25/07/29 14:01:42 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/snappy-java-1.1.10.5.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/snappy-java-1.1.10.5.jar
	25/07/29 14:01:42 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/snappy-java-1.1.10.5.jar' for reading
	25/07/29 14:01:42 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-acl-1.0.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-acl-1.0.0.jar
	25/07/29 14:01:42 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-acl-1.0.0.jar' for reading
	25/07/29 14:01:42 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-catalyst_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-catalyst_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:42 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-catalyst_2.12-3.5.2-amzn-1.jar' for reading
	25/07/29 14:01:42 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-common-utils_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-common-utils_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:42 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-common-utils_2.12-3.5.2-amzn-1.jar' for reading
	25/07/29 14:01:42 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-core_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-core_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:42 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-core_2.12-3.5.2-amzn-1.jar' for reading
	25/07/29 14:01:43 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-fgac-iceberg_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-fgac-iceberg_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:43 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-fgac-iceberg_2.12-3.5.2-amzn-1.jar' for reading
[INFO] 2025-07-29 14:01:44.204 +0000 -  ->
	25/07/29 14:01:43 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-fgac_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-fgac_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:43 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-fgac_2.12-3.5.2-amzn-1.jar' for reading
	25/07/29 14:01:43 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-ganglia-lgpl_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-ganglia-lgpl_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:43 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-ganglia-lgpl_2.12-3.5.2-amzn-1.jar' for reading
	25/07/29 14:01:43 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-graphx_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-graphx_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:43 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-graphx_2.12-3.5.2-amzn-1.jar' for reading
	25/07/29 14:01:43 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-hive-thriftserver_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-hive-thriftserver_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:43 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-hive-thriftserver_2.12-3.5.2-amzn-1.jar' for reading
	25/07/29 14:01:43 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-hive_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-hive_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:43 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-hive_2.12-3.5.2-amzn-1.jar' for reading
	25/07/29 14:01:43 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-kvstore_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-kvstore_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:43 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-kvstore_2.12-3.5.2-amzn-1.jar' for reading
	25/07/29 14:01:43 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-launcher_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-launcher_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:43 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-launcher_2.12-3.5.2-amzn-1.jar' for reading
	25/07/29 14:01:43 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-mllib-local_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-mllib-local_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:43 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-mllib-local_2.12-3.5.2-amzn-1.jar' for reading
	25/07/29 14:01:44 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-mllib_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-mllib_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-mllib_2.12-3.5.2-amzn-1.jar' for reading
	25/07/29 14:01:44 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-network-common_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-network-common_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-network-common_2.12-3.5.2-amzn-1.jar' for reading
[INFO] 2025-07-29 14:01:45.205 +0000 -  ->
	25/07/29 14:01:44 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-network-shuffle_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-network-shuffle_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-network-shuffle_2.12-3.5.2-amzn-1.jar' for reading
	25/07/29 14:01:44 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-repl_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-repl_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-repl_2.12-3.5.2-amzn-1.jar' for reading
	25/07/29 14:01:44 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-sketch_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-sketch_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-sketch_2.12-3.5.2-amzn-1.jar' for reading
	25/07/29 14:01:44 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-sql-api_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-sql-api_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-sql-api_2.12-3.5.2-amzn-1.jar' for reading
	25/07/29 14:01:44 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-sql_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-sql_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-sql_2.12-3.5.2-amzn-1.jar' for reading
	25/07/29 14:01:44 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-streaming_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-streaming_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-streaming_2.12-3.5.2-amzn-1.jar' for reading
	25/07/29 14:01:44 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-tags_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-tags_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-tags_2.12-3.5.2-amzn-1.jar' for reading
	25/07/29 14:01:44 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-unsafe_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-unsafe_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-unsafe_2.12-3.5.2-amzn-1.jar' for reading
	25/07/29 14:01:44 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spark-yarn_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spark-yarn_2.12-3.5.2-amzn-1.jar
	25/07/29 14:01:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spark-yarn_2.12-3.5.2-amzn-1.jar' for reading
	25/07/29 14:01:45 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spire-macros_2.12-0.17.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spire-macros_2.12-0.17.0.jar
	25/07/29 14:01:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spire-macros_2.12-0.17.0.jar' for reading
	25/07/29 14:01:45 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spire-platform_2.12-0.17.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spire-platform_2.12-0.17.0.jar
	25/07/29 14:01:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spire-platform_2.12-0.17.0.jar' for reading
[INFO] 2025-07-29 14:01:46.206 +0000 -  ->
	25/07/29 14:01:45 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spire-util_2.12-0.17.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spire-util_2.12-0.17.0.jar
	25/07/29 14:01:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spire-util_2.12-0.17.0.jar' for reading
	25/07/29 14:01:45 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/spire_2.12-0.17.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/spire_2.12-0.17.0.jar
	25/07/29 14:01:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/spire_2.12-0.17.0.jar' for reading
	25/07/29 14:01:45 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/stax-api-1.0.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/stax-api-1.0.1.jar
	25/07/29 14:01:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/stax-api-1.0.1.jar' for reading
	25/07/29 14:01:45 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/stream-2.9.6.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/stream-2.9.6.jar
	25/07/29 14:01:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/stream-2.9.6.jar' for reading
	25/07/29 14:01:45 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/super-csv-2.2.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/super-csv-2.2.0.jar
	25/07/29 14:01:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/super-csv-2.2.0.jar' for reading
	25/07/29 14:01:45 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/threeten-extra-1.7.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/threeten-extra-1.7.1.jar
	25/07/29 14:01:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/threeten-extra-1.7.1.jar' for reading
	25/07/29 14:01:45 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/tink-1.9.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/tink-1.9.0.jar
	25/07/29 14:01:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/tink-1.9.0.jar' for reading
	25/07/29 14:01:45 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/transaction-api-1.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/transaction-api-1.1.jar
	25/07/29 14:01:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/transaction-api-1.1.jar' for reading
	25/07/29 14:01:45 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/univocity-parsers-2.9.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/univocity-parsers-2.9.1.jar
	25/07/29 14:01:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/univocity-parsers-2.9.1.jar' for reading
	25/07/29 14:01:46 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/volcano-client-6.7.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/volcano-client-6.7.2.jar
	25/07/29 14:01:46 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/volcano-client-6.7.2.jar' for reading
	25/07/29 14:01:46 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/volcano-model-v1beta1-6.7.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/volcano-model-v1beta1-6.7.2.jar
	25/07/29 14:01:46 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/volcano-model-v1beta1-6.7.2.jar' for reading
	25/07/29 14:01:46 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/xbean-asm9-shaded-4.23.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/xbean-asm9-shaded-4.23.jar
	25/07/29 14:01:46 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/xbean-asm9-shaded-4.23.jar' for reading
[INFO] 2025-07-29 14:01:47.207 +0000 -  ->
	25/07/29 14:01:46 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/xz-1.9.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/xz-1.9.jar
	25/07/29 14:01:46 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/xz-1.9.jar' for reading
	25/07/29 14:01:46 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/zjsonpatch-0.3.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/zjsonpatch-0.3.0.jar
	25/07/29 14:01:46 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/zjsonpatch-0.3.0.jar' for reading
	25/07/29 14:01:46 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/zookeeper-3.9.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/zookeeper-3.9.1.jar
	25/07/29 14:01:46 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/zookeeper-3.9.1.jar' for reading
	25/07/29 14:01:46 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/zookeeper-jute-3.9.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/zookeeper-jute-3.9.1.jar
	25/07/29 14:01:46 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/zookeeper-jute-3.9.1.jar' for reading
	25/07/29 14:01:46 INFO Client: Uploading resource s3://exchanges-flink-test/batch/emr/spark-jars/zstd-jni-1.5.5-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/zstd-jni-1.5.5-4.jar
	25/07/29 14:01:46 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-test/batch/emr/spark-jars/zstd-jni-1.5.5-4.jar' for reading
	25/07/29 14:01:46 INFO Client: Uploading resource file:/dolphinscheduler/default/resources/mysql-connector-j-8.0.33.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/mysql-connector-j-8.0.33.jar
	25/07/29 14:01:46 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hive-site.xml
	25/07/29 14:01:46 INFO Client: Uploading resource file:/etc/hudi/conf/hudi-defaults.conf -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/hudi-defaults.conf
	25/07/29 14:01:46 INFO Client: Uploading resource file:/mnt/spark/python/lib/pyspark.zip -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/pyspark.zip
	25/07/29 14:01:46 INFO Client: Uploading resource file:/mnt/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/py4j-0.10.9.7-src.zip
	25/07/29 14:01:46 INFO Client: Uploading resource file:/tmp/spark-66ecfbc1-7f99-4c78-a6a8-36d88bafe73a/__spark_conf__14493540461551358604.zip -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1739849539217_3163/__spark_conf__.zip
	25/07/29 14:01:46 INFO SecurityManager: Changing view acls to: root
	25/07/29 14:01:46 INFO SecurityManager: Changing modify acls to: root
	25/07/29 14:01:46 INFO SecurityManager: Changing view acls groups to:
	25/07/29 14:01:46 INFO SecurityManager: Changing modify acls groups to:
	25/07/29 14:01:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
	25/07/29 14:01:46 INFO Client: Submitting application application_1739849539217_3163 to ResourceManager
	25/07/29 14:01:47 INFO YarnClientImpl: Submitted application application_1739849539217_3163
[INFO] 2025-07-29 14:01:48.208 +0000 -  ->
	25/07/29 14:01:48 INFO Client: Application report for application_1739849539217_3163 (state: ACCEPTED)
	25/07/29 14:01:48 INFO Client:
		 client token: N/A
		 diagnostics: AM container is launched, waiting for AM container to Register with RM
		 ApplicationMaster host: N/A
		 ApplicationMaster RPC port: -1
		 queue: root.default
		 start time: 1753797706945
		 final status: UNDEFINED
		 tracking URL: http://ip-172-31-9-196.ap-southeast-1.compute.internal:20888/proxy/application_1739849539217_3163/
		 user: root
[INFO] 2025-07-29 14:01:55.209 +0000 -  ->
	25/07/29 14:01:54 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-31-9-196.ap-southeast-1.compute.internal, PROXY_URI_BASES -> http://ip-172-31-9-196.ap-southeast-1.compute.internal:20888/proxy/application_1739849539217_3163, RM_HA_URLS -> ip-172-31-9-196.ap-southeast-1.compute.internal:8088,ip-172-31-9-114.ap-southeast-1.compute.internal:8088,ip-172-31-9-127.ap-southeast-1.compute.internal:8088), /proxy/application_1739849539217_3163
	25/07/29 14:01:54 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
	25/07/29 14:01:55 INFO Client: Application report for application_1739849539217_3163 (state: RUNNING)
	25/07/29 14:01:55 INFO Client:
		 client token: N/A
		 diagnostics: N/A
		 ApplicationMaster host: 172.31.9.95
		 ApplicationMaster RPC port: -1
		 queue: root.default
		 start time: 1753797706945
		 final status: UNDEFINED
		 tracking URL: http://ip-172-31-9-196.ap-southeast-1.compute.internal:20888/proxy/application_1739849539217_3163/
		 user: root
	25/07/29 14:01:55 INFO YarnClientSchedulerBackend: Application application_1739849539217_3163 has started running.
	25/07/29 14:01:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 17559.
	25/07/29 14:01:55 INFO NettyBlockTransferService: Server created on ip-172-31-9-77.ap-southeast-1.compute.internal:17559
	25/07/29 14:01:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	25/07/29 14:01:55 INFO BlockManager: external shuffle service port = 7337
	25/07/29 14:01:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-9-77.ap-southeast-1.compute.internal, 17559, None)
	25/07/29 14:01:55 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-9-77.ap-southeast-1.compute.internal:17559 with 127.2 MiB RAM, BlockManagerId(driver, ip-172-31-9-77.ap-southeast-1.compute.internal, 17559, None)
	25/07/29 14:01:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-9-77.ap-southeast-1.compute.internal, 17559, None)
	25/07/29 14:01:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-9-77.ap-southeast-1.compute.internal, 17559, None)
[INFO] 2025-07-29 14:01:56.209 +0000 -  ->
	25/07/29 14:01:55 INFO SingleEventLogFileWriter: Logging events to hdfs:/var/log/spark/apps/application_1739849539217_3163.inprogress
	25/07/29 14:01:55 INFO Utils: Using 100 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /executors/heapHistogram: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /executors/heapHistogram/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/07/29 14:01:55 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
	✅ Spark会话创建完成，版本: 3.5.2-amzn-1
	MySQL配置: cex-mysql-test.c5mgk4qm8m2z.ap-southeast-1.rds.amazonaws.com:3358/biz_statistics
	🗄️  HiveMeta初始化完成
	🗄️  MysqlMeta初始化完成
	🔍 TagRuleParser初始化完成
	🚀 TagEngine初始化完成

	🚀 执行全量标签计算...
	🚀 开始标签计算，模式: task-all
	📋 加载标签规则...
	📋 加载标签规则，指定标签: None
[INFO] 2025-07-29 14:02:04.211 +0000 -  ->
	✅ 标签规则加载完成: 50 个标签
[INFO] 2025-07-29 14:02:05.211 +0000 -  ->
	🎯 分析标签依赖关系...
	🔍 分析标签规则依赖关系...
[INFO] 2025-07-29 14:02:07.212 +0000 -  ->
	   📋 标签 1: 依赖表 ['tag_system.user_asset_summary']
	   📋 标签 2: 依赖表 ['tag_system.user_asset_summary']
	   📋 标签 3: 依赖表 ['tag_system.user_activity_summary']
	   📋 标签 4: 依赖表 ['tag_system.user_activity_summary']
	   📋 标签 5: 依赖表 ['tag_system.user_asset_summary']
	   📋 标签 6: 依赖表 ['tag_system.user_risk_profile']
	   📋 标签 7: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 8: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 9: 依赖表 ['tag_system.user_asset_summary']
	   📋 标签 10: 依赖表 ['tag_system.user_asset_summary']
	   📋 标签 11: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 12: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 13: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 14: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 15: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 16: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 17: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 18: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 19: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 20: 依赖表 ['tag_system.user_activity_summary']
	   📋 标签 21: 依赖表 ['tag_system.user_activity_summary']
	   📋 标签 22: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 23: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 24: 依赖表 ['tag_system.user_activity_summary']
	   📋 标签 25: 依赖表 ['tag_system.user_activity_summary']
	   📋 标签 26: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 27: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 28: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 29: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 30: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 31: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 32: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 33: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 34: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 35: 依赖表 ['tag_system.user_preferences']
	   📋 标签 36: 依赖表 ['tag_system.user_preferences']
	   📋 标签 37: 依赖表 ['tag_system.user_preferences']
	   📋 标签 38: 依赖表 ['tag_system.user_preferences']
	   📋 标签 39: 依赖表 ['tag_system.user_preferences']
	   📋 标签 40: 依赖表 ['tag_system.user_preferences']
	   📋 标签 41: 依赖表 ['tag_system.user_preferences']
	   📋 标签 42: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 43: 依赖表 ['tag_system.user_asset_summary', 'tag_system.user_activity_summary']
	   📋 标签 44: 依赖表 ['tag_system.user_basic_info', 'tag_system.user_asset_summary']
	   📋 标签 45: 依赖表 ['tag_system.user_activity_summary', 'tag_system.user_asset_summary']
	   📋 标签 46: 依赖表 ['tag_system.user_basic_info', 'tag_system.user_activity_summary']
	   📋 标签 47: 依赖表 ['tag_system.user_basic_info']
	   📋 标签 48: 依赖表 ['tag_system.user_preferences', 'tag_system.user_basic_info', 'tag_system.user_asset_summary']
	   📋 标签 49: 依赖表 ['tag_system.user_preferences', 'tag_system.user_basic_info']
	   📋 标签 50: 依赖表 ['tag_system.user_preferences', 'tag_system.user_basic_info']
	✅ 依赖分析完成: 50 个标签
	🎯 智能分组标签...
	📦 创建标签组: Group_5tags_1tables
	   🏷️  标签: [1, 2, 5, 9, 10]
	   📊 依赖表: ['tag_system.user_asset_summary']
	   🏷️  组 1: 标签[1, 2, 5, 9, 10] → 表['tag_system.user_asset_summary']
	📦 创建标签组: Group_6tags_1tables
	   🏷️  标签: [3, 4, 20, 21, 24, 25]
	   📊 依赖表: ['tag_system.user_activity_summary']
	   🏷️  组 2: 标签[3, 4, 20, 21, 24, 25] → 表['tag_system.user_activity_summary']
	📦 创建标签组: Group_1tags_1tables
	   🏷️  标签: [6]
	   📊 依赖表: ['tag_system.user_risk_profile']
	   🏷️  组 3: 标签[6] → 表['tag_system.user_risk_profile']
	📦 创建标签组: Group_24tags_1tables
	   🏷️  标签: [7, 8, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 26, 27, 28, 29, 30, 31, 32, 33, 34, 42, 47]
	   📊 依赖表: ['tag_system.user_basic_info']
	   🏷️  组 4: 标签[7, 8, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 26, 27, 28, 29, 30, 31, 32, 33, 34, 42, 47] → 表['tag_system.user_basic_info']
	📦 创建标签组: Group_7tags_1tables
	   🏷️  标签: [35, 36, 37, 38, 39, 40, 41]
	   📊 依赖表: ['tag_system.user_preferences']
	   🏷️  组 5: 标签[35, 36, 37, 38, 39, 40, 41] → 表['tag_system.user_preferences']
	📦 创建标签组: Group_2tags_2tables
	   🏷️  标签: [43, 45]
	   📊 依赖表: ['tag_system.user_activity_summary', 'tag_system.user_asset_summary']
	   🏷️  组 6: 标签[43, 45] → 表['tag_system.user_activity_summary', 'tag_system.user_asset_summary']
	📦 创建标签组: Group_1tags_2tables
	   🏷️  标签: [44]
	   📊 依赖表: ['tag_system.user_asset_summary', 'tag_system.user_basic_info']
	   🏷️  组 7: 标签[44] → 表['tag_system.user_asset_summary', 'tag_system.user_basic_info']
	📦 创建标签组: Group_1tags_2tables
	   🏷️  标签: [46]
	   📊 依赖表: ['tag_system.user_activity_summary', 'tag_system.user_basic_info']
	   🏷️  组 8: 标签[46] → 表['tag_system.user_activity_summary', 'tag_system.user_basic_info']
	📦 创建标签组: Group_1tags_3tables
	   🏷️  标签: [48]
	   📊 依赖表: ['tag_system.user_asset_summary', 'tag_system.user_basic_info', 'tag_system.user_preferences']
	   🏷️  组 9: 标签[48] → 表['tag_system.user_asset_summary', 'tag_system.user_basic_info', 'tag_system.user_preferences']
	📦 创建标签组: Group_2tags_2tables
	   🏷️  标签: [49, 50]
	   📊 依赖表: ['tag_system.user_basic_info', 'tag_system.user_preferences']
	   🏷️  组 10: 标签[49, 50] → 表['tag_system.user_basic_info', 'tag_system.user_preferences']
	✅ 分组完成: 10 个计算组
	🚀 并行计算 10 个标签组...
	   📦 计算标签组 1/10: Group_5tags_1tables
	🚀 开始计算标签组: Group_5tags_1tables
[INFO] 2025-07-29 14:02:09.213 +0000 -  ->
	   📋 该组标签规则数: 5
	🔍 TagRuleParser初始化完成
	🔍 分析字段依赖关系...
[INFO] 2025-07-29 14:02:10.213 +0000 -  ->
	✅ 字段依赖分析完成: 1 个表
	   📊 tag_system.user_asset_summary: ['debt_amount', 'total_asset_value', 'cash_balance', 'user_id']
	📖 加载Hive表: tag_system.user_asset_summary
	25/07/29 14:02:09 WARN HiveConf: HiveConf of name hive.server2.thrift.url does not exist
	✅ 表 tag_system.user_asset_summary 加载完成，字段: {'debt_amount', 'total_asset_value', 'cash_balance', 'user_id'}
[INFO] 2025-07-29 14:02:12.214 +0000 -  ->
	   🔗 JOIN完成，用户数: 1000
	   🎯 并行计算 5 个标签...
[INFO] 2025-07-29 14:02:13.214 +0000 -  ->
	      🏷️  计算标签 1...
	🔍 TagRuleParser初始化完成
	         🔍 标签 1 SQL条件: (`user_asset_summary`.`total_asset_value` = 100000)
	         ✅ 标签 1: 168 个用户
	      🏷️  计算标签 2...
	🔍 TagRuleParser初始化完成
	         🔍 标签 2 SQL条件: (`user_asset_summary`.`total_asset_value` != 0)
	         ✅ 标签 2: 824 个用户
	      🏷️  计算标签 5...
	🔍 TagRuleParser初始化完成
	         🔍 标签 5 SQL条件: (`user_asset_summary`.`cash_balance` >= 50000)
[INFO] 2025-07-29 14:02:14.214 +0000 -  ->
	         ✅ 标签 5: 596 个用户
	      🏷️  计算标签 9...
	🔍 TagRuleParser初始化完成
	         🔍 标签 9 SQL条件: (`user_asset_summary`.`debt_amount` IS NULL)
	         ✅ 标签 9: 462 个用户
	      🏷️  计算标签 10...
	🔍 TagRuleParser初始化完成
	         🔍 标签 10 SQL条件: (`user_asset_summary`.`total_asset_value` IS NOT NULL)
	         ✅ 标签 10: 1000 个用户
	   🔀 聚合用户标签...
	❌ 标签组计算失败: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "flatten(collect_list(tag_id))" due to data type mismatch: Parameter 1 requires the "ARRAY" of "ARRAY" type, however "collect_list(tag_id)" has the type "ARRAY<INT>".;
	'Aggregate [user_id#39], [user_id#39, array_distinct(array_sort(flatten(collect_list(tag_id#206, 0, 0)), lambdafunction(if ((isnull(lambda 'left) AND isnull(lambda 'right))) 0 else if (isnull(lambda 'left)) 1 else if (isnull(lambda 'right)) -1 else if ((lambda 'left < lambda 'right)) -1 else if ((lambda 'left > lambda 'right)) 1 else 0, lambda 'left, lambda 'right, false), false)) AS tag_ids_array#895]
	+- Union false, false
	   :- Project [user_id#39, 1 AS tag_id#206]
	   :  +- Project [user_id#39]
	   :     +- Filter (total_asset_value#40 = cast(100000 as double))
	   :        +- Project [debt_amount#42, total_asset_value#40, cash_balance#41, user_id#39]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_asset_summary
	   :              +- Relation spark_catalog.tag_system.user_asset_summary[user_id#39,total_asset_value#40,cash_balance#41,debt_amount#42,dt#43] parquet
	   :- Project [user_id#860, 2 AS tag_id#337]
	   :  +- Project [user_id#860]
	   :     +- Filter NOT (total_asset_value#861 = cast(0 as double))
	   :        +- Project [debt_amount#863, total_asset_value#861, cash_balance#862, user_id#860]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_asset_summary
	   :              +- Relation spark_catalog.tag_system.user_asset_summary[user_id#860,total_asset_value#861,cash_balance#862,debt_amount#863,dt#864] parquet
	   :- Project [user_id#868, 5 AS tag_id#468]
	   :  +- Project [user_id#868]
	   :     +- Filter (cash_balance#870 >= cast(50000 as double))
	   :        +- Project [debt_amount#871, total_asset_value#869, cash_balance#870, user_id#868]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_asset_summary
	   :              +- Relation spark_catalog.tag_system.user_asset_summary[user_id#868,total_asset_value#869,cash_balance#870,debt_amount#871,dt#872] parquet
	   :- Project [user_id#876, 9 AS tag_id#599]
	   :  +- Project [user_id#876]
	   :     +- Filter isnull(debt_amount#879)
	   :        +- Project [debt_amount#879, total_asset_value#877, cash_balance#878, user_id#876]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_asset_summary
	   :              +- Relation spark_catalog.tag_system.user_asset_summary[user_id#876,total_asset_value#877,cash_balance#878,debt_amount#879,dt#880] parquet
	   +- Project [user_id#884, 10 AS tag_id#730]
	      +- Project [user_id#884]
	         +- Filter isnotnull(total_asset_value#885)
	            +- Project [debt_amount#887, total_asset_value#885, cash_balance#886, user_id#884]
	               +- SubqueryAlias spark_catalog.tag_system.user_asset_summary
	                  +- Relation spark_catalog.tag_system.user_asset_summary[user_id#884,total_asset_value#885,cash_balance#886,debt_amount#887,dt#888] parquet

	Traceback (most recent call last):
	  File "/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py", line 66, in computeTags
	    userTagsDF = self._aggregateUserTags(tagResultsDF)
	                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py", line 155, in _aggregateUserTags
	    userTagsDF = tagResultsDF.groupBy("user_id").agg(
	                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/sql/group.py", line 186, in agg
	    jdf = self._jgd.agg(exprs[0]._jc, _to_seq(self.session._sc, [c._jc for c in exprs[1:]]))
	          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
	    raise converted from None
	pyspark.errors.exceptions.captured.AnalysisException: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "flatten(collect_list(tag_id))" due to data type mismatch: Parameter 1 requires the "ARRAY" of "ARRAY" type, however "collect_list(tag_id)" has the type "ARRAY<INT>".;
	'Aggregate [user_id#39], [user_id#39, array_distinct(array_sort(flatten(collect_list(tag_id#206, 0, 0)), lambdafunction(if ((isnull(lambda 'left) AND isnull(lambda 'right))) 0 else if (isnull(lambda 'left)) 1 else if (isnull(lambda 'right)) -1 else if ((lambda 'left < lambda 'right)) -1 else if ((lambda 'left > lambda 'right)) 1 else 0, lambda 'left, lambda 'right, false), false)) AS tag_ids_array#895]
	+- Union false, false
	   :- Project [user_id#39, 1 AS tag_id#206]
	   :  +- Project [user_id#39]
	   :     +- Filter (total_asset_value#40 = cast(100000 as double))
	   :        +- Project [debt_amount#42, total_asset_value#40, cash_balance#41, user_id#39]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_asset_summary
	   :              +- Relation spark_catalog.tag_system.user_asset_summary[user_id#39,total_asset_value#40,cash_balance#41,debt_amount#42,dt#43] parquet
	   :- Project [user_id#860, 2 AS tag_id#337]
	   :  +- Project [user_id#860]
	   :     +- Filter NOT (total_asset_value#861 = cast(0 as double))
	   :        +- Project [debt_amount#863, total_asset_value#861, cash_balance#862, user_id#860]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_asset_summary
	   :              +- Relation spark_catalog.tag_system.user_asset_summary[user_id#860,total_asset_value#861,cash_balance#862,debt_amount#863,dt#864] parquet
	   :- Project [user_id#868, 5 AS tag_id#468]
	   :  +- Project [user_id#868]
	   :     +- Filter (cash_balance#870 >= cast(50000 as double))
	   :        +- Project [debt_amount#871, total_asset_value#869, cash_balance#870, user_id#868]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_asset_summary
	   :              +- Relation spark_catalog.tag_system.user_asset_summary[user_id#868,total_asset_value#869,cash_balance#870,debt_amount#871,dt#872] parquet
	   :- Project [user_id#876, 9 AS tag_id#599]
	   :  +- Project [user_id#876]
	   :     +- Filter isnull(debt_amount#879)
	   :        +- Project [debt_amount#879, total_asset_value#877, cash_balance#878, user_id#876]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_asset_summary
	   :              +- Relation spark_catalog.tag_system.user_asset_summary[user_id#876,total_asset_value#877,cash_balance#878,debt_amount#879,dt#880] parquet
	   +- Project [user_id#884, 10 AS tag_id#730]
	      +- Project [user_id#884]
	         +- Filter isnotnull(total_asset_value#885)
	            +- Project [debt_amount#887, total_asset_value#885, cash_balance#886, user_id#884]
	               +- SubqueryAlias spark_catalog.tag_system.user_asset_summary
	                  +- Relation spark_catalog.tag_system.user_asset_summary[user_id#884,total_asset_value#885,cash_balance#886,debt_amount#887,dt#888] parquet

	   📦 计算标签组 2/10: Group_6tags_1tables
	🚀 开始计算标签组: Group_6tags_1tables
	   📋 该组标签规则数: 6
	🔍 TagRuleParser初始化完成
	🔍 分析字段依赖关系...
	✅ 字段依赖分析完成: 1 个表
	   📊 tag_system.user_activity_summary: ['user_id', 'last_login_date', 'trade_count_30d', 'last_trade_date']
	📖 加载Hive表: tag_system.user_activity_summary
	✅ 表 tag_system.user_activity_summary 加载完成，字段: {'user_id', 'last_login_date', 'trade_count_30d', 'last_trade_date'}
[INFO] 2025-07-29 14:02:15.215 +0000 -  ->
	OpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended
[INFO] 2025-07-29 14:02:16.216 +0000 -  ->
	   🔗 JOIN完成，用户数: 1000
	   🎯 并行计算 6 个标签...
[INFO] 2025-07-29 14:02:17.216 +0000 -  ->
	      🏷️  计算标签 3...
	🔍 TagRuleParser初始化完成
	         🔍 标签 3 SQL条件: (`user_activity_summary`.`trade_count_30d` > 10)
	         ✅ 标签 3: 161 个用户
	      🏷️  计算标签 4...
	🔍 TagRuleParser初始化完成
	         🔍 标签 4 SQL条件: (`user_activity_summary`.`trade_count_30d` < 5)
	         ✅ 标签 4: 519 个用户
	      🏷️  计算标签 20...
	🔍 TagRuleParser初始化完成
	         🔍 标签 20 SQL条件: (`user_activity_summary`.`last_login_date` != '2025-01-01')
[INFO] 2025-07-29 14:02:18.217 +0000 -  ->
	         ✅ 标签 20: 947 个用户
	      🏷️  计算标签 21...
	🔍 TagRuleParser初始化完成
	         🔍 标签 21 SQL条件: (`user_activity_summary`.`last_login_date` > '2025-01-01')
	         ✅ 标签 21: 306 个用户
	      🏷️  计算标签 24...
	🔍 TagRuleParser初始化完成
	         🔍 标签 24 SQL条件: (`user_activity_summary`.`last_login_date` NOT BETWEEN '2023-01-01' AND '2023-12-31')
	         ✅ 标签 24: 1000 个用户
	      🏷️  计算标签 25...
	🔍 TagRuleParser初始化完成
	         🔍 标签 25 SQL条件: (`user_activity_summary`.`last_trade_date` IS NULL)
	         ✅ 标签 25: 310 个用户
	   🔀 聚合用户标签...
	❌ 标签组计算失败: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "flatten(collect_list(tag_id))" due to data type mismatch: Parameter 1 requires the "ARRAY" of "ARRAY" type, however "collect_list(tag_id)" has the type "ARRAY<INT>".;
	'Aggregate [user_id#911], [user_id#911, array_distinct(array_sort(flatten(collect_list(tag_id#1078, 0, 0)), lambdafunction(if ((isnull(lambda 'left) AND isnull(lambda 'right))) 0 else if (isnull(lambda 'left)) 1 else if (isnull(lambda 'right)) -1 else if ((lambda 'left < lambda 'right)) -1 else if ((lambda 'left > lambda 'right)) 1 else 0, lambda 'left, lambda 'right, false), false)) AS tag_ids_array#1906]
	+- Union false, false
	   :- Project [user_id#911, 3 AS tag_id#1078]
	   :  +- Project [user_id#911]
	   :     +- Filter (trade_count_30d#912 > 10)
	   :        +- Project [user_id#911, last_login_date#913, trade_count_30d#912, last_trade_date#914]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_activity_summary
	   :              +- Relation spark_catalog.tag_system.user_activity_summary[user_id#911,trade_count_30d#912,last_login_date#913,last_trade_date#914,dt#915] parquet
	   :- Project [user_id#1863, 4 AS tag_id#1209]
	   :  +- Project [user_id#1863]
	   :     +- Filter (trade_count_30d#1864 < 5)
	   :        +- Project [user_id#1863, last_login_date#1865, trade_count_30d#1864, last_trade_date#1866]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_activity_summary
	   :              +- Relation spark_catalog.tag_system.user_activity_summary[user_id#1863,trade_count_30d#1864,last_login_date#1865,last_trade_date#1866,dt#1867] parquet
	   :- Project [user_id#1871, 20 AS tag_id#1340]
	   :  +- Project [user_id#1871]
	   :     +- Filter NOT (last_login_date#1873 = 2025-01-01)
	   :        +- Project [user_id#1871, last_login_date#1873, trade_count_30d#1872, last_trade_date#1874]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_activity_summary
	   :              +- Relation spark_catalog.tag_system.user_activity_summary[user_id#1871,trade_count_30d#1872,last_login_date#1873,last_trade_date#1874,dt#1875] parquet
	   :- Project [user_id#1879, 21 AS tag_id#1471]
	   :  +- Project [user_id#1879]
	   :     +- Filter (last_login_date#1881 > 2025-01-01)
	   :        +- Project [user_id#1879, last_login_date#1881, trade_count_30d#1880, last_trade_date#1882]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_activity_summary
	   :              +- Relation spark_catalog.tag_system.user_activity_summary[user_id#1879,trade_count_30d#1880,last_login_date#1881,last_trade_date#1882,dt#1883] parquet
	   :- Project [user_id#1887, 24 AS tag_id#1602]
	   :  +- Project [user_id#1887]
	   :     +- Filter NOT ((last_login_date#1889 >= 2023-01-01) AND (last_login_date#1889 <= 2023-12-31))
	   :        +- Project [user_id#1887, last_login_date#1889, trade_count_30d#1888, last_trade_date#1890]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_activity_summary
	   :              +- Relation spark_catalog.tag_system.user_activity_summary[user_id#1887,trade_count_30d#1888,last_login_date#1889,last_trade_date#1890,dt#1891] parquet
	   +- Project [user_id#1895, 25 AS tag_id#1733]
	      +- Project [user_id#1895]
	         +- Filter isnull(last_trade_date#1898)
	            +- Project [user_id#1895, last_login_date#1897, trade_count_30d#1896, last_trade_date#1898]
	               +- SubqueryAlias spark_catalog.tag_system.user_activity_summary
	                  +- Relation spark_catalog.tag_system.user_activity_summary[user_id#1895,trade_count_30d#1896,last_login_date#1897,last_trade_date#1898,dt#1899] parquet

	Traceback (most recent call last):
	  File "/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py", line 66, in computeTags
	    userTagsDF = self._aggregateUserTags(tagResultsDF)
	                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py", line 155, in _aggregateUserTags
	    userTagsDF = tagResultsDF.groupBy("user_id").agg(
	                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/sql/group.py", line 186, in agg
	    jdf = self._jgd.agg(exprs[0]._jc, _to_seq(self.session._sc, [c._jc for c in exprs[1:]]))
	          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
	    raise converted from None
	pyspark.errors.exceptions.captured.AnalysisException: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "flatten(collect_list(tag_id))" due to data type mismatch: Parameter 1 requires the "ARRAY" of "ARRAY" type, however "collect_list(tag_id)" has the type "ARRAY<INT>".;
	'Aggregate [user_id#911], [user_id#911, array_distinct(array_sort(flatten(collect_list(tag_id#1078, 0, 0)), lambdafunction(if ((isnull(lambda 'left) AND isnull(lambda 'right))) 0 else if (isnull(lambda 'left)) 1 else if (isnull(lambda 'right)) -1 else if ((lambda 'left < lambda 'right)) -1 else if ((lambda 'left > lambda 'right)) 1 else 0, lambda 'left, lambda 'right, false), false)) AS tag_ids_array#1906]
	+- Union false, false
	   :- Project [user_id#911, 3 AS tag_id#1078]
	   :  +- Project [user_id#911]
	   :     +- Filter (trade_count_30d#912 > 10)
	   :        +- Project [user_id#911, last_login_date#913, trade_count_30d#912, last_trade_date#914]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_activity_summary
	   :              +- Relation spark_catalog.tag_system.user_activity_summary[user_id#911,trade_count_30d#912,last_login_date#913,last_trade_date#914,dt#915] parquet
	   :- Project [user_id#1863, 4 AS tag_id#1209]
	   :  +- Project [user_id#1863]
	   :     +- Filter (trade_count_30d#1864 < 5)
	   :        +- Project [user_id#1863, last_login_date#1865, trade_count_30d#1864, last_trade_date#1866]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_activity_summary
	   :              +- Relation spark_catalog.tag_system.user_activity_summary[user_id#1863,trade_count_30d#1864,last_login_date#1865,last_trade_date#1866,dt#1867] parquet
	   :- Project [user_id#1871, 20 AS tag_id#1340]
	   :  +- Project [user_id#1871]
	   :     +- Filter NOT (last_login_date#1873 = 2025-01-01)
	   :        +- Project [user_id#1871, last_login_date#1873, trade_count_30d#1872, last_trade_date#1874]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_activity_summary
	   :              +- Relation spark_catalog.tag_system.user_activity_summary[user_id#1871,trade_count_30d#1872,last_login_date#1873,last_trade_date#1874,dt#1875] parquet
	   :- Project [user_id#1879, 21 AS tag_id#1471]
	   :  +- Project [user_id#1879]
	   :     +- Filter (last_login_date#1881 > 2025-01-01)
	   :        +- Project [user_id#1879, last_login_date#1881, trade_count_30d#1880, last_trade_date#1882]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_activity_summary
	   :              +- Relation spark_catalog.tag_system.user_activity_summary[user_id#1879,trade_count_30d#1880,last_login_date#1881,last_trade_date#1882,dt#1883] parquet
	   :- Project [user_id#1887, 24 AS tag_id#1602]
	   :  +- Project [user_id#1887]
	   :     +- Filter NOT ((last_login_date#1889 >= 2023-01-01) AND (last_login_date#1889 <= 2023-12-31))
	   :        +- Project [user_id#1887, last_login_date#1889, trade_count_30d#1888, last_trade_date#1890]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_activity_summary
	   :              +- Relation spark_catalog.tag_system.user_activity_summary[user_id#1887,trade_count_30d#1888,last_login_date#1889,last_trade_date#1890,dt#1891] parquet
	   +- Project [user_id#1895, 25 AS tag_id#1733]
	      +- Project [user_id#1895]
	         +- Filter isnull(last_trade_date#1898)
	            +- Project [user_id#1895, last_login_date#1897, trade_count_30d#1896, last_trade_date#1898]
	               +- SubqueryAlias spark_catalog.tag_system.user_activity_summary
	                  +- Relation spark_catalog.tag_system.user_activity_summary[user_id#1895,trade_count_30d#1896,last_login_date#1897,last_trade_date#1898,dt#1899] parquet

	   📦 计算标签组 3/10: Group_1tags_1tables
	🚀 开始计算标签组: Group_1tags_1tables
[INFO] 2025-07-29 14:02:19.218 +0000 -  ->
	   📋 该组标签规则数: 1
	🔍 TagRuleParser初始化完成
	🔍 分析字段依赖关系...
	✅ 字段依赖分析完成: 1 个表
	   📊 tag_system.user_risk_profile: ['risk_score', 'user_id']
	📖 加载Hive表: tag_system.user_risk_profile
	✅ 表 tag_system.user_risk_profile 加载完成，字段: {'risk_score', 'user_id'}
[INFO] 2025-07-29 14:02:21.218 +0000 -  ->
	   🔗 JOIN完成，用户数: 1000
	   🎯 并行计算 1 个标签...
	      🏷️  计算标签 6...
	🔍 TagRuleParser初始化完成
	         🔍 标签 6 SQL条件: (`user_risk_profile`.`risk_score` <= 30)
[INFO] 2025-07-29 14:02:22.219 +0000 -  ->
	         ✅ 标签 6: 610 个用户
	   🔀 聚合用户标签...
	❌ 标签组计算失败: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "flatten(collect_list(tag_id))" due to data type mismatch: Parameter 1 requires the "ARRAY" of "ARRAY" type, however "collect_list(tag_id)" has the type "ARRAY<INT>".;
	'Aggregate [user_id#1922], [user_id#1922, array_distinct(array_sort(flatten(collect_list(tag_id#2011, 0, 0)), lambdafunction(if ((isnull(lambda 'left) AND isnull(lambda 'right))) 0 else if (isnull(lambda 'left)) 1 else if (isnull(lambda 'right)) -1 else if ((lambda 'left < lambda 'right)) -1 else if ((lambda 'left > lambda 'right)) 1 else 0, lambda 'left, lambda 'right, false), false)) AS tag_ids_array#2084]
	+- Project [user_id#1922, 6 AS tag_id#2011]
	   +- Project [user_id#1922]
	      +- Filter (risk_score#1923 <= 30)
	         +- Project [risk_score#1923, user_id#1922]
	            +- SubqueryAlias spark_catalog.tag_system.user_risk_profile
	               +- Relation spark_catalog.tag_system.user_risk_profile[user_id#1922,risk_score#1923,dt#1924] parquet

	Traceback (most recent call last):
	  File "/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py", line 66, in computeTags
	    userTagsDF = self._aggregateUserTags(tagResultsDF)
	                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py", line 155, in _aggregateUserTags
	    userTagsDF = tagResultsDF.groupBy("user_id").agg(
	                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/sql/group.py", line 186, in agg
	    jdf = self._jgd.agg(exprs[0]._jc, _to_seq(self.session._sc, [c._jc for c in exprs[1:]]))
	          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
	    raise converted from None
	pyspark.errors.exceptions.captured.AnalysisException: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "flatten(collect_list(tag_id))" due to data type mismatch: Parameter 1 requires the "ARRAY" of "ARRAY" type, however "collect_list(tag_id)" has the type "ARRAY<INT>".;
	'Aggregate [user_id#1922], [user_id#1922, array_distinct(array_sort(flatten(collect_list(tag_id#2011, 0, 0)), lambdafunction(if ((isnull(lambda 'left) AND isnull(lambda 'right))) 0 else if (isnull(lambda 'left)) 1 else if (isnull(lambda 'right)) -1 else if ((lambda 'left < lambda 'right)) -1 else if ((lambda 'left > lambda 'right)) 1 else 0, lambda 'left, lambda 'right, false), false)) AS tag_ids_array#2084]
	+- Project [user_id#1922, 6 AS tag_id#2011]
	   +- Project [user_id#1922]
	      +- Filter (risk_score#1923 <= 30)
	         +- Project [risk_score#1923, user_id#1922]
	            +- SubqueryAlias spark_catalog.tag_system.user_risk_profile
	               +- Relation spark_catalog.tag_system.user_risk_profile[user_id#1922,risk_score#1923,dt#1924] parquet

	   📦 计算标签组 4/10: Group_24tags_1tables
	🚀 开始计算标签组: Group_24tags_1tables
	   📋 该组标签规则数: 24
	🔍 TagRuleParser初始化完成
	🔍 分析字段依赖关系...
	✅ 字段依赖分析完成: 1 个表
	   📊 tag_system.user_basic_info: ['birthday', 'kyc_status', 'secondary_status', 'primary_status', 'first_name', 'middle_name', 'registration_date', 'is_banned', 'account_status', 'age', 'is_vip', 'user_id', 'user_level', 'phone_number', 'email']
	📖 加载Hive表: tag_system.user_basic_info
	✅ 表 tag_system.user_basic_info 加载完成，字段: {'birthday', 'kyc_status', 'secondary_status', 'primary_status', 'first_name', 'middle_name', 'registration_date', 'is_banned', 'account_status', 'age', 'is_vip', 'user_id', 'user_level', 'phone_number', 'email'}
[INFO] 2025-07-29 14:02:23.219 +0000 -  ->
	   🔗 JOIN完成，用户数: 1000
	   🎯 并行计算 24 个标签...
	      🏷️  计算标签 7...
	🔍 TagRuleParser初始化完成
	         🔍 标签 7 SQL条件: (`user_basic_info`.`age` BETWEEN 18 AND 65)
	         ✅ 标签 7: 768 个用户
	      🏷️  计算标签 8...
	🔍 TagRuleParser初始化完成
	         🔍 标签 8 SQL条件: (`user_basic_info`.`age` NOT BETWEEN 0 AND 17)
	         ✅ 标签 8: 1000 个用户
	      🏷️  计算标签 11...
	🔍 TagRuleParser初始化完成
	         🔍 标签 11 SQL条件: (`user_basic_info`.`user_level` = 'VIP3')
	         ✅ 标签 11: 162 个用户
	      🏷️  计算标签 12...
	🔍 TagRuleParser初始化完成
	         🔍 标签 12 SQL条件: (`user_basic_info`.`user_level` != 'VIP1')
	         ✅ 标签 12: 822 个用户
	      🏷️  计算标签 13...
	🔍 TagRuleParser初始化完成
	         🔍 标签 13 SQL条件: (`user_basic_info`.`phone_number` LIKE '%138%')
	         ✅ 标签 13: 345 个用户
	      🏷️  计算标签 14...
	🔍 TagRuleParser初始化完成
	         🔍 标签 14 SQL条件: (`user_basic_info`.`email` NOT LIKE '%temp%')
	         ✅ 标签 14: 763 个用户
	      🏷️  计算标签 15...
	🔍 TagRuleParser初始化完成
	         🔍 标签 15 SQL条件: (`user_basic_info`.`phone_number` LIKE '+86%')
[INFO] 2025-07-29 14:02:24.220 +0000 -  ->
	         ✅ 标签 15: 351 个用户
	      🏷️  计算标签 16...
	🔍 TagRuleParser初始化完成
	         🔍 标签 16 SQL条件: (`user_basic_info`.`email` LIKE '%gmail.com')
	         ✅ 标签 16: 241 个用户
	      🏷️  计算标签 17...
	🔍 TagRuleParser初始化完成
	         🔍 标签 17 SQL条件: (`user_basic_info`.`middle_name` IS NULL)
	         ✅ 标签 17: 502 个用户
	      🏷️  计算标签 18...
	🔍 TagRuleParser初始化完成
	         🔍 标签 18 SQL条件: (`user_basic_info`.`first_name` IS NOT NULL)
	         ✅ 标签 18: 876 个用户
	      🏷️  计算标签 19...
	🔍 TagRuleParser初始化完成
	         🔍 标签 19 SQL条件: (`user_basic_info`.`registration_date` = '2025-01-01')
	         ✅ 标签 19: 0 个用户
	      🏷️  计算标签 22...
	🔍 TagRuleParser初始化完成
	         🔍 标签 22 SQL条件: (`user_basic_info`.`registration_date` < '2024-12-31')
	         ✅ 标签 22: 1000 个用户
	      🏷️  计算标签 23...
	🔍 TagRuleParser初始化完成
	         🔍 标签 23 SQL条件: (`user_basic_info`.`registration_date` BETWEEN '2024-01-01' AND '2024-12-31')
	         ✅ 标签 23: 31 个用户
	      🏷️  计算标签 26...
	🔍 TagRuleParser初始化完成
	         🔍 标签 26 SQL条件: (`user_basic_info`.`birthday` IS NOT NULL)
	         ✅ 标签 26: 1000 个用户
	      🏷️  计算标签 27...
	🔍 TagRuleParser初始化完成
	         🔍 标签 27 SQL条件: (`user_basic_info`.`is_vip` = TRUE)
	         ✅ 标签 27: 482 个用户
	      🏷️  计算标签 28...
	🔍 TagRuleParser初始化完成
	         🔍 标签 28 SQL条件: (`user_basic_info`.`is_banned` = FALSE)
	         ✅ 标签 28: 749 个用户
	      🏷️  计算标签 29...
	🔍 TagRuleParser初始化完成
	         🔍 标签 29 SQL条件: (`user_basic_info`.`kyc_status` = 'verified')
[INFO] 2025-07-29 14:02:25.220 +0000 -  ->
	         ✅ 标签 29: 337 个用户
	      🏷️  计算标签 30...
	🔍 TagRuleParser初始化完成
	         🔍 标签 30 SQL条件: (`user_basic_info`.`account_status` != 'suspended')
	         ✅ 标签 30: 735 个用户
	      🏷️  计算标签 31...
	🔍 TagRuleParser初始化完成
	         🔍 标签 31 SQL条件: (`user_basic_info`.`user_level` IN ('VIP1','VIP2','VIP3'))
	         ✅ 标签 31: 497 个用户
	      🏷️  计算标签 32...
	🔍 TagRuleParser初始化完成
	         🔍 标签 32 SQL条件: (`user_basic_info`.`account_status` NOT IN ('suspended','banned'))
	         ✅ 标签 32: 483 个用户
	      🏷️  计算标签 33...
	🔍 TagRuleParser初始化完成
	         🔍 标签 33 SQL条件: (`user_basic_info`.`secondary_status` IS NULL)
	         ✅ 标签 33: 517 个用户
	      🏷️  计算标签 34...
	🔍 TagRuleParser初始化完成
	         🔍 标签 34 SQL条件: (`user_basic_info`.`primary_status` IS NOT NULL)
	         ✅ 标签 34: 767 个用户
	      🏷️  计算标签 42...
	🔍 TagRuleParser初始化完成
	         🔍 标签 42 SQL条件: (`user_basic_info`.`user_level` = 'VIP1')
	         ✅ 标签 42: 178 个用户
	      🏷️  计算标签 47...
	🔍 TagRuleParser初始化完成
	         🔍 标签 47 SQL条件: (`user_basic_info`.`email` LIKE '%gmail.com' OR `user_basic_info`.`email` LIKE '%yahoo.com') AND (`user_basic_info`.`phone_number` LIKE '+86%')
	         ✅ 标签 47: 176 个用户
	   🔀 聚合用户标签...
	❌ 标签组计算失败: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "flatten(collect_list(tag_id))" due to data type mismatch: Parameter 1 requires the "ARRAY" of "ARRAY" type, however "collect_list(tag_id)" has the type "ARRAY<INT>".;
	'Aggregate [user_id#2100], [user_id#2100, array_distinct(array_sort(flatten(collect_list(tag_id#2698, 0, 0)), lambdafunction(if ((isnull(lambda 'left) AND isnull(lambda 'right))) 0 else if (isnull(lambda 'left)) 1 else if (isnull(lambda 'right)) -1 else if ((lambda 'left < lambda 'right)) -1 else if ((lambda 'left > lambda 'right)) 1 else 0, lambda 'left, lambda 'right, false), false)) AS tag_ids_array#14224]
	+- Union false, false
	   :- Project [user_id#2100, 7 AS tag_id#2698]
	   :  +- Project [user_id#2100]
	   :     +- Filter ((age#2101 >= 18) AND (age#2101 <= 65))
	   :        +- Project [birthday#2104, kyc_status#2112, secondary_status#2115, primary_status#2114, first_name#2105, middle_name#2107, registration_date#2103, is_banned#2111, account_status#2113, age#2101, is_vip#2110, user_id#2100, user_level#2102, phone_number#2108, email#2109]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#2100,age#2101,user_level#2102,registration_date#2103,birthday#2104,first_name#2105,last_name#2106,middle_name#2107,phone_number#2108,email#2109,is_vip#2110,is_banned#2111,kyc_status#2112,account_status#2113,primary_status#2114,secondary_status#2115,dt#2116] parquet
	   :- Project [user_id#13761, 8 AS tag_id#3159]
	   :  +- Project [user_id#13761]
	   :     +- Filter NOT ((age#13762 >= 0) AND (age#13762 <= 17))
	   :        +- Project [birthday#13765, kyc_status#13773, secondary_status#13776, primary_status#13775, first_name#13766, middle_name#13768, registration_date#13764, is_banned#13772, account_status#13774, age#13762, is_vip#13771, user_id#13761, user_level#13763, phone_number#13769, email#13770]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13761,age#13762,user_level#13763,registration_date#13764,birthday#13765,first_name#13766,last_name#13767,middle_name#13768,phone_number#13769,email#13770,is_vip#13771,is_banned#13772,kyc_status#13773,account_status#13774,primary_status#13775,secondary_status#13776,dt#13777] parquet
	   :- Project [user_id#13781, 11 AS tag_id#3620]
	   :  +- Project [user_id#13781]
	   :     +- Filter (user_level#13783 = VIP3)
	   :        +- Project [birthday#13785, kyc_status#13793, secondary_status#13796, primary_status#13795, first_name#13786, middle_name#13788, registration_date#13784, is_banned#13792, account_status#13794, age#13782, is_vip#13791, user_id#13781, user_level#13783, phone_number#13789, email#13790]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13781,age#13782,user_level#13783,registration_date#13784,birthday#13785,first_name#13786,last_name#13787,middle_name#13788,phone_number#13789,email#13790,is_vip#13791,is_banned#13792,kyc_status#13793,account_status#13794,primary_status#13795,secondary_status#13796,dt#13797] parquet
	   :- Project [user_id#13801, 12 AS tag_id#4081]
	   :  +- Project [user_id#13801]
	   :     +- Filter NOT (user_level#13803 = VIP1)
	   :        +- Project [birthday#13805, kyc_status#13813, secondary_status#13816, primary_status#13815, first_name#13806, middle_name#13808, registration_date#13804, is_banned#13812, account_status#13814, age#13802, is_vip#13811, user_id#13801, user_level#13803, phone_number#13809, email#13810]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13801,age#13802,user_level#13803,registration_date#13804,birthday#13805,first_name#13806,last_name#13807,middle_name#13808,phone_number#13809,email#13810,is_vip#13811,is_banned#13812,kyc_status#13813,account_status#13814,primary_status#13815,secondary_status#13816,dt#13817] parquet
	   :- Project [user_id#13821, 13 AS tag_id#4542]
	   :  +- Project [user_id#13821]
	   :     +- Filter phone_number#13829 LIKE %138%
	   :        +- Project [birthday#13825, kyc_status#13833, secondary_status#13836, primary_status#13835, first_name#13826, middle_name#13828, registration_date#13824, is_banned#13832, account_status#13834, age#13822, is_vip#13831, user_id#13821, user_level#13823, phone_number#13829, email#13830]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13821,age#13822,user_level#13823,registration_date#13824,birthday#13825,first_name#13826,last_name#13827,middle_name#13828,phone_number#13829,email#13830,is_vip#13831,is_banned#13832,kyc_status#13833,account_status#13834,primary_status#13835,secondary_status#13836,dt#13837] parquet
	   :- Project [user_id#13841, 14 AS tag_id#5003]
	   :  +- Project [user_id#13841]
	   :     +- Filter NOT email#13850 LIKE %temp%
	   :        +- Project [birthday#13845, kyc_status#13853, secondary_status#13856, primary_status#13855, first_name#13846, middle_name#13848, registration_date#13844, is_banned#13852, account_status#13854, age#13842, is_vip#13851, user_id#13841, user_level#13843, phone_number#13849, email#13850]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13841,age#13842,user_level#13843,registration_date#13844,birthday#13845,first_name#13846,last_name#13847,middle_name#13848,phone_number#13849,email#13850,is_vip#13851,is_banned#13852,kyc_status#13853,account_status#13854,primary_status#13855,secondary_status#13856,dt#13857] parquet
	   :- Project [user_id#13861, 15 AS tag_id#5464]
	   :  +- Project [user_id#13861]
	   :     +- Filter phone_number#13869 LIKE +86%
	   :        +- Project [birthday#13865, kyc_status#13873, secondary_status#13876, primary_status#13875, first_name#13866, middle_name#13868, registration_date#13864, is_banned#13872, account_status#13874, age#13862, is_vip#13871, user_id#13861, user_level#13863, phone_number#13869, email#13870]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13861,age#13862,user_level#13863,registration_date#13864,birthday#13865,first_name#13866,last_name#13867,middle_name#13868,phone_number#13869,email#13870,is_vip#13871,is_banned#13872,kyc_status#13873,account_status#13874,primary_status#13875,secondary_status#13876,dt#13877] parquet
	   :- Project [user_id#13881, 16 AS tag_id#5925]
	   :  +- Project [user_id#13881]
	   :     +- Filter email#13890 LIKE %gmail.com
	   :        +- Project [birthday#13885, kyc_status#13893, secondary_status#13896, primary_status#13895, first_name#13886, middle_name#13888, registration_date#13884, is_banned#13892, account_status#13894, age#13882, is_vip#13891, user_id#13881, user_level#13883, phone_number#13889, email#13890]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13881,age#13882,user_level#13883,registration_date#13884,birthday#13885,first_name#13886,last_name#13887,middle_name#13888,phone_number#13889,email#13890,is_vip#13891,is_banned#13892,kyc_status#13893,account_status#13894,primary_status#13895,secondary_status#13896,dt#13897] parquet
	   :- Project [user_id#13901, 17 AS tag_id#6386]
	   :  +- Project [user_id#13901]
	   :     +- Filter isnull(middle_name#13908)
	   :        +- Project [birthday#13905, kyc_status#13913, secondary_status#13916, primary_status#13915, first_name#13906, middle_name#13908, registration_date#13904, is_banned#13912, account_status#13914, age#13902, is_vip#13911, user_id#13901, user_level#13903, phone_number#13909, email#13910]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13901,age#13902,user_level#13903,registration_date#13904,birthday#13905,first_name#13906,last_name#13907,middle_name#13908,phone_number#13909,email#13910,is_vip#13911,is_banned#13912,kyc_status#13913,account_status#13914,primary_status#13915,secondary_status#13916,dt#13917] parquet
	   :- Project [user_id#13921, 18 AS tag_id#6847]
	   :  +- Project [user_id#13921]
	   :     +- Filter isnotnull(first_name#13926)
	   :        +- Project [birthday#13925, kyc_status#13933, secondary_status#13936, primary_status#13935, first_name#13926, middle_name#13928, registration_date#13924, is_banned#13932, account_status#13934, age#13922, is_vip#13931, user_id#13921, user_level#13923, phone_number#13929, email#13930]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13921,age#13922,user_level#13923,registration_date#13924,birthday#13925,first_name#13926,last_name#13927,middle_name#13928,phone_number#13929,email#13930,is_vip#13931,is_banned#13932,kyc_status#13933,account_status#13934,primary_status#13935,secondary_status#13936,dt#13937] parquet
	   :- Project [user_id#13941, 19 AS tag_id#7308]
	   :  +- Project [user_id#13941]
	   :     +- Filter (registration_date#13944 = 2025-01-01)
	   :        +- Project [birthday#13945, kyc_status#13953, secondary_status#13956, primary_status#13955, first_name#13946, middle_name#13948, registration_date#13944, is_banned#13952, account_status#13954, age#13942, is_vip#13951, user_id#13941, user_level#13943, phone_number#13949, email#13950]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13941,age#13942,user_level#13943,registration_date#13944,birthday#13945,first_name#13946,last_name#13947,middle_name#13948,phone_number#13949,email#13950,is_vip#13951,is_banned#13952,kyc_status#13953,account_status#13954,primary_status#13955,secondary_status#13956,dt#13957] parquet
	   :- Project [user_id#13961, 22 AS tag_id#7769]
	   :  +- Project [user_id#13961]
	   :     +- Filter (registration_date#13964 < 2024-12-31)
	   :        +- Project [birthday#13965, kyc_status#13973, secondary_status#13976, primary_status#13975, first_name#13966, middle_name#13968, registration_date#13964, is_banned#13972, account_status#13974, age#13962, is_vip#13971, user_id#13961, user_level#13963, phone_number#13969, email#13970]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13961,age#13962,user_level#13963,registration_date#13964,birthday#13965,first_name#13966,last_name#13967,middle_name#13968,phone_number#13969,email#13970,is_vip#13971,is_banned#13972,kyc_status#13973,account_status#13974,primary_status#13975,secondary_status#13976,dt#13977] parquet
	   :- Project [user_id#13981, 23 AS tag_id#8230]
	   :  +- Project [user_id#13981]
	   :     +- Filter ((registration_date#13984 >= 2024-01-01) AND (registration_date#13984 <= 2024-12-31))
	   :        +- Project [birthday#13985, kyc_status#13993, secondary_status#13996, primary_status#13995, first_name#13986, middle_name#13988, registration_date#13984, is_banned#13992, account_status#13994, age#13982, is_vip#13991, user_id#13981, user_level#13983, phone_number#13989, email#13990]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13981,age#13982,user_level#13983,registration_date#13984,birthday#13985,first_name#13986,last_name#13987,middle_name#13988,phone_number#13989,email#13990,is_vip#13991,is_banned#13992,kyc_status#13993,account_status#13994,primary_status#13995,secondary_status#13996,dt#13997] parquet
	   :- Project [user_id#14001, 26 AS tag_id#8691]
	   :  +- Project [user_id#14001]
	   :     +- Filter isnotnull(birthday#14005)
	   :        +- Project [birthday#14005, kyc_status#14013, secondary_status#14016, primary_status#14015, first_name#14006, middle_name#14008, registration_date#14004, is_banned#14012, account_status#14014, age#14002, is_vip#14011, user_id#14001, user_level#14003, phone_number#14009, email#14010]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#14001,age#14002,user_level#14003,registration_date#14004,birthday#14005,first_name#14006,last_name#14007,middle_name#14008,phone_number#14009,email#14010,is_vip#14011,is_banned#14012,kyc_status#14013,account_status#14014,primary_status#14015,secondary_status#14016,dt#14017] parquet
	   :- Project [user_id#14021, 27 AS tag_id#9152]
	   :  +- Project [user_id#14021]
	   :     +- Filter (is_vip#14031 = true)
	   :        +- Project [birthday#14025, kyc_status#14033, secondary_status#14036, primary_status#14035, first_name#14026, middle_name#14028, registration_date#14024, is_banned#14032, account_status#14034, age#14022, is_vip#14031, user_id#14021, user_level#14023, phone_number#14029, email#14030]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#14021,age#14022,user_level#14023,registration_date#14024,birthday#14025,first_name#14026,last_name#14027,middle_name#14028,phone_number#14029,email#14030,is_vip#14031,is_banned#14032,kyc_status#14033,account_status#14034,primary_status#14035,secondary_status#14036,dt#14037] parquet
	   :- Project [user_id#14041, 28 AS tag_id#9613]
	   :  +- Project [user_id#14041]
	   :     +- Filter (is_banned#14052 = false)
	   :        +- Project [birthday#14045, kyc_status#14053, secondary_status#14056, primary_status#14055, first_name#14046, middle_name#14048, registration_date#14044, is_banned#14052, account_status#14054, age#14042, is_vip#14051, user_id#14041, user_level#14043, phone_number#14049, email#14050]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#14041,age#14042,user_level#14043,registration_date#14044,birthday#14045,first_name#14046,last_name#14047,middle_name#14048,phone_number#14049,email#14050,is_vip#14051,is_banned#14052,kyc_status#14053,account_status#14054,primary_status#14055,secondary_status#14056,dt#14057] parquet
	   :- Project [user_id#14061, 29 AS tag_id#10074]
	   :  +- Project [user_id#14061]
	   :     +- Filter (kyc_status#14073 = verified)
	   :        +- Project [birthday#14065, kyc_status#14073, secondary_status#14076, primary_status#14075, first_name#14066, middle_name#14068, registration_date#14064, is_banned#14072, account_status#14074, age#14062, is_vip#14071, user_id#14061, user_level#14063, phone_number#14069, email#14070]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#14061,age#14062,user_level#14063,registration_date#14064,birthday#14065,first_name#14066,last_name#14067,middle_name#14068,phone_number#14069,email#14070,is_vip#14071,is_banned#14072,kyc_status#14073,account_status#14074,primary_status#14075,secondary_status#14076,dt#14077] parquet
	   :- Project [user_id#14081, 30 AS tag_id#10535]
	   :  +- Project [user_id#14081]
	   :     +- Filter NOT (account_status#14094 = suspended)
	   :        +- Project [birthday#14085, kyc_status#14093, secondary_status#14096, primary_status#14095, first_name#14086, middle_name#14088, registration_date#14084, is_banned#14092, account_status#14094, age#14082, is_vip#14091, user_id#14081, user_level#14083, phone_number#14089, email#14090]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#14081,age#14082,user_level#14083,registration_date#14084,birthday#14085,first_name#14086,last_name#14087,middle_name#14088,phone_number#14089,email#14090,is_vip#14091,is_banned#14092,kyc_status#14093,account_status#14094,primary_status#14095,secondary_status#14096,dt#14097] parquet
	   :- Project [user_id#14101, 31 AS tag_id#10996]
	   :  +- Project [user_id#14101]
	   :     +- Filter user_level#14103 IN (VIP1,VIP2,VIP3)
	   :        +- Project [birthday#14105, kyc_status#14113, secondary_status#14116, primary_status#14115, first_name#14106, middle_name#14108, registration_date#14104, is_banned#14112, account_status#14114, age#14102, is_vip#14111, user_id#14101, user_level#14103, phone_number#14109, email#14110]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#14101,age#14102,user_level#14103,registration_date#14104,birthday#14105,first_name#14106,last_name#14107,middle_name#14108,phone_number#14109,email#14110,is_vip#14111,is_banned#14112,kyc_status#14113,account_status#14114,primary_status#14115,secondary_status#14116,dt#14117] parquet
	   :- Project [user_id#14121, 32 AS tag_id#11457]
	   :  +- Project [user_id#14121]
	   :     +- Filter NOT account_status#14134 IN (suspended,banned)
	   :        +- Project [birthday#14125, kyc_status#14133, secondary_status#14136, primary_status#14135, first_name#14126, middle_name#14128, registration_date#14124, is_banned#14132, account_status#14134, age#14122, is_vip#14131, user_id#14121, user_level#14123, phone_number#14129, email#14130]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#14121,age#14122,user_level#14123,registration_date#14124,birthday#14125,first_name#14126,last_name#14127,middle_name#14128,phone_number#14129,email#14130,is_vip#14131,is_banned#14132,kyc_status#14133,account_status#14134,primary_status#14135,secondary_status#14136,dt#14137] parquet
	   :- Project [user_id#14141, 33 AS tag_id#11918]
	   :  +- Project [user_id#14141]
	   :     +- Filter isnull(secondary_status#14156)
	   :        +- Project [birthday#14145, kyc_status#14153, secondary_status#14156, primary_status#14155, first_name#14146, middle_name#14148, registration_date#14144, is_banned#14152, account_status#14154, age#14142, is_vip#14151, user_id#14141, user_level#14143, phone_number#14149, email#14150]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#14141,age#14142,user_level#14143,registration_date#14144,birthday#14145,first_name#14146,last_name#14147,middle_name#14148,phone_number#14149,email#14150,is_vip#14151,is_banned#14152,kyc_status#14153,account_status#14154,primary_status#14155,secondary_status#14156,dt#14157] parquet
	   :- Project [user_id#14161, 34 AS tag_id#12379]
	   :  +- Project [user_id#14161]
	   :     +- Filter isnotnull(primary_status#14175)
	   :        +- Project [birthday#14165, kyc_status#14173, secondary_status#14176, primary_status#14175, first_name#14166, middle_name#14168, registration_date#14164, is_banned#14172, account_status#14174, age#14162, is_vip#14171, user_id#14161, user_level#14163, phone_number#14169, email#14170]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#14161,age#14162,user_level#14163,registration_date#14164,birthday#14165,first_name#14166,last_name#14167,middle_name#14168,phone_number#14169,email#14170,is_vip#14171,is_banned#14172,kyc_status#14173,account_status#14174,primary_status#14175,secondary_status#14176,dt#14177] parquet
	   :- Project [user_id#14181, 42 AS tag_id#12840]
	   :  +- Project [user_id#14181]
	   :     +- Filter (user_level#14183 = VIP1)
	   :        +- Project [birthday#14185, kyc_status#14193, secondary_status#14196, primary_status#14195, first_name#14186, middle_name#14188, registration_date#14184, is_banned#14192, account_status#14194, age#14182, is_vip#14191, user_id#14181, user_level#14183, phone_number#14189, email#14190]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#14181,age#14182,user_level#14183,registration_date#14184,birthday#14185,first_name#14186,last_name#14187,middle_name#14188,phone_number#14189,email#14190,is_vip#14191,is_banned#14192,kyc_status#14193,account_status#14194,primary_status#14195,secondary_status#14196,dt#14197] parquet
	   +- Project [user_id#14201, 47 AS tag_id#13301]
	      +- Project [user_id#14201]
	         +- Filter ((email#14210 LIKE %gmail.com OR email#14210 LIKE %yahoo.com) AND phone_number#14209 LIKE +86%)
	            +- Project [birthday#14205, kyc_status#14213, secondary_status#14216, primary_status#14215, first_name#14206, middle_name#14208, registration_date#14204, is_banned#14212, account_status#14214, age#14202, is_vip#14211, user_id#14201, user_level#14203, phone_number#14209, email#14210]
	               +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	                  +- Relation spark_catalog.tag_system.user_basic_info[user_id#14201,age#14202,user_level#14203,registration_date#14204,birthday#14205,first_name#14206,last_name#14207,middle_name#14208,phone_number#14209,email#14210,is_vip#14211,is_banned#14212,kyc_status#14213,account_status#14214,primary_status#14215,secondary_status#14216,dt#14217] parquet

	Traceback (most recent call last):
	  File "/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py", line 66, in computeTags
	    userTagsDF = self._aggregateUserTags(tagResultsDF)
	                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py", line 155, in _aggregateUserTags
	    userTagsDF = tagResultsDF.groupBy("user_id").agg(
	                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/sql/group.py", line 186, in agg
	    jdf = self._jgd.agg(exprs[0]._jc, _to_seq(self.session._sc, [c._jc for c in exprs[1:]]))
	          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
	    raise converted from None
	pyspark.errors.exceptions.captured.AnalysisException: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "flatten(collect_list(tag_id))" due to data type mismatch: Parameter 1 requires the "ARRAY" of "ARRAY" type, however "collect_list(tag_id)" has the type "ARRAY<INT>".;
	'Aggregate [user_id#2100], [user_id#2100, array_distinct(array_sort(flatten(collect_list(tag_id#2698, 0, 0)), lambdafunction(if ((isnull(lambda 'left) AND isnull(lambda 'right))) 0 else if (isnull(lambda 'left)) 1 else if (isnull(lambda 'right)) -1 else if ((lambda 'left < lambda 'right)) -1 else if ((lambda 'left > lambda 'right)) 1 else 0, lambda 'left, lambda 'right, false), false)) AS tag_ids_array#14224]
	+- Union false, false
	   :- Project [user_id#2100, 7 AS tag_id#2698]
	   :  +- Project [user_id#2100]
	   :     +- Filter ((age#2101 >= 18) AND (age#2101 <= 65))
	   :        +- Project [birthday#2104, kyc_status#2112, secondary_status#2115, primary_status#2114, first_name#2105, middle_name#2107, registration_date#2103, is_banned#2111, account_status#2113, age#2101, is_vip#2110, user_id#2100, user_level#2102, phone_number#2108, email#2109]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#2100,age#2101,user_level#2102,registration_date#2103,birthday#2104,first_name#2105,last_name#2106,middle_name#2107,phone_number#2108,email#2109,is_vip#2110,is_banned#2111,kyc_status#2112,account_status#2113,primary_status#2114,secondary_status#2115,dt#2116] parquet
	   :- Project [user_id#13761, 8 AS tag_id#3159]
	   :  +- Project [user_id#13761]
	   :     +- Filter NOT ((age#13762 >= 0) AND (age#13762 <= 17))
	   :        +- Project [birthday#13765, kyc_status#13773, secondary_status#13776, primary_status#13775, first_name#13766, middle_name#13768, registration_date#13764, is_banned#13772, account_status#13774, age#13762, is_vip#13771, user_id#13761, user_level#13763, phone_number#13769, email#13770]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13761,age#13762,user_level#13763,registration_date#13764,birthday#13765,first_name#13766,last_name#13767,middle_name#13768,phone_number#13769,email#13770,is_vip#13771,is_banned#13772,kyc_status#13773,account_status#13774,primary_status#13775,secondary_status#13776,dt#13777] parquet
	   :- Project [user_id#13781, 11 AS tag_id#3620]
	   :  +- Project [user_id#13781]
	   :     +- Filter (user_level#13783 = VIP3)
	   :        +- Project [birthday#13785, kyc_status#13793, secondary_status#13796, primary_status#13795, first_name#13786, middle_name#13788, registration_date#13784, is_banned#13792, account_status#13794, age#13782, is_vip#13791, user_id#13781, user_level#13783, phone_number#13789, email#13790]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13781,age#13782,user_level#13783,registration_date#13784,birthday#13785,first_name#13786,last_name#13787,middle_name#13788,phone_number#13789,email#13790,is_vip#13791,is_banned#13792,kyc_status#13793,account_status#13794,primary_status#13795,secondary_status#13796,dt#13797] parquet
	   :- Project [user_id#13801, 12 AS tag_id#4081]
	   :  +- Project [user_id#13801]
	   :     +- Filter NOT (user_level#13803 = VIP1)
	   :        +- Project [birthday#13805, kyc_status#13813, secondary_status#13816, primary_status#13815, first_name#13806, middle_name#13808, registration_date#13804, is_banned#13812, account_status#13814, age#13802, is_vip#13811, user_id#13801, user_level#13803, phone_number#13809, email#13810]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13801,age#13802,user_level#13803,registration_date#13804,birthday#13805,first_name#13806,last_name#13807,middle_name#13808,phone_number#13809,email#13810,is_vip#13811,is_banned#13812,kyc_status#13813,account_status#13814,primary_status#13815,secondary_status#13816,dt#13817] parquet
	   :- Project [user_id#13821, 13 AS tag_id#4542]
	   :  +- Project [user_id#13821]
	   :     +- Filter phone_number#13829 LIKE %138%
	   :        +- Project [birthday#13825, kyc_status#13833, secondary_status#13836, primary_status#13835, first_name#13826, middle_name#13828, registration_date#13824, is_banned#13832, account_status#13834, age#13822, is_vip#13831, user_id#13821, user_level#13823, phone_number#13829, email#13830]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13821,age#13822,user_level#13823,registration_date#13824,birthday#13825,first_name#13826,last_name#13827,middle_name#13828,phone_number#13829,email#13830,is_vip#13831,is_banned#13832,kyc_status#13833,account_status#13834,primary_status#13835,secondary_status#13836,dt#13837] parquet
	   :- Project [user_id#13841, 14 AS tag_id#5003]
	   :  +- Project [user_id#13841]
	   :     +- Filter NOT email#13850 LIKE %temp%
	   :        +- Project [birthday#13845, kyc_status#13853, secondary_status#13856, primary_status#13855, first_name#13846, middle_name#13848, registration_date#13844, is_banned#13852, account_status#13854, age#13842, is_vip#13851, user_id#13841, user_level#13843, phone_number#13849, email#13850]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13841,age#13842,user_level#13843,registration_date#13844,birthday#13845,first_name#13846,last_name#13847,middle_name#13848,phone_number#13849,email#13850,is_vip#13851,is_banned#13852,kyc_status#13853,account_status#13854,primary_status#13855,secondary_status#13856,dt#13857] parquet
	   :- Project [user_id#13861, 15 AS tag_id#5464]
	   :  +- Project [user_id#13861]
	   :     +- Filter phone_number#13869 LIKE +86%
	   :        +- Project [birthday#13865, kyc_status#13873, secondary_status#13876, primary_status#13875, first_name#13866, middle_name#13868, registration_date#13864, is_banned#13872, account_status#13874, age#13862, is_vip#13871, user_id#13861, user_level#13863, phone_number#13869, email#13870]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13861,age#13862,user_level#13863,registration_date#13864,birthday#13865,first_name#13866,last_name#13867,middle_name#13868,phone_number#13869,email#13870,is_vip#13871,is_banned#13872,kyc_status#13873,account_status#13874,primary_status#13875,secondary_status#13876,dt#13877] parquet
	   :- Project [user_id#13881, 16 AS tag_id#5925]
	   :  +- Project [user_id#13881]
	   :     +- Filter email#13890 LIKE %gmail.com
	   :        +- Project [birthday#13885, kyc_status#13893, secondary_status#13896, primary_status#13895, first_name#13886, middle_name#13888, registration_date#13884, is_banned#13892, account_status#13894, age#13882, is_vip#13891, user_id#13881, user_level#13883, phone_number#13889, email#13890]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13881,age#13882,user_level#13883,registration_date#13884,birthday#13885,first_name#13886,last_name#13887,middle_name#13888,phone_number#13889,email#13890,is_vip#13891,is_banned#13892,kyc_status#13893,account_status#13894,primary_status#13895,secondary_status#13896,dt#13897] parquet
	   :- Project [user_id#13901, 17 AS tag_id#6386]
	   :  +- Project [user_id#13901]
	   :     +- Filter isnull(middle_name#13908)
	   :        +- Project [birthday#13905, kyc_status#13913, secondary_status#13916, primary_status#13915, first_name#13906, middle_name#13908, registration_date#13904, is_banned#13912, account_status#13914, age#13902, is_vip#13911, user_id#13901, user_level#13903, phone_number#13909, email#13910]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13901,age#13902,user_level#13903,registration_date#13904,birthday#13905,first_name#13906,last_name#13907,middle_name#13908,phone_number#13909,email#13910,is_vip#13911,is_banned#13912,kyc_status#13913,account_status#13914,primary_status#13915,secondary_status#13916,dt#13917] parquet
	   :- Project [user_id#13921, 18 AS tag_id#6847]
	   :  +- Project [user_id#13921]
	   :     +- Filter isnotnull(first_name#13926)
	   :        +- Project [birthday#13925, kyc_status#13933, secondary_status#13936, primary_status#13935, first_name#13926, middle_name#13928, registration_date#13924, is_banned#13932, account_status#13934, age#13922, is_vip#13931, user_id#13921, user_level#13923, phone_number#13929, email#13930]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13921,age#13922,user_level#13923,registration_date#13924,birthday#13925,first_name#13926,last_name#13927,middle_name#13928,phone_number#13929,email#13930,is_vip#13931,is_banned#13932,kyc_status#13933,account_status#13934,primary_status#13935,secondary_status#13936,dt#13937] parquet
	   :- Project [user_id#13941, 19 AS tag_id#7308]
	   :  +- Project [user_id#13941]
	   :     +- Filter (registration_date#13944 = 2025-01-01)
	   :        +- Project [birthday#13945, kyc_status#13953, secondary_status#13956, primary_status#13955, first_name#13946, middle_name#13948, registration_date#13944, is_banned#13952, account_status#13954, age#13942, is_vip#13951, user_id#13941, user_level#13943, phone_number#13949, email#13950]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13941,age#13942,user_level#13943,registration_date#13944,birthday#13945,first_name#13946,last_name#13947,middle_name#13948,phone_number#13949,email#13950,is_vip#13951,is_banned#13952,kyc_status#13953,account_status#13954,primary_status#13955,secondary_status#13956,dt#13957] parquet
	   :- Project [user_id#13961, 22 AS tag_id#7769]
	   :  +- Project [user_id#13961]
	   :     +- Filter (registration_date#13964 < 2024-12-31)
	   :        +- Project [birthday#13965, kyc_status#13973, secondary_status#13976, primary_status#13975, first_name#13966, middle_name#13968, registration_date#13964, is_banned#13972, account_status#13974, age#13962, is_vip#13971, user_id#13961, user_level#13963, phone_number#13969, email#13970]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13961,age#13962,user_level#13963,registration_date#13964,birthday#13965,first_name#13966,last_name#13967,middle_name#13968,phone_number#13969,email#13970,is_vip#13971,is_banned#13972,kyc_status#13973,account_status#13974,primary_status#13975,secondary_status#13976,dt#13977] parquet
	   :- Project [user_id#13981, 23 AS tag_id#8230]
	   :  +- Project [user_id#13981]
	   :     +- Filter ((registration_date#13984 >= 2024-01-01) AND (registration_date#13984 <= 2024-12-31))
	   :        +- Project [birthday#13985, kyc_status#13993, secondary_status#13996, primary_status#13995, first_name#13986, middle_name#13988, registration_date#13984, is_banned#13992, account_status#13994, age#13982, is_vip#13991, user_id#13981, user_level#13983, phone_number#13989, email#13990]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#13981,age#13982,user_level#13983,registration_date#13984,birthday#13985,first_name#13986,last_name#13987,middle_name#13988,phone_number#13989,email#13990,is_vip#13991,is_banned#13992,kyc_status#13993,account_status#13994,primary_status#13995,secondary_status#13996,dt#13997] parquet
	   :- Project [user_id#14001, 26 AS tag_id#8691]
	   :  +- Project [user_id#14001]
	   :     +- Filter isnotnull(birthday#14005)
	   :        +- Project [birthday#14005, kyc_status#14013, secondary_status#14016, primary_status#14015, first_name#14006, middle_name#14008, registration_date#14004, is_banned#14012, account_status#14014, age#14002, is_vip#14011, user_id#14001, user_level#14003, phone_number#14009, email#14010]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#14001,age#14002,user_level#14003,registration_date#14004,birthday#14005,first_name#14006,last_name#14007,middle_name#14008,phone_number#14009,email#14010,is_vip#14011,is_banned#14012,kyc_status#14013,account_status#14014,primary_status#14015,secondary_status#14016,dt#14017] parquet
	   :- Project [user_id#14021, 27 AS tag_id#9152]
	   :  +- Project [user_id#14021]
	   :     +- Filter (is_vip#14031 = true)
	   :        +- Project [birthday#14025, kyc_status#14033, secondary_status#14036, primary_status#14035, first_name#14026, middle_name#14028, registration_date#14024, is_banned#14032, account_status#14034, age#14022, is_vip#14031, user_id#14021, user_level#14023, phone_number#14029, email#14030]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#14021,age#14022,user_level#14023,registration_date#14024,birthday#14025,first_name#14026,last_name#14027,middle_name#14028,phone_number#14029,email#14030,is_vip#14031,is_banned#14032,kyc_status#14033,account_status#14034,primary_status#14035,secondary_status#14036,dt#14037] parquet
	   :- Project [user_id#14041, 28 AS tag_id#9613]
	   :  +- Project [user_id#14041]
	   :     +- Filter (is_banned#14052 = false)
	   :        +- Project [birthday#14045, kyc_status#14053, secondary_status#14056, primary_status#14055, first_name#14046, middle_name#14048, registration_date#14044, is_banned#14052, account_status#14054, age#14042, is_vip#14051, user_id#14041, user_level#14043, phone_number#14049, email#14050]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#14041,age#14042,user_level#14043,registration_date#14044,birthday#14045,first_name#14046,last_name#14047,middle_name#14048,phone_number#14049,email#14050,is_vip#14051,is_banned#14052,kyc_status#14053,account_status#14054,primary_status#14055,secondary_status#14056,dt#14057] parquet
	   :- Project [user_id#14061, 29 AS tag_id#10074]
	   :  +- Project [user_id#14061]
	   :     +- Filter (kyc_status#14073 = verified)
	   :        +- Project [birthday#14065, kyc_status#14073, secondary_status#14076, primary_status#14075, first_name#14066, middle_name#14068, registration_date#14064, is_banned#14072, account_status#14074, age#14062, is_vip#14071, user_id#14061, user_level#14063, phone_number#14069, email#14070]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#14061,age#14062,user_level#14063,registration_date#14064,birthday#14065,first_name#14066,last_name#14067,middle_name#14068,phone_number#14069,email#14070,is_vip#14071,is_banned#14072,kyc_status#14073,account_status#14074,primary_status#14075,secondary_status#14076,dt#14077] parquet
	   :- Project [user_id#14081, 30 AS tag_id#10535]
	   :  +- Project [user_id#14081]
	   :     +- Filter NOT (account_status#14094 = suspended)
	   :        +- Project [birthday#14085, kyc_status#14093, secondary_status#14096, primary_status#14095, first_name#14086, middle_name#14088, registration_date#14084, is_banned#14092, account_status#14094, age#14082, is_vip#14091, user_id#14081, user_level#14083, phone_number#14089, email#14090]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#14081,age#14082,user_level#14083,registration_date#14084,birthday#14085,first_name#14086,last_name#14087,middle_name#14088,phone_number#14089,email#14090,is_vip#14091,is_banned#14092,kyc_status#14093,account_status#14094,primary_status#14095,secondary_status#14096,dt#14097] parquet
	   :- Project [user_id#14101, 31 AS tag_id#10996]
	   :  +- Project [user_id#14101]
	   :     +- Filter user_level#14103 IN (VIP1,VIP2,VIP3)
	   :        +- Project [birthday#14105, kyc_status#14113, secondary_status#14116, primary_status#14115, first_name#14106, middle_name#14108, registration_date#14104, is_banned#14112, account_status#14114, age#14102, is_vip#14111, user_id#14101, user_level#14103, phone_number#14109, email#14110]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#14101,age#14102,user_level#14103,registration_date#14104,birthday#14105,first_name#14106,last_name#14107,middle_name#14108,phone_number#14109,email#14110,is_vip#14111,is_banned#14112,kyc_status#14113,account_status#14114,primary_status#14115,secondary_status#14116,dt#14117] parquet
	   :- Project [user_id#14121, 32 AS tag_id#11457]
	   :  +- Project [user_id#14121]
	   :     +- Filter NOT account_status#14134 IN (suspended,banned)
	   :        +- Project [birthday#14125, kyc_status#14133, secondary_status#14136, primary_status#14135, first_name#14126, middle_name#14128, registration_date#14124, is_banned#14132, account_status#14134, age#14122, is_vip#14131, user_id#14121, user_level#14123, phone_number#14129, email#14130]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#14121,age#14122,user_level#14123,registration_date#14124,birthday#14125,first_name#14126,last_name#14127,middle_name#14128,phone_number#14129,email#14130,is_vip#14131,is_banned#14132,kyc_status#14133,account_status#14134,primary_status#14135,secondary_status#14136,dt#14137] parquet
	   :- Project [user_id#14141, 33 AS tag_id#11918]
	   :  +- Project [user_id#14141]
	   :     +- Filter isnull(secondary_status#14156)
	   :        +- Project [birthday#14145, kyc_status#14153, secondary_status#14156, primary_status#14155, first_name#14146, middle_name#14148, registration_date#14144, is_banned#14152, account_status#14154, age#14142, is_vip#14151, user_id#14141, user_level#14143, phone_number#14149, email#14150]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#14141,age#14142,user_level#14143,registration_date#14144,birthday#14145,first_name#14146,last_name#14147,middle_name#14148,phone_number#14149,email#14150,is_vip#14151,is_banned#14152,kyc_status#14153,account_status#14154,primary_status#14155,secondary_status#14156,dt#14157] parquet
	   :- Project [user_id#14161, 34 AS tag_id#12379]
	   :  +- Project [user_id#14161]
	   :     +- Filter isnotnull(primary_status#14175)
	   :        +- Project [birthday#14165, kyc_status#14173, secondary_status#14176, primary_status#14175, first_name#14166, middle_name#14168, registration_date#14164, is_banned#14172, account_status#14174, age#14162, is_vip#14171, user_id#14161, user_level#14163, phone_number#14169, email#14170]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#14161,age#14162,user_level#14163,registration_date#14164,birthday#14165,first_name#14166,last_name#14167,middle_name#14168,phone_number#14169,email#14170,is_vip#14171,is_banned#14172,kyc_status#14173,account_status#14174,primary_status#14175,secondary_status#14176,dt#14177] parquet
	   :- Project [user_id#14181, 42 AS tag_id#12840]
	   :  +- Project [user_id#14181]
	   :     +- Filter (user_level#14183 = VIP1)
	   :        +- Project [birthday#14185, kyc_status#14193, secondary_status#14196, primary_status#14195, first_name#14186, middle_name#14188, registration_date#14184, is_banned#14192, account_status#14194, age#14182, is_vip#14191, user_id#14181, user_level#14183, phone_number#14189, email#14190]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#14181,age#14182,user_level#14183,registration_date#14184,birthday#14185,first_name#14186,last_name#14187,middle_name#14188,phone_number#14189,email#14190,is_vip#14191,is_banned#14192,kyc_status#14193,account_status#14194,primary_status#14195,secondary_status#14196,dt#14197] parquet
	   +- Project [user_id#14201, 47 AS tag_id#13301]
	      +- Project [user_id#14201]
	         +- Filter ((email#14210 LIKE %gmail.com OR email#14210 LIKE %yahoo.com) AND phone_number#14209 LIKE +86%)
	            +- Project [birthday#14205, kyc_status#14213, secondary_status#14216, primary_status#14215, first_name#14206, middle_name#14208, registration_date#14204, is_banned#14212, account_status#14214, age#14202, is_vip#14211, user_id#14201, user_level#14203, phone_number#14209, email#14210]
	               +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	                  +- Relation spark_catalog.tag_system.user_basic_info[user_id#14201,age#14202,user_level#14203,registration_date#14204,birthday#14205,first_name#14206,last_name#14207,middle_name#14208,phone_number#14209,email#14210,is_vip#14211,is_banned#14212,kyc_status#14213,account_status#14214,primary_status#14215,secondary_status#14216,dt#14217] parquet

	   📦 计算标签组 5/10: Group_7tags_1tables
	🚀 开始计算标签组: Group_7tags_1tables
	   📋 该组标签规则数: 7
	🔍 TagRuleParser初始化完成
	🔍 分析字段依赖关系...
[INFO] 2025-07-29 14:02:26.224 +0000 -  ->
	✅ 字段依赖分析完成: 1 个表
	   📊 tag_system.user_preferences: ['active_products', 'required_services', 'interested_products', 'user_id', 'blacklisted_products', 'expired_products', 'owned_products', 'optional_services']
	📖 加载Hive表: tag_system.user_preferences
	✅ 表 tag_system.user_preferences 加载完成，字段: {'active_products', 'required_services', 'interested_products', 'user_id', 'blacklisted_products', 'expired_products', 'owned_products', 'optional_services'}
[INFO] 2025-07-29 14:02:27.225 +0000 -  ->
	   🔗 JOIN完成，用户数: 1000
	   🎯 并行计算 7 个标签...
	      🏷️  计算标签 35...
	🔍 TagRuleParser初始化完成
	         🔍 标签 35 SQL条件: ((array_contains(`user_preferences`.`interested_products`, 'stocks') OR array_contains(`user_preferences`.`interested_products`, 'bonds')))
	         ✅ 标签 35: 768 个用户
	      🏷️  计算标签 36...
	🔍 TagRuleParser初始化完成
	         🔍 标签 36 SQL条件: ((array_contains(`user_preferences`.`owned_products`, 'savings') AND array_contains(`user_preferences`.`owned_products`, 'checking')))
	         ✅ 标签 36: 123 个用户
	      🏷️  计算标签 37...
	🔍 TagRuleParser初始化完成
	         🔍 标签 37 SQL条件: ((NOT array_contains(`user_preferences`.`blacklisted_products`, 'forex')))
	         ✅ 标签 37: 803 个用户
	      🏷️  计算标签 38...
	🔍 TagRuleParser初始化完成
	         🔍 标签 38 SQL条件: ((array_contains(`user_preferences`.`active_products`, 'premium') OR array_contains(`user_preferences`.`active_products`, 'gold')))
	         ✅ 标签 38: 566 个用户
	      🏷️  计算标签 39...
	🔍 TagRuleParser初始化完成
	         🔍 标签 39 SQL条件: ((NOT array_contains(`user_preferences`.`expired_products`, 'premium') AND NOT array_contains(`user_preferences`.`expired_products`, 'platinum')))
	         ✅ 标签 39: 522 个用户
	      🏷️  计算标签 40...
	🔍 TagRuleParser初始化完成
	         🔍 标签 40 SQL条件: (`user_preferences`.`optional_services` IS NULL)
	         ✅ 标签 40: 0 个用户
	      🏷️  计算标签 41...
	🔍 TagRuleParser初始化完成
	         🔍 标签 41 SQL条件: (`user_preferences`.`required_services` IS NOT NULL)
[INFO] 2025-07-29 14:02:28.225 +0000 -  ->
	         ✅ 标签 41: 1000 个用户
	   🔀 聚合用户标签...
	❌ 标签组计算失败: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "flatten(collect_list(tag_id))" due to data type mismatch: Parameter 1 requires the "ARRAY" of "ARRAY" type, however "collect_list(tag_id)" has the type "ARRAY<INT>".;
	'Aggregate [user_id#14240], [user_id#14240, array_distinct(array_sort(flatten(collect_list(tag_id#14563, 0, 0)), lambdafunction(if ((isnull(lambda 'left) AND isnull(lambda 'right))) 0 else if (isnull(lambda 'left)) 1 else if (isnull(lambda 'right)) -1 else if ((lambda 'left < lambda 'right)) -1 else if ((lambda 'left > lambda 'right)) 1 else 0, lambda 'left, lambda 'right, false), false)) AS tag_ids_array#16394]
	+- Union false, false
	   :- Project [user_id#14240, 35 AS tag_id#14563]
	   :  +- Project [user_id#14240]
	   :     +- Filter (array_contains(interested_products#14241, stocks) OR array_contains(interested_products#14241, bonds))
	   :        +- Project [active_products#14244, required_services#14247, interested_products#14241, user_id#14240, blacklisted_products#14243, expired_products#14245, owned_products#14242, optional_services#14246]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_preferences
	   :              +- Relation spark_catalog.tag_system.user_preferences[user_id#14240,interested_products#14241,owned_products#14242,blacklisted_products#14243,active_products#14244,expired_products#14245,optional_services#14246,required_services#14247,dt#14248] parquet
	   :- Project [user_id#16319, 36 AS tag_id#14814]
	   :  +- Project [user_id#16319]
	   :     +- Filter (array_contains(owned_products#16321, savings) AND array_contains(owned_products#16321, checking))
	   :        +- Project [active_products#16323, required_services#16326, interested_products#16320, user_id#16319, blacklisted_products#16322, expired_products#16324, owned_products#16321, optional_services#16325]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_preferences
	   :              +- Relation spark_catalog.tag_system.user_preferences[user_id#16319,interested_products#16320,owned_products#16321,blacklisted_products#16322,active_products#16323,expired_products#16324,optional_services#16325,required_services#16326,dt#16327] parquet
	   :- Project [user_id#16331, 37 AS tag_id#15065]
	   :  +- Project [user_id#16331]
	   :     +- Filter NOT array_contains(blacklisted_products#16334, forex)
	   :        +- Project [active_products#16335, required_services#16338, interested_products#16332, user_id#16331, blacklisted_products#16334, expired_products#16336, owned_products#16333, optional_services#16337]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_preferences
	   :              +- Relation spark_catalog.tag_system.user_preferences[user_id#16331,interested_products#16332,owned_products#16333,blacklisted_products#16334,active_products#16335,expired_products#16336,optional_services#16337,required_services#16338,dt#16339] parquet
	   :- Project [user_id#16343, 38 AS tag_id#15316]
	   :  +- Project [user_id#16343]
	   :     +- Filter (array_contains(active_products#16347, premium) OR array_contains(active_products#16347, gold))
	   :        +- Project [active_products#16347, required_services#16350, interested_products#16344, user_id#16343, blacklisted_products#16346, expired_products#16348, owned_products#16345, optional_services#16349]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_preferences
	   :              +- Relation spark_catalog.tag_system.user_preferences[user_id#16343,interested_products#16344,owned_products#16345,blacklisted_products#16346,active_products#16347,expired_products#16348,optional_services#16349,required_services#16350,dt#16351] parquet
	   :- Project [user_id#16355, 39 AS tag_id#15567]
	   :  +- Project [user_id#16355]
	   :     +- Filter (NOT array_contains(expired_products#16360, premium) AND NOT array_contains(expired_products#16360, platinum))
	   :        +- Project [active_products#16359, required_services#16362, interested_products#16356, user_id#16355, blacklisted_products#16358, expired_products#16360, owned_products#16357, optional_services#16361]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_preferences
	   :              +- Relation spark_catalog.tag_system.user_preferences[user_id#16355,interested_products#16356,owned_products#16357,blacklisted_products#16358,active_products#16359,expired_products#16360,optional_services#16361,required_services#16362,dt#16363] parquet
	   :- Project [user_id#16367, 40 AS tag_id#15818]
	   :  +- Project [user_id#16367]
	   :     +- Filter isnull(optional_services#16373)
	   :        +- Project [active_products#16371, required_services#16374, interested_products#16368, user_id#16367, blacklisted_products#16370, expired_products#16372, owned_products#16369, optional_services#16373]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_preferences
	   :              +- Relation spark_catalog.tag_system.user_preferences[user_id#16367,interested_products#16368,owned_products#16369,blacklisted_products#16370,active_products#16371,expired_products#16372,optional_services#16373,required_services#16374,dt#16375] parquet
	   +- Project [user_id#16379, 41 AS tag_id#16069]
	      +- Project [user_id#16379]
	         +- Filter isnotnull(required_services#16386)
	            +- Project [active_products#16383, required_services#16386, interested_products#16380, user_id#16379, blacklisted_products#16382, expired_products#16384, owned_products#16381, optional_services#16385]
	               +- SubqueryAlias spark_catalog.tag_system.user_preferences
	                  +- Relation spark_catalog.tag_system.user_preferences[user_id#16379,interested_products#16380,owned_products#16381,blacklisted_products#16382,active_products#16383,expired_products#16384,optional_services#16385,required_services#16386,dt#16387] parquet

	Traceback (most recent call last):
	  File "/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py", line 66, in computeTags
	    userTagsDF = self._aggregateUserTags(tagResultsDF)
	                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py", line 155, in _aggregateUserTags
	    userTagsDF = tagResultsDF.groupBy("user_id").agg(
	                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/sql/group.py", line 186, in agg
	    jdf = self._jgd.agg(exprs[0]._jc, _to_seq(self.session._sc, [c._jc for c in exprs[1:]]))
	          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
	    raise converted from None
	pyspark.errors.exceptions.captured.AnalysisException: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "flatten(collect_list(tag_id))" due to data type mismatch: Parameter 1 requires the "ARRAY" of "ARRAY" type, however "collect_list(tag_id)" has the type "ARRAY<INT>".;
	'Aggregate [user_id#14240], [user_id#14240, array_distinct(array_sort(flatten(collect_list(tag_id#14563, 0, 0)), lambdafunction(if ((isnull(lambda 'left) AND isnull(lambda 'right))) 0 else if (isnull(lambda 'left)) 1 else if (isnull(lambda 'right)) -1 else if ((lambda 'left < lambda 'right)) -1 else if ((lambda 'left > lambda 'right)) 1 else 0, lambda 'left, lambda 'right, false), false)) AS tag_ids_array#16394]
	+- Union false, false
	   :- Project [user_id#14240, 35 AS tag_id#14563]
	   :  +- Project [user_id#14240]
	   :     +- Filter (array_contains(interested_products#14241, stocks) OR array_contains(interested_products#14241, bonds))
	   :        +- Project [active_products#14244, required_services#14247, interested_products#14241, user_id#14240, blacklisted_products#14243, expired_products#14245, owned_products#14242, optional_services#14246]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_preferences
	   :              +- Relation spark_catalog.tag_system.user_preferences[user_id#14240,interested_products#14241,owned_products#14242,blacklisted_products#14243,active_products#14244,expired_products#14245,optional_services#14246,required_services#14247,dt#14248] parquet
	   :- Project [user_id#16319, 36 AS tag_id#14814]
	   :  +- Project [user_id#16319]
	   :     +- Filter (array_contains(owned_products#16321, savings) AND array_contains(owned_products#16321, checking))
	   :        +- Project [active_products#16323, required_services#16326, interested_products#16320, user_id#16319, blacklisted_products#16322, expired_products#16324, owned_products#16321, optional_services#16325]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_preferences
	   :              +- Relation spark_catalog.tag_system.user_preferences[user_id#16319,interested_products#16320,owned_products#16321,blacklisted_products#16322,active_products#16323,expired_products#16324,optional_services#16325,required_services#16326,dt#16327] parquet
	   :- Project [user_id#16331, 37 AS tag_id#15065]
	   :  +- Project [user_id#16331]
	   :     +- Filter NOT array_contains(blacklisted_products#16334, forex)
	   :        +- Project [active_products#16335, required_services#16338, interested_products#16332, user_id#16331, blacklisted_products#16334, expired_products#16336, owned_products#16333, optional_services#16337]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_preferences
	   :              +- Relation spark_catalog.tag_system.user_preferences[user_id#16331,interested_products#16332,owned_products#16333,blacklisted_products#16334,active_products#16335,expired_products#16336,optional_services#16337,required_services#16338,dt#16339] parquet
	   :- Project [user_id#16343, 38 AS tag_id#15316]
	   :  +- Project [user_id#16343]
	   :     +- Filter (array_contains(active_products#16347, premium) OR array_contains(active_products#16347, gold))
	   :        +- Project [active_products#16347, required_services#16350, interested_products#16344, user_id#16343, blacklisted_products#16346, expired_products#16348, owned_products#16345, optional_services#16349]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_preferences
	   :              +- Relation spark_catalog.tag_system.user_preferences[user_id#16343,interested_products#16344,owned_products#16345,blacklisted_products#16346,active_products#16347,expired_products#16348,optional_services#16349,required_services#16350,dt#16351] parquet
	   :- Project [user_id#16355, 39 AS tag_id#15567]
	   :  +- Project [user_id#16355]
	   :     +- Filter (NOT array_contains(expired_products#16360, premium) AND NOT array_contains(expired_products#16360, platinum))
	   :        +- Project [active_products#16359, required_services#16362, interested_products#16356, user_id#16355, blacklisted_products#16358, expired_products#16360, owned_products#16357, optional_services#16361]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_preferences
	   :              +- Relation spark_catalog.tag_system.user_preferences[user_id#16355,interested_products#16356,owned_products#16357,blacklisted_products#16358,active_products#16359,expired_products#16360,optional_services#16361,required_services#16362,dt#16363] parquet
	   :- Project [user_id#16367, 40 AS tag_id#15818]
	   :  +- Project [user_id#16367]
	   :     +- Filter isnull(optional_services#16373)
	   :        +- Project [active_products#16371, required_services#16374, interested_products#16368, user_id#16367, blacklisted_products#16370, expired_products#16372, owned_products#16369, optional_services#16373]
	   :           +- SubqueryAlias spark_catalog.tag_system.user_preferences
	   :              +- Relation spark_catalog.tag_system.user_preferences[user_id#16367,interested_products#16368,owned_products#16369,blacklisted_products#16370,active_products#16371,expired_products#16372,optional_services#16373,required_services#16374,dt#16375] parquet
	   +- Project [user_id#16379, 41 AS tag_id#16069]
	      +- Project [user_id#16379]
	         +- Filter isnotnull(required_services#16386)
	            +- Project [active_products#16383, required_services#16386, interested_products#16380, user_id#16379, blacklisted_products#16382, expired_products#16384, owned_products#16381, optional_services#16385]
	               +- SubqueryAlias spark_catalog.tag_system.user_preferences
	                  +- Relation spark_catalog.tag_system.user_preferences[user_id#16379,interested_products#16380,owned_products#16381,blacklisted_products#16382,active_products#16383,expired_products#16384,optional_services#16385,required_services#16386,dt#16387] parquet

	   📦 计算标签组 6/10: Group_2tags_2tables
	🚀 开始计算标签组: Group_2tags_2tables
	   📋 该组标签规则数: 2
	🔍 TagRuleParser初始化完成
	🔍 分析字段依赖关系...
	✅ 字段依赖分析完成: 2 个表
	   📊 tag_system.user_asset_summary: ['total_asset_value', 'cash_balance', 'user_id']
	   📊 tag_system.user_activity_summary: ['last_login_date', 'trade_count_30d', 'user_id']
	🔗 开始JOIN表: ['tag_system.user_activity_summary', 'tag_system.user_asset_summary']
	📖 加载Hive表: tag_system.user_activity_summary
	✅ 表 tag_system.user_activity_summary 加载完成，字段: {'last_login_date', 'trade_count_30d', 'user_id'}
	📖 加载Hive表: tag_system.user_asset_summary
	✅ 表 tag_system.user_asset_summary 加载完成，字段: {'total_asset_value', 'cash_balance', 'user_id'}
	📚 批量加载完成: 2 个表
	   📋 基础表: tag_system.user_activity_summary
	   🔗 LEFT JOIN: tag_system.user_asset_summary
	✅ JOIN完成，涉及 2 个表
[INFO] 2025-07-29 14:02:30.227 +0000 -  ->
	   🔗 JOIN完成，用户数: 1000
	   🎯 并行计算 2 个标签...
	      🏷️  计算标签 43...
	🔍 TagRuleParser初始化完成
	         🔍 标签 43 SQL条件: (`tag_system.user_asset_summary`.`total_asset_value` < 1000 AND `tag_system.user_activity_summary`.`trade_count_30d` = 0)
	         ✅ 标签 43: 31 个用户
	      🏷️  计算标签 45...
	🔍 TagRuleParser初始化完成
	         🔍 标签 45 SQL条件: (`tag_system.user_activity_summary`.`last_login_date` BETWEEN '2025-01-01' AND '2025-07-26' AND `tag_system.user_activity_summary`.`trade_count_30d` > 5) OR (`tag_system.user_asset_summary`.`cash_balance` >= 50000)
	         ✅ 标签 45: 657 个用户
	   🔀 聚合用户标签...
	❌ 标签组计算失败: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "flatten(collect_list(tag_id))" due to data type mismatch: Parameter 1 requires the "ARRAY" of "ARRAY" type, however "collect_list(tag_id)" has the type "ARRAY<INT>".;
	'Aggregate [user_id#911], [user_id#911, array_distinct(array_sort(flatten(collect_list(tag_id#16654, 0, 0)), lambdafunction(if ((isnull(lambda 'left) AND isnull(lambda 'right))) 0 else if (isnull(lambda 'left)) 1 else if (isnull(lambda 'right)) -1 else if ((lambda 'left < lambda 'right)) -1 else if ((lambda 'left > lambda 'right)) 1 else 0, lambda 'left, lambda 'right, false), false)) AS tag_ids_array#17052]
	+- Union false, false
	   :- Project [user_id#911, 43 AS tag_id#16654]
	   :  +- Project [user_id#911]
	   :     +- Filter ((total_asset_value#40 < cast(1000 as double)) AND (trade_count_30d#912 = 0))
	   :        +- Project [user_id#911, last_login_date#913, trade_count_30d#912, total_asset_value#40, cash_balance#41]
	   :           +- Join LeftOuter, (user_id#911 = user_id#39)
	   :              :- SubqueryAlias `tag_system.user_activity_summary`
	   :              :  +- Project [last_login_date#913, trade_count_30d#912, user_id#911]
	   :              :     +- SubqueryAlias spark_catalog.tag_system.user_activity_summary
	   :              :        +- Relation spark_catalog.tag_system.user_activity_summary[user_id#911,trade_count_30d#912,last_login_date#913,last_trade_date#914,dt#915] parquet
	   :              +- SubqueryAlias `tag_system.user_asset_summary`
	   :                 +- Project [total_asset_value#40, cash_balance#41, user_id#39]
	   :                    +- SubqueryAlias spark_catalog.tag_system.user_asset_summary
	   :                       +- Relation spark_catalog.tag_system.user_asset_summary[user_id#39,total_asset_value#40,cash_balance#41,debt_amount#42,dt#43] parquet
	   +- Project [user_id#17035, 45 AS tag_id#16845]
	      +- Project [user_id#17035]
	         +- Filter ((((last_login_date#17037 >= 2025-01-01) AND (last_login_date#17037 <= 2025-07-26)) AND (trade_count_30d#17036 > 5)) OR (cash_balance#17042 >= cast(50000 as double)))
	            +- Project [user_id#17035, last_login_date#17037, trade_count_30d#17036, total_asset_value#17041, cash_balance#17042]
	               +- Join LeftOuter, (user_id#17035 = user_id#17040)
	                  :- SubqueryAlias `tag_system.user_activity_summary`
	                  :  +- Project [last_login_date#17037, trade_count_30d#17036, user_id#17035]
	                  :     +- SubqueryAlias spark_catalog.tag_system.user_activity_summary
	                  :        +- Relation spark_catalog.tag_system.user_activity_summary[user_id#17035,trade_count_30d#17036,last_login_date#17037,last_trade_date#17038,dt#17039] parquet
	                  +- SubqueryAlias `tag_system.user_asset_summary`
	                     +- Project [total_asset_value#17041, cash_balance#17042, user_id#17040]
	                        +- SubqueryAlias spark_catalog.tag_system.user_asset_summary
	                           +- Relation spark_catalog.tag_system.user_asset_summary[user_id#17040,total_asset_value#17041,cash_balance#17042,debt_amount#17043,dt#17044] parquet

	Traceback (most recent call last):
	  File "/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py", line 66, in computeTags
	    userTagsDF = self._aggregateUserTags(tagResultsDF)
	                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py", line 155, in _aggregateUserTags
	    userTagsDF = tagResultsDF.groupBy("user_id").agg(
	                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/sql/group.py", line 186, in agg
	    jdf = self._jgd.agg(exprs[0]._jc, _to_seq(self.session._sc, [c._jc for c in exprs[1:]]))
	          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
	    raise converted from None
	pyspark.errors.exceptions.captured.AnalysisException: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "flatten(collect_list(tag_id))" due to data type mismatch: Parameter 1 requires the "ARRAY" of "ARRAY" type, however "collect_list(tag_id)" has the type "ARRAY<INT>".;
	'Aggregate [user_id#911], [user_id#911, array_distinct(array_sort(flatten(collect_list(tag_id#16654, 0, 0)), lambdafunction(if ((isnull(lambda 'left) AND isnull(lambda 'right))) 0 else if (isnull(lambda 'left)) 1 else if (isnull(lambda 'right)) -1 else if ((lambda 'left < lambda 'right)) -1 else if ((lambda 'left > lambda 'right)) 1 else 0, lambda 'left, lambda 'right, false), false)) AS tag_ids_array#17052]
	+- Union false, false
	   :- Project [user_id#911, 43 AS tag_id#16654]
	   :  +- Project [user_id#911]
	   :     +- Filter ((total_asset_value#40 < cast(1000 as double)) AND (trade_count_30d#912 = 0))
	   :        +- Project [user_id#911, last_login_date#913, trade_count_30d#912, total_asset_value#40, cash_balance#41]
	   :           +- Join LeftOuter, (user_id#911 = user_id#39)
	   :              :- SubqueryAlias `tag_system.user_activity_summary`
	   :              :  +- Project [last_login_date#913, trade_count_30d#912, user_id#911]
	   :              :     +- SubqueryAlias spark_catalog.tag_system.user_activity_summary
	   :              :        +- Relation spark_catalog.tag_system.user_activity_summary[user_id#911,trade_count_30d#912,last_login_date#913,last_trade_date#914,dt#915] parquet
	   :              +- SubqueryAlias `tag_system.user_asset_summary`
	   :                 +- Project [total_asset_value#40, cash_balance#41, user_id#39]
	   :                    +- SubqueryAlias spark_catalog.tag_system.user_asset_summary
	   :                       +- Relation spark_catalog.tag_system.user_asset_summary[user_id#39,total_asset_value#40,cash_balance#41,debt_amount#42,dt#43] parquet
	   +- Project [user_id#17035, 45 AS tag_id#16845]
	      +- Project [user_id#17035]
	         +- Filter ((((last_login_date#17037 >= 2025-01-01) AND (last_login_date#17037 <= 2025-07-26)) AND (trade_count_30d#17036 > 5)) OR (cash_balance#17042 >= cast(50000 as double)))
	            +- Project [user_id#17035, last_login_date#17037, trade_count_30d#17036, total_asset_value#17041, cash_balance#17042]
	               +- Join LeftOuter, (user_id#17035 = user_id#17040)
	                  :- SubqueryAlias `tag_system.user_activity_summary`
	                  :  +- Project [last_login_date#17037, trade_count_30d#17036, user_id#17035]
	                  :     +- SubqueryAlias spark_catalog.tag_system.user_activity_summary
	                  :        +- Relation spark_catalog.tag_system.user_activity_summary[user_id#17035,trade_count_30d#17036,last_login_date#17037,last_trade_date#17038,dt#17039] parquet
	                  +- SubqueryAlias `tag_system.user_asset_summary`
	                     +- Project [total_asset_value#17041, cash_balance#17042, user_id#17040]
	                        +- SubqueryAlias spark_catalog.tag_system.user_asset_summary
	                           +- Relation spark_catalog.tag_system.user_asset_summary[user_id#17040,total_asset_value#17041,cash_balance#17042,debt_amount#17043,dt#17044] parquet

	   📦 计算标签组 7/10: Group_1tags_2tables
	🚀 开始计算标签组: Group_1tags_2tables
[INFO] 2025-07-29 14:02:31.228 +0000 -  ->
	   📋 该组标签规则数: 1
	🔍 TagRuleParser初始化完成
	🔍 分析字段依赖关系...
	✅ 字段依赖分析完成: 2 个表
	   📊 tag_system.user_basic_info: ['user_level', 'is_banned', 'kyc_status', 'user_id']
	   📊 tag_system.user_asset_summary: ['total_asset_value', 'user_id']
	🔗 开始JOIN表: ['tag_system.user_asset_summary', 'tag_system.user_basic_info']
	📖 加载Hive表: tag_system.user_asset_summary
	✅ 表 tag_system.user_asset_summary 加载完成，字段: {'total_asset_value', 'user_id'}
	📖 加载Hive表: tag_system.user_basic_info
	✅ 表 tag_system.user_basic_info 加载完成，字段: {'user_level', 'is_banned', 'kyc_status', 'user_id'}
	📚 批量加载完成: 2 个表
	   📋 基础表: tag_system.user_asset_summary
	   🔗 LEFT JOIN: tag_system.user_basic_info
	✅ JOIN完成，涉及 2 个表
	   🔗 JOIN完成，用户数: 1000
	   🎯 并行计算 1 个标签...
[INFO] 2025-07-29 14:02:32.228 +0000 -  ->
	      🏷️  计算标签 44...
	🔍 TagRuleParser初始化完成
	         🔍 标签 44 SQL条件: (`tag_system.user_basic_info`.`user_level` IN ('VIP2','VIP3') OR `tag_system.user_asset_summary`.`total_asset_value` >= 100000) AND (`tag_system.user_basic_info`.`kyc_status` = 'verified' AND `tag_system.user_basic_info`.`is_banned` = FALSE)
	         ✅ 标签 44: 183 个用户
	   🔀 聚合用户标签...
	❌ 标签组计算失败: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "flatten(collect_list(tag_id))" due to data type mismatch: Parameter 1 requires the "ARRAY" of "ARRAY" type, however "collect_list(tag_id)" has the type "ARRAY<INT>".;
	'Aggregate [user_id#39], [user_id#39, array_distinct(array_sort(flatten(collect_list(tag_id#17324, 0, 0)), lambdafunction(if ((isnull(lambda 'left) AND isnull(lambda 'right))) 0 else if (isnull(lambda 'left)) 1 else if (isnull(lambda 'right)) -1 else if ((lambda 'left < lambda 'right)) -1 else if ((lambda 'left > lambda 'right)) 1 else 0, lambda 'left, lambda 'right, false), false)) AS tag_ids_array#17517]
	+- Project [user_id#39, 44 AS tag_id#17324]
	   +- Project [user_id#39]
	      +- Filter ((user_level#2102 IN (VIP2,VIP3) OR (total_asset_value#40 >= cast(100000 as double))) AND ((kyc_status#2112 = verified) AND (is_banned#2111 = false)))
	         +- Project [user_id#39, total_asset_value#40, user_level#2102, is_banned#2111, kyc_status#2112]
	            +- Join LeftOuter, (user_id#39 = user_id#2100)
	               :- SubqueryAlias `tag_system.user_asset_summary`
	               :  +- Project [total_asset_value#40, user_id#39]
	               :     +- SubqueryAlias spark_catalog.tag_system.user_asset_summary
	               :        +- Relation spark_catalog.tag_system.user_asset_summary[user_id#39,total_asset_value#40,cash_balance#41,debt_amount#42,dt#43] parquet
	               +- SubqueryAlias `tag_system.user_basic_info`
	                  +- Project [user_level#2102, is_banned#2111, kyc_status#2112, user_id#2100]
	                     +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	                        +- Relation spark_catalog.tag_system.user_basic_info[user_id#2100,age#2101,user_level#2102,registration_date#2103,birthday#2104,first_name#2105,last_name#2106,middle_name#2107,phone_number#2108,email#2109,is_vip#2110,is_banned#2111,kyc_status#2112,account_status#2113,primary_status#2114,secondary_status#2115,dt#2116] parquet

	Traceback (most recent call last):
	  File "/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py", line 66, in computeTags
	    userTagsDF = self._aggregateUserTags(tagResultsDF)
	                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py", line 155, in _aggregateUserTags
	    userTagsDF = tagResultsDF.groupBy("user_id").agg(
	                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/sql/group.py", line 186, in agg
	    jdf = self._jgd.agg(exprs[0]._jc, _to_seq(self.session._sc, [c._jc for c in exprs[1:]]))
	          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
	    raise converted from None
	pyspark.errors.exceptions.captured.AnalysisException: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "flatten(collect_list(tag_id))" due to data type mismatch: Parameter 1 requires the "ARRAY" of "ARRAY" type, however "collect_list(tag_id)" has the type "ARRAY<INT>".;
	'Aggregate [user_id#39], [user_id#39, array_distinct(array_sort(flatten(collect_list(tag_id#17324, 0, 0)), lambdafunction(if ((isnull(lambda 'left) AND isnull(lambda 'right))) 0 else if (isnull(lambda 'left)) 1 else if (isnull(lambda 'right)) -1 else if ((lambda 'left < lambda 'right)) -1 else if ((lambda 'left > lambda 'right)) 1 else 0, lambda 'left, lambda 'right, false), false)) AS tag_ids_array#17517]
	+- Project [user_id#39, 44 AS tag_id#17324]
	   +- Project [user_id#39]
	      +- Filter ((user_level#2102 IN (VIP2,VIP3) OR (total_asset_value#40 >= cast(100000 as double))) AND ((kyc_status#2112 = verified) AND (is_banned#2111 = false)))
	         +- Project [user_id#39, total_asset_value#40, user_level#2102, is_banned#2111, kyc_status#2112]
	            +- Join LeftOuter, (user_id#39 = user_id#2100)
	               :- SubqueryAlias `tag_system.user_asset_summary`
	               :  +- Project [total_asset_value#40, user_id#39]
	               :     +- SubqueryAlias spark_catalog.tag_system.user_asset_summary
	               :        +- Relation spark_catalog.tag_system.user_asset_summary[user_id#39,total_asset_value#40,cash_balance#41,debt_amount#42,dt#43] parquet
	               +- SubqueryAlias `tag_system.user_basic_info`
	                  +- Project [user_level#2102, is_banned#2111, kyc_status#2112, user_id#2100]
	                     +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	                        +- Relation spark_catalog.tag_system.user_basic_info[user_id#2100,age#2101,user_level#2102,registration_date#2103,birthday#2104,first_name#2105,last_name#2106,middle_name#2107,phone_number#2108,email#2109,is_vip#2110,is_banned#2111,kyc_status#2112,account_status#2113,primary_status#2114,secondary_status#2115,dt#2116] parquet

	   📦 计算标签组 8/10: Group_1tags_2tables
	🚀 开始计算标签组: Group_1tags_2tables
	   📋 该组标签规则数: 1
	🔍 TagRuleParser初始化完成
	🔍 分析字段依赖关系...
	✅ 字段依赖分析完成: 2 个表
	   📊 tag_system.user_basic_info: ['account_status', 'user_id']
	   📊 tag_system.user_activity_summary: ['last_login_date', 'user_id']
	🔗 开始JOIN表: ['tag_system.user_activity_summary', 'tag_system.user_basic_info']
	📖 加载Hive表: tag_system.user_activity_summary
	✅ 表 tag_system.user_activity_summary 加载完成，字段: {'last_login_date', 'user_id'}
	📖 加载Hive表: tag_system.user_basic_info
	✅ 表 tag_system.user_basic_info 加载完成，字段: {'account_status', 'user_id'}
	📚 批量加载完成: 2 个表
	   📋 基础表: tag_system.user_activity_summary
	   🔗 LEFT JOIN: tag_system.user_basic_info
	✅ JOIN完成，涉及 2 个表
[INFO] 2025-07-29 14:02:35.229 +0000 -  ->
	   🔗 JOIN完成，用户数: 1000
	   🎯 并行计算 1 个标签...
	      🏷️  计算标签 46...
	🔍 TagRuleParser初始化完成
	         🔍 标签 46 SQL条件: (`tag_system.user_basic_info`.`account_status` IN ('suspended','banned') OR `tag_system.user_activity_summary`.`last_login_date` NOT BETWEEN '2024-01-01' AND '2025-07-26')
	         ✅ 标签 46: 517 个用户
	   🔀 聚合用户标签...
	❌ 标签组计算失败: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "flatten(collect_list(tag_id))" due to data type mismatch: Parameter 1 requires the "ARRAY" of "ARRAY" type, however "collect_list(tag_id)" has the type "ARRAY<INT>".;
	'Aggregate [user_id#911], [user_id#911, array_distinct(array_sort(flatten(collect_list(tag_id#17713, 0, 0)), lambdafunction(if ((isnull(lambda 'left) AND isnull(lambda 'right))) 0 else if (isnull(lambda 'left)) 1 else if (isnull(lambda 'right)) -1 else if ((lambda 'left < lambda 'right)) -1 else if ((lambda 'left > lambda 'right)) 1 else 0, lambda 'left, lambda 'right, false), false)) AS tag_ids_array#17846]
	+- Project [user_id#911, 46 AS tag_id#17713]
	   +- Project [user_id#911]
	      +- Filter (account_status#2113 IN (suspended,banned) OR NOT ((last_login_date#913 >= 2024-01-01) AND (last_login_date#913 <= 2025-07-26)))
	         +- Project [user_id#911, last_login_date#913, account_status#2113]
	            +- Join LeftOuter, (user_id#911 = user_id#2100)
	               :- SubqueryAlias `tag_system.user_activity_summary`
	               :  +- Project [last_login_date#913, user_id#911]
	               :     +- SubqueryAlias spark_catalog.tag_system.user_activity_summary
	               :        +- Relation spark_catalog.tag_system.user_activity_summary[user_id#911,trade_count_30d#912,last_login_date#913,last_trade_date#914,dt#915] parquet
	               +- SubqueryAlias `tag_system.user_basic_info`
	                  +- Project [account_status#2113, user_id#2100]
	                     +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	                        +- Relation spark_catalog.tag_system.user_basic_info[user_id#2100,age#2101,user_level#2102,registration_date#2103,birthday#2104,first_name#2105,last_name#2106,middle_name#2107,phone_number#2108,email#2109,is_vip#2110,is_banned#2111,kyc_status#2112,account_status#2113,primary_status#2114,secondary_status#2115,dt#2116] parquet

	Traceback (most recent call last):
	  File "/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py", line 66, in computeTags
	    userTagsDF = self._aggregateUserTags(tagResultsDF)
	                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py", line 155, in _aggregateUserTags
	    userTagsDF = tagResultsDF.groupBy("user_id").agg(
	                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/sql/group.py", line 186, in agg
	    jdf = self._jgd.agg(exprs[0]._jc, _to_seq(self.session._sc, [c._jc for c in exprs[1:]]))
	          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
	    raise converted from None
	pyspark.errors.exceptions.captured.AnalysisException: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "flatten(collect_list(tag_id))" due to data type mismatch: Parameter 1 requires the "ARRAY" of "ARRAY" type, however "collect_list(tag_id)" has the type "ARRAY<INT>".;
	'Aggregate [user_id#911], [user_id#911, array_distinct(array_sort(flatten(collect_list(tag_id#17713, 0, 0)), lambdafunction(if ((isnull(lambda 'left) AND isnull(lambda 'right))) 0 else if (isnull(lambda 'left)) 1 else if (isnull(lambda 'right)) -1 else if ((lambda 'left < lambda 'right)) -1 else if ((lambda 'left > lambda 'right)) 1 else 0, lambda 'left, lambda 'right, false), false)) AS tag_ids_array#17846]
	+- Project [user_id#911, 46 AS tag_id#17713]
	   +- Project [user_id#911]
	      +- Filter (account_status#2113 IN (suspended,banned) OR NOT ((last_login_date#913 >= 2024-01-01) AND (last_login_date#913 <= 2025-07-26)))
	         +- Project [user_id#911, last_login_date#913, account_status#2113]
	            +- Join LeftOuter, (user_id#911 = user_id#2100)
	               :- SubqueryAlias `tag_system.user_activity_summary`
	               :  +- Project [last_login_date#913, user_id#911]
	               :     +- SubqueryAlias spark_catalog.tag_system.user_activity_summary
	               :        +- Relation spark_catalog.tag_system.user_activity_summary[user_id#911,trade_count_30d#912,last_login_date#913,last_trade_date#914,dt#915] parquet
	               +- SubqueryAlias `tag_system.user_basic_info`
	                  +- Project [account_status#2113, user_id#2100]
	                     +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	                        +- Relation spark_catalog.tag_system.user_basic_info[user_id#2100,age#2101,user_level#2102,registration_date#2103,birthday#2104,first_name#2105,last_name#2106,middle_name#2107,phone_number#2108,email#2109,is_vip#2110,is_banned#2111,kyc_status#2112,account_status#2113,primary_status#2114,secondary_status#2115,dt#2116] parquet

	   📦 计算标签组 9/10: Group_1tags_3tables
	🚀 开始计算标签组: Group_1tags_3tables
	   📋 该组标签规则数: 1
	🔍 TagRuleParser初始化完成
	🔍 分析字段依赖关系...
	✅ 字段依赖分析完成: 3 个表
	   📊 tag_system.user_basic_info: ['user_level', 'age', 'user_id']
	   📊 tag_system.user_asset_summary: ['total_asset_value', 'user_id']
	   📊 tag_system.user_preferences: ['owned_products', 'user_id']
	🔗 开始JOIN表: ['tag_system.user_asset_summary', 'tag_system.user_basic_info', 'tag_system.user_preferences']
	📖 加载Hive表: tag_system.user_basic_info
	✅ 表 tag_system.user_basic_info 加载完成，字段: {'user_level', 'age', 'user_id'}
	📖 加载Hive表: tag_system.user_preferences
[INFO] 2025-07-29 14:02:36.230 +0000 -  ->
	✅ 表 tag_system.user_preferences 加载完成，字段: {'owned_products', 'user_id'}
	📚 批量加载完成: 3 个表
	   📋 基础表: tag_system.user_asset_summary
	   🔗 LEFT JOIN: tag_system.user_basic_info
	   🔗 LEFT JOIN: tag_system.user_preferences
	✅ JOIN完成，涉及 3 个表
[INFO] 2025-07-29 14:02:37.230 +0000 -  ->
	   🔗 JOIN完成，用户数: 1000
	   🎯 并行计算 1 个标签...
	      🏷️  计算标签 48...
	🔍 TagRuleParser初始化完成
	         🔍 标签 48 SQL条件: (`tag_system.user_basic_info`.`age` BETWEEN 25 AND 45 AND `tag_system.user_asset_summary`.`total_asset_value` NOT BETWEEN 0 AND 1000) AND (`tag_system.user_basic_info`.`user_level` IN ('VIP3','VIP4','VIP5') OR (array_contains(`tag_system.user_preferences`.`owned_products`, 'premium') OR array_contains(`tag_system.user_preferences`.`owned_products`, 'platinum')))
[INFO] 2025-07-29 14:02:38.231 +0000 -  ->
	         ✅ 标签 48: 143 个用户
	   🔀 聚合用户标签...
	❌ 标签组计算失败: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "flatten(collect_list(tag_id))" due to data type mismatch: Parameter 1 requires the "ARRAY" of "ARRAY" type, however "collect_list(tag_id)" has the type "ARRAY<INT>".;
	'Aggregate [user_id#39], [user_id#39, array_distinct(array_sort(flatten(collect_list(tag_id#18150, 0, 0)), lambdafunction(if ((isnull(lambda 'left) AND isnull(lambda 'right))) 0 else if (isnull(lambda 'left)) 1 else if (isnull(lambda 'right)) -1 else if ((lambda 'left < lambda 'right)) -1 else if ((lambda 'left > lambda 'right)) 1 else 0, lambda 'left, lambda 'right, false), false)) AS tag_ids_array#18373]
	+- Project [user_id#39, 48 AS tag_id#18150]
	   +- Project [user_id#39]
	      +- Filter ((((age#2101 >= 25) AND (age#2101 <= 45)) AND NOT ((total_asset_value#40 >= cast(0 as double)) AND (total_asset_value#40 <= cast(1000 as double)))) AND (user_level#2102 IN (VIP3,VIP4,VIP5) OR (array_contains(owned_products#14242, premium) OR array_contains(owned_products#14242, platinum))))
	         +- Project [user_id#39, total_asset_value#40, user_level#2102, age#2101, owned_products#14242]
	            +- Join LeftOuter, (user_id#39 = user_id#14240)
	               :- Project [user_id#39, total_asset_value#40, user_level#2102, age#2101]
	               :  +- Join LeftOuter, (user_id#39 = user_id#2100)
	               :     :- SubqueryAlias `tag_system.user_asset_summary`
	               :     :  +- Project [total_asset_value#40, user_id#39]
	               :     :     +- SubqueryAlias spark_catalog.tag_system.user_asset_summary
	               :     :        +- Relation spark_catalog.tag_system.user_asset_summary[user_id#39,total_asset_value#40,cash_balance#41,debt_amount#42,dt#43] parquet
	               :     +- SubqueryAlias `tag_system.user_basic_info`
	               :        +- Project [user_level#2102, age#2101, user_id#2100]
	               :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	               :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#2100,age#2101,user_level#2102,registration_date#2103,birthday#2104,first_name#2105,last_name#2106,middle_name#2107,phone_number#2108,email#2109,is_vip#2110,is_banned#2111,kyc_status#2112,account_status#2113,primary_status#2114,secondary_status#2115,dt#2116] parquet
	               +- SubqueryAlias `tag_system.user_preferences`
	                  +- Project [owned_products#14242, user_id#14240]
	                     +- SubqueryAlias spark_catalog.tag_system.user_preferences
	                        +- Relation spark_catalog.tag_system.user_preferences[user_id#14240,interested_products#14241,owned_products#14242,blacklisted_products#14243,active_products#14244,expired_products#14245,optional_services#14246,required_services#14247,dt#14248] parquet

	Traceback (most recent call last):
	  File "/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py", line 66, in computeTags
	    userTagsDF = self._aggregateUserTags(tagResultsDF)
	                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py", line 155, in _aggregateUserTags
	    userTagsDF = tagResultsDF.groupBy("user_id").agg(
	                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/sql/group.py", line 186, in agg
	    jdf = self._jgd.agg(exprs[0]._jc, _to_seq(self.session._sc, [c._jc for c in exprs[1:]]))
	          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
	    raise converted from None
	pyspark.errors.exceptions.captured.AnalysisException: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "flatten(collect_list(tag_id))" due to data type mismatch: Parameter 1 requires the "ARRAY" of "ARRAY" type, however "collect_list(tag_id)" has the type "ARRAY<INT>".;
	'Aggregate [user_id#39], [user_id#39, array_distinct(array_sort(flatten(collect_list(tag_id#18150, 0, 0)), lambdafunction(if ((isnull(lambda 'left) AND isnull(lambda 'right))) 0 else if (isnull(lambda 'left)) 1 else if (isnull(lambda 'right)) -1 else if ((lambda 'left < lambda 'right)) -1 else if ((lambda 'left > lambda 'right)) 1 else 0, lambda 'left, lambda 'right, false), false)) AS tag_ids_array#18373]
	+- Project [user_id#39, 48 AS tag_id#18150]
	   +- Project [user_id#39]
	      +- Filter ((((age#2101 >= 25) AND (age#2101 <= 45)) AND NOT ((total_asset_value#40 >= cast(0 as double)) AND (total_asset_value#40 <= cast(1000 as double)))) AND (user_level#2102 IN (VIP3,VIP4,VIP5) OR (array_contains(owned_products#14242, premium) OR array_contains(owned_products#14242, platinum))))
	         +- Project [user_id#39, total_asset_value#40, user_level#2102, age#2101, owned_products#14242]
	            +- Join LeftOuter, (user_id#39 = user_id#14240)
	               :- Project [user_id#39, total_asset_value#40, user_level#2102, age#2101]
	               :  +- Join LeftOuter, (user_id#39 = user_id#2100)
	               :     :- SubqueryAlias `tag_system.user_asset_summary`
	               :     :  +- Project [total_asset_value#40, user_id#39]
	               :     :     +- SubqueryAlias spark_catalog.tag_system.user_asset_summary
	               :     :        +- Relation spark_catalog.tag_system.user_asset_summary[user_id#39,total_asset_value#40,cash_balance#41,debt_amount#42,dt#43] parquet
	               :     +- SubqueryAlias `tag_system.user_basic_info`
	               :        +- Project [user_level#2102, age#2101, user_id#2100]
	               :           +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	               :              +- Relation spark_catalog.tag_system.user_basic_info[user_id#2100,age#2101,user_level#2102,registration_date#2103,birthday#2104,first_name#2105,last_name#2106,middle_name#2107,phone_number#2108,email#2109,is_vip#2110,is_banned#2111,kyc_status#2112,account_status#2113,primary_status#2114,secondary_status#2115,dt#2116] parquet
	               +- SubqueryAlias `tag_system.user_preferences`
	                  +- Project [owned_products#14242, user_id#14240]
	                     +- SubqueryAlias spark_catalog.tag_system.user_preferences
	                        +- Relation spark_catalog.tag_system.user_preferences[user_id#14240,interested_products#14241,owned_products#14242,blacklisted_products#14243,active_products#14244,expired_products#14245,optional_services#14246,required_services#14247,dt#14248] parquet

	   📦 计算标签组 10/10: Group_2tags_2tables
	🚀 开始计算标签组: Group_2tags_2tables
	   📋 该组标签规则数: 2
	🔍 TagRuleParser初始化完成
	🔍 分析字段依赖关系...
	✅ 字段依赖分析完成: 2 个表
	   📊 tag_system.user_preferences: ['interested_products', 'optional_services', 'user_id']
	   📊 tag_system.user_basic_info: ['first_name', 'birthday', 'user_id', 'is_vip', 'middle_name', 'last_name']
	🔗 开始JOIN表: ['tag_system.user_basic_info', 'tag_system.user_preferences']
	📖 加载Hive表: tag_system.user_basic_info
	✅ 表 tag_system.user_basic_info 加载完成，字段: {'first_name', 'birthday', 'user_id', 'is_vip', 'middle_name', 'last_name'}
	📖 加载Hive表: tag_system.user_preferences
	✅ 表 tag_system.user_preferences 加载完成，字段: {'interested_products', 'optional_services', 'user_id'}
	📚 批量加载完成: 2 个表
	   📋 基础表: tag_system.user_basic_info
	   🔗 LEFT JOIN: tag_system.user_preferences
	✅ JOIN完成，涉及 2 个表
[INFO] 2025-07-29 14:02:39.232 +0000 -  ->
	   🔗 JOIN完成，用户数: 1000
	   🎯 并行计算 2 个标签...
	      🏷️  计算标签 49...
	🔍 TagRuleParser初始化完成
	         🔍 标签 49 SQL条件: ((array_contains(`tag_system.user_preferences`.`interested_products`, 'high_risk') AND array_contains(`tag_system.user_preferences`.`interested_products`, 'speculative')) AND `tag_system.user_basic_info`.`is_vip` = FALSE)
	         ✅ 标签 49: 0 个用户
	      🏷️  计算标签 50...
	🔍 TagRuleParser初始化完成
	         🔍 标签 50 SQL条件: (`tag_system.user_basic_info`.`first_name` IS NOT NULL AND `tag_system.user_basic_info`.`last_name` IS NOT NULL AND `tag_system.user_basic_info`.`birthday` IS NOT NULL) AND (`tag_system.user_basic_info`.`middle_name` IS NULL OR `tag_system.user_preferences`.`optional_services` IS NULL)
[INFO] 2025-07-29 14:02:40.232 +0000 -  ->
	         ✅ 标签 50: 386 个用户
	   🔀 聚合用户标签...
	❌ 标签组计算失败: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "flatten(collect_list(tag_id))" due to data type mismatch: Parameter 1 requires the "ARRAY" of "ARRAY" type, however "collect_list(tag_id)" has the type "ARRAY<INT>".;
	'Aggregate [user_id#2100], [user_id#2100, array_distinct(array_sort(flatten(collect_list(tag_id#18763, 0, 0)), lambdafunction(if ((isnull(lambda 'left) AND isnull(lambda 'right))) 0 else if (isnull(lambda 'left)) 1 else if (isnull(lambda 'right)) -1 else if ((lambda 'left < lambda 'right)) -1 else if ((lambda 'left > lambda 'right)) 1 else 0, lambda 'left, lambda 'right, false), false)) AS tag_ids_array#19357]
	+- Union false, false
	   :- Project [user_id#2100, 49 AS tag_id#18763]
	   :  +- Project [user_id#2100]
	   :     +- Filter ((array_contains(interested_products#14241, high_risk) AND array_contains(interested_products#14241, speculative)) AND (is_vip#2110 = false))
	   :        +- Project [user_id#2100, first_name#2105, birthday#2104, is_vip#2110, middle_name#2107, last_name#2106, interested_products#14241, optional_services#14246]
	   :           +- Join LeftOuter, (user_id#2100 = user_id#14240)
	   :              :- SubqueryAlias `tag_system.user_basic_info`
	   :              :  +- Project [first_name#2105, birthday#2104, user_id#2100, is_vip#2110, middle_name#2107, last_name#2106]
	   :              :     +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              :        +- Relation spark_catalog.tag_system.user_basic_info[user_id#2100,age#2101,user_level#2102,registration_date#2103,birthday#2104,first_name#2105,last_name#2106,middle_name#2107,phone_number#2108,email#2109,is_vip#2110,is_banned#2111,kyc_status#2112,account_status#2113,primary_status#2114,secondary_status#2115,dt#2116] parquet
	   :              +- SubqueryAlias `tag_system.user_preferences`
	   :                 +- Project [interested_products#14241, optional_services#14246, user_id#14240]
	   :                    +- SubqueryAlias spark_catalog.tag_system.user_preferences
	   :                       +- Relation spark_catalog.tag_system.user_preferences[user_id#14240,interested_products#14241,owned_products#14242,blacklisted_products#14243,active_products#14244,expired_products#14245,optional_services#14246,required_services#14247,dt#14248] parquet
	   +- Project [user_id#19324, 50 AS tag_id#19044]
	      +- Project [user_id#19324]
	         +- Filter (((isnotnull(first_name#19329) AND isnotnull(last_name#19330)) AND isnotnull(birthday#19328)) AND (isnull(middle_name#19331) OR isnull(optional_services#19347)))
	            +- Project [user_id#19324, first_name#19329, birthday#19328, is_vip#19334, middle_name#19331, last_name#19330, interested_products#19342, optional_services#19347]
	               +- Join LeftOuter, (user_id#19324 = user_id#19341)
	                  :- SubqueryAlias `tag_system.user_basic_info`
	                  :  +- Project [first_name#19329, birthday#19328, user_id#19324, is_vip#19334, middle_name#19331, last_name#19330]
	                  :     +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	                  :        +- Relation spark_catalog.tag_system.user_basic_info[user_id#19324,age#19325,user_level#19326,registration_date#19327,birthday#19328,first_name#19329,last_name#19330,middle_name#19331,phone_number#19332,email#19333,is_vip#19334,is_banned#19335,kyc_status#19336,account_status#19337,primary_status#19338,secondary_status#19339,dt#19340] parquet
	                  +- SubqueryAlias `tag_system.user_preferences`
	                     +- Project [interested_products#19342, optional_services#19347, user_id#19341]
	                        +- SubqueryAlias spark_catalog.tag_system.user_preferences
	                           +- Relation spark_catalog.tag_system.user_preferences[user_id#19341,interested_products#19342,owned_products#19343,blacklisted_products#19344,active_products#19345,expired_products#19346,optional_services#19347,required_services#19348,dt#19349] parquet

	Traceback (most recent call last):
	  File "/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py", line 66, in computeTags
	    userTagsDF = self._aggregateUserTags(tagResultsDF)
	                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py", line 155, in _aggregateUserTags
	    userTagsDF = tagResultsDF.groupBy("user_id").agg(
	                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/sql/group.py", line 186, in agg
	    jdf = self._jgd.agg(exprs[0]._jc, _to_seq(self.session._sc, [c._jc for c in exprs[1:]]))
	          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
	    raise converted from None
	pyspark.errors.exceptions.captured.AnalysisException: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve "flatten(collect_list(tag_id))" due to data type mismatch: Parameter 1 requires the "ARRAY" of "ARRAY" type, however "collect_list(tag_id)" has the type "ARRAY<INT>".;
	'Aggregate [user_id#2100], [user_id#2100, array_distinct(array_sort(flatten(collect_list(tag_id#18763, 0, 0)), lambdafunction(if ((isnull(lambda 'left) AND isnull(lambda 'right))) 0 else if (isnull(lambda 'left)) 1 else if (isnull(lambda 'right)) -1 else if ((lambda 'left < lambda 'right)) -1 else if ((lambda 'left > lambda 'right)) 1 else 0, lambda 'left, lambda 'right, false), false)) AS tag_ids_array#19357]
	+- Union false, false
	   :- Project [user_id#2100, 49 AS tag_id#18763]
	   :  +- Project [user_id#2100]
	   :     +- Filter ((array_contains(interested_products#14241, high_risk) AND array_contains(interested_products#14241, speculative)) AND (is_vip#2110 = false))
	   :        +- Project [user_id#2100, first_name#2105, birthday#2104, is_vip#2110, middle_name#2107, last_name#2106, interested_products#14241, optional_services#14246]
	   :           +- Join LeftOuter, (user_id#2100 = user_id#14240)
	   :              :- SubqueryAlias `tag_system.user_basic_info`
	   :              :  +- Project [first_name#2105, birthday#2104, user_id#2100, is_vip#2110, middle_name#2107, last_name#2106]
	   :              :     +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	   :              :        +- Relation spark_catalog.tag_system.user_basic_info[user_id#2100,age#2101,user_level#2102,registration_date#2103,birthday#2104,first_name#2105,last_name#2106,middle_name#2107,phone_number#2108,email#2109,is_vip#2110,is_banned#2111,kyc_status#2112,account_status#2113,primary_status#2114,secondary_status#2115,dt#2116] parquet
	   :              +- SubqueryAlias `tag_system.user_preferences`
	   :                 +- Project [interested_products#14241, optional_services#14246, user_id#14240]
	   :                    +- SubqueryAlias spark_catalog.tag_system.user_preferences
	   :                       +- Relation spark_catalog.tag_system.user_preferences[user_id#14240,interested_products#14241,owned_products#14242,blacklisted_products#14243,active_products#14244,expired_products#14245,optional_services#14246,required_services#14247,dt#14248] parquet
	   +- Project [user_id#19324, 50 AS tag_id#19044]
	      +- Project [user_id#19324]
	         +- Filter (((isnotnull(first_name#19329) AND isnotnull(last_name#19330)) AND isnotnull(birthday#19328)) AND (isnull(middle_name#19331) OR isnull(optional_services#19347)))
	            +- Project [user_id#19324, first_name#19329, birthday#19328, is_vip#19334, middle_name#19331, last_name#19330, interested_products#19342, optional_services#19347]
	               +- Join LeftOuter, (user_id#19324 = user_id#19341)
	                  :- SubqueryAlias `tag_system.user_basic_info`
	                  :  +- Project [first_name#19329, birthday#19328, user_id#19324, is_vip#19334, middle_name#19331, last_name#19330]
	                  :     +- SubqueryAlias spark_catalog.tag_system.user_basic_info
	                  :        +- Relation spark_catalog.tag_system.user_basic_info[user_id#19324,age#19325,user_level#19326,registration_date#19327,birthday#19328,first_name#19329,last_name#19330,middle_name#19331,phone_number#19332,email#19333,is_vip#19334,is_banned#19335,kyc_status#19336,account_status#19337,primary_status#19338,secondary_status#19339,dt#19340] parquet
	                  +- SubqueryAlias `tag_system.user_preferences`
	                     +- Project [interested_products#19342, optional_services#19347, user_id#19341]
	                        +- SubqueryAlias spark_catalog.tag_system.user_preferences
	                           +- Relation spark_catalog.tag_system.user_preferences[user_id#19341,interested_products#19342,owned_products#19343,blacklisted_products#19344,active_products#19345,expired_products#19346,optional_services#19347,required_services#19348,dt#19349] parquet

	✅ 所有标签组计算完成，成功: 10 个
	🔀 合并所有标签组结果...
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 1.0 in stage 287.0 (TID 204) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	25/07/29 14:02:40 WARN TaskSetManager: Lost task 12.0 in stage 287.0 (TID 215) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	25/07/29 14:02:40 ERROR TaskSetManager: Task 26 in stage 287.0 failed 4 times; aborting job
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 0.0 in stage 287.0 (TID 203) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 11): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000013/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	25/07/29 14:02:40 WARN TaskSetManager: Lost task 14.3 in stage 287.0 (TID 243) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 26 in stage 287.0 failed 4 times, most recent failure: Lost task 26.3 in stage 287.0 (TID 240) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 25.3 in stage 287.0 (TID 244) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 26 in stage 287.0 failed 4 times, most recent failure: Lost task 26.3 in stage 287.0 (TID 240) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
[INFO] 2025-07-29 14:02:41.236 +0000 -  ->
	   ❌ 合并过程异常: An error occurred while calling o1392.count.
	: org.apache.spark.SparkException: Job aborted due to stage failure: Task 26 in stage 287.0 failed 4 times, most recent failure: Lost task 26.3 in stage 287.0 (TID 240) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:
		at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3083)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3019)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3018)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3018)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1324)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1324)
		at scala.Option.foreach(Option.scala:407)
		at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1324)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3301)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3235)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3224)
		at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:175)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:97)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:75)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:59)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:290)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:289)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:583)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:545)
		at org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3662)
		at org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3661)
		at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4392)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:711)
		at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4390)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)
		at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)
		at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:157)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$10(SQLExecution.scala:220)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)
		at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:220)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:405)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:219)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:83)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)
		at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4390)
		at org.apache.spark.sql.Dataset.count(Dataset.scala:3661)
		at jdk.internal.reflect.GeneratedMethodAccessor112.invoke(Unknown Source)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.base/java.lang.Thread.run(Thread.java:840)
	Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	25/07/29 14:02:40 WARN TaskSetManager: Lost task 3.0 in stage 287.0 (TID 206) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 26 in stage 287.0 failed 4 times, most recent failure: Lost task 26.3 in stage 287.0 (TID 240) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 8.0 in stage 287.0 (TID 211) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 9): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 26 in stage 287.0 failed 4 times, most recent failure: Lost task 26.3 in stage 287.0 (TID 240) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 16.0 in stage 287.0 (TID 219) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 26 in stage 287.0 failed 4 times, most recent failure: Lost task 26.3 in stage 287.0 (TID 240) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 21.0 in stage 287.0 (TID 224) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 9): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 26 in stage 287.0 failed 4 times, most recent failure: Lost task 26.3 in stage 287.0 (TID 240) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	💾 与现有标签合并并保存...
	📖 加载现有用户标签数据...
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 20.0 in stage 287.0 (TID 223) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 26 in stage 287.0 failed 4 times, most recent failure: Lost task 26.3 in stage 287.0 (TID 240) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 7.0 in stage 287.0 (TID 210) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 26 in stage 287.0 failed 4 times, most recent failure: Lost task 26.3 in stage 287.0 (TID 240) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 18.0 in stage 287.0 (TID 221) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 10): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 26 in stage 287.0 failed 4 times, most recent failure: Lost task 26.3 in stage 287.0 (TID 240) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 5.0 in stage 287.0 (TID 208) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 10): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 26 in stage 287.0 failed 4 times, most recent failure: Lost task 26.3 in stage 287.0 (TID 240) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	✅ 现有标签数据加载完成: 1000 个用户
	💾 开始写入标签结果到MySQL...
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 4.0 in stage 287.0 (TID 207) (ip-172-31-9-7.ap-southeast-1.compute.internal executor 4): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 26 in stage 287.0 failed 4 times, most recent failure: Lost task 26.3 in stage 287.0 (TID 240) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 17.0 in stage 287.0 (TID 220) (ip-172-31-9-7.ap-southeast-1.compute.internal executor 4): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 26 in stage 287.0 failed 4 times, most recent failure: Lost task 26.3 in stage 287.0 (TID 240) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 2.0 in stage 287.0 (TID 205) (ip-172-31-9-7.ap-southeast-1.compute.internal executor 3): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 26 in stage 287.0 failed 4 times, most recent failure: Lost task 26.3 in stage 287.0 (TID 240) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 15.0 in stage 287.0 (TID 218) (ip-172-31-9-7.ap-southeast-1.compute.internal executor 3): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 26 in stage 287.0 failed 4 times, most recent failure: Lost task 26.3 in stage 287.0 (TID 240) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 14.0 in stage 291.0 (TID 261) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	25/07/29 14:02:40 WARN TaskSetManager: Lost task 2.0 in stage 291.0 (TID 249) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	25/07/29 14:02:40 WARN TaskSetManager: Lost task 3.0 in stage 291.0 (TID 250) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000002/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	25/07/29 14:02:40 WARN TaskSetManager: Lost task 0.0 in stage 291.0 (TID 247) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000003/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	25/07/29 14:02:40 WARN TaskSetManager: Lost task 9.0 in stage 287.0 (TID 212) (ip-172-31-9-7.ap-southeast-1.compute.internal executor 8): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 26 in stage 287.0 failed 4 times, most recent failure: Lost task 26.3 in stage 287.0 (TID 240) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 22.0 in stage 287.0 (TID 225) (ip-172-31-9-7.ap-southeast-1.compute.internal executor 8): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 26 in stage 287.0 failed 4 times, most recent failure: Lost task 26.3 in stage 287.0 (TID 240) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 13.0 in stage 291.0 (TID 260) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 9): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000010/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	25/07/29 14:02:40 WARN TaskSetManager: Lost task 19.0 in stage 287.0 (TID 222) (ip-172-31-9-7.ap-southeast-1.compute.internal executor 13): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 26 in stage 287.0 failed 4 times, most recent failure: Lost task 26.3 in stage 287.0 (TID 240) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 6.0 in stage 287.0 (TID 209) (ip-172-31-9-7.ap-southeast-1.compute.internal executor 13): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 26 in stage 287.0 failed 4 times, most recent failure: Lost task 26.3 in stage 287.0 (TID 240) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 ERROR TaskSetManager: Task 14 in stage 291.0 failed 4 times; aborting job
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 2.2 in stage 291.0 (TID 272) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 14 in stage 291.0 failed 4 times, most recent failure: Lost task 14.3 in stage 291.0 (TID 280) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 3.1 in stage 291.0 (TID 274) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 14 in stage 291.0 failed 4 times, most recent failure: Lost task 14.3 in stage 291.0 (TID 280) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 20.0 in stage 291.0 (TID 279) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 14 in stage 291.0 failed 4 times, most recent failure: Lost task 14.3 in stage 291.0 (TID 280) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 22.0 in stage 291.0 (TID 285) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 14 in stage 291.0 failed 4 times, most recent failure: Lost task 14.3 in stage 291.0 (TID 280) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 6.2 in stage 291.0 (TID 273) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 14 in stage 291.0 failed 4 times, most recent failure: Lost task 14.3 in stage 291.0 (TID 280) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 5.1 in stage 291.0 (TID 283) (ip-172-31-9-7.ap-southeast-1.compute.internal executor 13): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 14 in stage 291.0 failed 4 times, most recent failure: Lost task 14.3 in stage 291.0 (TID 280) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 11.1 in stage 291.0 (TID 275) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 14 in stage 291.0 failed 4 times, most recent failure: Lost task 14.3 in stage 291.0 (TID 280) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 19.0 in stage 291.0 (TID 277) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 14 in stage 291.0 failed 4 times, most recent failure: Lost task 14.3 in stage 291.0 (TID 280) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 1.0 in stage 291.0 (TID 248) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 10): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 14 in stage 291.0 failed 4 times, most recent failure: Lost task 14.3 in stage 291.0 (TID 280) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 9.0 in stage 291.0 (TID 256) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 10): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 14 in stage 291.0 failed 4 times, most recent failure: Lost task 14.3 in stage 291.0 (TID 280) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 7.0 in stage 291.0 (TID 254) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 11): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 14 in stage 291.0 failed 4 times, most recent failure: Lost task 14.3 in stage 291.0 (TID 280) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 15.0 in stage 291.0 (TID 262) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 11): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 14 in stage 291.0 failed 4 times, most recent failure: Lost task 14.3 in stage 291.0 (TID 280) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 10.2 in stage 291.0 (TID 281) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 9): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 14 in stage 291.0 failed 4 times, most recent failure: Lost task 14.3 in stage 291.0 (TID 280) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 13.1 in stage 291.0 (TID 282) (ip-172-31-9-95.ap-southeast-1.compute.internal executor 9): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 14 in stage 291.0 failed 4 times, most recent failure: Lost task 14.3 in stage 291.0 (TID 280) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	❌ 写入标签结果失败: An error occurred while calling o1506.collectToPython.
	: org.apache.spark.SparkException: Job aborted due to stage failure: Task 14 in stage 291.0 failed 4 times, most recent failure: Lost task 14.3 in stage 291.0 (TID 280) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:
		at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3083)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3019)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3018)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3018)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1324)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1324)
		at scala.Option.foreach(Option.scala:407)
		at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1324)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3301)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3235)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3224)
		at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:175)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:97)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:75)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:59)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:290)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:289)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:583)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:545)
		at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4222)
		at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4392)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:711)
		at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4390)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)
		at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)
		at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:157)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$10(SQLExecution.scala:220)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)
		at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:220)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:405)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:219)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:83)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)
		at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4390)
		at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4219)
		at jdk.internal.reflect.GeneratedMethodAccessor124.invoke(Unknown Source)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.base/java.lang.Thread.run(Thread.java:840)
	Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Traceback (most recent call last):
	  File "/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/meta/MysqlMeta.py", line 137, in writeTagResults
	    results = resultsDF.select("user_id", "final_tag_ids_json").collect()
	              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 1264, in collect
	    sock_info = self._jdf.collectToPython()
	                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
	    return f(*a, **kw)
	           ^^^^^^^^^^^
	  File "/mnt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
	    raise Py4JJavaError(
	py4j.protocol.Py4JJavaError: An error occurred while calling o1506.collectToPython.
	: org.apache.spark.SparkException: Job aborted due to stage failure: Task 14 in stage 291.0 failed 4 times, most recent failure: Lost task 14.3 in stage 291.0 (TID 280) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:
		at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3083)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3019)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3018)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3018)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1324)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1324)
		at scala.Option.foreach(Option.scala:407)
		at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1324)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3301)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3235)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3224)
		at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:175)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:97)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:75)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:59)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:290)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:289)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:583)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:545)
		at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4222)
		at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4392)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:711)
		at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4390)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)
		at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)
		at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:157)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$10(SQLExecution.scala:220)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)
		at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:220)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:405)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:219)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:83)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)
		at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4390)
		at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4219)
		at jdk.internal.reflect.GeneratedMethodAccessor124.invoke(Unknown Source)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.base/java.lang.Thread.run(Thread.java:840)
	Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	❌ 标签计算失败

	============================================================
	❌ 任务执行失败
	============================================================
	🧹 HiveMeta缓存已清理
	🧹 TagEngine资源清理完成
	🧹 关闭Spark会话...
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 17.0 in stage 291.0 (TID 264) (ip-172-31-9-7.ap-southeast-1.compute.internal executor 3): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 14 in stage 291.0 failed 4 times, most recent failure: Lost task 14.3 in stage 291.0 (TID 280) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 16.0 in stage 291.0 (TID 263) (ip-172-31-9-7.ap-southeast-1.compute.internal executor 3): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 14 in stage 291.0 failed 4 times, most recent failure: Lost task 14.3 in stage 291.0 (TID 280) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 10.0 in stage 287.0 (TID 213) (ip-172-31-9-7.ap-southeast-1.compute.internal executor 12): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 26 in stage 287.0 failed 4 times, most recent failure: Lost task 26.3 in stage 287.0 (TID 240) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 23.0 in stage 287.0 (TID 226) (ip-172-31-9-7.ap-southeast-1.compute.internal executor 12): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 26 in stage 287.0 failed 4 times, most recent failure: Lost task 26.3 in stage 287.0 (TID 240) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000006/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 8.1 in stage 291.0 (TID 278) (ip-172-31-9-7.ap-southeast-1.compute.internal executor 8): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 14 in stage 291.0 failed 4 times, most recent failure: Lost task 14.3 in stage 291.0 (TID 280) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 0.1 in stage 291.0 (TID 276) (ip-172-31-9-7.ap-southeast-1.compute.internal executor 8): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 14 in stage 291.0 failed 4 times, most recent failure: Lost task 14.3 in stage 291.0 (TID 280) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
	25/07/29 14:02:40 WARN TaskSetManager: Lost task 21.0 in stage 291.0 (TID 284) (ip-172-31-9-7.ap-southeast-1.compute.internal executor 13): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 14 in stage 291.0 failed 4 times, most recent failure: Lost task 14.3 in stage 291.0 (TID 280) (ip-172-31-9-238.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt/yarn/usercache/root/appcache/application_1739849539217_3163/container_1739849539217_3163_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.UnsafeRowInterceptor.hasNext(UnsafeRowInterceptor.java:24)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.start(UnsafeShuffleWriter.java:229)
		at org.apache.spark.shuffle.DirectShuffleWriteProcessor.doWrite(DirectShuffleWriteProcessor.scala:44)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
[INFO] 2025-07-29 14:02:42.250 +0000 -  ->
	👋 程序退出
	🧹 HiveMeta缓存已清理
	🧹 TagEngine资源清理完成
[INFO] 2025-07-29 14:02:42.251 +0000 - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_16/904/3459, processId:767673 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[INFO] 2025-07-29 14:02:42.251 +0000 - Start finding appId in /usr/local/installed/dolphinscheduler/apache-dolphinscheduler-3.2.0-bin/worker-server/logs/20250729/18466477060960/16/904/3459.log, fetch way: log
[INFO] 2025-07-29 14:02:42.251 +0000 - Find appId: application_1739849539217_3163 from /usr/local/installed/dolphinscheduler/apache-dolphinscheduler-3.2.0-bin/worker-server/logs/20250729/18466477060960/16/904/3459.log
[INFO] 2025-07-29 14:02:42.253 +0000 - ***********************************************************************************************
[INFO] 2025-07-29 14:02:42.253 +0000 - *********************************  Finalize task instance  ************************************
[INFO] 2025-07-29 14:02:42.253 +0000 - ***********************************************************************************************
[INFO] 2025-07-29 14:02:42.253 +0000 - Upload output files: [] successfully
[INFO] 2025-07-29 14:02:42.253 +0000 - Send task execute status: FAILURE to master : 172.31.9.77:1234
[INFO] 2025-07-29 14:02:42.253 +0000 - Remove the current task execute context from worker cache
[INFO] 2025-07-29 14:02:42.253 +0000 - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_16/904/3459
[INFO] 2025-07-29 14:02:42.254 +0000 - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_16/904/3459
[INFO] 2025-07-29 14:02:42.254 +0000 - FINALIZE_SESSION
