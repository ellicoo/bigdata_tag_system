[LOG-PATH]: /usr/local/installed/dolphinscheduler/apache-dolphinscheduler-3.2.0-bin/worker-server/logs/20250728/18466477060960/7/881/3436.log, [HOST]:  Host(ip=172.31.9.77, port=1234)
[INFO] 2025-07-28 13:26:35.359 +0000 - ***********************************************************************************************
[INFO] 2025-07-28 13:26:35.360 +0000 - *********************************  Initialize task context  ***********************************
[INFO] 2025-07-28 13:26:35.360 +0000 - ***********************************************************************************************
[INFO] 2025-07-28 13:26:35.360 +0000 - Begin to initialize task
[INFO] 2025-07-28 13:26:35.360 +0000 - Set task startTime: 1753709195360
[INFO] 2025-07-28 13:26:35.360 +0000 - Set task appId: 881_3436
[INFO] 2025-07-28 13:26:35.360 +0000 - End initialize task {
  "taskInstanceId" : 3436,
  "taskName" : "health_check",
  "firstSubmitTime" : 1753709195341,
  "startTime" : 1753709195360,
  "taskType" : "SPARK",
  "workflowInstanceHost" : "172.31.9.77:5678",
  "host" : "172.31.9.77:1234",
  "logPath" : "/usr/local/installed/dolphinscheduler/apache-dolphinscheduler-3.2.0-bin/worker-server/logs/20250728/18466477060960/7/881/3436.log",
  "processId" : 0,
  "processDefineCode" : 18466477060960,
  "processDefineVersion" : 7,
  "processInstanceId" : 881,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 18466461502048,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"\",\"resourceList\":[],\"programType\":\"PYTHON\",\"mainClass\":\"\",\"mainJar\":{\"id\":-1,\"resourceName\":\"file:/dolphinscheduler/default/resources/bigdata_tag_system/main.py\",\"res\":null},\"deployMode\":\"local\",\"appName\":\"health_check\",\"mainArgs\":\"--mode health\",\"others\":\"--jars /dolphinscheduler/default/resources/mysql-connector-j-8.0.33.jar\",\"yarnQueue\":\"\",\"driverCores\":1,\"driverMemory\":\"512M\",\"numExecutors\":2,\"executorMemory\":\"2G\",\"executorCores\":2,\"sqlExecutionType\":\"SCRIPT\"}",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "health_check"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "18466461502048"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "881"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20250728"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20250727"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3436"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "pyspark_tag_system"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "18466524825825"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "18466477060960"
    },
    "StartNodeList" : {
      "prop" : "StartNodeList",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "18466524825825"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20250728132635"
    }
  },
  "taskAppId" : "881_3436",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "resources" : {
    "file:/dolphinscheduler/default/resources/bigdata_tag_system/main.py" : ""
  },
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[INFO] 2025-07-28 13:26:35.361 +0000 - ***********************************************************************************************
[INFO] 2025-07-28 13:26:35.361 +0000 - *********************************  Load task instance plugin  *********************************
[INFO] 2025-07-28 13:26:35.361 +0000 - ***********************************************************************************************
[INFO] 2025-07-28 13:26:35.361 +0000 - Send task status RUNNING_EXECUTION master: 172.31.9.77:1234
[WARN] 2025-07-28 13:26:35.361 +0000 - Current tenant is default tenant, will use root to execute the task
[INFO] 2025-07-28 13:26:35.361 +0000 - TenantCode: default check successfully
[INFO] 2025-07-28 13:26:35.361 +0000 - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_7/881/3436 check successfully
[INFO] 2025-07-28 13:26:35.362 +0000 - get resource file from path:file:/dolphinscheduler/default/resources/bigdata_tag_system/main.py
[INFO] 2025-07-28 13:26:35.363 +0000 - Download resources: {file:/dolphinscheduler/default/resources/bigdata_tag_system/main.py=file:/dolphinscheduler/default/resources/bigdata_tag_system/main.py} successfully
[INFO] 2025-07-28 13:26:35.363 +0000 - Download upstream files: [] successfully
[INFO] 2025-07-28 13:26:35.363 +0000 - Task plugin instance: SPARK create successfully
[INFO] 2025-07-28 13:26:35.363 +0000 - Initialize spark task params {
  "localParams" : [ ],
  "varPool" : null,
  "mainJar" : {
    "id" : -1,
    "resourceName" : "file:/dolphinscheduler/default/resources/bigdata_tag_system/main.py",
    "res" : null
  },
  "mainClass" : "",
  "deployMode" : "local",
  "mainArgs" : "--mode health",
  "driverCores" : 1,
  "driverMemory" : "512M",
  "numExecutors" : 2,
  "executorCores" : 2,
  "executorMemory" : "2G",
  "appName" : "health_check",
  "yarnQueue" : "",
  "others" : "--jars /dolphinscheduler/default/resources/mysql-connector-j-8.0.33.jar",
  "programType" : "PYTHON",
  "rawScript" : "",
  "namespace" : null,
  "resourceList" : [ ],
  "sqlExecutionType" : "SCRIPT"
}
[INFO] 2025-07-28 13:26:35.363 +0000 - Success initialized task plugin instance successfully
[INFO] 2025-07-28 13:26:35.363 +0000 - Set taskVarPool: null successfully
[INFO] 2025-07-28 13:26:35.363 +0000 - ***********************************************************************************************
[INFO] 2025-07-28 13:26:35.363 +0000 - *********************************  Execute task instance  *************************************
[INFO] 2025-07-28 13:26:35.363 +0000 - ***********************************************************************************************
[INFO] 2025-07-28 13:26:35.364 +0000 - Final Shell file is :
#!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
${SPARK_HOME}/bin/spark-submit --master local --conf spark.driver.cores=1 --conf spark.driver.memory=512M --conf spark.executor.instances=2 --conf spark.executor.cores=2 --conf spark.executor.memory=2G --name health_check --jars /dolphinscheduler/default/resources/mysql-connector-j-8.0.33.jar file:/dolphinscheduler/default/resources/bigdata_tag_system/main.py --mode health
[INFO] 2025-07-28 13:26:35.364 +0000 - Executing shell command : sudo -u root -i /tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_7/881/3436/881_3436.sh
[INFO] 2025-07-28 13:26:35.367 +0000 - process start, process id is: 741702
[INFO] 2025-07-28 13:26:37.367 +0000 -  ->
	25/07/28 13:26:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	============================================================
	üè∑Ô∏è  Â§ßÊï∞ÊçÆÊ†áÁ≠æËÆ°ÁÆóÁ≥ªÁªü
	============================================================
	ÊâßË°åÊ®°Âºè: health
	ÂΩìÂâçÂ∑•‰ΩúÁõÆÂΩï: /tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_7/881/3436
	ËÑöÊú¨ÁõÆÂΩï: /dolphinscheduler/default/resources/bigdata_tag_system
	È°πÁõÆÊ†πÁõÆÂΩï: /dolphinscheduler/default
	PythonË∑ØÂæÑÂâç3È°π: ['/dolphinscheduler/default', '/dolphinscheduler/default/resources/bigdata_tag_system', '/mnt/spark/python/lib/pyspark.zip']
	üöÄ ÂàõÂª∫Spark‰ºöËØù: TagComputeEngine
[INFO] 2025-07-28 13:26:38.368 +0000 -  ->
	25/07/28 13:26:37 INFO EMRParamSideChannel: Setting FGAC mode to false
	25/07/28 13:26:37 INFO SparkContext: Running Spark version 3.5.2-amzn-1
	25/07/28 13:26:37 INFO SparkContext: OS info Linux, 6.8.0-1029-aws, amd64
	25/07/28 13:26:37 INFO SparkContext: Java version 17.0.15
	25/07/28 13:26:37 INFO ResourceUtils: ==============================================================
	25/07/28 13:26:37 INFO ResourceUtils: No custom resources configured for spark.driver.
	25/07/28 13:26:37 INFO ResourceUtils: ==============================================================
	25/07/28 13:26:37 INFO SparkContext: Submitted application: TagComputeEngine
	25/07/28 13:26:37 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(executorType -> name: executorType, amount: 1, script: , vendor: , cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	25/07/28 13:26:37 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor
	25/07/28 13:26:37 INFO ResourceProfileManager: Added ResourceProfile id: 0
	25/07/28 13:26:37 INFO ResourceProfile: User executor ResourceProfile created, executor resources: Map(executorType -> name: executorType, amount: 1, script: , vendor: , cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	25/07/28 13:26:37 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor
	25/07/28 13:26:37 INFO ResourceProfileManager: Added ResourceProfile id: 1
	25/07/28 13:26:37 INFO SecurityManager: Changing view acls to: root
	25/07/28 13:26:37 INFO SecurityManager: Changing modify acls to: root
	25/07/28 13:26:37 INFO SecurityManager: Changing view acls groups to:
	25/07/28 13:26:37 INFO SecurityManager: Changing modify acls groups to:
	25/07/28 13:26:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
	25/07/28 13:26:37 INFO Utils: Successfully started service 'sparkDriver' on port 6411.
	25/07/28 13:26:37 INFO SparkEnv: Registering MapOutputTracker
	25/07/28 13:26:37 INFO SparkEnv: Registering BlockManagerMaster
	25/07/28 13:26:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	25/07/28 13:26:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	25/07/28 13:26:37 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	25/07/28 13:26:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ff2bc52c-2712-4c25-bd15-12e7a72d9477
	25/07/28 13:26:37 INFO MemoryStore: MemoryStore started with capacity 127.2 MiB
	25/07/28 13:26:37 INFO SparkEnv: Registering OutputCommitCoordinator
	25/07/28 13:26:37 INFO SubResultCacheManager: Sub-result caches are disabled.
	25/07/28 13:26:37 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	25/07/28 13:26:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	25/07/28 13:26:38 INFO SparkContext: Added JAR file:///dolphinscheduler/default/resources/mysql-connector-j-8.0.33.jar at spark://ip-172-31-9-77.ap-southeast-1.compute.internal:6411/jars/mysql-connector-j-8.0.33.jar with timestamp 1753709197555
	25/07/28 13:26:38 INFO Executor: Starting executor ID driver on host ip-172-31-9-77.ap-southeast-1.compute.internal
	25/07/28 13:26:38 INFO Executor: OS info Linux, 6.8.0-1029-aws, amd64
	25/07/28 13:26:38 INFO Executor: Java version 17.0.15
	25/07/28 13:26:38 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/usr/lib/hadoop-lzo/lib/*,file:/usr/lib/hadoop/hadoop-aws.jar,file:/usr/share/aws/aws-java-sdk/*,file:/usr/share/aws/aws-java-sdk-v2/*,file:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar,file:/usr/share/aws/emr/security/conf,file:/usr/share/aws/emr/security/lib/*,file:/usr/share/aws/redshift/jdbc/*,file:/usr/share/aws/redshift/spark-redshift/lib/*,file:/usr/share/aws/kinesis/spark-sql-kinesis/lib/*,file:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar,file:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar,file:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar,file:/docker/usr/lib/hadoop-lzo/lib/*,file:/docker/usr/lib/hadoop/hadoop-aws.jar,file:/docker/usr/share/aws/aws-java-sdk/*,file:/docker/usr/share/aws/aws-java-sdk-v2/*,file:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar,file:/docker/usr/share/aws/emr/security/conf,file:/docker/usr/share/aws/emr/security/lib/*,file:/docker/usr/share/aws/redshift/jdbc/*,file:/docker/usr/share/aws/redshift/spark-redshift/lib/*,file:/docker/usr/share/aws/kinesis/spark-sql-kinesis/lib/*,file:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar,file:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar,file:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar,file:/tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_7/881/3436/emr-s3-select-spark-connector.jar,file:/tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_7/881/3436/*,file:/tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_7/881/3436/aws-glue-datacatalog-spark-client.jar,file:/tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_7/881/3436/hadoop-aws.jar,file:/tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_7/881/3436/emr-spark-goodies.jar,file:/tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_7/881/3436/hive-openx-serde.jar,file:/tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_7/881/3436/conf'
	25/07/28 13:26:38 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2f675780 for default.
	25/07/28 13:26:38 INFO Executor: Fetching spark://ip-172-31-9-77.ap-southeast-1.compute.internal:6411/jars/mysql-connector-j-8.0.33.jar with timestamp 1753709197555
	25/07/28 13:26:38 INFO TransportClientFactory: Successfully created connection to ip-172-31-9-77.ap-southeast-1.compute.internal/172.31.9.77:6411 after 15 ms (0 ms spent in bootstraps)
	25/07/28 13:26:38 INFO Utils: Fetching spark://ip-172-31-9-77.ap-southeast-1.compute.internal:6411/jars/mysql-connector-j-8.0.33.jar to /tmp/spark-bcc84e3e-8a08-410e-b1b0-dafd55f099eb/userFiles-9e0b25f3-cd5e-4a90-9c99-8ee6622b28cd/fetchFileTemp9900320711727186257.tmp
	25/07/28 13:26:38 INFO Executor: Adding file:/tmp/spark-bcc84e3e-8a08-410e-b1b0-dafd55f099eb/userFiles-9e0b25f3-cd5e-4a90-9c99-8ee6622b28cd/mysql-connector-j-8.0.33.jar to class loader default
	25/07/28 13:26:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 20449.
	25/07/28 13:26:38 INFO NettyBlockTransferService: Server created on ip-172-31-9-77.ap-southeast-1.compute.internal:20449
	25/07/28 13:26:38 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	25/07/28 13:26:38 INFO BlockManager: external shuffle service port = 7337
	25/07/28 13:26:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-9-77.ap-southeast-1.compute.internal, 20449, None)
	25/07/28 13:26:38 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-9-77.ap-southeast-1.compute.internal:20449 with 127.2 MiB RAM, BlockManagerId(driver, ip-172-31-9-77.ap-southeast-1.compute.internal, 20449, None)
	25/07/28 13:26:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-9-77.ap-southeast-1.compute.internal, 20449, None)
	25/07/28 13:26:38 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-9-77.ap-southeast-1.compute.internal, 20449, None)
[INFO] 2025-07-28 13:26:40.369 +0000 -  ->
	25/07/28 13:26:39 INFO SingleEventLogFileWriter: Logging events to hdfs:/var/log/spark/apps/local-1753709198028.inprogress
	‚úÖ Spark‰ºöËØùÂàõÂª∫ÂÆåÊàêÔºåÁâàÊú¨: 3.5.2-amzn-1
	MySQLÈÖçÁΩÆ: cex-mysql-test.c5mgk4qm8m2z.ap-southeast-1.rds.amazonaws.com:3358/biz_statistics
	üóÑÔ∏è  HiveMetaÂàùÂßãÂåñÂÆåÊàê
	üóÑÔ∏è  MysqlMetaÂàùÂßãÂåñÂÆåÊàê
	üîç TagRuleParserÂàùÂßãÂåñÂÆåÊàê
	üöÄ TagEngineÂàùÂßãÂåñÂÆåÊàê

	üîç ÊâßË°åÂÅ•Â∫∑Ê£ÄÊü•...
	üîç ÊâßË°åÊ†áÁ≠æÁ≥ªÁªüÂÅ•Â∫∑Ê£ÄÊü•...
	‚úÖ MySQLËøûÊé•ÊµãËØïÊàêÂäü
[INFO] 2025-07-28 13:26:41.370 +0000 -  ->
	25/07/28 13:26:41 WARN HiveConf: HiveConf of name hive.server2.thrift.url does not exist
[INFO] 2025-07-28 13:26:42.370 +0000 -  ->
	   ‚úÖ HiveËÆøÈóÆÊ≠£Â∏∏ÔºåÂèëÁé∞ 1 ‰∏™Ë°®
[INFO] 2025-07-28 13:26:44.370 +0000 -  ->
	   ‚úÖ UDFÂáΩÊï∞ÊµãËØïÈÄöËøáÔºåÂ§ÑÁêÜ 2 Êù°Êï∞ÊçÆ
	üìã Âä†ËΩΩÊ†áÁ≠æËßÑÂàôÔºåÊåáÂÆöÊ†áÁ≠æ: None
	‚ùå Âä†ËΩΩÊ†áÁ≠æËßÑÂàôÂ§±Ë¥•: An error occurred while calling o296.load.
	: java.sql.SQLException: Unsupported character encoding 'utf8mb4'
		at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:130)
		at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:98)
		at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:90)
		at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:64)
		at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:74)
		at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:85)
		at com.mysql.cj.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:442)
		at com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:239)
		at com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:188)
		at org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)
		at org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)
		at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)
		at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:65)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:60)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:345)
		at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:272)
		at org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:210)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
		at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.base/java.lang.Thread.run(Thread.java:840)
	Caused by: com.mysql.cj.exceptions.WrongArgumentException: Unsupported character encoding 'utf8mb4'
		at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
		at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
		at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
		at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
		at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:62)
		at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:105)
		at com.mysql.cj.util.StringUtils.getBytes(StringUtils.java:236)
		at com.mysql.cj.jdbc.JdbcPropertySetImpl.postInitialization(JdbcPropertySetImpl.java:66)
		at com.mysql.cj.conf.DefaultPropertySet.initializeProperties(DefaultPropertySet.java:231)
		at com.mysql.cj.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:388)
		... 29 more
	Caused by: java.io.UnsupportedEncodingException: utf8mb4
		at java.base/java.lang.String.lookupCharset(String.java:829)
		at java.base/java.lang.String.getBytes(String.java:1765)
		at com.mysql.cj.util.StringUtils.getBytes(StringUtils.java:234)
		... 32 more

	   ‚ö†Ô∏è  Ê≤°ÊúâÂèëÁé∞Ê¥ªË∑ÉÁöÑÊ†áÁ≠æËßÑÂàô
	‚ùå Á≥ªÁªüÂÅ•Â∫∑Ê£ÄÊü•Â§±Ë¥•
	   MySQL: ‚úÖ
	   Hive: ‚úÖ
	   UDF: ‚úÖ
	   Rules: ‚ùå

	============================================================
	‚ùå ‰ªªÂä°ÊâßË°åÂ§±Ë¥•
	============================================================
	üßπ HiveMetaÁºìÂ≠òÂ∑≤Ê∏ÖÁêÜ
	üßπ TagEngineËµÑÊ∫êÊ∏ÖÁêÜÂÆåÊàê
	üßπ ÂÖ≥Èó≠Spark‰ºöËØù...
[INFO] 2025-07-28 13:26:45.371 +0000 -  ->
	üëã Á®ãÂ∫èÈÄÄÂá∫
	üßπ HiveMetaÁºìÂ≠òÂ∑≤Ê∏ÖÁêÜ
	üßπ TagEngineËµÑÊ∫êÊ∏ÖÁêÜÂÆåÊàê
[INFO] 2025-07-28 13:26:46.372 +0000 - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_7/881/3436, processId:741702 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[INFO] 2025-07-28 13:26:46.373 +0000 - Start finding appId in /usr/local/installed/dolphinscheduler/apache-dolphinscheduler-3.2.0-bin/worker-server/logs/20250728/18466477060960/7/881/3436.log, fetch way: log
[INFO] 2025-07-28 13:26:46.373 +0000 - ***********************************************************************************************
[INFO] 2025-07-28 13:26:46.373 +0000 - *********************************  Finalize task instance  ************************************
[INFO] 2025-07-28 13:26:46.373 +0000 - ***********************************************************************************************
[INFO] 2025-07-28 13:26:46.373 +0000 - Upload output files: [] successfully
[INFO] 2025-07-28 13:26:46.373 +0000 - Send task execute status: FAILURE to master : 172.31.9.77:1234
[INFO] 2025-07-28 13:26:46.373 +0000 - Remove the current task execute context from worker cache
[INFO] 2025-07-28 13:26:46.373 +0000 - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_7/881/3436
[INFO] 2025-07-28 13:26:46.375 +0000 - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_7/881/3436
[INFO] 2025-07-28 13:26:46.375 +0000 - FINALIZE_SESSION
