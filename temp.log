‚úÖ Spark‰ºöËØùÂàõÂª∫ÂÆåÊàêÔºåÁâàÊú¨: 3.5.2-amzn-1
	MySQLÈÖçÁΩÆ: rm-3ns765y13i6wf0hp3.mysql.rds.aliyuncs.com:3358/biz_user
	üóÑÔ∏è  HiveMetaÂàùÂßãÂåñÂÆåÊàêÔºåÂ∞Ü‰ΩøÁî®ÂàÜÂå∫: 2025-08-19
	üîó JDBC URL: jdbc:mysql://rm-3ns765y13i6wf0hp3.mysql.rds.aliyuncs.com:3358/biz_user?useSSL=false&useUnicode=true&connectionCollation=utf8mb4_unicode_ci&autoReconnect=true&useCursorFetch=true&serverTimezone=UTC
	üîß ÂºÄÂßãMySQLËøûÊé•ÊµãËØï...
	üîç ÊµãËØïÁΩëÁªúËøûÈÄöÊÄß: rm-3ns765y13i6wf0hp3.mysql.rds.aliyuncs.com:3358
[INFO] 2025-08-19 10:29:11.025 +0000 -  ->
	‚ùå ÁΩëÁªúËøûÊé•Â§±Ë¥•: rm-3ns765y13i6wf0hp3.mysql.rds.aliyuncs.com:3358 (ÈîôËØØÁ†Å: 11)
	   ÂèØËÉΩÂéüÂõ†: 1) ÂÆâÂÖ®ÁªÑÈôêÂà∂ 2) Èò≤ÁÅ´Â¢ôÈôêÂà∂ 3) ÁΩëÁªú‰∏çÈÄö
	‚ùå MySQLËøûÊé•ÊµãËØïÂ§±Ë¥•Ôºå‰ΩÜÁªßÁª≠ÂàùÂßãÂåñ
	üóÑÔ∏è  MysqlMetaÂàùÂßãÂåñÂÆåÊàê
	üîç TagRuleParserÂàùÂßãÂåñÂÆåÊàê
	üöÄ TagEngineÂàùÂßãÂåñÂÆåÊàê

	üéØ ÊâßË°åÊåáÂÆöÊ†áÁ≠æËÆ°ÁÆó: [1, 2, 3, 4, 5]
	üöÄ ÂºÄÂßãÊ†áÁ≠æËÆ°ÁÆóÔºåÊ®°Âºè: task-tags
	üìã Âä†ËΩΩÊ†áÁ≠æËßÑÂàô...
	üìã Âä†ËΩΩÊ†áÁ≠æËßÑÂàôÔºåÊåáÂÆöÊ†áÁ≠æ: [1, 2, 3, 4, 5]
[INFO] 2025-08-19 10:35:57.068 +0000 -  ->
	‚ùå Âä†ËΩΩÊ†áÁ≠æËßÑÂàôÂ§±Ë¥•: An error occurred while calling o262.load.
	: java.sql.SQLNonTransientConnectionException: Could not create connection to database server. Attempted reconnect 3 times. Giving up.
		at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:111)
		at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:98)
		at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:90)
		at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:64)
		at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:74)
		at com.mysql.cj.jdbc.ConnectionImpl.connectWithRetries(ConnectionImpl.java:895)
		at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:820)
		at com.mysql.cj.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:446)
		at com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:239)
		at com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:188)
		at org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)
		at org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)
		at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)
		at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:65)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:60)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:345)
		at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:272)
		at org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:210)
		at scala.Option.getOrElse(Option.scala:189)
		at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
		at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.base/java.lang.Thread.run(Thread.java:840)
	Caused by: com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure

	The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
		at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
		at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
		at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
		at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
		at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:62)
		at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:105)
		at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:150)
		at com.mysql.cj.exceptions.ExceptionFactory.createCommunicationsException(ExceptionFactory.java:166)
		at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:89)
		at com.mysql.cj.NativeSession.connect(NativeSession.java:121)
		at com.mysql.cj.jdbc.ConnectionImpl.connectWithRetries(ConnectionImpl.java:839)
		... 31 more
	Caused by: java.net.ConnectException: Connection timed out
		at java.base/sun.nio.ch.Net.connect0(Native Method)
		at java.base/sun.nio.ch.Net.connect(Net.java:579)
		at java.base/sun.nio.ch.Net.connect(Net.java:568)
		at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)
		at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
		at java.base/java.net.Socket.connect(Socket.java:633)
		at com.mysql.cj.protocol.StandardSocketFactory.connect(StandardSocketFactory.java:153)
		at com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:63)
		... 33 more

[INFO] 2025-08-19 10:36:03.069 +0000 -  ->
	25/08/19 10:36:02 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (ip-172-24-15-124.ap-southeast-1.compute.internal executor 101): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1755501357359_1140/container_e06_1755501357359_1140_01_000165/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	25/08/19 10:36:02 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job
	‚ùå Ê†áÁ≠æËÆ°ÁÆóÂºÇÂ∏∏: An error occurred while calling o279.count.
	: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6) (ip-172-24-15-124.ap-southeast-1.compute.internal executor 101): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1755501357359_1140/container_e06_1755501357359_1140_01_000165/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:
		at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3083)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3019)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3018)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3018)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1324)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1324)
		at scala.Option.foreach(Option.scala:407)
		at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1324)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3301)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3235)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3224)
		at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:175)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:97)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:75)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:59)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:290)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:289)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:583)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:545)
		at org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3662)
		at org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3661)
		at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4392)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:711)
		at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4390)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)
		at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)
		at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:157)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$10(SQLExecution.scala:220)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)
		at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:220)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:405)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:219)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:83)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)
		at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4390)
		at org.apache.spark.sql.Dataset.count(Dataset.scala:3661)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.base/java.lang.Thread.run(Thread.java:840)
	Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1755501357359_1140/container_e06_1755501357359_1140_01_000165/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Traceback (most recent call last):
	  File "/tmp/spark-efb33bc3-d290-49ee-bc75-5c78da371320/bigdata_tag_system.zip/src/tag_engine/engine/TagEngine.py", line 62, in computeTags
	    if rulesDF.count() == 0:
	       ^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 1241, in count
	    return int(self._jdf.count())
	               ^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
	    return_value = get_return_value(
	                   ^^^^^^^^^^^^^^^^^
	  File "/mnt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
	    return f(*a, **kw)
	           ^^^^^^^^^^^
	  File "/mnt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
	    raise Py4JJavaError(
	py4j.protocol.Py4JJavaError: An error occurred while calling o279.count.
	: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6) (ip-172-24-15-124.ap-southeast-1.compute.internal executor 101): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1755501357359_1140/container_e06_1755501357359_1140_01_000165/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:
		at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3083)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3019)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3018)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3018)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1324)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1324)
		at scala.Option.foreach(Option.scala:407)
		at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1324)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3301)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3235)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3224)
		at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:175)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:97)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:75)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:59)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:290)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:289)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:583)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:545)
		at org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3662)
		at org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3661)
		at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4392)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:711)
		at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4390)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)
		at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)
		at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:157)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$10(SQLExecution.scala:220)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)
		at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:220)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:405)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:219)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:83)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)
		at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4390)
		at org.apache.spark.sql.Dataset.count(Dataset.scala:3661)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.base/java.lang.Thread.run(Thread.java:840)
	Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1755501357359_1140/container_e06_1755501357359_1140_01_000165/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	‚ö†Ô∏è  5 ‰∏™Ê†áÁ≠æÂõ†Ë°®Âä†ËΩΩÂ§±Ë¥•ËÄåË∑≥Ëøá: [1, 2, 3, 4, 5]

	============================================================
	‚ùå ‰ªªÂä°ÊâßË°åÂ§±Ë¥•
	============================================================
	üßπ HiveMetaÁºìÂ≠òÂ∑≤Ê∏ÖÁêÜ
	üßπ TagEngineËµÑÊ∫êÊ∏ÖÁêÜÂÆåÊàê
	üßπ ÂÖ≥Èó≠Spark‰ºöËØù...
[INFO] 2025-08-19 10:36:04.073 +0000 -  ->
	üëã Á®ãÂ∫èÈÄÄÂá∫
[INFO] 2025-08-19 10:36:04.073 +0000 - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/18540427537920/18540583925120_73/18099/108110, processId:4158829 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[INFO] 2025-08-19 10:36:04.074 +0000 - Start finding appId in /data/installed/dolphinscheduler/apache-dolphinscheduler-3.2.0-bin/worker-server/logs/20250819/18540583925120/73/18099/108110.log, fetch way: log
[INFO] 2025-08-19 10:36:04.074 +0000 - Find appId: application_1755501357359_1140 from /data/installed/dolphinscheduler/apache-dolphinscheduler-3.2.0-bin/worker-server/logs/20250819/18540583925120/73/18099/108110.log
[INFO] 2025-08-19 10:36:04.075 +0000 - ***********************************************************************************************
[INFO] 2025-08-19 10:36:04.075 +0000 - *********************************  Finalize task instance  ************************************
[INFO] 2025-08-19 10:36:04.075 +0000 - ***********************************************************************************************
[INFO] 2025-08-19 10:36:04.075 +0000 - Upload output files: [] successfully
[INFO] 2025-08-19 10:36:04.075 +0000 - Send task execute status: FAILURE to master : 172.24.14.168:1234
[INFO] 2025-08-19 10:36:04.075 +0000 - Remove the current task execute context from worker cache
[INFO] 2025-08-19 10:36:04.075 +0000 - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/18540427537920/18540583925120_73/18099/108110
[INFO] 2025-08-19 10:36:04.076 +0000 - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/18540427537920/18540583925120_73/18099/108110
[INFO] 2025-08-19 10:36:04.076 +0000 - FINALIZE_SESSION
