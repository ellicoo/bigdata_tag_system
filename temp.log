[LOG-PATH]: /usr/local/installed/dolphinscheduler/apache-dolphinscheduler-3.2.0-bin/worker-server/logs/20250728/18466477060960/8/885/3440.log, [HOST]:  Host(ip=172.31.9.77, port=1234)
[INFO] 2025-07-28 16:13:58.551 +0000 - ***********************************************************************************************
[INFO] 2025-07-28 16:13:58.552 +0000 - *********************************  Initialize task context  ***********************************
[INFO] 2025-07-28 16:13:58.552 +0000 - ***********************************************************************************************
[INFO] 2025-07-28 16:13:58.552 +0000 - Begin to initialize task
[INFO] 2025-07-28 16:13:58.553 +0000 - Set task startTime: 1753719238553
[INFO] 2025-07-28 16:13:58.553 +0000 - Set task appId: 885_3440
[INFO] 2025-07-28 16:13:58.553 +0000 - End initialize task {
  "taskInstanceId" : 3440,
  "taskName" : "full_compute_tags",
  "firstSubmitTime" : 1753719238528,
  "startTime" : 1753719238553,
  "taskType" : "SPARK",
  "workflowInstanceHost" : "172.31.9.77:5678",
  "host" : "172.31.9.77:1234",
  "logPath" : "/usr/local/installed/dolphinscheduler/apache-dolphinscheduler-3.2.0-bin/worker-server/logs/20250728/18466477060960/8/885/3440.log",
  "processId" : 0,
  "processDefineCode" : 18466477060960,
  "processDefineVersion" : 8,
  "processInstanceId" : 885,
  "scheduleTime" : 0,
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 18466461502048,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"\",\"resourceList\":[],\"programType\":\"PYTHON\",\"mainClass\":\"\",\"mainJar\":{\"id\":-1,\"resourceName\":\"file:/dolphinscheduler/default/resources/bigdata_tag_system/main.py\",\"res\":null},\"deployMode\":\"local\",\"mainArgs\":\"--mode task-all\",\"others\":\"--jars /dolphinscheduler/default/resources/mysql-connector-j-8.0.33.jar\",\"yarnQueue\":\"\",\"driverCores\":1,\"driverMemory\":\"512M\",\"numExecutors\":2,\"executorMemory\":\"2G\",\"executorCores\":2,\"sqlExecutionType\":\"SCRIPT\"}",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "full_compute_tags"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "18466461502048"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "885"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20250728"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20250727"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3440"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "pyspark_tag_system"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "18466551838433"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "18466477060960"
    },
    "StartNodeList" : {
      "prop" : "StartNodeList",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "18466551838433"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20250728161358"
    }
  },
  "taskAppId" : "885_3440",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "resources" : {
    "file:/dolphinscheduler/default/resources/bigdata_tag_system/main.py" : ""
  },
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[INFO] 2025-07-28 16:13:58.553 +0000 - ***********************************************************************************************
[INFO] 2025-07-28 16:13:58.553 +0000 - *********************************  Load task instance plugin  *********************************
[INFO] 2025-07-28 16:13:58.553 +0000 - ***********************************************************************************************
[INFO] 2025-07-28 16:13:58.553 +0000 - Send task status RUNNING_EXECUTION master: 172.31.9.77:1234
[WARN] 2025-07-28 16:13:58.553 +0000 - Current tenant is default tenant, will use root to execute the task
[INFO] 2025-07-28 16:13:58.553 +0000 - TenantCode: default check successfully
[INFO] 2025-07-28 16:13:58.554 +0000 - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_8/885/3440 check successfully
[INFO] 2025-07-28 16:13:58.554 +0000 - get resource file from path:file:/dolphinscheduler/default/resources/bigdata_tag_system/main.py
[INFO] 2025-07-28 16:13:58.555 +0000 - Download resources: {file:/dolphinscheduler/default/resources/bigdata_tag_system/main.py=file:/dolphinscheduler/default/resources/bigdata_tag_system/main.py} successfully
[INFO] 2025-07-28 16:13:58.555 +0000 - Download upstream files: [] successfully
[INFO] 2025-07-28 16:13:58.555 +0000 - Task plugin instance: SPARK create successfully
[INFO] 2025-07-28 16:13:58.555 +0000 - Initialize spark task params {
  "localParams" : [ ],
  "varPool" : null,
  "mainJar" : {
    "id" : -1,
    "resourceName" : "file:/dolphinscheduler/default/resources/bigdata_tag_system/main.py",
    "res" : null
  },
  "mainClass" : "",
  "deployMode" : "local",
  "mainArgs" : "--mode task-all",
  "driverCores" : 1,
  "driverMemory" : "512M",
  "numExecutors" : 2,
  "executorCores" : 2,
  "executorMemory" : "2G",
  "appName" : null,
  "yarnQueue" : "",
  "others" : "--jars /dolphinscheduler/default/resources/mysql-connector-j-8.0.33.jar",
  "programType" : "PYTHON",
  "rawScript" : "",
  "namespace" : null,
  "resourceList" : [ ],
  "sqlExecutionType" : "SCRIPT"
}
[INFO] 2025-07-28 16:13:58.556 +0000 - Success initialized task plugin instance successfully
[INFO] 2025-07-28 16:13:58.556 +0000 - Set taskVarPool: null successfully
[INFO] 2025-07-28 16:13:58.556 +0000 - ***********************************************************************************************
[INFO] 2025-07-28 16:13:58.556 +0000 - *********************************  Execute task instance  *************************************
[INFO] 2025-07-28 16:13:58.556 +0000 - ***********************************************************************************************
[INFO] 2025-07-28 16:13:58.556 +0000 - Final Shell file is :
#!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
${SPARK_HOME}/bin/spark-submit --master local --conf spark.driver.cores=1 --conf spark.driver.memory=512M --conf spark.executor.instances=2 --conf spark.executor.cores=2 --conf spark.executor.memory=2G --jars /dolphinscheduler/default/resources/mysql-connector-j-8.0.33.jar file:/dolphinscheduler/default/resources/bigdata_tag_system/main.py --mode task-all
[INFO] 2025-07-28 16:13:58.557 +0000 - Executing shell command : sudo -u root -i /tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_8/885/3440/885_3440.sh
[INFO] 2025-07-28 16:13:58.559 +0000 - process start, process id is: 744882
[INFO] 2025-07-28 16:14:00.560 +0000 -  ->
	25/07/28 16:13:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2025-07-28 16:14:01.561 +0000 -  ->
	============================================================
	🏷️  大数据标签计算系统
	============================================================
	执行模式: task-all
	当前工作目录: /tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_8/885/3440
	脚本目录: /dolphinscheduler/default/resources/bigdata_tag_system
	项目根目录: /dolphinscheduler/default
	Python路径前3项: ['/dolphinscheduler/default', '/dolphinscheduler/default/resources/bigdata_tag_system', '/mnt/spark/python/lib/pyspark.zip']
	🚀 创建Spark会话: TagComputeEngine
	25/07/28 16:14:00 INFO EMRParamSideChannel: Setting FGAC mode to false
	25/07/28 16:14:00 INFO SparkContext: Running Spark version 3.5.2-amzn-1
	25/07/28 16:14:00 INFO SparkContext: OS info Linux, 6.8.0-1029-aws, amd64
	25/07/28 16:14:00 INFO SparkContext: Java version 17.0.15
	25/07/28 16:14:00 INFO ResourceUtils: ==============================================================
	25/07/28 16:14:00 INFO ResourceUtils: No custom resources configured for spark.driver.
	25/07/28 16:14:00 INFO ResourceUtils: ==============================================================
	25/07/28 16:14:00 INFO SparkContext: Submitted application: TagComputeEngine
	25/07/28 16:14:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(executorType -> name: executorType, amount: 1, script: , vendor: , cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	25/07/28 16:14:00 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor
	25/07/28 16:14:00 INFO ResourceProfileManager: Added ResourceProfile id: 0
	25/07/28 16:14:00 INFO ResourceProfile: User executor ResourceProfile created, executor resources: Map(executorType -> name: executorType, amount: 1, script: , vendor: , cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	25/07/28 16:14:00 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor
	25/07/28 16:14:00 INFO ResourceProfileManager: Added ResourceProfile id: 1
	25/07/28 16:14:00 INFO SecurityManager: Changing view acls to: root
	25/07/28 16:14:00 INFO SecurityManager: Changing modify acls to: root
	25/07/28 16:14:00 INFO SecurityManager: Changing view acls groups to:
	25/07/28 16:14:00 INFO SecurityManager: Changing modify acls groups to:
	25/07/28 16:14:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
	25/07/28 16:14:00 INFO Utils: Successfully started service 'sparkDriver' on port 4791.
	25/07/28 16:14:00 INFO SparkEnv: Registering MapOutputTracker
	25/07/28 16:14:01 INFO SparkEnv: Registering BlockManagerMaster
	25/07/28 16:14:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	25/07/28 16:14:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	25/07/28 16:14:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	25/07/28 16:14:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c5eb0331-88a9-4048-9034-ee9a40407025
	25/07/28 16:14:01 INFO MemoryStore: MemoryStore started with capacity 127.2 MiB
	25/07/28 16:14:01 INFO SparkEnv: Registering OutputCommitCoordinator
	25/07/28 16:14:01 INFO SubResultCacheManager: Sub-result caches are disabled.
	25/07/28 16:14:01 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	25/07/28 16:14:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	25/07/28 16:14:01 INFO SparkContext: Added JAR file:///dolphinscheduler/default/resources/mysql-connector-j-8.0.33.jar at spark://ip-172-31-9-77.ap-southeast-1.compute.internal:4791/jars/mysql-connector-j-8.0.33.jar with timestamp 1753719240754
	25/07/28 16:14:01 INFO Executor: Starting executor ID driver on host ip-172-31-9-77.ap-southeast-1.compute.internal
	25/07/28 16:14:01 INFO Executor: OS info Linux, 6.8.0-1029-aws, amd64
	25/07/28 16:14:01 INFO Executor: Java version 17.0.15
	25/07/28 16:14:01 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/usr/lib/hadoop-lzo/lib/*,file:/usr/lib/hadoop/hadoop-aws.jar,file:/usr/share/aws/aws-java-sdk/*,file:/usr/share/aws/aws-java-sdk-v2/*,file:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar,file:/usr/share/aws/emr/security/conf,file:/usr/share/aws/emr/security/lib/*,file:/usr/share/aws/redshift/jdbc/*,file:/usr/share/aws/redshift/spark-redshift/lib/*,file:/usr/share/aws/kinesis/spark-sql-kinesis/lib/*,file:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar,file:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar,file:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar,file:/docker/usr/lib/hadoop-lzo/lib/*,file:/docker/usr/lib/hadoop/hadoop-aws.jar,file:/docker/usr/share/aws/aws-java-sdk/*,file:/docker/usr/share/aws/aws-java-sdk-v2/*,file:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar,file:/docker/usr/share/aws/emr/security/conf,file:/docker/usr/share/aws/emr/security/lib/*,file:/docker/usr/share/aws/redshift/jdbc/*,file:/docker/usr/share/aws/redshift/spark-redshift/lib/*,file:/docker/usr/share/aws/kinesis/spark-sql-kinesis/lib/*,file:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar,file:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar,file:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar,file:/tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_8/885/3440/conf,file:/tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_8/885/3440/hive-openx-serde.jar,file:/tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_8/885/3440/*,file:/tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_8/885/3440/emr-spark-goodies.jar,file:/tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_8/885/3440/hadoop-aws.jar,file:/tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_8/885/3440/aws-glue-datacatalog-spark-client.jar,file:/tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_8/885/3440/emr-s3-select-spark-connector.jar'
	25/07/28 16:14:01 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1bdc1a72 for default.
	25/07/28 16:14:01 INFO Executor: Fetching spark://ip-172-31-9-77.ap-southeast-1.compute.internal:4791/jars/mysql-connector-j-8.0.33.jar with timestamp 1753719240754
	25/07/28 16:14:01 INFO TransportClientFactory: Successfully created connection to ip-172-31-9-77.ap-southeast-1.compute.internal/172.31.9.77:4791 after 16 ms (0 ms spent in bootstraps)
	25/07/28 16:14:01 INFO Utils: Fetching spark://ip-172-31-9-77.ap-southeast-1.compute.internal:4791/jars/mysql-connector-j-8.0.33.jar to /tmp/spark-eec8929b-95eb-4ffa-90c1-098ea267ea7c/userFiles-e473efdc-1f47-4d93-8bc6-c6c5d274b156/fetchFileTemp3421525033606839988.tmp
	25/07/28 16:14:01 INFO Executor: Adding file:/tmp/spark-eec8929b-95eb-4ffa-90c1-098ea267ea7c/userFiles-e473efdc-1f47-4d93-8bc6-c6c5d274b156/mysql-connector-j-8.0.33.jar to class loader default
	25/07/28 16:14:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 23579.
	25/07/28 16:14:01 INFO NettyBlockTransferService: Server created on ip-172-31-9-77.ap-southeast-1.compute.internal:23579
	25/07/28 16:14:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	25/07/28 16:14:01 INFO BlockManager: external shuffle service port = 7337
	25/07/28 16:14:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-9-77.ap-southeast-1.compute.internal, 23579, None)
	25/07/28 16:14:01 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-9-77.ap-southeast-1.compute.internal:23579 with 127.2 MiB RAM, BlockManagerId(driver, ip-172-31-9-77.ap-southeast-1.compute.internal, 23579, None)
	25/07/28 16:14:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-9-77.ap-southeast-1.compute.internal, 23579, None)
	25/07/28 16:14:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-9-77.ap-southeast-1.compute.internal, 23579, None)
[INFO] 2025-07-28 16:14:02.562 +0000 -  ->
	25/07/28 16:14:01 INFO SingleEventLogFileWriter: Logging events to hdfs:/var/log/spark/apps/local-1753719241252.inprogress
	✅ Spark会话创建完成，版本: 3.5.2-amzn-1
	MySQL配置: cex-mysql-test.c5mgk4qm8m2z.ap-southeast-1.rds.amazonaws.com:3358/biz_statistics
	🗄️  HiveMeta初始化完成
	🗄️  MysqlMeta初始化完成
	🔍 TagRuleParser初始化完成
	🚀 TagEngine初始化完成

	🚀 执行全量标签计算...
	🚀 开始标签计算，模式: task-all
	📋 加载标签规则...
	📋 加载标签规则，指定标签: None
[INFO] 2025-07-28 16:14:05.562 +0000 -  ->
	✅ 标签规则加载完成: 50 个标签
	🎯 分析标签依赖关系...
	🔍 分析标签规则依赖关系...
	   📋 标签 1: 依赖表 []
	   📋 标签 2: 依赖表 []
	   📋 标签 3: 依赖表 []
	   📋 标签 4: 依赖表 []
	   📋 标签 5: 依赖表 []
	   📋 标签 6: 依赖表 []
	   📋 标签 7: 依赖表 []
	   📋 标签 8: 依赖表 []
	   📋 标签 9: 依赖表 []
	   📋 标签 10: 依赖表 []
	   📋 标签 11: 依赖表 []
	   📋 标签 12: 依赖表 []
	   📋 标签 13: 依赖表 []
	   📋 标签 14: 依赖表 []
	   📋 标签 15: 依赖表 []
	   📋 标签 16: 依赖表 []
	   📋 标签 17: 依赖表 []
	   📋 标签 18: 依赖表 []
	   📋 标签 19: 依赖表 []
	   📋 标签 20: 依赖表 []
	   📋 标签 21: 依赖表 []
	   📋 标签 22: 依赖表 []
	   📋 标签 23: 依赖表 []
	   📋 标签 24: 依赖表 []
	   📋 标签 25: 依赖表 []
	   📋 标签 26: 依赖表 []
	   📋 标签 27: 依赖表 []
	   📋 标签 28: 依赖表 []
	   📋 标签 29: 依赖表 []
	   📋 标签 30: 依赖表 []
	   📋 标签 31: 依赖表 []
	   📋 标签 32: 依赖表 []
	   📋 标签 33: 依赖表 []
	   📋 标签 34: 依赖表 []
	   📋 标签 35: 依赖表 []
	   📋 标签 36: 依赖表 []
	   📋 标签 37: 依赖表 []
	   📋 标签 38: 依赖表 []
	   📋 标签 39: 依赖表 []
	   📋 标签 40: 依赖表 []
	   📋 标签 41: 依赖表 []
	   📋 标签 42: 依赖表 []
	   📋 标签 43: 依赖表 []
	   📋 标签 44: 依赖表 []
	   📋 标签 45: 依赖表 []
	   📋 标签 46: 依赖表 []
	   📋 标签 47: 依赖表 []
	   📋 标签 48: 依赖表 []
	   📋 标签 49: 依赖表 []
	   📋 标签 50: 依赖表 []
	✅ 依赖分析完成: 50 个标签
	🎯 智能分组标签...
	📦 创建标签组: Group_50tags_0tables
	   🏷️  标签: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]
	   📊 依赖表: []
	   🏷️  组 1: 标签[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50] → 表[]
	✅ 分组完成: 1 个计算组
	🚀 并行计算 1 个标签组...
	   📦 计算标签组 1/1: Group_50tags_0tables
	🚀 开始计算标签组: Group_50tags_0tables
	   📋 该组标签规则数: 50
	🔍 TagRuleParser初始化完成
	🔍 分析字段依赖关系...
	✅ 字段依赖分析完成: 0 个表
[INFO] 2025-07-28 16:14:06.563 +0000 -  ->
	   🔗 JOIN完成，用户数: 0
	   🎯 并行计算 50 个标签...
	      🏷️  计算标签 1...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 1: 0 个用户
	      🏷️  计算标签 2...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 2: 0 个用户
	      🏷️  计算标签 3...
	🔍 TagRuleParser初始化完成
[INFO] 2025-07-28 16:14:07.563 +0000 -  ->
	         ✅ 标签 3: 0 个用户
	      🏷️  计算标签 4...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 4: 0 个用户
	      🏷️  计算标签 5...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 5: 0 个用户
	      🏷️  计算标签 6...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 6: 0 个用户
	      🏷️  计算标签 7...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 7: 0 个用户
	      🏷️  计算标签 8...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 8: 0 个用户
	      🏷️  计算标签 9...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 9: 0 个用户
	      🏷️  计算标签 10...
	🔍 TagRuleParser初始化完成
[INFO] 2025-07-28 16:14:08.563 +0000 -  ->
	         ✅ 标签 10: 0 个用户
	      🏷️  计算标签 11...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 11: 0 个用户
	      🏷️  计算标签 12...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 12: 0 个用户
	      🏷️  计算标签 13...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 13: 0 个用户
	      🏷️  计算标签 14...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 14: 0 个用户
	      🏷️  计算标签 15...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 15: 0 个用户
	      🏷️  计算标签 16...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 16: 0 个用户
	      🏷️  计算标签 17...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 17: 0 个用户
	      🏷️  计算标签 18...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 18: 0 个用户
	      🏷️  计算标签 19...
	🔍 TagRuleParser初始化完成
[INFO] 2025-07-28 16:14:09.564 +0000 -  ->
	         ✅ 标签 19: 0 个用户
	      🏷️  计算标签 20...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 20: 0 个用户
	      🏷️  计算标签 21...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 21: 0 个用户
	      🏷️  计算标签 22...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 22: 0 个用户
	      🏷️  计算标签 23...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 23: 0 个用户
	      🏷️  计算标签 24...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 24: 0 个用户
	      🏷️  计算标签 25...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 25: 0 个用户
	      🏷️  计算标签 26...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 26: 0 个用户
	      🏷️  计算标签 27...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 27: 0 个用户
	      🏷️  计算标签 28...
	🔍 TagRuleParser初始化完成
[INFO] 2025-07-28 16:14:10.564 +0000 -  ->
	         ✅ 标签 28: 0 个用户
	      🏷️  计算标签 29...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 29: 0 个用户
	      🏷️  计算标签 30...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 30: 0 个用户
	      🏷️  计算标签 31...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 31: 0 个用户
	      🏷️  计算标签 32...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 32: 0 个用户
	      🏷️  计算标签 33...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 33: 0 个用户
	      🏷️  计算标签 34...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 34: 0 个用户
	      🏷️  计算标签 35...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 35: 0 个用户
	      🏷️  计算标签 36...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 36: 0 个用户
	      🏷️  计算标签 37...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 37: 0 个用户
	      🏷️  计算标签 38...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 38: 0 个用户
	      🏷️  计算标签 39...
	🔍 TagRuleParser初始化完成
[INFO] 2025-07-28 16:14:11.564 +0000 -  ->
	         ✅ 标签 39: 0 个用户
	      🏷️  计算标签 40...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 40: 0 个用户
	      🏷️  计算标签 41...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 41: 0 个用户
	      🏷️  计算标签 42...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 42: 0 个用户
	      🏷️  计算标签 43...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 43: 0 个用户
	      🏷️  计算标签 44...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 44: 0 个用户
	      🏷️  计算标签 45...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 45: 0 个用户
	      🏷️  计算标签 46...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 46: 0 个用户
	      🏷️  计算标签 47...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 47: 0 个用户
	      🏷️  计算标签 48...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 48: 0 个用户
	      🏷️  计算标签 49...
	🔍 TagRuleParser初始化完成
[INFO] 2025-07-28 16:14:12.565 +0000 -  ->
	         ✅ 标签 49: 0 个用户
	      🏷️  计算标签 50...
	🔍 TagRuleParser初始化完成
	         ✅ 标签 50: 0 个用户
	   🔀 聚合用户标签...
[INFO] 2025-07-28 16:14:16.565 +0000 -  ->
	   ✅ 用户标签聚合完成: 0 个用户
[INFO] 2025-07-28 16:14:19.566 +0000 -  ->
	✅ 标签组计算完成: 0 个用户
	✅ 所有标签组计算完成，成功: 1 个
	🔀 合并所有标签组结果...
[INFO] 2025-07-28 16:14:22.566 +0000 -  ->
	   ⚠️  所有标签组结果都为空
	💾 与现有标签合并并保存...
	📖 加载现有用户标签数据...
	✅ 现有标签数据加载完成: 0 个用户
	💾 开始写入标签结果到MySQL...
	⚠️  没有结果需要写入
[INFO] 2025-07-28 16:14:23.566 +0000 -  ->
	   ✅ 标签结果保存成功: 0 个用户
	✅ 标签计算完成

	📊 标签系统统计信息:
	   活跃标签数: 50
	   有标签用户数: 0
	   总标签数: 0
	   平均每用户标签数: 0

	============================================================
	✅ 任务执行成功
	============================================================
	🧹 HiveMeta缓存已清理
	🧹 TagEngine资源清理完成
	🧹 关闭Spark会话...
[INFO] 2025-07-28 16:14:24.567 +0000 -  ->
	👋 程序退出
	🧹 HiveMeta缓存已清理
	🧹 TagEngine资源清理完成
[INFO] 2025-07-28 16:14:24.567 +0000 - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_8/885/3440, processId:744882 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[INFO] 2025-07-28 16:14:24.568 +0000 - Start finding appId in /usr/local/installed/dolphinscheduler/apache-dolphinscheduler-3.2.0-bin/worker-server/logs/20250728/18466477060960/8/885/3440.log, fetch way: log
[INFO] 2025-07-28 16:14:24.568 +0000 - ***********************************************************************************************
[INFO] 2025-07-28 16:14:24.568 +0000 - *********************************  Finalize task instance  ************************************
[INFO] 2025-07-28 16:14:24.568 +0000 - ***********************************************************************************************
[INFO] 2025-07-28 16:14:24.569 +0000 - Upload output files: [] successfully
[INFO] 2025-07-28 16:14:24.569 +0000 - Send task execute status: SUCCESS to master : 172.31.9.77:1234
[INFO] 2025-07-28 16:14:24.569 +0000 - Remove the current task execute context from worker cache
[INFO] 2025-07-28 16:14:24.569 +0000 - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_8/885/3440
[INFO] 2025-07-28 16:14:24.570 +0000 - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/18466461502048/18466477060960_8/885/3440
[INFO] 2025-07-28 16:14:24.570 +0000 - FINALIZE_SESSION
