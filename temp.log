LOG-PATH]: /data/installed/dolphinscheduler/apache-dolphinscheduler-3.2.0-bin/worker-server/logs/20250813/18540583925120/38/16574/97748.log, [HOST]:  Host(ip=172.24.14.168, port=1234)
[INFO] 2025-08-13 01:45:17.285 +0000 - ***********************************************************************************************
[INFO] 2025-08-13 01:45:17.286 +0000 - *********************************  Initialize task context  ***********************************
[INFO] 2025-08-13 01:45:17.286 +0000 - ***********************************************************************************************
[INFO] 2025-08-13 01:45:17.286 +0000 - Begin to initialize task
[INFO] 2025-08-13 01:45:17.286 +0000 - Set task startTime: 1755049517286
[INFO] 2025-08-13 01:45:17.286 +0000 - Set task appId: 16574_97748
[INFO] 2025-08-13 01:45:17.286 +0000 - End initialize task {
  "taskInstanceId" : 97748,
  "taskName" : "health_check",
  "firstSubmitTime" : 1755049517256,
  "startTime" : 1755049517286,
  "taskType" : "SPARK",
  "workflowInstanceHost" : "172.24.14.90:5678",
  "host" : "172.24.14.168:1234",
  "logPath" : "/data/installed/dolphinscheduler/apache-dolphinscheduler-3.2.0-bin/worker-server/logs/20250813/18540583925120/38/16574/97748.log",
  "processId" : 0,
  "processDefineCode" : 18540583925120,
  "processDefineVersion" : 38,
  "processInstanceId" : 16574,
  "scheduleTime" : 0,
  "executorId" : 6,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 18540427537920,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"\",\"resourceList\":[{\"id\":-1,\"resourceName\":\"dolphinscheduler/default/resources/bigdata_tag_system/create_test_tables.sql\",\"res\":null},{\"id\":-1,\"resourceName\":\"dolphinscheduler/default/resources/bigdata_tag_system/generate_test_data.py\",\"res\":null},{\"id\":-1,\"resourceName\":\"dolphinscheduler/default/resources/bigdata_tag_system/requirements.txt\",\"res\":null},{\"id\":-1,\"resourceName\":\"dolphinscheduler/default/resources/bigdata_tag_system/src/__init__.py\",\"res\":null},{\"id\":-1,\"resourceName\":\"dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/__init__.py\",\"res\":null},{\"id\":-1,\"resourceName\":\"dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/main.py\",\"res\":null},{\"id\":-1,\"resourceName\":\"dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagEngine.py\",\"res\":null},{\"id\":-1,\"resourceName\":\"dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py\",\"res\":null},{\"id\":-1,\"resourceName\":\"dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/__init__.py\",\"res\":null},{\"id\":-1,\"resourceName\":\"dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/meta/HiveMeta.py\",\"res\":null},{\"id\":-1,\"resourceName\":\"dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/meta/MysqlMeta.py\",\"res\":null},{\"id\":-1,\"resourceName\":\"dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/meta/__init__.py\",\"res\":null},{\"id\":-1,\"resourceName\":\"dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/parser/TagRuleParser.py\",\"res\":null},{\"id\":-1,\"resourceName\":\"dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/parser/__init__.py\",\"res\":null},{\"id\":-1,\"resourceName\":\"dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/utils/SparkUdfs.py\",\"res\":null},{\"id\":-1,\"resourceName\":\"dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/utils/__init__.py\",\"res\":null},{\"id\":-1,\"resourceName\":\"dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/utils/tagExpressionUtils.py\",\"res\":null}],\"programType\":\"PYTHON\",\"mainClass\":\"\",\"mainJar\":{\"id\":-1,\"resourceName\":\"dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/main.py\",\"res\":null},\"deployMode\":\"client\",\"mainArgs\":\"--mode health\",\"others\":\"--jars s3://exchanges-flink-prod/dolphinscheduler/default/resources/connectors/mysql-connector-j-8.0.33.jar\",\"yarnQueue\":\"\",\"driverCores\":1,\"driverMemory\":\"512M\",\"numExecutors\":2,\"executorMemory\":\"2G\",\"executorCores\":2,\"sqlExecutionType\":\"SCRIPT\"}",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "health_check"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "18540427537920"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "16574"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20250813"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20250812"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "97748"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "pyspark_tag_system"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "18639041774720"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "18540583925120"
    },
    "StartNodeList" : {
      "prop" : "StartNodeList",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "18639041774720"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20250813014517"
    }
  },
  "taskAppId" : "16574_97748",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "resources" : {
    "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagEngine.py" : "",
    "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/parser/TagRuleParser.py" : "",
    "dolphinscheduler/default/resources/bigdata_tag_system/src/__init__.py" : "",
    "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/__init__.py" : "",
    "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/parser/__init__.py" : "",
    "dolphinscheduler/default/resources/bigdata_tag_system/generate_test_data.py" : "",
    "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/utils/__init__.py" : "",
    "dolphinscheduler/default/resources/bigdata_tag_system/create_test_tables.sql" : "",
    "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/main.py" : "",
    "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/utils/tagExpressionUtils.py" : "",
    "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/meta/MysqlMeta.py" : "",
    "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/utils/SparkUdfs.py" : "",
    "dolphinscheduler/default/resources/bigdata_tag_system/requirements.txt" : "",
    "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py" : "",
    "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/meta/HiveMeta.py" : "",
    "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/__init__.py" : "",
    "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/meta/__init__.py" : ""
  },
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[INFO] 2025-08-13 01:45:17.287 +0000 - ***********************************************************************************************
[INFO] 2025-08-13 01:45:17.287 +0000 - *********************************  Load task instance plugin  *********************************
[INFO] 2025-08-13 01:45:17.287 +0000 - ***********************************************************************************************
[INFO] 2025-08-13 01:45:17.287 +0000 - Send task status RUNNING_EXECUTION master: 172.24.14.168:1234
[WARN] 2025-08-13 01:45:17.287 +0000 - Current tenant is default tenant, will use root to execute the task
[INFO] 2025-08-13 01:45:17.287 +0000 - TenantCode: default check successfully
[INFO] 2025-08-13 01:45:17.287 +0000 - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/18540427537920/18540583925120_38/16574/97748 check successfully
[INFO] 2025-08-13 01:45:17.287 +0000 - get resource file from path:dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagEngine.py
[INFO] 2025-08-13 01:45:17.348 +0000 - get resource file from path:dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/parser/TagRuleParser.py
[INFO] 2025-08-13 01:45:17.372 +0000 - get resource file from path:dolphinscheduler/default/resources/bigdata_tag_system/src/__init__.py
[INFO] 2025-08-13 01:45:17.385 +0000 - get resource file from path:dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/__init__.py
[INFO] 2025-08-13 01:45:17.431 +0000 - get resource file from path:dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/parser/__init__.py
[INFO] 2025-08-13 01:45:17.461 +0000 - get resource file from path:dolphinscheduler/default/resources/bigdata_tag_system/generate_test_data.py
[INFO] 2025-08-13 01:45:17.479 +0000 - get resource file from path:dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/utils/__init__.py
[INFO] 2025-08-13 01:45:17.513 +0000 - get resource file from path:dolphinscheduler/default/resources/bigdata_tag_system/create_test_tables.sql
[INFO] 2025-08-13 01:45:17.536 +0000 - get resource file from path:dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/main.py
[INFO] 2025-08-13 01:45:17.565 +0000 - get resource file from path:dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/utils/tagExpressionUtils.py
[INFO] 2025-08-13 01:45:17.589 +0000 - get resource file from path:dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/meta/MysqlMeta.py
[INFO] 2025-08-13 01:45:17.620 +0000 - get resource file from path:dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/utils/SparkUdfs.py
[INFO] 2025-08-13 01:45:17.652 +0000 - get resource file from path:dolphinscheduler/default/resources/bigdata_tag_system/requirements.txt
[INFO] 2025-08-13 01:45:17.674 +0000 - get resource file from path:dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py
[INFO] 2025-08-13 01:45:17.697 +0000 - get resource file from path:dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/meta/HiveMeta.py
[INFO] 2025-08-13 01:45:17.732 +0000 - get resource file from path:dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/__init__.py
[INFO] 2025-08-13 01:45:17.756 +0000 - get resource file from path:dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/meta/__init__.py
[INFO] 2025-08-13 01:45:17.796 +0000 - Download resources: {dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagEngine.py=dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagEngine.py, dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/parser/TagRuleParser.py=dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/parser/TagRuleParser.py, dolphinscheduler/default/resources/bigdata_tag_system/src/__init__.py=dolphinscheduler/default/resources/bigdata_tag_system/src/__init__.py, dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/__init__.py=dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/__init__.py, dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/parser/__init__.py=dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/parser/__init__.py, dolphinscheduler/default/resources/bigdata_tag_system/generate_test_data.py=dolphinscheduler/default/resources/bigdata_tag_system/generate_test_data.py, dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/utils/__init__.py=dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/utils/__init__.py, dolphinscheduler/default/resources/bigdata_tag_system/create_test_tables.sql=dolphinscheduler/default/resources/bigdata_tag_system/create_test_tables.sql, dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/main.py=dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/main.py, dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/utils/tagExpressionUtils.py=dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/utils/tagExpressionUtils.py, dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/meta/MysqlMeta.py=dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/meta/MysqlMeta.py, dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/utils/SparkUdfs.py=dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/utils/SparkUdfs.py, dolphinscheduler/default/resources/bigdata_tag_system/requirements.txt=dolphinscheduler/default/resources/bigdata_tag_system/requirements.txt, dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py=dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py, dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/meta/HiveMeta.py=dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/meta/HiveMeta.py, dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/__init__.py=dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/__init__.py, dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/meta/__init__.py=dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/meta/__init__.py} successfully
[INFO] 2025-08-13 01:45:17.796 +0000 - Download upstream files: [] successfully
[INFO] 2025-08-13 01:45:17.796 +0000 - Task plugin instance: SPARK create successfully
[INFO] 2025-08-13 01:45:17.796 +0000 - Initialize spark task params {
  "localParams" : [ ],
  "varPool" : null,
  "mainJar" : {
    "id" : -1,
    "resourceName" : "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/main.py",
    "res" : null
  },
  "mainClass" : "",
  "deployMode" : "client",
  "mainArgs" : "--mode health",
  "driverCores" : 1,
  "driverMemory" : "512M",
  "numExecutors" : 2,
  "executorCores" : 2,
  "executorMemory" : "2G",
  "appName" : null,
  "yarnQueue" : "",
  "others" : "--jars s3://exchanges-flink-prod/dolphinscheduler/default/resources/connectors/mysql-connector-j-8.0.33.jar",
  "programType" : "PYTHON",
  "rawScript" : "",
  "namespace" : null,
  "resourceList" : [ {
    "id" : -1,
    "resourceName" : "dolphinscheduler/default/resources/bigdata_tag_system/create_test_tables.sql",
    "res" : null
  }, {
    "id" : -1,
    "resourceName" : "dolphinscheduler/default/resources/bigdata_tag_system/generate_test_data.py",
    "res" : null
  }, {
    "id" : -1,
    "resourceName" : "dolphinscheduler/default/resources/bigdata_tag_system/requirements.txt",
    "res" : null
  }, {
    "id" : -1,
    "resourceName" : "dolphinscheduler/default/resources/bigdata_tag_system/src/__init__.py",
    "res" : null
  }, {
    "id" : -1,
    "resourceName" : "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/__init__.py",
    "res" : null
  }, {
    "id" : -1,
    "resourceName" : "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/main.py",
    "res" : null
  }, {
    "id" : -1,
    "resourceName" : "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagEngine.py",
    "res" : null
  }, {
    "id" : -1,
    "resourceName" : "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/TagGroup.py",
    "res" : null
  }, {
    "id" : -1,
    "resourceName" : "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/engine/__init__.py",
    "res" : null
  }, {
    "id" : -1,
    "resourceName" : "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/meta/HiveMeta.py",
    "res" : null
  }, {
    "id" : -1,
    "resourceName" : "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/meta/MysqlMeta.py",
    "res" : null
  }, {
    "id" : -1,
    "resourceName" : "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/meta/__init__.py",
    "res" : null
  }, {
    "id" : -1,
    "resourceName" : "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/parser/TagRuleParser.py",
    "res" : null
  }, {
    "id" : -1,
    "resourceName" : "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/parser/__init__.py",
    "res" : null
  }, {
    "id" : -1,
    "resourceName" : "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/utils/SparkUdfs.py",
    "res" : null
  }, {
    "id" : -1,
    "resourceName" : "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/utils/__init__.py",
    "res" : null
  }, {
    "id" : -1,
    "resourceName" : "dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/utils/tagExpressionUtils.py",
    "res" : null
  } ],
  "sqlExecutionType" : "SCRIPT"
}
[INFO] 2025-08-13 01:45:17.797 +0000 - Success initialized task plugin instance successfully
[INFO] 2025-08-13 01:45:17.797 +0000 - Set taskVarPool: null successfully
[INFO] 2025-08-13 01:45:17.797 +0000 - ***********************************************************************************************
[INFO] 2025-08-13 01:45:17.797 +0000 - *********************************  Execute task instance  *************************************
[INFO] 2025-08-13 01:45:17.797 +0000 - ***********************************************************************************************
[INFO] 2025-08-13 01:45:17.797 +0000 - Final Shell file is :
#!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
${SPARK_HOME}/bin/spark-submit --master yarn --deploy-mode client --conf spark.driver.cores=1 --conf spark.driver.memory=512M --conf spark.executor.instances=2 --conf spark.executor.cores=2 --conf spark.executor.memory=2G --jars s3://exchanges-flink-prod/dolphinscheduler/default/resources/connectors/mysql-connector-j-8.0.33.jar dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine/main.py --mode health
[INFO] 2025-08-13 01:45:17.797 +0000 - Executing shell command : sudo -u root -i /tmp/dolphinscheduler/exec/process/default/18540427537920/18540583925120_38/16574/97748/16574_97748.sh
[INFO] 2025-08-13 01:45:17.799 +0000 - process start, process id is: 2457780
[INFO] 2025-08-13 01:45:19.800 +0000 -  ->
	25/08/13 01:45:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO] 2025-08-13 01:45:21.801 +0000 -  ->
	============================================================
	🏷️  大数据标签计算系统
	============================================================
	执行模式: health
	当前工作目录: /tmp/dolphinscheduler/exec/process/default/18540427537920/18540583925120_38/16574/97748
	脚本目录: /tmp/dolphinscheduler/exec/process/default/18540427537920/18540583925120_38/16574/97748/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine
	项目根目录: /tmp/dolphinscheduler/exec/process/default/18540427537920/18540583925120_38/16574/97748/dolphinscheduler/default/resources/bigdata_tag_system
	Python路径前3项: ['/tmp/dolphinscheduler/exec/process/default/18540427537920/18540583925120_38/16574/97748/dolphinscheduler/default/resources/bigdata_tag_system', '/tmp/dolphinscheduler/exec/process/default/18540427537920/18540583925120_38/16574/97748/dolphinscheduler/default/resources/bigdata_tag_system/src/tag_engine', '/mnt/spark/python/lib/pyspark.zip']
	🚀 创建Spark会话: TagComputeEngine
	25/08/13 01:45:21 INFO EMRParamSideChannel: Setting FGAC mode to false
	25/08/13 01:45:21 INFO SparkContext: Running Spark version 3.5.2-amzn-1
	25/08/13 01:45:21 INFO SparkContext: OS info Linux, 6.8.0-1030-aws, amd64
	25/08/13 01:45:21 INFO SparkContext: Java version 17.0.16
	25/08/13 01:45:21 INFO ResourceUtils: ==============================================================
	25/08/13 01:45:21 INFO ResourceUtils: No custom resources configured for spark.driver.
	25/08/13 01:45:21 INFO ResourceUtils: ==============================================================
	25/08/13 01:45:21 INFO SparkContext: Submitted application: TagComputeEngine
	25/08/13 01:45:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(executorType -> name: executorType, amount: 1, script: , vendor: , cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	25/08/13 01:45:21 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor
	25/08/13 01:45:21 INFO ResourceProfileManager: Added ResourceProfile id: 0
	25/08/13 01:45:21 INFO ResourceProfile: User executor ResourceProfile created, executor resources: Map(executorType -> name: executorType, amount: 1, script: , vendor: , cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	25/08/13 01:45:21 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor
	25/08/13 01:45:21 INFO ResourceProfileManager: Added ResourceProfile id: 1
	25/08/13 01:45:21 INFO SecurityManager: Changing view acls to: root
	25/08/13 01:45:21 INFO SecurityManager: Changing modify acls to: root
	25/08/13 01:45:21 INFO SecurityManager: Changing view acls groups to:
	25/08/13 01:45:21 INFO SecurityManager: Changing modify acls groups to:
	25/08/13 01:45:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
	25/08/13 01:45:21 INFO Utils: Successfully started service 'sparkDriver' on port 34753.
	25/08/13 01:45:21 INFO SparkEnv: Registering MapOutputTracker
	25/08/13 01:45:21 INFO SparkEnv: Registering BlockManagerMaster
	25/08/13 01:45:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	25/08/13 01:45:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	25/08/13 01:45:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	25/08/13 01:45:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fd114502-c768-4468-bd14-45412b108715
	25/08/13 01:45:21 INFO MemoryStore: MemoryStore started with capacity 127.2 MiB
	25/08/13 01:45:21 INFO SparkEnv: Registering OutputCommitCoordinator
	25/08/13 01:45:21 INFO SubResultCacheManager: Sub-result caches are disabled.
	25/08/13 01:45:21 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	25/08/13 01:45:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	25/08/13 01:45:21 INFO Utils: Using 100 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
	25/08/13 01:45:21 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm2
	25/08/13 01:45:21 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm3
[INFO] 2025-08-13 01:45:22.802 +0000 -  ->
	25/08/13 01:45:21 INFO Configuration: resource-types.xml not found
	25/08/13 01:45:21 INFO ResourceUtils: Unable to find 'resource-types.xml'.
	25/08/13 01:45:21 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (54272 MB per container)
	25/08/13 01:45:21 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
	25/08/13 01:45:21 INFO Client: Setting up container launch context for our AM
	25/08/13 01:45:21 INFO Client: Setting up the launch environment for our AM container
	25/08/13 01:45:21 INFO Client: Preparing resources for our AM container
	25/08/13 01:45:22 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/HikariCP-2.5.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/HikariCP-2.5.1.jar
	25/08/13 01:45:22 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/HikariCP-2.5.1.jar' for reading
[INFO] 2025-08-13 01:45:23.804 +0000 -  ->
	25/08/13 01:45:22 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/JLargeArrays-1.5.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/JLargeArrays-1.5.jar
	25/08/13 01:45:22 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/JLargeArrays-1.5.jar' for reading
	25/08/13 01:45:22 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/JTransforms-3.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/JTransforms-3.1.jar
	25/08/13 01:45:22 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/JTransforms-3.1.jar' for reading
	25/08/13 01:45:23 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/RoaringBitmap-0.9.45.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/RoaringBitmap-0.9.45.jar
	25/08/13 01:45:23 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/RoaringBitmap-0.9.45.jar' for reading
	25/08/13 01:45:23 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/ST4-4.0.4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/ST4-4.0.4.jar
	25/08/13 01:45:23 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/ST4-4.0.4.jar' for reading
	25/08/13 01:45:23 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/activation-1.1.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/activation-1.1.1.jar
	25/08/13 01:45:23 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/activation-1.1.1.jar' for reading
	25/08/13 01:45:23 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/aggdesigner-algorithm-6.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/aggdesigner-algorithm-6.0.jar
	25/08/13 01:45:23 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/aggdesigner-algorithm-6.0.jar' for reading
	25/08/13 01:45:23 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/aircompressor-0.27.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/aircompressor-0.27.jar
	25/08/13 01:45:23 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/aircompressor-0.27.jar' for reading
	25/08/13 01:45:23 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/algebra_2.12-2.0.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/algebra_2.12-2.0.1.jar
	25/08/13 01:45:23 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/algebra_2.12-2.0.1.jar' for reading
	25/08/13 01:45:23 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/animal-sniffer-annotations-1.14.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/animal-sniffer-annotations-1.14.jar
	25/08/13 01:45:23 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/animal-sniffer-annotations-1.14.jar' for reading
	25/08/13 01:45:23 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/animal-sniffer-annotations-1.23.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/animal-sniffer-annotations-1.23.jar
	25/08/13 01:45:23 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/animal-sniffer-annotations-1.23.jar' for reading
	25/08/13 01:45:23 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/annotations-16.0.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/annotations-16.0.2.jar
	25/08/13 01:45:23 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/annotations-16.0.2.jar' for reading
[INFO] 2025-08-13 01:45:24.805 +0000 -  ->
	25/08/13 01:45:23 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/annotations-17.0.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/annotations-17.0.0.jar
	25/08/13 01:45:23 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/annotations-17.0.0.jar' for reading
	25/08/13 01:45:23 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/annotations-4.1.1.4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/annotations-4.1.1.4.jar
	25/08/13 01:45:23 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/annotations-4.1.1.4.jar' for reading
	25/08/13 01:45:23 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/antlr-runtime-3.5.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/antlr-runtime-3.5.2.jar
	25/08/13 01:45:23 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/antlr-runtime-3.5.2.jar' for reading
	25/08/13 01:45:24 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/antlr4-runtime-4.9.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/antlr4-runtime-4.9.3.jar
	25/08/13 01:45:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/antlr4-runtime-4.9.3.jar' for reading
	25/08/13 01:45:24 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/aopalliance-1.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/aopalliance-1.0.jar
	25/08/13 01:45:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/aopalliance-1.0.jar' for reading
	25/08/13 01:45:24 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/aopalliance-repackaged-2.6.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/aopalliance-repackaged-2.6.1.jar
	25/08/13 01:45:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/aopalliance-repackaged-2.6.1.jar' for reading
	25/08/13 01:45:24 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/arpack-3.0.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/arpack-3.0.3.jar
	25/08/13 01:45:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/arpack-3.0.3.jar' for reading
	25/08/13 01:45:24 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/arpack_combined_all-0.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/arpack_combined_all-0.1.jar
	25/08/13 01:45:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/arpack_combined_all-0.1.jar' for reading
	25/08/13 01:45:24 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/arrow-format-12.0.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/arrow-format-12.0.1.jar
	25/08/13 01:45:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/arrow-format-12.0.1.jar' for reading
	25/08/13 01:45:24 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/arrow-memory-core-12.0.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/arrow-memory-core-12.0.1.jar
	25/08/13 01:45:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/arrow-memory-core-12.0.1.jar' for reading
	25/08/13 01:45:24 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/arrow-memory-netty-12.0.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/arrow-memory-netty-12.0.1.jar
	25/08/13 01:45:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/arrow-memory-netty-12.0.1.jar' for reading
	25/08/13 01:45:24 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/arrow-vector-12.0.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/arrow-vector-12.0.1.jar
	25/08/13 01:45:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/arrow-vector-12.0.1.jar' for reading
[INFO] 2025-08-13 01:45:25.806 +0000 -  ->
	25/08/13 01:45:24 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/audience-annotations-0.12.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/audience-annotations-0.12.0.jar
	25/08/13 01:45:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/audience-annotations-0.12.0.jar' for reading
	25/08/13 01:45:24 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/avro-1.11.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/avro-1.11.2.jar
	25/08/13 01:45:24 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/avro-1.11.2.jar' for reading
	25/08/13 01:45:25 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/avro-ipc-1.11.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/avro-ipc-1.11.2.jar
	25/08/13 01:45:25 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/avro-ipc-1.11.2.jar' for reading
	25/08/13 01:45:25 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/avro-mapred-1.11.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/avro-mapred-1.11.2.jar
	25/08/13 01:45:25 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/avro-mapred-1.11.2.jar' for reading
	25/08/13 01:45:25 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/bcprov-ext-jdk15on-1.66.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/bcprov-ext-jdk15on-1.66.jar
	25/08/13 01:45:25 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/bcprov-ext-jdk15on-1.66.jar' for reading
	25/08/13 01:45:25 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/blas-3.0.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/blas-3.0.3.jar
	25/08/13 01:45:25 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/blas-3.0.3.jar' for reading
	25/08/13 01:45:25 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/bonecp-0.8.0.RELEASE.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/bonecp-0.8.0.RELEASE.jar
	25/08/13 01:45:25 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/bonecp-0.8.0.RELEASE.jar' for reading
	25/08/13 01:45:25 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/breeze-macros_2.12-2.1.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/breeze-macros_2.12-2.1.0.jar
	25/08/13 01:45:25 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/breeze-macros_2.12-2.1.0.jar' for reading
	25/08/13 01:45:25 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/breeze_2.12-2.1.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/breeze_2.12-2.1.0.jar
	25/08/13 01:45:25 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/breeze_2.12-2.1.0.jar' for reading
	25/08/13 01:45:25 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/cats-kernel_2.12-2.1.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/cats-kernel_2.12-2.1.1.jar
	25/08/13 01:45:25 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/cats-kernel_2.12-2.1.1.jar' for reading
[INFO] 2025-08-13 01:45:26.807 +0000 -  ->
	25/08/13 01:45:25 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/checker-qual-2.5.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/checker-qual-2.5.2.jar
	25/08/13 01:45:25 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/checker-qual-2.5.2.jar' for reading
	25/08/13 01:45:25 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/chill-java-0.10.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/chill-java-0.10.0.jar
	25/08/13 01:45:25 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/chill-java-0.10.0.jar' for reading
	25/08/13 01:45:25 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/chill_2.12-0.10.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/chill_2.12-0.10.0.jar
	25/08/13 01:45:25 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/chill_2.12-0.10.0.jar' for reading
	25/08/13 01:45:26 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/commons-cli-1.5.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/commons-cli-1.5.0.jar
	25/08/13 01:45:26 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/commons-cli-1.5.0.jar' for reading
	25/08/13 01:45:26 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/commons-codec-1.16.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/commons-codec-1.16.1.jar
	25/08/13 01:45:26 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/commons-codec-1.16.1.jar' for reading
	25/08/13 01:45:26 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/commons-collections-3.2.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/commons-collections-3.2.2.jar
	25/08/13 01:45:26 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/commons-collections-3.2.2.jar' for reading
	25/08/13 01:45:26 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/commons-collections4-4.4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/commons-collections4-4.4.jar
	25/08/13 01:45:26 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/commons-collections4-4.4.jar' for reading
	25/08/13 01:45:26 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/commons-compiler-3.1.9.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/commons-compiler-3.1.9.jar
	25/08/13 01:45:26 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/commons-compiler-3.1.9.jar' for reading
	25/08/13 01:45:26 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/commons-compress-1.23.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/commons-compress-1.23.0.jar
	25/08/13 01:45:26 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/commons-compress-1.23.0.jar' for reading
	25/08/13 01:45:26 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/commons-crypto-1.1.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/commons-crypto-1.1.0.jar
	25/08/13 01:45:26 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/commons-crypto-1.1.0.jar' for reading
	25/08/13 01:45:26 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/commons-dbcp-1.4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/commons-dbcp-1.4.jar
	25/08/13 01:45:26 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/commons-dbcp-1.4.jar' for reading
	25/08/13 01:45:26 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/commons-io-2.16.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/commons-io-2.16.1.jar
	25/08/13 01:45:26 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/commons-io-2.16.1.jar' for reading
[INFO] 2025-08-13 01:45:27.808 +0000 -  ->
	25/08/13 01:45:26 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/commons-lang-2.6.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/commons-lang-2.6.jar
	25/08/13 01:45:26 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/commons-lang-2.6.jar' for reading
	25/08/13 01:45:26 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/commons-lang3-3.12.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/commons-lang3-3.12.0.jar
	25/08/13 01:45:26 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/commons-lang3-3.12.0.jar' for reading
	25/08/13 01:45:27 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/commons-logging-1.1.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/commons-logging-1.1.3.jar
	25/08/13 01:45:27 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/commons-logging-1.1.3.jar' for reading
	25/08/13 01:45:27 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/commons-math3-3.6.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/commons-math3-3.6.1.jar
	25/08/13 01:45:27 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/commons-math3-3.6.1.jar' for reading
	25/08/13 01:45:27 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/commons-pool-1.5.4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/commons-pool-1.5.4.jar
	25/08/13 01:45:27 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/commons-pool-1.5.4.jar' for reading
	25/08/13 01:45:27 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/commons-text-1.10.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/commons-text-1.10.0.jar
	25/08/13 01:45:27 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/commons-text-1.10.0.jar' for reading
	25/08/13 01:45:27 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/compress-lzf-1.1.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/compress-lzf-1.1.2.jar
	25/08/13 01:45:27 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/compress-lzf-1.1.2.jar' for reading
	25/08/13 01:45:27 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/curator-client-2.13.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/curator-client-2.13.0.jar
	25/08/13 01:45:27 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/curator-client-2.13.0.jar' for reading
	25/08/13 01:45:27 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/curator-framework-2.13.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/curator-framework-2.13.0.jar
	25/08/13 01:45:27 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/curator-framework-2.13.0.jar' for reading
	25/08/13 01:45:27 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/curator-recipes-2.13.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/curator-recipes-2.13.0.jar
	25/08/13 01:45:27 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/curator-recipes-2.13.0.jar' for reading
	25/08/13 01:45:27 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/datanucleus-api-jdo-4.2.4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/datanucleus-api-jdo-4.2.4.jar
	25/08/13 01:45:27 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/datanucleus-api-jdo-4.2.4.jar' for reading
[INFO] 2025-08-13 01:45:28.810 +0000 -  ->
	25/08/13 01:45:27 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/datanucleus-core-4.1.17.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/datanucleus-core-4.1.17.jar
	25/08/13 01:45:27 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/datanucleus-core-4.1.17.jar' for reading
	25/08/13 01:45:27 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/datanucleus-rdbms-4.1.19.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/datanucleus-rdbms-4.1.19.jar
	25/08/13 01:45:28 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/datanucleus-rdbms-4.1.19.jar' for reading
	25/08/13 01:45:28 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/datasketches-java-3.3.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/datasketches-java-3.3.0.jar
	25/08/13 01:45:28 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/datasketches-java-3.3.0.jar' for reading
	25/08/13 01:45:28 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/datasketches-memory-2.1.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/datasketches-memory-2.1.0.jar
	25/08/13 01:45:28 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/datasketches-memory-2.1.0.jar' for reading
	25/08/13 01:45:28 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/derby-10.14.2.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/derby-10.14.2.0.jar
	25/08/13 01:45:28 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/derby-10.14.2.0.jar' for reading
	25/08/13 01:45:28 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/disruptor-3.3.7.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/disruptor-3.3.7.jar
	25/08/13 01:45:28 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/disruptor-3.3.7.jar' for reading
	25/08/13 01:45:28 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar
	25/08/13 01:45:28 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar' for reading
	25/08/13 01:45:28 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/emr-spark-goodies.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/emr-spark-goodies.jar
	25/08/13 01:45:28 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/emr-spark-goodies.jar' for reading
	25/08/13 01:45:28 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/emrfs-hadoop-assembly-2.66.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/emrfs-hadoop-assembly-2.66.0.jar
	25/08/13 01:45:28 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/emrfs-hadoop-assembly-2.66.0.jar' for reading
[INFO] 2025-08-13 01:45:29.811 +0000 -  ->
	25/08/13 01:45:28 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/error_prone_annotations-2.1.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/error_prone_annotations-2.1.3.jar
	25/08/13 01:45:28 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/error_prone_annotations-2.1.3.jar' for reading
	25/08/13 01:45:28 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/error_prone_annotations-2.18.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/error_prone_annotations-2.18.0.jar
	25/08/13 01:45:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/error_prone_annotations-2.18.0.jar' for reading
	25/08/13 01:45:29 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/findbugs-annotations-3.0.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/findbugs-annotations-3.0.1.jar
	25/08/13 01:45:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/findbugs-annotations-3.0.1.jar' for reading
	25/08/13 01:45:29 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/flatbuffers-java-1.12.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/flatbuffers-java-1.12.0.jar
	25/08/13 01:45:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/flatbuffers-java-1.12.0.jar' for reading
	25/08/13 01:45:29 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/gmetric4j-1.0.10.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/gmetric4j-1.0.10.jar
	25/08/13 01:45:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/gmetric4j-1.0.10.jar' for reading
	25/08/13 01:45:29 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/grpc-api-1.56.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/grpc-api-1.56.0.jar
	25/08/13 01:45:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/grpc-api-1.56.0.jar' for reading
	25/08/13 01:45:29 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/grpc-context-1.56.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/grpc-context-1.56.0.jar
	25/08/13 01:45:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/grpc-context-1.56.0.jar' for reading
	25/08/13 01:45:29 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/grpc-core-1.56.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/grpc-core-1.56.0.jar
	25/08/13 01:45:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/grpc-core-1.56.0.jar' for reading
	25/08/13 01:45:29 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/grpc-netty-1.56.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/grpc-netty-1.56.0.jar
	25/08/13 01:45:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/grpc-netty-1.56.0.jar' for reading
	25/08/13 01:45:29 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/grpc-protobuf-1.56.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/grpc-protobuf-1.56.0.jar
	25/08/13 01:45:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/grpc-protobuf-1.56.0.jar' for reading
	25/08/13 01:45:29 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/grpc-protobuf-lite-1.56.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/grpc-protobuf-lite-1.56.0.jar
	25/08/13 01:45:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/grpc-protobuf-lite-1.56.0.jar' for reading
	25/08/13 01:45:29 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/grpc-services-1.56.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/grpc-services-1.56.0.jar
	25/08/13 01:45:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/grpc-services-1.56.0.jar' for reading
	25/08/13 01:45:29 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/grpc-stub-1.56.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/grpc-stub-1.56.0.jar
	25/08/13 01:45:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/grpc-stub-1.56.0.jar' for reading
	25/08/13 01:45:29 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/gson-2.10.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/gson-2.10.1.jar
[INFO] 2025-08-13 01:45:30.812 +0000 -  ->
	25/08/13 01:45:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/gson-2.10.1.jar' for reading
	25/08/13 01:45:29 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/guava-14.0.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/guava-14.0.1.jar
	25/08/13 01:45:29 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/guava-14.0.1.jar' for reading
	25/08/13 01:45:30 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/hadoop-client-api-3.4.0-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hadoop-client-api-3.4.0-amzn-1.jar
	25/08/13 01:45:30 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/hadoop-client-api-3.4.0-amzn-1.jar' for reading
	25/08/13 01:45:30 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/hadoop-client-runtime-3.4.0-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hadoop-client-runtime-3.4.0-amzn-1.jar
	25/08/13 01:45:30 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/hadoop-client-runtime-3.4.0-amzn-1.jar' for reading
	25/08/13 01:45:30 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/hadoop-shaded-guava-1.2.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hadoop-shaded-guava-1.2.0.jar
	25/08/13 01:45:30 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/hadoop-shaded-guava-1.2.0.jar' for reading
	25/08/13 01:45:30 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/hadoop-yarn-server-web-proxy-3.4.0-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hadoop-yarn-server-web-proxy-3.4.0-amzn-1.jar
	25/08/13 01:45:30 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/hadoop-yarn-server-web-proxy-3.4.0-amzn-1.jar' for reading
[INFO] 2025-08-13 01:45:31.813 +0000 -  ->
	25/08/13 01:45:30 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/hive-beeline-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hive-beeline-2.3.9-amzn-4.jar
	25/08/13 01:45:30 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/hive-beeline-2.3.9-amzn-4.jar' for reading
	25/08/13 01:45:30 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/hive-cli-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hive-cli-2.3.9-amzn-4.jar
	25/08/13 01:45:30 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/hive-cli-2.3.9-amzn-4.jar' for reading
	25/08/13 01:45:30 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/hive-common-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hive-common-2.3.9-amzn-4.jar
	25/08/13 01:45:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/hive-common-2.3.9-amzn-4.jar' for reading
	25/08/13 01:45:31 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/hive-exec-2.3.9-amzn-4-core.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hive-exec-2.3.9-amzn-4-core.jar
	25/08/13 01:45:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/hive-exec-2.3.9-amzn-4-core.jar' for reading
	25/08/13 01:45:31 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/hive-jdbc-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hive-jdbc-2.3.9-amzn-4.jar
	25/08/13 01:45:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/hive-jdbc-2.3.9-amzn-4.jar' for reading
	25/08/13 01:45:31 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/hive-llap-common-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hive-llap-common-2.3.9-amzn-4.jar
	25/08/13 01:45:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/hive-llap-common-2.3.9-amzn-4.jar' for reading
	25/08/13 01:45:31 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/hive-metastore-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hive-metastore-2.3.9-amzn-4.jar
	25/08/13 01:45:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/hive-metastore-2.3.9-amzn-4.jar' for reading
	25/08/13 01:45:31 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/hive-serde-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hive-serde-2.3.9-amzn-4.jar
	25/08/13 01:45:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/hive-serde-2.3.9-amzn-4.jar' for reading
	25/08/13 01:45:31 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/hive-service-rpc-3.1.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hive-service-rpc-3.1.3.jar
	25/08/13 01:45:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/hive-service-rpc-3.1.3.jar' for reading
	25/08/13 01:45:31 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/hive-shims-0.23-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hive-shims-0.23-2.3.9-amzn-4.jar
	25/08/13 01:45:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/hive-shims-0.23-2.3.9-amzn-4.jar' for reading
	25/08/13 01:45:31 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/hive-shims-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hive-shims-2.3.9-amzn-4.jar
	25/08/13 01:45:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/hive-shims-2.3.9-amzn-4.jar' for reading
[INFO] 2025-08-13 01:45:32.814 +0000 -  ->
	25/08/13 01:45:31 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/hive-shims-common-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hive-shims-common-2.3.9-amzn-4.jar
	25/08/13 01:45:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/hive-shims-common-2.3.9-amzn-4.jar' for reading
	25/08/13 01:45:31 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/hive-shims-scheduler-2.3.9-amzn-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hive-shims-scheduler-2.3.9-amzn-4.jar
	25/08/13 01:45:31 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/hive-shims-scheduler-2.3.9-amzn-4.jar' for reading
	25/08/13 01:45:32 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/hive-storage-api-2.8.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hive-storage-api-2.8.1.jar
	25/08/13 01:45:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/hive-storage-api-2.8.1.jar' for reading
	25/08/13 01:45:32 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/hk2-api-2.6.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hk2-api-2.6.1.jar
	25/08/13 01:45:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/hk2-api-2.6.1.jar' for reading
	25/08/13 01:45:32 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/hk2-locator-2.6.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hk2-locator-2.6.1.jar
	25/08/13 01:45:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/hk2-locator-2.6.1.jar' for reading
	25/08/13 01:45:32 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/hk2-utils-2.6.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hk2-utils-2.6.1.jar
	25/08/13 01:45:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/hk2-utils-2.6.1.jar' for reading
	25/08/13 01:45:32 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/httpclient-4.5.13.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/httpclient-4.5.13.jar
	25/08/13 01:45:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/httpclient-4.5.13.jar' for reading
	25/08/13 01:45:32 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/httpcore-4.4.13.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/httpcore-4.4.13.jar
	25/08/13 01:45:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/httpcore-4.4.13.jar' for reading
	25/08/13 01:45:32 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/istack-commons-runtime-3.0.8.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/istack-commons-runtime-3.0.8.jar
	25/08/13 01:45:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/istack-commons-runtime-3.0.8.jar' for reading
	25/08/13 01:45:32 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/ivy-2.5.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/ivy-2.5.1.jar
	25/08/13 01:45:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/ivy-2.5.1.jar' for reading
	25/08/13 01:45:32 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/j2objc-annotations-1.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/j2objc-annotations-1.1.jar
	25/08/13 01:45:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/j2objc-annotations-1.1.jar' for reading
	25/08/13 01:45:32 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jackson-annotations-2.15.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jackson-annotations-2.15.2.jar
	25/08/13 01:45:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jackson-annotations-2.15.2.jar' for reading
	25/08/13 01:45:32 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jackson-core-2.15.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jackson-core-2.15.2.jar
	25/08/13 01:45:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jackson-core-2.15.2.jar' for reading
[INFO] 2025-08-13 01:45:33.816 +0000 -  ->
	25/08/13 01:45:32 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jackson-core-asl-1.9.13.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jackson-core-asl-1.9.13.jar
	25/08/13 01:45:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jackson-core-asl-1.9.13.jar' for reading
	25/08/13 01:45:32 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jackson-databind-2.15.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jackson-databind-2.15.2.jar
	25/08/13 01:45:32 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jackson-databind-2.15.2.jar' for reading
	25/08/13 01:45:33 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jackson-dataformat-yaml-2.15.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jackson-dataformat-yaml-2.15.2.jar
	25/08/13 01:45:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jackson-dataformat-yaml-2.15.2.jar' for reading
	25/08/13 01:45:33 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jackson-datatype-jsr310-2.15.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jackson-datatype-jsr310-2.15.2.jar
	25/08/13 01:45:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jackson-datatype-jsr310-2.15.2.jar' for reading
	25/08/13 01:45:33 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jackson-mapper-asl-1.9.13.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jackson-mapper-asl-1.9.13.jar
	25/08/13 01:45:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jackson-mapper-asl-1.9.13.jar' for reading
	25/08/13 01:45:33 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jackson-module-scala_2.12-2.15.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jackson-module-scala_2.12-2.15.2.jar
	25/08/13 01:45:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jackson-module-scala_2.12-2.15.2.jar' for reading
	25/08/13 01:45:33 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jakarta.annotation-api-1.3.5.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jakarta.annotation-api-1.3.5.jar
	25/08/13 01:45:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jakarta.annotation-api-1.3.5.jar' for reading
	25/08/13 01:45:33 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jakarta.inject-2.6.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jakarta.inject-2.6.1.jar
	25/08/13 01:45:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jakarta.inject-2.6.1.jar' for reading
	25/08/13 01:45:33 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jakarta.servlet-api-4.0.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jakarta.servlet-api-4.0.3.jar
	25/08/13 01:45:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jakarta.servlet-api-4.0.3.jar' for reading
	25/08/13 01:45:33 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jakarta.validation-api-2.0.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jakarta.validation-api-2.0.2.jar
	25/08/13 01:45:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jakarta.validation-api-2.0.2.jar' for reading
	25/08/13 01:45:33 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jakarta.ws.rs-api-2.1.6.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jakarta.ws.rs-api-2.1.6.jar
	25/08/13 01:45:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jakarta.ws.rs-api-2.1.6.jar' for reading
	25/08/13 01:45:33 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jakarta.xml.bind-api-2.3.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jakarta.xml.bind-api-2.3.2.jar
	25/08/13 01:45:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jakarta.xml.bind-api-2.3.2.jar' for reading
	25/08/13 01:45:33 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/janino-3.1.9.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/janino-3.1.9.jar
	25/08/13 01:45:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/janino-3.1.9.jar' for reading
[INFO] 2025-08-13 01:45:34.817 +0000 -  ->
	25/08/13 01:45:33 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/javassist-3.29.2-GA.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/javassist-3.29.2-GA.jar
	25/08/13 01:45:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/javassist-3.29.2-GA.jar' for reading
	25/08/13 01:45:33 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/javax.inject-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/javax.inject-1.jar
	25/08/13 01:45:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/javax.inject-1.jar' for reading
	25/08/13 01:45:33 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/javax.jdo-3.2.0-m3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/javax.jdo-3.2.0-m3.jar
	25/08/13 01:45:33 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/javax.jdo-3.2.0-m3.jar' for reading
	25/08/13 01:45:34 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/javax.servlet-api-3.1.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/javax.servlet-api-3.1.0.jar
	25/08/13 01:45:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/javax.servlet-api-3.1.0.jar' for reading
	25/08/13 01:45:34 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/javolution-5.5.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/javolution-5.5.1.jar
	25/08/13 01:45:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/javolution-5.5.1.jar' for reading
	25/08/13 01:45:34 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jaxb-runtime-2.3.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jaxb-runtime-2.3.2.jar
	25/08/13 01:45:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jaxb-runtime-2.3.2.jar' for reading
	25/08/13 01:45:34 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jcl-over-slf4j-2.0.7.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jcl-over-slf4j-2.0.7.jar
	25/08/13 01:45:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jcl-over-slf4j-2.0.7.jar' for reading
	25/08/13 01:45:34 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jdo-api-3.0.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jdo-api-3.0.1.jar
	25/08/13 01:45:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jdo-api-3.0.1.jar' for reading
	25/08/13 01:45:34 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jersey-client-2.40.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jersey-client-2.40.jar
	25/08/13 01:45:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jersey-client-2.40.jar' for reading
	25/08/13 01:45:34 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jersey-common-2.40.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jersey-common-2.40.jar
	25/08/13 01:45:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jersey-common-2.40.jar' for reading
	25/08/13 01:45:34 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jersey-container-servlet-2.40.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jersey-container-servlet-2.40.jar
	25/08/13 01:45:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jersey-container-servlet-2.40.jar' for reading
	25/08/13 01:45:34 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jersey-container-servlet-core-2.40.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jersey-container-servlet-core-2.40.jar
	25/08/13 01:45:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jersey-container-servlet-core-2.40.jar' for reading
	25/08/13 01:45:34 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jersey-hk2-2.40.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jersey-hk2-2.40.jar
	25/08/13 01:45:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jersey-hk2-2.40.jar' for reading
[INFO] 2025-08-13 01:45:35.818 +0000 -  ->
	25/08/13 01:45:34 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jersey-server-2.40.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jersey-server-2.40.jar
	25/08/13 01:45:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jersey-server-2.40.jar' for reading
	25/08/13 01:45:34 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jetty-rewrite-9.3.27.v20190418.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jetty-rewrite-9.3.27.v20190418.jar
	25/08/13 01:45:34 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jetty-rewrite-9.3.27.v20190418.jar' for reading
	25/08/13 01:45:35 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jline-2.14.6.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jline-2.14.6.jar
	25/08/13 01:45:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jline-2.14.6.jar' for reading
	25/08/13 01:45:35 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jmespath-java-1.12.705.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jmespath-java-1.12.705.jar
	25/08/13 01:45:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jmespath-java-1.12.705.jar' for reading
	25/08/13 01:45:35 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/joda-time-2.12.5.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/joda-time-2.12.5.jar
	25/08/13 01:45:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/joda-time-2.12.5.jar' for reading
	25/08/13 01:45:35 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jodd-core-3.5.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jodd-core-3.5.2.jar
	25/08/13 01:45:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jodd-core-3.5.2.jar' for reading
	25/08/13 01:45:35 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jpam-1.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jpam-1.1.jar
	25/08/13 01:45:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jpam-1.1.jar' for reading
	25/08/13 01:45:35 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/json-1.8.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/json-1.8.jar
	25/08/13 01:45:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/json-1.8.jar' for reading
	25/08/13 01:45:35 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/json4s-ast_2.12-3.7.0-M11.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/json4s-ast_2.12-3.7.0-M11.jar
	25/08/13 01:45:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/json4s-ast_2.12-3.7.0-M11.jar' for reading
	25/08/13 01:45:35 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/json4s-core_2.12-3.7.0-M11.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/json4s-core_2.12-3.7.0-M11.jar
	25/08/13 01:45:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/json4s-core_2.12-3.7.0-M11.jar' for reading
	25/08/13 01:45:35 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/json4s-jackson_2.12-3.7.0-M11.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/json4s-jackson_2.12-3.7.0-M11.jar
	25/08/13 01:45:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/json4s-jackson_2.12-3.7.0-M11.jar' for reading
	25/08/13 01:45:35 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/json4s-scalap_2.12-3.7.0-M11.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/json4s-scalap_2.12-3.7.0-M11.jar
	25/08/13 01:45:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/json4s-scalap_2.12-3.7.0-M11.jar' for reading
	25/08/13 01:45:35 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jsr305-3.0.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jsr305-3.0.0.jar
	25/08/13 01:45:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jsr305-3.0.0.jar' for reading
[INFO] 2025-08-13 01:45:36.819 +0000 -  ->
	25/08/13 01:45:35 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jsr305-3.0.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jsr305-3.0.2.jar
	25/08/13 01:45:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jsr305-3.0.2.jar' for reading
	25/08/13 01:45:35 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jta-1.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jta-1.1.jar
	25/08/13 01:45:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jta-1.1.jar' for reading
	25/08/13 01:45:35 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/jul-to-slf4j-2.0.7.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/jul-to-slf4j-2.0.7.jar
	25/08/13 01:45:35 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/jul-to-slf4j-2.0.7.jar' for reading
	25/08/13 01:45:36 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/kryo-shaded-4.0.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/kryo-shaded-4.0.2.jar
	25/08/13 01:45:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/kryo-shaded-4.0.2.jar' for reading
	25/08/13 01:45:36 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/lapack-3.0.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/lapack-3.0.3.jar
	25/08/13 01:45:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/lapack-3.0.3.jar' for reading
	25/08/13 01:45:36 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/leveldbjni-all-1.8.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/leveldbjni-all-1.8.jar
	25/08/13 01:45:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/leveldbjni-all-1.8.jar' for reading
	25/08/13 01:45:36 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/libfb303-0.9.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/libfb303-0.9.3.jar
	25/08/13 01:45:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/libfb303-0.9.3.jar' for reading
	25/08/13 01:45:36 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/libthrift-0.12.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/libthrift-0.12.0.jar
	25/08/13 01:45:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/libthrift-0.12.0.jar' for reading
	25/08/13 01:45:36 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/log4j-1.2-api-2.20.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/log4j-1.2-api-2.20.0.jar
	25/08/13 01:45:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/log4j-1.2-api-2.20.0.jar' for reading
	25/08/13 01:45:36 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/log4j-api-2.20.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/log4j-api-2.20.0.jar
	25/08/13 01:45:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/log4j-api-2.20.0.jar' for reading
	25/08/13 01:45:36 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/log4j-core-2.20.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/log4j-core-2.20.0.jar
	25/08/13 01:45:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/log4j-core-2.20.0.jar' for reading
	25/08/13 01:45:36 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/log4j-slf4j2-impl-2.20.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/log4j-slf4j2-impl-2.20.0.jar
	25/08/13 01:45:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/log4j-slf4j2-impl-2.20.0.jar' for reading
	25/08/13 01:45:36 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/logging-interceptor-3.12.12.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/logging-interceptor-3.12.12.jar
	25/08/13 01:45:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/logging-interceptor-3.12.12.jar' for reading
	25/08/13 01:45:36 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/lz4-java-1.8.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/lz4-java-1.8.0.jar
	25/08/13 01:45:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/lz4-java-1.8.0.jar' for reading
[INFO] 2025-08-13 01:45:37.820 +0000 -  ->
	25/08/13 01:45:36 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/mariadb-connector-java.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/mariadb-connector-java.jar
	25/08/13 01:45:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/mariadb-connector-java.jar' for reading
	25/08/13 01:45:36 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/metrics-core-4.2.19.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/metrics-core-4.2.19.jar
	25/08/13 01:45:36 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/metrics-core-4.2.19.jar' for reading
	25/08/13 01:45:37 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/metrics-graphite-4.2.19.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/metrics-graphite-4.2.19.jar
	25/08/13 01:45:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/metrics-graphite-4.2.19.jar' for reading
	25/08/13 01:45:37 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/metrics-jmx-4.2.19.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/metrics-jmx-4.2.19.jar
	25/08/13 01:45:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/metrics-jmx-4.2.19.jar' for reading
	25/08/13 01:45:37 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/metrics-json-4.2.19.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/metrics-json-4.2.19.jar
	25/08/13 01:45:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/metrics-json-4.2.19.jar' for reading
	25/08/13 01:45:37 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/metrics-jvm-4.2.19.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/metrics-jvm-4.2.19.jar
	25/08/13 01:45:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/metrics-jvm-4.2.19.jar' for reading
	25/08/13 01:45:37 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/minlog-1.3.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/minlog-1.3.0.jar
	25/08/13 01:45:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/minlog-1.3.0.jar' for reading
	25/08/13 01:45:37 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/netty-all-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/netty-all-4.1.100.Final.jar
	25/08/13 01:45:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/netty-all-4.1.100.Final.jar' for reading
	25/08/13 01:45:37 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/netty-buffer-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/netty-buffer-4.1.100.Final.jar
	25/08/13 01:45:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/netty-buffer-4.1.100.Final.jar' for reading
	25/08/13 01:45:37 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/netty-codec-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/netty-codec-4.1.100.Final.jar
	25/08/13 01:45:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/netty-codec-4.1.100.Final.jar' for reading
	25/08/13 01:45:37 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/netty-codec-http-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/netty-codec-http-4.1.100.Final.jar
	25/08/13 01:45:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/netty-codec-http-4.1.100.Final.jar' for reading
	25/08/13 01:45:37 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/netty-codec-http2-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/netty-codec-http2-4.1.100.Final.jar
	25/08/13 01:45:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/netty-codec-http2-4.1.100.Final.jar' for reading
	25/08/13 01:45:37 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/netty-codec-socks-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/netty-codec-socks-4.1.100.Final.jar
	25/08/13 01:45:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/netty-codec-socks-4.1.100.Final.jar' for reading
[INFO] 2025-08-13 01:45:38.822 +0000 -  ->
	25/08/13 01:45:37 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/netty-common-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/netty-common-4.1.100.Final.jar
	25/08/13 01:45:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/netty-common-4.1.100.Final.jar' for reading
	25/08/13 01:45:37 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/netty-handler-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/netty-handler-4.1.100.Final.jar
	25/08/13 01:45:37 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/netty-handler-4.1.100.Final.jar' for reading
	25/08/13 01:45:37 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/netty-handler-proxy-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/netty-handler-proxy-4.1.100.Final.jar
	25/08/13 01:45:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/netty-handler-proxy-4.1.100.Final.jar' for reading
	25/08/13 01:45:38 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/netty-resolver-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/netty-resolver-4.1.100.Final.jar
	25/08/13 01:45:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/netty-resolver-4.1.100.Final.jar' for reading
	25/08/13 01:45:38 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/netty-transport-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/netty-transport-4.1.100.Final.jar
	25/08/13 01:45:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/netty-transport-4.1.100.Final.jar' for reading
	25/08/13 01:45:38 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/netty-transport-classes-epoll-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/netty-transport-classes-epoll-4.1.100.Final.jar
	25/08/13 01:45:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/netty-transport-classes-epoll-4.1.100.Final.jar' for reading
	25/08/13 01:45:38 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/netty-transport-classes-kqueue-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/netty-transport-classes-kqueue-4.1.100.Final.jar
	25/08/13 01:45:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/netty-transport-classes-kqueue-4.1.100.Final.jar' for reading
	25/08/13 01:45:38 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar
	25/08/13 01:45:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar' for reading
	25/08/13 01:45:38 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar
	25/08/13 01:45:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar' for reading
	25/08/13 01:45:38 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar
	25/08/13 01:45:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar' for reading
	25/08/13 01:45:38 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar
	25/08/13 01:45:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar' for reading
	25/08/13 01:45:38 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/netty-transport-native-unix-common-4.1.100.Final.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/netty-transport-native-unix-common-4.1.100.Final.jar
	25/08/13 01:45:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/netty-transport-native-unix-common-4.1.100.Final.jar' for reading
	25/08/13 01:45:38 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/objenesis-2.5.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/objenesis-2.5.1.jar
	25/08/13 01:45:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/objenesis-2.5.1.jar' for reading
	25/08/13 01:45:38 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/objenesis-3.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/objenesis-3.3.jar
	25/08/13 01:45:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/objenesis-3.3.jar' for reading
	25/08/13 01:45:38 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/okhttp-3.12.12.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/okhttp-3.12.12.jar
	25/08/13 01:45:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/okhttp-3.12.12.jar' for reading
[INFO] 2025-08-13 01:45:39.823 +0000 -  ->
	25/08/13 01:45:38 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/okio-1.15.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/okio-1.15.0.jar
	25/08/13 01:45:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/okio-1.15.0.jar' for reading
	25/08/13 01:45:38 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/opencsv-2.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/opencsv-2.3.jar
	25/08/13 01:45:38 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/opencsv-2.3.jar' for reading
	25/08/13 01:45:39 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/orc-core-1.9.4-shaded-protobuf.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/orc-core-1.9.4-shaded-protobuf.jar
	25/08/13 01:45:39 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/orc-core-1.9.4-shaded-protobuf.jar' for reading
	25/08/13 01:45:39 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/orc-mapreduce-1.9.4-shaded-protobuf.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/orc-mapreduce-1.9.4-shaded-protobuf.jar
	25/08/13 01:45:39 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/orc-mapreduce-1.9.4-shaded-protobuf.jar' for reading
	25/08/13 01:45:39 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/orc-shims-1.9.4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/orc-shims-1.9.4.jar
	25/08/13 01:45:39 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/orc-shims-1.9.4.jar' for reading
	25/08/13 01:45:39 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/oro-2.0.8.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/oro-2.0.8.jar
	25/08/13 01:45:39 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/oro-2.0.8.jar' for reading
	25/08/13 01:45:39 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/osgi-resource-locator-1.0.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/osgi-resource-locator-1.0.3.jar
	25/08/13 01:45:39 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/osgi-resource-locator-1.0.3.jar' for reading
	25/08/13 01:45:39 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/paranamer-2.8.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/paranamer-2.8.jar
	25/08/13 01:45:39 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/paranamer-2.8.jar' for reading
	25/08/13 01:45:39 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/parquet-column-1.13.1-amzn-3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/parquet-column-1.13.1-amzn-3.jar
	25/08/13 01:45:39 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/parquet-column-1.13.1-amzn-3.jar' for reading
	25/08/13 01:45:39 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/parquet-common-1.13.1-amzn-3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/parquet-common-1.13.1-amzn-3.jar
	25/08/13 01:45:39 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/parquet-common-1.13.1-amzn-3.jar' for reading
	25/08/13 01:45:39 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/parquet-encoding-1.13.1-amzn-3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/parquet-encoding-1.13.1-amzn-3.jar
	25/08/13 01:45:39 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/parquet-encoding-1.13.1-amzn-3.jar' for reading
	25/08/13 01:45:39 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/parquet-format-structures-1.13.1-amzn-3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/parquet-format-structures-1.13.1-amzn-3.jar
	25/08/13 01:45:39 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/parquet-format-structures-1.13.1-amzn-3.jar' for reading
[INFO] 2025-08-13 01:45:40.824 +0000 -  ->
	25/08/13 01:45:39 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/parquet-hadoop-1.13.1-amzn-3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/parquet-hadoop-1.13.1-amzn-3.jar
	25/08/13 01:45:39 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/parquet-hadoop-1.13.1-amzn-3.jar' for reading
	25/08/13 01:45:39 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/parquet-jackson-1.13.1-amzn-3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/parquet-jackson-1.13.1-amzn-3.jar
	25/08/13 01:45:39 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/parquet-jackson-1.13.1-amzn-3.jar' for reading
	25/08/13 01:45:40 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/perfmark-api-0.26.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/perfmark-api-0.26.0.jar
	25/08/13 01:45:40 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/perfmark-api-0.26.0.jar' for reading
	25/08/13 01:45:40 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/pickle-1.3.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/pickle-1.3.jar
	25/08/13 01:45:40 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/pickle-1.3.jar' for reading
	25/08/13 01:45:40 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/proto-google-common-protos-2.17.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/proto-google-common-protos-2.17.0.jar
	25/08/13 01:45:40 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/proto-google-common-protos-2.17.0.jar' for reading
	25/08/13 01:45:40 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/py4j-0.10.9.7.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/py4j-0.10.9.7.jar
	25/08/13 01:45:40 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/py4j-0.10.9.7.jar' for reading
	25/08/13 01:45:40 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/remotetea-oncrpc-1.1.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/remotetea-oncrpc-1.1.2.jar
	25/08/13 01:45:40 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/remotetea-oncrpc-1.1.2.jar' for reading
	25/08/13 01:45:40 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/rocksdbjni-8.3.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/rocksdbjni-8.3.2.jar
	25/08/13 01:45:40 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/rocksdbjni-8.3.2.jar' for reading
[INFO] 2025-08-13 01:45:41.825 +0000 -  ->
	25/08/13 01:45:41 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/scala-collection-compat_2.12-2.7.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/scala-collection-compat_2.12-2.7.0.jar
	25/08/13 01:45:41 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/scala-collection-compat_2.12-2.7.0.jar' for reading
	25/08/13 01:45:41 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/scala-compiler-2.12.18.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/scala-compiler-2.12.18.jar
	25/08/13 01:45:41 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/scala-compiler-2.12.18.jar' for reading
	25/08/13 01:45:41 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/scala-library-2.12.18.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/scala-library-2.12.18.jar
	25/08/13 01:45:41 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/scala-library-2.12.18.jar' for reading
	25/08/13 01:45:41 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/scala-parser-combinators_2.12-2.3.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/scala-parser-combinators_2.12-2.3.0.jar
	25/08/13 01:45:41 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/scala-parser-combinators_2.12-2.3.0.jar' for reading
	25/08/13 01:45:41 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/scala-reflect-2.12.18.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/scala-reflect-2.12.18.jar
	25/08/13 01:45:41 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/scala-reflect-2.12.18.jar' for reading
	25/08/13 01:45:41 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/scala-xml_2.12-2.1.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/scala-xml_2.12-2.1.0.jar
	25/08/13 01:45:41 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/scala-xml_2.12-2.1.0.jar' for reading
	25/08/13 01:45:41 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/shims-0.9.45.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/shims-0.9.45.jar
	25/08/13 01:45:41 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/shims-0.9.45.jar' for reading
	25/08/13 01:45:41 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/slf4j-api-2.0.7.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/slf4j-api-2.0.7.jar
	25/08/13 01:45:41 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/slf4j-api-2.0.7.jar' for reading
[INFO] 2025-08-13 01:45:42.825 +0000 -  ->
	25/08/13 01:45:41 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/snakeyaml-2.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/snakeyaml-2.1.jar
	25/08/13 01:45:41 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/snakeyaml-2.1.jar' for reading
	25/08/13 01:45:41 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/snakeyaml-engine-2.6.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/snakeyaml-engine-2.6.jar
	25/08/13 01:45:41 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/snakeyaml-engine-2.6.jar' for reading
	25/08/13 01:45:41 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/snappy-java-1.1.10.5.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/snappy-java-1.1.10.5.jar
	25/08/13 01:45:42 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/snappy-java-1.1.10.5.jar' for reading
	25/08/13 01:45:42 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-acl-1.0.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-acl-1.0.0.jar
	25/08/13 01:45:42 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-acl-1.0.0.jar' for reading
	25/08/13 01:45:42 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-catalyst_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-catalyst_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:42 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-catalyst_2.12-3.5.2-amzn-1.jar' for reading
	25/08/13 01:45:42 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-common-utils_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-common-utils_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:42 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-common-utils_2.12-3.5.2-amzn-1.jar' for reading
	25/08/13 01:45:42 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-core_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-core_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:42 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-core_2.12-3.5.2-amzn-1.jar' for reading
	25/08/13 01:45:42 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-fgac-iceberg_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-fgac-iceberg_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:42 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-fgac-iceberg_2.12-3.5.2-amzn-1.jar' for reading
	25/08/13 01:45:42 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-fgac_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-fgac_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:42 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-fgac_2.12-3.5.2-amzn-1.jar' for reading
[INFO] 2025-08-13 01:45:43.827 +0000 -  ->
	25/08/13 01:45:42 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-ganglia-lgpl_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-ganglia-lgpl_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:42 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-ganglia-lgpl_2.12-3.5.2-amzn-1.jar' for reading
	25/08/13 01:45:42 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-graphx_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-graphx_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:42 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-graphx_2.12-3.5.2-amzn-1.jar' for reading
	25/08/13 01:45:43 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-hive-thriftserver_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-hive-thriftserver_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:43 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-hive-thriftserver_2.12-3.5.2-amzn-1.jar' for reading
	25/08/13 01:45:43 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-hive_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-hive_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:43 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-hive_2.12-3.5.2-amzn-1.jar' for reading
	25/08/13 01:45:43 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-kvstore_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-kvstore_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:43 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-kvstore_2.12-3.5.2-amzn-1.jar' for reading
	25/08/13 01:45:43 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-launcher_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-launcher_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:43 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-launcher_2.12-3.5.2-amzn-1.jar' for reading
	25/08/13 01:45:43 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-mllib-local_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-mllib-local_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:43 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-mllib-local_2.12-3.5.2-amzn-1.jar' for reading
	25/08/13 01:45:43 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-mllib_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-mllib_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:43 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-mllib_2.12-3.5.2-amzn-1.jar' for reading
	25/08/13 01:45:43 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-network-common_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-network-common_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:43 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-network-common_2.12-3.5.2-amzn-1.jar' for reading
	25/08/13 01:45:43 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-network-shuffle_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-network-shuffle_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:43 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-network-shuffle_2.12-3.5.2-amzn-1.jar' for reading
[INFO] 2025-08-13 01:45:44.828 +0000 -  ->
	25/08/13 01:45:43 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-repl_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-repl_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:43 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-repl_2.12-3.5.2-amzn-1.jar' for reading
	25/08/13 01:45:43 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-sketch_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-sketch_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:43 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-sketch_2.12-3.5.2-amzn-1.jar' for reading
	25/08/13 01:45:43 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-sql-api_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-sql-api_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:43 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-sql-api_2.12-3.5.2-amzn-1.jar' for reading
	25/08/13 01:45:44 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-sql_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-sql_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-sql_2.12-3.5.2-amzn-1.jar' for reading
	25/08/13 01:45:44 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-streaming_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-streaming_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-streaming_2.12-3.5.2-amzn-1.jar' for reading
	25/08/13 01:45:44 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-tags_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-tags_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-tags_2.12-3.5.2-amzn-1.jar' for reading
	25/08/13 01:45:44 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-unsafe_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-unsafe_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-unsafe_2.12-3.5.2-amzn-1.jar' for reading
	25/08/13 01:45:44 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spark-yarn_2.12-3.5.2-amzn-1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spark-yarn_2.12-3.5.2-amzn-1.jar
	25/08/13 01:45:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spark-yarn_2.12-3.5.2-amzn-1.jar' for reading
	25/08/13 01:45:44 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spire-macros_2.12-0.17.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spire-macros_2.12-0.17.0.jar
	25/08/13 01:45:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spire-macros_2.12-0.17.0.jar' for reading
	25/08/13 01:45:44 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spire-platform_2.12-0.17.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spire-platform_2.12-0.17.0.jar
	25/08/13 01:45:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spire-platform_2.12-0.17.0.jar' for reading
	25/08/13 01:45:44 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spire-util_2.12-0.17.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spire-util_2.12-0.17.0.jar
	25/08/13 01:45:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spire-util_2.12-0.17.0.jar' for reading
	25/08/13 01:45:44 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/spire_2.12-0.17.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/spire_2.12-0.17.0.jar
	25/08/13 01:45:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/spire_2.12-0.17.0.jar' for reading
[INFO] 2025-08-13 01:45:45.829 +0000 -  ->
	25/08/13 01:45:44 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/stax-api-1.0.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/stax-api-1.0.1.jar
	25/08/13 01:45:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/stax-api-1.0.1.jar' for reading
	25/08/13 01:45:44 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/stream-2.9.6.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/stream-2.9.6.jar
	25/08/13 01:45:44 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/stream-2.9.6.jar' for reading
	25/08/13 01:45:45 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/super-csv-2.2.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/super-csv-2.2.0.jar
	25/08/13 01:45:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/super-csv-2.2.0.jar' for reading
	25/08/13 01:45:45 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/threeten-extra-1.7.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/threeten-extra-1.7.1.jar
	25/08/13 01:45:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/threeten-extra-1.7.1.jar' for reading
	25/08/13 01:45:45 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/tink-1.9.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/tink-1.9.0.jar
	25/08/13 01:45:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/tink-1.9.0.jar' for reading
	25/08/13 01:45:45 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/transaction-api-1.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/transaction-api-1.1.jar
	25/08/13 01:45:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/transaction-api-1.1.jar' for reading
	25/08/13 01:45:45 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/univocity-parsers-2.9.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/univocity-parsers-2.9.1.jar
	25/08/13 01:45:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/univocity-parsers-2.9.1.jar' for reading
	25/08/13 01:45:45 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/volcano-client-6.7.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/volcano-client-6.7.2.jar
	25/08/13 01:45:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/volcano-client-6.7.2.jar' for reading
	25/08/13 01:45:45 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/volcano-model-v1beta1-6.7.2.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/volcano-model-v1beta1-6.7.2.jar
	25/08/13 01:45:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/volcano-model-v1beta1-6.7.2.jar' for reading
	25/08/13 01:45:45 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/xbean-asm9-shaded-4.23.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/xbean-asm9-shaded-4.23.jar
	25/08/13 01:45:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/xbean-asm9-shaded-4.23.jar' for reading
	25/08/13 01:45:45 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/xz-1.9.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/xz-1.9.jar
	25/08/13 01:45:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/xz-1.9.jar' for reading
	25/08/13 01:45:45 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/zjsonpatch-0.3.0.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/zjsonpatch-0.3.0.jar
	25/08/13 01:45:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/zjsonpatch-0.3.0.jar' for reading
[INFO] 2025-08-13 01:45:46.830 +0000 -  ->
	25/08/13 01:45:45 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/zookeeper-3.9.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/zookeeper-3.9.1.jar
	25/08/13 01:45:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/zookeeper-3.9.1.jar' for reading
	25/08/13 01:45:45 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/zookeeper-jute-3.9.1.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/zookeeper-jute-3.9.1.jar
	25/08/13 01:45:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/zookeeper-jute-3.9.1.jar' for reading
	25/08/13 01:45:45 INFO Client: Uploading resource s3://exchanges-flink-prod/batch/emr/spark-jars/zstd-jni-1.5.5-4.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/zstd-jni-1.5.5-4.jar
	25/08/13 01:45:45 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/batch/emr/spark-jars/zstd-jni-1.5.5-4.jar' for reading
	25/08/13 01:45:46 INFO Client: Uploading resource s3://exchanges-flink-prod/dolphinscheduler/default/resources/connectors/mysql-connector-j-8.0.33.jar -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/mysql-connector-j-8.0.33.jar
	25/08/13 01:45:46 INFO S3NativeFileSystem: Opening 's3://exchanges-flink-prod/dolphinscheduler/default/resources/connectors/mysql-connector-j-8.0.33.jar' for reading
	25/08/13 01:45:46 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hive-site.xml
	25/08/13 01:45:46 INFO Client: Uploading resource file:/etc/hudi/conf/hudi-defaults.conf -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/hudi-defaults.conf
	25/08/13 01:45:46 INFO Client: Uploading resource file:/mnt/spark/python/lib/pyspark.zip -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/pyspark.zip
	25/08/13 01:45:46 INFO Client: Uploading resource file:/mnt/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/py4j-0.10.9.7-src.zip
	25/08/13 01:45:46 INFO Client: Uploading resource file:/tmp/spark-94bf0aa0-e6a3-401c-8cd5-ef6b95318503/__spark_conf__5041893426354324587.zip -> hdfs://ha-nn-uri/user/root/.sparkStaging/application_1754991343133_0495/__spark_conf__.zip
	25/08/13 01:45:46 INFO SecurityManager: Changing view acls to: root
	25/08/13 01:45:46 INFO SecurityManager: Changing modify acls to: root
	25/08/13 01:45:46 INFO SecurityManager: Changing view acls groups to:
	25/08/13 01:45:46 INFO SecurityManager: Changing modify acls groups to:
	25/08/13 01:45:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
	25/08/13 01:45:46 INFO Client: Submitting application application_1754991343133_0495 to ResourceManager
	25/08/13 01:45:46 INFO YarnClientImpl: Submitted application application_1754991343133_0495
[INFO] 2025-08-13 01:45:47.831 +0000 -  ->
	25/08/13 01:45:47 INFO Client: Application report for application_1754991343133_0495 (state: ACCEPTED)
	25/08/13 01:45:47 INFO Client:
		 client token: N/A
		 diagnostics: AM container is launched, waiting for AM container to Register with RM
		 ApplicationMaster host: N/A
		 ApplicationMaster RPC port: -1
		 queue: root.default
		 start time: 1755049546427
		 final status: UNDEFINED
		 tracking URL: http://ip-172-24-13-28.ap-southeast-1.compute.internal:20888/proxy/application_1754991343133_0495/
		 user: root
[INFO] 2025-08-13 01:45:52.833 +0000 -  ->
	25/08/13 01:45:52 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-24-13-28.ap-southeast-1.compute.internal, PROXY_URI_BASES -> http://ip-172-24-13-28.ap-southeast-1.compute.internal:20888/proxy/application_1754991343133_0495, RM_HA_URLS -> ip-172-24-13-28.ap-southeast-1.compute.internal:8088,ip-172-24-13-23.ap-southeast-1.compute.internal:8088,ip-172-24-14-248.ap-southeast-1.compute.internal:8088), /proxy/application_1754991343133_0495
	25/08/13 01:45:52 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
	25/08/13 01:45:52 INFO Client: Application report for application_1754991343133_0495 (state: RUNNING)
	25/08/13 01:45:52 INFO Client:
		 client token: N/A
		 diagnostics: N/A
		 ApplicationMaster host: 172.24.12.37
		 ApplicationMaster RPC port: -1
		 queue: root.default
		 start time: 1755049546427
		 final status: UNDEFINED
		 tracking URL: http://ip-172-24-13-28.ap-southeast-1.compute.internal:20888/proxy/application_1754991343133_0495/
		 user: root
	25/08/13 01:45:52 INFO YarnClientSchedulerBackend: Application application_1754991343133_0495 has started running.
	25/08/13 01:45:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38549.
	25/08/13 01:45:52 INFO NettyBlockTransferService: Server created on ip-172-24-14-168.ap-southeast-1.compute.internal:38549
	25/08/13 01:45:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	25/08/13 01:45:52 INFO BlockManager: external shuffle service port = 7337
	25/08/13 01:45:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-24-14-168.ap-southeast-1.compute.internal, 38549, None)
	25/08/13 01:45:52 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-24-14-168.ap-southeast-1.compute.internal:38549 with 127.2 MiB RAM, BlockManagerId(driver, ip-172-24-14-168.ap-southeast-1.compute.internal, 38549, None)
	25/08/13 01:45:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-24-14-168.ap-southeast-1.compute.internal, 38549, None)
	25/08/13 01:45:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-24-14-168.ap-southeast-1.compute.internal, 38549, None)
	25/08/13 01:45:52 INFO SingleEventLogFileWriter: Logging events to hdfs:/var/log/spark/apps/application_1754991343133_0495.inprogress
[INFO] 2025-08-13 01:45:53.834 +0000 -  ->
	25/08/13 01:45:52 INFO Utils: Using 100 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /executors/heapHistogram: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /executors/heapHistogram/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	25/08/13 01:45:52 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
	✅ Spark会话创建完成，版本: 3.5.2-amzn-1
	MySQL配置: cex-mysql-ex-test-cluster.cluster-c5mgk4qm8m2z.ap-southeast-1.rds.amazonaws.com:3358/biz_statistics
	🗄️  HiveMeta初始化完成，将使用分区: 2025-08-13
	🗄️  MysqlMeta初始化完成
	🔍 TagRuleParser初始化完成
	🚀 TagEngine初始化完成

	🔍 执行健康检查...
	🔍 执行系统健康检查...
	✅ MySQL连接测试成功
[INFO] 2025-08-13 01:45:54.835 +0000 -  ->
	25/08/13 01:45:54 WARN HiveConf: HiveConf of name hive.server2.thrift.url does not exist
	   ✅ Hive访问正常，发现 0 个表
[INFO] 2025-08-13 01:45:59.836 +0000 -  ->
	25/08/13 01:45:59 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1) (ip-172-24-12-37.ap-southeast-1.compute.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1754991343133_0495/container_e03_1754991343133_0495_01_000007/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

[INFO] 2025-08-13 01:46:01.837 +0000 -  ->
	25/08/13 01:46:01 WARN TaskSetManager: Lost task 0.1 in stage 0.0 (TID 2) (ip-172-24-12-185.ap-southeast-1.compute.internal executor 56): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt2/yarn/usercache/root/appcache/application_1754991343133_0495/container_e03_1754991343133_0495_01_000057/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	25/08/13 01:46:01 WARN TaskSetManager: Lost task 1.1 in stage 0.0 (TID 3) (ip-172-24-14-121.ap-southeast-1.compute.internal executor 62): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt2/yarn/usercache/root/appcache/application_1754991343133_0495/container_e03_1754991343133_0495_01_000063/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

[INFO] 2025-08-13 01:46:02.838 +0000 -  ->
	25/08/13 01:46:02 WARN TaskSetManager: Lost task 0.2 in stage 0.0 (TID 4) (ip-172-24-12-37.ap-southeast-1.compute.internal executor 30): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1754991343133_0495/container_e03_1754991343133_0495_01_000031/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	25/08/13 01:46:02 WARN TaskSetManager: Lost task 1.2 in stage 0.0 (TID 5) (ip-172-24-14-83.ap-southeast-1.compute.internal executor 94): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt2/yarn/usercache/root/appcache/application_1754991343133_0495/container_e03_1754991343133_0495_01_000095/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

[INFO] 2025-08-13 01:46:03.839 +0000 -  ->
	25/08/13 01:46:03 WARN TaskSetManager: Lost task 0.3 in stage 0.0 (TID 6) (ip-172-24-12-27.ap-southeast-1.compute.internal executor 23): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1754991343133_0495/container_e03_1754991343133_0495_01_000024/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	25/08/13 01:46:03 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job
	   ❌ 工具函数测试失败: An error occurred while calling o294.count.
	: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6) (ip-172-24-12-27.ap-southeast-1.compute.internal executor 23): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1754991343133_0495/container_e03_1754991343133_0495_01_000024/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:
		at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3083)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3019)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3018)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3018)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1324)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1324)
		at scala.Option.foreach(Option.scala:407)
		at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1324)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3301)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3235)
		at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3224)
		at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:175)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:97)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:75)
		at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:59)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:290)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:289)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:583)
		at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:545)
		at org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3662)
		at org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3661)
		at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4392)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:711)
		at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4390)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)
		at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)
		at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:157)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$10(SQLExecution.scala:220)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)
		at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:220)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:405)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:219)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:83)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)
		at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4390)
		at org.apache.spark.sql.Dataset.count(Dataset.scala:3661)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
		at py4j.Gateway.invoke(Gateway.java:282)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
		at java.base/java.lang.Thread.run(Thread.java:840)
	Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1754991343133_0495/container_e03_1754991343133_0495_01_000024/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	📋 加载标签规则，指定标签: None
[INFO] 2025-08-13 01:46:04.841 +0000 -  ->
	25/08/13 01:46:03 WARN TaskSetManager: Lost task 1.3 in stage 0.0 (TID 7) (ip-172-24-12-185.ap-southeast-1.compute.internal executor 55): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6) (ip-172-24-12-27.ap-southeast-1.compute.internal executor 23): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
	  File "/mnt1/yarn/usercache/root/appcache/application_1754991343133_0495/container_e03_1754991343133_0495_01_000024/pyspark.zip/pyspark/worker.py", line 1100, in main
	    raise PySparkRuntimeError(
	pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 9) than that in driver 3.12, PySpark cannot run with different minor versions.
	Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
		at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
		at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
		at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
		at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)
		at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)
		at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
		at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
		at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45)
		at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:69)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
		at org.apache.spark.scheduler.Task.run(Task.scala:152)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

	Driver stacktrace:)
[INFO] 2025-08-13 01:46:05.842 +0000 -  ->
	✅ 标签规则加载完成: 50 个标签
[INFO] 2025-08-13 01:46:07.843 +0000 -  ->
	   ✅ 标签规则检查通过，发现 50 个活跃标签
	📋 健康检查结果:
	   MySQL连接: ✅
	   Hive访问: ✅
	   UDF功能: ❌
	   标签规则: ✅

	============================================================
	❌ 任务执行失败
	============================================================
	🧹 HiveMeta缓存已清理
	🧹 TagEngine资源清理完成
	🧹 关闭Spark会话...
	👋 程序退出
[INFO] 2025-08-13 01:46:07.843 +0000 - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/18540427537920/18540583925120_38/16574/97748, processId:2457780 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[INFO] 2025-08-13 01:46:07.844 +0000 - Start finding appId in /data/installed/dolphinscheduler/apache-dolphinscheduler-3.2.0-bin/worker-server/logs/20250813/18540583925120/38/16574/97748.log, fetch way: log
[INFO] 2025-08-13 01:46:07.844 +0000 - Find appId: application_1754991343133_0495 from /data/installed/dolphinscheduler/apache-dolphinscheduler-3.2.0-bin/worker-server/logs/20250813/18540583925120/38/16574/97748.log
[INFO] 2025-08-13 01:46:07.845 +0000 - ***********************************************************************************************
[INFO] 2025-08-13 01:46:07.845 +0000 - *********************************  Finalize task instance  ************************************
[INFO] 2025-08-13 01:46:07.845 +0000 - ***********************************************************************************************
[INFO] 2025-08-13 01:46:07.845 +0000 - Upload output files: [] successfully
[INFO] 2025-08-13 01:46:07.845 +0000 - Send task execute status: FAILURE to master : 172.24.14.168:1234
[INFO] 2025-08-13 01:46:07.845 +0000 - Remove the current task execute context from worker cache
[INFO] 2025-08-13 01:46:07.845 +0000 - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/18540427537920/18540583925120_38/16574/97748
[INFO] 2025-08-13 01:46:07.849 +0000 - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/18540427537920/18540583925120_38/16574/97748
[INFO] 2025-08-13 01:46:07.849 +0000 - FINALIZE_SESSION