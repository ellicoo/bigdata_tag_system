#!/usr/bin/env python3
"""
S3æ•°æ®æ£€æŸ¥æµ‹è¯•è„šæœ¬
ä½¿ç”¨Sparkè¯»å–S3/Hiveæ•°æ®ï¼ŒæŸ¥çœ‹æ•°æ®ç»“æ„å’Œå†…å®¹
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import logging
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, desc, asc
from src.common.config.manager import ConfigManager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class S3DataInspector:
    """S3æ•°æ®æ£€æŸ¥å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        self.config = ConfigManager.load_config('local')
        self.spark = self._create_spark_session()
        
    def _create_spark_session(self):
        """åˆ›å»ºSparkä¼šè¯"""
        logger.info("ğŸš€ åˆ›å»ºSparkä¼šè¯...")
        
        # è·å–JARæ–‡ä»¶è·¯å¾„
        jars_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "environments/local/jars")
        jar_files = [
            os.path.join(jars_dir, "mysql-connector-j-8.0.33.jar"),
            os.path.join(jars_dir, "hadoop-aws-3.3.4.jar"),
            os.path.join(jars_dir, "aws-java-sdk-bundle-1.11.1034.jar")
        ]
        existing_jars = [jar for jar in jar_files if os.path.exists(jar)]
        
        spark = SparkSession.builder \
            .appName("S3DataInspector") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000") \
            .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
            .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
            .config("spark.hadoop.fs.s3a.path.style.access", "true") \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .config("spark.jars", ",".join(existing_jars) if existing_jars else "") \
            .getOrCreate()
        
        # è®¾ç½®æ—¥å¿—çº§åˆ«
        spark.sparkContext.setLogLevel("ERROR")
        
        logger.info(f"âœ… Sparkä¼šè¯åˆ›å»ºæˆåŠŸ: {spark.sparkContext.applicationId}")
        return spark
    
    def inspect_table(self, table_name: str, s3_path: str):
        """æ£€æŸ¥æŒ‡å®šè¡¨çš„æ•°æ®"""
        logger.info(f"\nğŸ” æ£€æŸ¥è¡¨: {table_name}")
        logger.info("=" * 80)
        
        try:
            # è¯»å–S3æ•°æ®
            logger.info(f"ğŸ“– ä»S3è¯»å–æ•°æ®: {s3_path}")
            df = self.spark.read.parquet(s3_path)
            
            # åŸºæœ¬ä¿¡æ¯
            total_count = df.count()
            logger.info(f"ğŸ“Š æ€»è®°å½•æ•°: {total_count:,}")
            
            # æ•°æ®ç»“æ„
            logger.info(f"ğŸ“‹ æ•°æ®ç»“æ„:")
            df.printSchema()
            
            # æ•°æ®æ ·æœ¬ï¼ˆå‰10è¡Œï¼‰
            logger.info(f"ğŸ“ æ•°æ®æ ·æœ¬ï¼ˆå‰10è¡Œï¼‰:")
            df.show(10, truncate=False)
            
            # æ•°æ®ç»Ÿè®¡
            logger.info(f"ğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
            
            # å¯¹æ•°å€¼å‹åˆ—è¿›è¡Œç»Ÿè®¡
            numeric_cols = [field.name for field in df.schema.fields 
                          if field.dataType.typeName() in ['integer', 'long', 'float', 'double']]
            
            if numeric_cols:
                logger.info(f"ğŸ“Š æ•°å€¼å‹å­—æ®µç»Ÿè®¡: {numeric_cols}")
                df.select(numeric_cols).describe().show()
            
            # å¯¹å­—ç¬¦ä¸²å‹åˆ—è¿›è¡Œåˆ†ç»„ç»Ÿè®¡
            string_cols = [field.name for field in df.schema.fields 
                         if field.dataType.typeName() == 'string' and field.name != 'user_id']
            
            for col_name in string_cols[:3]:  # åªå±•ç¤ºå‰3ä¸ªå­—ç¬¦ä¸²åˆ—çš„åˆ†å¸ƒ
                logger.info(f"ğŸ“Š å­—æ®µ '{col_name}' åˆ†å¸ƒ:")
                df.groupBy(col_name).count().orderBy(desc("count")).show(10)
            
            # æ£€æŸ¥ç©ºå€¼
            logger.info(f"ğŸ” ç©ºå€¼æ£€æŸ¥:")
            null_counts = df.select([count(col(c)).alias(c) for c in df.columns]).collect()[0]
            total_records = df.count()
            
            null_info = []
            for col_name in df.columns:
                null_count = total_records - null_counts[col_name]
                null_pct = (null_count / total_records) * 100 if total_records > 0 else 0
                null_info.append((col_name, null_count, null_pct))
            
            for col_name, null_count, null_pct in null_info:
                if null_count > 0:
                    logger.info(f"   - {col_name}: {null_count:,} ä¸ªç©ºå€¼ ({null_pct:.1f}%)")
                else:
                    logger.info(f"   - {col_name}: æ— ç©ºå€¼ âœ…")
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥è¡¨ {table_name} å¤±è´¥: {str(e)}")
            return None
    
    def inspect_user_basic_info(self):
        """æ£€æŸ¥ç”¨æˆ·åŸºç¡€ä¿¡æ¯è¡¨"""
        s3_path = "s3a://test-data-lake/warehouse/user_basic_info"
        df = self.inspect_table("user_basic_info", s3_path)
        
        if df:
            logger.info(f"\nğŸ¯ ç”¨æˆ·åŸºç¡€ä¿¡æ¯è¡¨ - ä¸šåŠ¡é€»è¾‘éªŒè¯:")
            
            # å¹´é¾„åˆ†å¸ƒ
            logger.info("ğŸ“Š å¹´é¾„åˆ†å¸ƒ:")
            df.groupBy("age").count().orderBy(asc("age")).show(20)
            
            # ç”¨æˆ·ç­‰çº§åˆ†å¸ƒ
            logger.info("ğŸ“Š ç”¨æˆ·ç­‰çº§åˆ†å¸ƒ:")
            df.groupBy("user_level").count().orderBy(desc("count")).show()
            
            # KYCçŠ¶æ€åˆ†å¸ƒ
            logger.info("ğŸ“Š KYCçŠ¶æ€åˆ†å¸ƒ:")
            df.groupBy("kyc_status").count().orderBy(desc("count")).show()
            
            # å¹´è½»ç”¨æˆ·ç»Ÿè®¡ (â‰¤30å²)
            young_users = df.filter(col("age") <= 30).count()
            logger.info(f"ğŸ‘¶ å¹´è½»ç”¨æˆ· (â‰¤30å²): {young_users:,} ä¸ª")
            
            # VIPç”¨æˆ·ç»Ÿè®¡
            vip_users = df.filter(
                (col("user_level").isin(["VIP2", "VIP3"])) & 
                (col("kyc_status") == "verified")
            ).count()
            logger.info(f"ğŸ’ VIPç”¨æˆ· (VIP2/3+éªŒè¯): {vip_users:,} ä¸ª")
    
    def inspect_user_asset_summary(self):
        """æ£€æŸ¥ç”¨æˆ·èµ„äº§æ±‡æ€»è¡¨"""
        s3_path = "s3a://test-data-lake/warehouse/user_asset_summary"
        df = self.inspect_table("user_asset_summary", s3_path)
        
        if df:
            logger.info(f"\nğŸ¯ ç”¨æˆ·èµ„äº§æ±‡æ€»è¡¨ - ä¸šåŠ¡é€»è¾‘éªŒè¯:")
            
            # èµ„äº§åˆ†å¸ƒç»Ÿè®¡
            logger.info("ğŸ’° æ€»èµ„äº§åˆ†å¸ƒ:")
            df.select("total_asset_value").describe().show()
            
            logger.info("ğŸ’µ ç°é‡‘ä½™é¢åˆ†å¸ƒ:")
            df.select("cash_balance").describe().show()
            
            # é«˜å‡€å€¼ç”¨æˆ·ç»Ÿè®¡ (â‰¥150K)
            high_net_worth = df.filter(col("total_asset_value") >= 150000).count()
            logger.info(f"ğŸ† é«˜å‡€å€¼ç”¨æˆ· (â‰¥150K): {high_net_worth:,} ä¸ª")
            
            # ç°é‡‘å¯Œè£•ç”¨æˆ·ç»Ÿè®¡ (â‰¥60Kç°é‡‘)
            cash_rich = df.filter(col("cash_balance") >= 60000).count()
            logger.info(f"ğŸ’¸ ç°é‡‘å¯Œè£•ç”¨æˆ· (â‰¥60K): {cash_rich:,} ä¸ª")
            
            # èµ„äº§åŒºé—´åˆ†å¸ƒ
            logger.info("ğŸ“Š æ€»èµ„äº§åŒºé—´åˆ†å¸ƒ:")
            df.select(
                (col("total_asset_value") / 1000).cast("int").alias("asset_k")
            ).groupBy("asset_k").count().orderBy("asset_k").show(20)
    
    def inspect_user_activity_summary(self):
        """æ£€æŸ¥ç”¨æˆ·æ´»åŠ¨æ±‡æ€»è¡¨"""
        s3_path = "s3a://test-data-lake/warehouse/user_activity_summary"
        df = self.inspect_table("user_activity_summary", s3_path)
        
        if df:
            logger.info(f"\nğŸ¯ ç”¨æˆ·æ´»åŠ¨æ±‡æ€»è¡¨ - ä¸šåŠ¡é€»è¾‘éªŒè¯:")
            
            # äº¤æ˜“æ¬¡æ•°åˆ†å¸ƒ
            logger.info("ğŸ“ˆ 30å¤©äº¤æ˜“æ¬¡æ•°åˆ†å¸ƒ:")
            df.select("trade_count_30d").describe().show()
            
            # é£é™©è¯„åˆ†åˆ†å¸ƒ
            logger.info("âš ï¸ é£é™©è¯„åˆ†åˆ†å¸ƒ:")
            df.select("risk_score").describe().show()
            
            # æ´»è·ƒäº¤æ˜“è€…ç»Ÿè®¡ (>15æ¬¡äº¤æ˜“)
            active_traders = df.filter(col("trade_count_30d") > 15).count()
            logger.info(f"ğŸ”¥ æ´»è·ƒäº¤æ˜“è€… (>15æ¬¡äº¤æ˜“): {active_traders:,} ä¸ª")
            
            # ä½é£é™©ç”¨æˆ·ç»Ÿè®¡ (â‰¤30åˆ†)
            low_risk = df.filter(col("risk_score") <= 30).count()
            logger.info(f"ğŸ›¡ï¸ ä½é£é™©ç”¨æˆ· (â‰¤30åˆ†): {low_risk:,} ä¸ª")
            
            # æœ€è¿‘æ´»è·ƒç”¨æˆ·ç»Ÿè®¡ï¼ˆæœ€è¿‘7å¤©ç™»å½•ï¼‰
            from pyspark.sql.functions import date_sub, current_date
            recent_active = df.filter(
                col("last_login_date") >= date_sub(current_date(), 7)
            ).count()
            logger.info(f"ğŸŒŸ æœ€è¿‘æ´»è·ƒç”¨æˆ· (7å¤©å†…ç™»å½•): {recent_active:,} ä¸ª")
            
            # éªŒè¯last_login_dateå­—æ®µæ˜¯å¦å­˜åœ¨
            if "last_login_date" in df.columns:
                logger.info("âœ… last_login_date å­—æ®µå·²æ­£ç¡®æ·»åŠ ")
                logger.info("ğŸ“… æœ€è¿‘ç™»å½•æ—¥æœŸæ ·æœ¬:")
                df.select("user_id", "last_login_date").show(10)
            else:
                logger.error("âŒ last_login_date å­—æ®µç¼ºå¤±")
    
    def inspect_user_transaction_detail(self):
        """æ£€æŸ¥ç”¨æˆ·äº¤æ˜“æ˜ç»†è¡¨"""
        s3_path = "s3a://test-data-lake/warehouse/user_transaction_detail"
        df = self.inspect_table("user_transaction_detail", s3_path)
        
        if df:
            logger.info(f"\nğŸ¯ ç”¨æˆ·äº¤æ˜“æ˜ç»†è¡¨ - ä¸šåŠ¡é€»è¾‘éªŒè¯:")
            
            # äº¤æ˜“ç±»å‹åˆ†å¸ƒ
            logger.info("ğŸ“Š äº¤æ˜“ç±»å‹åˆ†å¸ƒ:")
            df.groupBy("transaction_type").count().orderBy(desc("count")).show()
            
            # äº§å“ç±»å‹åˆ†å¸ƒ
            logger.info("ğŸ“Š äº§å“ç±»å‹åˆ†å¸ƒ:")
            df.groupBy("product_type").count().orderBy(desc("count")).show()
            
            # äº¤æ˜“çŠ¶æ€åˆ†å¸ƒ
            logger.info("ğŸ“Š äº¤æ˜“çŠ¶æ€åˆ†å¸ƒ:")
            df.groupBy("status").count().orderBy(desc("count")).show()
            
            # äº¤æ˜“é‡‘é¢åˆ†å¸ƒ
            logger.info("ğŸ’° äº¤æ˜“é‡‘é¢åˆ†å¸ƒ:")
            df.select("amount").describe().show()
    
    def run_full_inspection(self):
        """è¿è¡Œå®Œæ•´æ•°æ®æ£€æŸ¥"""
        logger.info("ğŸ” å¼€å§‹S3æ•°æ®å…¨é¢æ£€æŸ¥...")
        logger.info("ğŸ¯ è®¾ç½®NO_PROXYç¯å¢ƒå˜é‡ä»¥ç»•è¿‡ä»£ç†é™åˆ¶")
        
        try:
            # æ£€æŸ¥æ‰€æœ‰è¡¨
            self.inspect_user_basic_info()
            self.inspect_user_asset_summary() 
            self.inspect_user_activity_summary()
            self.inspect_user_transaction_detail()
            
            logger.info("\nğŸ‰ S3æ•°æ®æ£€æŸ¥å®Œæˆï¼")
            logger.info("ğŸ“Š æ‰€æœ‰è¡¨çš„æ•°æ®ç»“æ„å’Œå†…å®¹å·²å±•ç¤º")
            logger.info("âœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¯ç”¨äºæ ‡ç­¾è®¡ç®—æµ‹è¯•")
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®æ£€æŸ¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        
        finally:
            self.cleanup()
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.spark:
            self.spark.stop()
            logger.info("ğŸ§¹ Sparkä¼šè¯å·²å…³é—­")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¯åŠ¨S3æ•°æ®æ£€æŸ¥å·¥å…·")
    logger.info("ğŸ“ ç”¨é€”: ä½¿ç”¨Sparkè¯»å–S3æ•°æ®ï¼ŒæŸ¥çœ‹æ•°æ®ç»“æ„å’Œå†…å®¹")
    logger.info("âš ï¸ æ³¨æ„: è¯·ç¡®ä¿è®¾ç½®äº†NO_PROXYç¯å¢ƒå˜é‡")
    
    inspector = S3DataInspector()
    inspector.run_full_inspection()


if __name__ == "__main__":
    main()