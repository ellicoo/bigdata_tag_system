#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ‡ç­¾è´¨é‡æµ‹è¯•
æµ‹è¯•æ ‡ç­¾æ‰“æ ‡é€»è¾‘çš„æ­£ç¡®æ€§ï¼ŒåŒ…å«ç®€å•æ¡ä»¶å’Œå¤æ‚æ¡ä»¶çš„å…¨é¢éªŒè¯
"""
import pytest
import json
from pyspark.sql.functions import *
from pyspark.sql.types import *
from src.tag_engine.parser.TagRuleParser import TagRuleParser
from src.tag_engine.utils.tagExpressionUtils import buildParallelTagExpression


class TestTagQuality:
    """æ ‡ç­¾è´¨é‡æµ‹è¯•ç±»"""
    
    def test_simple_age_tag_quality(self, spark):
        """æµ‹è¯•ç®€å•å¹´é¾„æ ‡ç­¾è´¨é‡ - æˆå¹´ç”¨æˆ·æ ‡ç­¾"""
        print("\nğŸ§ª æµ‹è¯•ç®€å•æ¡ä»¶ï¼šæˆå¹´ç”¨æˆ·æ ‡ç­¾(age >= 18)")
        
        # æ ‡ç­¾è§„åˆ™ï¼šæˆå¹´ç”¨æˆ·
        rule_json = json.dumps({
            "logic": "AND",
            "conditions": [{
                "condition": {
                    "logic": "None",
                    "fields": [{
                        "table": "user_basic_info",
                        "field": "age", 
                        "operator": ">=",
                        "value": "18",
                        "type": "number"
                    }]
                }
            }]
        })
        
        # åˆ›å»ºæµ‹è¯•ç”¨æˆ·æ•°æ®
        test_data = [
            ("user001", 25),    # âœ… åº”è¯¥æœ‰æ ‡ç­¾
            ("user002", 17),    # âŒ ä¸åº”è¯¥æœ‰æ ‡ç­¾ 
            ("user003", 18),    # âœ… åº”è¯¥æœ‰æ ‡ç­¾(è¾¹ç•Œå€¼)
            ("user004", 30),    # âœ… åº”è¯¥æœ‰æ ‡ç­¾
            ("user005", 16)     # âŒ ä¸åº”è¯¥æœ‰æ ‡ç­¾
        ]
        
        testDF = spark.createDataFrame(test_data, ["user_id", "age"])
        
        # è§£æè§„åˆ™ä¸ºSQLæ¡ä»¶
        parser = TagRuleParser()
        sql_condition = parser.parseRuleToSql(rule_json, ["user_basic_info"])
        print(f"   SQLæ¡ä»¶: {sql_condition}")
        
        # æ„å»ºæ ‡ç­¾è¡¨è¾¾å¼ - éœ€è¦å°†è¡¨å‰ç¼€æ›¿æ¢ä¸ºå®é™…çš„åˆ—åæ ¼å¼
        simple_condition = sql_condition.replace("user_basic_info.", "")
        tag_conditions = [{'tag_id': 1, 'condition': simple_condition}]
        tag_expr = buildParallelTagExpression(tag_conditions)
        
        # åº”ç”¨æ ‡ç­¾é€»è¾‘
        resultDF = testDF.withColumn("tag_ids_array", tag_expr)
        results = resultDF.collect()
        
        # éªŒè¯ç»“æœ
        expected_results = {
            "user001": [1],  # age=25 >= 18 âœ…
            "user002": [],   # age=17 < 18 âŒ  
            "user003": [1],  # age=18 >= 18 âœ…
            "user004": [1],  # age=30 >= 18 âœ…
            "user005": []    # age=16 < 18 âŒ
        }
        
        for row in results:
            user_id = row.user_id
            actual_tags = row.tag_ids_array
            expected_tags = expected_results[user_id]
            print(f"   {user_id}: age={row.age} â†’ é¢„æœŸ{expected_tags}, å®é™…{actual_tags}")
            assert actual_tags == expected_tags, f"ç”¨æˆ·{user_id}æ ‡ç­¾ä¸åŒ¹é…"
        
        print("   âœ… ç®€å•å¹´é¾„æ ‡ç­¾æµ‹è¯•é€šè¿‡")
    
    def test_vip_level_tag_quality(self, spark):
        """æµ‹è¯•VIPç­‰çº§æ ‡ç­¾è´¨é‡ - æšä¸¾æ¡ä»¶"""
        print("\nğŸ§ª æµ‹è¯•æšä¸¾æ¡ä»¶ï¼šé«˜çº§VIPæ ‡ç­¾(VIP2,VIP3,VIP4)")
        
        # æ ‡ç­¾è§„åˆ™ï¼šé«˜çº§VIPç”¨æˆ·
        rule_json = json.dumps({
            "logic": "AND", 
            "conditions": [{
                "condition": {
                    "logic": "None",
                    "fields": [{
                        "table": "user_basic_info",
                        "field": "user_level",
                        "operator": "belongs_to", 
                        "value": ["VIP2", "VIP3", "VIP4"],
                        "type": "enum"
                    }]
                }
            }]
        })
        
        # åˆ›å»ºæµ‹è¯•ç”¨æˆ·æ•°æ®
        test_data = [
            ("user001", "VIP1"),     # âŒ ä¸åº”è¯¥æœ‰æ ‡ç­¾
            ("user002", "VIP2"),     # âœ… åº”è¯¥æœ‰æ ‡ç­¾
            ("user003", "VIP3"),     # âœ… åº”è¯¥æœ‰æ ‡ç­¾
            ("user004", "VIP4"),     # âœ… åº”è¯¥æœ‰æ ‡ç­¾
            ("user005", "NORMAL"),   # âŒ ä¸åº”è¯¥æœ‰æ ‡ç­¾
            ("user006", "VIP5")      # âŒ ä¸åº”è¯¥æœ‰æ ‡ç­¾(ä¸åœ¨åˆ—è¡¨ä¸­)
        ]
        
        testDF = spark.createDataFrame(test_data, ["user_id", "user_level"])
        
        # è§£æè§„åˆ™å¹¶åº”ç”¨
        parser = TagRuleParser()
        sql_condition = parser.parseRuleToSql(rule_json, ["user_basic_info"])
        print(f"   SQLæ¡ä»¶: {sql_condition}")
        
        # æ„å»ºæ ‡ç­¾è¡¨è¾¾å¼ - éœ€è¦å°†è¡¨å‰ç¼€æ›¿æ¢ä¸ºå®é™…çš„åˆ—åæ ¼å¼
        simple_condition = sql_condition.replace("user_basic_info.", "")
        tag_conditions = [{'tag_id': 2, 'condition': simple_condition}]
        tag_expr = buildParallelTagExpression(tag_conditions)
        
        resultDF = testDF.withColumn("tag_ids_array", tag_expr)
        results = resultDF.collect()
        
        # éªŒè¯ç»“æœ
        expected_results = {
            "user001": [],   # VIP1 ä¸åœ¨[VIP2,VIP3,VIP4] âŒ
            "user002": [2],  # VIP2 åœ¨åˆ—è¡¨ä¸­ âœ…
            "user003": [2],  # VIP3 åœ¨åˆ—è¡¨ä¸­ âœ… 
            "user004": [2],  # VIP4 åœ¨åˆ—è¡¨ä¸­ âœ…
            "user005": [],   # NORMAL ä¸åœ¨åˆ—è¡¨ä¸­ âŒ
            "user006": []    # VIP5 ä¸åœ¨åˆ—è¡¨ä¸­ âŒ
        }
        
        for row in results:
            user_id = row.user_id
            actual_tags = row.tag_ids_array
            expected_tags = expected_results[user_id]
            print(f"   {user_id}: level={row.user_level} â†’ é¢„æœŸ{expected_tags}, å®é™…{actual_tags}")
            assert actual_tags == expected_tags, f"ç”¨æˆ·{user_id}æ ‡ç­¾ä¸åŒ¹é…"
            
        print("   âœ… VIPç­‰çº§æ ‡ç­¾æµ‹è¯•é€šè¿‡")
    
    def test_complex_high_value_user_tag_quality(self, spark):
        """æµ‹è¯•å¤æ‚æ¡ä»¶ï¼šé«˜å‡€å€¼ç”¨æˆ·æ ‡ç­¾ - å¤šè¡¨ANDé€»è¾‘"""
        print("\nğŸ§ª æµ‹è¯•å¤æ‚ANDæ¡ä»¶ï¼šé«˜å‡€å€¼ç”¨æˆ·æ ‡ç­¾")
        print("   è§„åˆ™ï¼š(age >= 30 AND assets >= 100000) AND (kyc_status = 'verified')")
        
        # å¤æ‚æ ‡ç­¾è§„åˆ™ï¼šé«˜å‡€å€¼ç”¨æˆ·
        rule_json = json.dumps({
            "logic": "AND",
            "conditions": [
                {
                    "condition": {
                        "logic": "AND",
                        "fields": [
                            {
                                "table": "user_basic_info",
                                "field": "age",
                                "operator": ">=",
                                "value": "30", 
                                "type": "number"
                            },
                            {
                                "table": "user_asset_summary",
                                "field": "total_asset_value",
                                "operator": ">=",
                                "value": "100000",
                                "type": "number"
                            }
                        ]
                    }
                },
                {
                    "condition": {
                        "logic": "None",
                        "fields": [{
                            "table": "user_basic_info", 
                            "field": "kyc_status",
                            "operator": "=",
                            "value": "verified",
                            "type": "string"
                        }]
                    }
                }
            ]
        })
        
        # åˆ›å»ºå¤šè¡¨æµ‹è¯•æ•°æ®
        test_data = [
            # user_id, age, total_asset_value, kyc_status
            ("user001", 35, 150000, "verified"),    # âœ… å…¨éƒ¨æ»¡è¶³
            ("user002", 25, 200000, "verified"),    # âŒ å¹´é¾„ä¸å¤Ÿ
            ("user003", 40, 50000, "verified"),     # âŒ èµ„äº§ä¸å¤Ÿ
            ("user004", 35, 150000, "pending"),     # âŒ KYCæœªéªŒè¯
            ("user005", 45, 500000, "verified"),    # âœ… å…¨éƒ¨æ»¡è¶³
            ("user006", 28, 80000, "pending"),      # âŒ å…¨éƒ¨ä¸æ»¡è¶³
            ("user007", 30, 100000, "verified")     # âœ… è¾¹ç•Œå€¼å…¨éƒ¨æ»¡è¶³
        ]
        
        # åˆ›å»ºDataFrameï¼ˆæ¨¡æ‹Ÿå¤šè¡¨JOINåçš„ç»“æœï¼‰
        schema = StructType([
            StructField("user_id", StringType(), False),
            StructField("age", IntegerType(), False), 
            StructField("total_asset_value", IntegerType(), False),
            StructField("kyc_status", StringType(), False)
        ])
        
        testDF = spark.createDataFrame(test_data, schema)
        
        # è§£æè§„åˆ™å¹¶åº”ç”¨
        parser = TagRuleParser()
        sql_condition = parser.parseRuleToSql(rule_json, ["user_basic_info", "user_asset_summary"])
        print(f"   SQLæ¡ä»¶: {sql_condition}")
        
        # æ„å»ºæ ‡ç­¾è¡¨è¾¾å¼ - ç§»é™¤è¡¨å‰ç¼€
        simple_condition = sql_condition.replace("user_basic_info.", "").replace("user_asset_summary.", "")
        tag_conditions = [{'tag_id': 3, 'condition': simple_condition}]
        tag_expr = buildParallelTagExpression(tag_conditions)
        
        resultDF = testDF.withColumn("tag_ids_array", tag_expr)
        results = resultDF.collect()
        
        # éªŒè¯ç»“æœ
        expected_results = {
            "user001": [3],  # 35>=30 âœ…, 150000>=100000 âœ…, verified âœ…
            "user002": [],   # 25>=30 âŒ, 200000>=100000 âœ…, verified âœ…
            "user003": [],   # 40>=30 âœ…, 50000>=100000 âŒ, verified âœ…
            "user004": [],   # 35>=30 âœ…, 150000>=100000 âœ…, pendingâ‰ verified âŒ
            "user005": [3],  # 45>=30 âœ…, 500000>=100000 âœ…, verified âœ…
            "user006": [],   # 28>=30 âŒ, 80000>=100000 âŒ, pendingâ‰ verified âŒ
            "user007": [3]   # 30>=30 âœ…, 100000>=100000 âœ…, verified âœ…
        }
        
        for row in results:
            user_id = row.user_id
            actual_tags = row.tag_ids_array
            expected_tags = expected_results[user_id]
            
            # è¯¦ç»†éªŒè¯é€»è¾‘
            age_ok = row.age >= 30
            asset_ok = row.total_asset_value >= 100000
            kyc_ok = row.kyc_status == "verified"
            should_have_tag = age_ok and asset_ok and kyc_ok
            
            print(f"   {user_id}: age={row.age}({age_ok}), assets={row.total_asset_value}({asset_ok}), kyc={row.kyc_status}({kyc_ok}) â†’ é¢„æœŸ{expected_tags}, å®é™…{actual_tags}")
            
            assert actual_tags == expected_tags, f"ç”¨æˆ·{user_id}æ ‡ç­¾ä¸åŒ¹é…"
            assert (len(actual_tags) > 0) == should_have_tag, f"ç”¨æˆ·{user_id}é€»è¾‘åˆ¤æ–­é”™è¯¯"
            
        print("   âœ… å¤æ‚é«˜å‡€å€¼ç”¨æˆ·æ ‡ç­¾æµ‹è¯•é€šè¿‡")
    
    def test_complex_or_logic_tag_quality(self, spark):
        """æµ‹è¯•å¤æ‚ORæ¡ä»¶ï¼šæ´»è·ƒç”¨æˆ·æ ‡ç­¾"""
        print("\nğŸ§ª æµ‹è¯•å¤æ‚ORæ¡ä»¶ï¼šæ´»è·ƒç”¨æˆ·æ ‡ç­¾")
        print("   è§„åˆ™ï¼š(trade_count > 10) OR (last_login within 7 days) OR (cash_balance > 50000)")
        
        # å¤æ‚ORé€»è¾‘æ ‡ç­¾è§„åˆ™
        rule_json = json.dumps({
            "logic": "OR",
            "conditions": [
                {
                    "condition": {
                        "logic": "None",
                        "fields": [{
                            "table": "user_activity_summary",
                            "field": "trade_count_30d", 
                            "operator": ">",
                            "value": "10",
                            "type": "number"
                        }]
                    }
                },
                {
                    "condition": {
                        "logic": "None", 
                        "fields": [{
                            "table": "user_activity_summary",
                            "field": "days_since_login",
                            "operator": "<=",
                            "value": "7",
                            "type": "number"
                        }]
                    }
                },
                {
                    "condition": {
                        "logic": "None",
                        "fields": [{
                            "table": "user_asset_summary",
                            "field": "cash_balance",
                            "operator": ">", 
                            "value": "50000",
                            "type": "number"
                        }]
                    }
                }
            ]
        })
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = [
            # user_id, trade_count_30d, days_since_login, cash_balance
            ("user001", 15, 3, 30000),    # âœ… äº¤æ˜“å¤š
            ("user002", 5, 2, 20000),     # âœ… æœ€è¿‘ç™»å½•
            ("user003", 3, 15, 80000),    # âœ… ç°é‡‘å¤š
            ("user004", 20, 1, 100000),   # âœ… å…¨éƒ¨æ»¡è¶³
            ("user005", 2, 30, 10000),    # âŒ å…¨éƒ¨ä¸æ»¡è¶³
            ("user006", 11, 10, 60000),   # âœ… äº¤æ˜“å¤š+ç°é‡‘å¤š
            ("user007", 10, 7, 50000)     # âŒ éƒ½æ˜¯è¾¹ç•Œå€¼ä½†ä¸æ»¡è¶³
        ]
        
        schema = StructType([
            StructField("user_id", StringType(), False),
            StructField("trade_count_30d", IntegerType(), False),
            StructField("days_since_login", IntegerType(), False), 
            StructField("cash_balance", IntegerType(), False)
        ])
        
        testDF = spark.createDataFrame(test_data, schema)
        
        # è§£æè§„åˆ™å¹¶åº”ç”¨
        parser = TagRuleParser()
        sql_condition = parser.parseRuleToSql(rule_json, ["user_activity_summary", "user_asset_summary"])
        print(f"   SQLæ¡ä»¶: {sql_condition}")
        
        # æ„å»ºæ ‡ç­¾è¡¨è¾¾å¼ - ç§»é™¤è¡¨å‰ç¼€
        simple_condition = sql_condition.replace("user_activity_summary.", "").replace("user_asset_summary.", "")
        tag_conditions = [{'tag_id': 4, 'condition': simple_condition}]
        tag_expr = buildParallelTagExpression(tag_conditions)
        
        resultDF = testDF.withColumn("tag_ids_array", tag_expr)
        results = resultDF.collect()
        
        # éªŒè¯ç»“æœ
        for row in results:
            user_id = row.user_id
            actual_tags = row.tag_ids_array
            
            # ORé€»è¾‘éªŒè¯
            trade_ok = row.trade_count_30d > 10
            login_ok = row.days_since_login <= 7  
            cash_ok = row.cash_balance > 50000
            should_have_tag = trade_ok or login_ok or cash_ok
            
            expected_tags = [4] if should_have_tag else []
            
            print(f"   {user_id}: trade={row.trade_count_30d}({trade_ok}), login_days={row.days_since_login}({login_ok}), cash={row.cash_balance}({cash_ok}) â†’ é¢„æœŸ{expected_tags}, å®é™…{actual_tags}")
            
            assert actual_tags == expected_tags, f"ç”¨æˆ·{user_id}æ ‡ç­¾ä¸åŒ¹é…"
            
        print("   âœ… å¤æ‚ORé€»è¾‘æ ‡ç­¾æµ‹è¯•é€šè¿‡")
    
    def test_complex_nested_logic_tag_quality(self, spark):
        """æµ‹è¯•åµŒå¥—é€»è¾‘ï¼šè¶…çº§ç”¨æˆ·æ ‡ç­¾"""
        print("\nğŸ§ª æµ‹è¯•åµŒå¥—æ¡ä»¶ï¼šè¶…çº§ç”¨æˆ·æ ‡ç­¾")
        print("   è§„åˆ™ï¼š(age >= 25 AND assets >= 50000) AND ((vip_level IN [VIP3,VIP4]) OR (trade_count > 20))")
        
        # å¤æ‚åµŒå¥—é€»è¾‘æ ‡ç­¾è§„åˆ™
        rule_json = json.dumps({
            "logic": "AND",
            "conditions": [
                {
                    "condition": {
                        "logic": "AND",
                        "fields": [
                            {
                                "table": "user_basic_info",
                                "field": "age",
                                "operator": ">=", 
                                "value": "25",
                                "type": "number"
                            },
                            {
                                "table": "user_asset_summary", 
                                "field": "total_asset_value",
                                "operator": ">=",
                                "value": "50000",
                                "type": "number"
                            }
                        ]
                    }
                },
                {
                    "condition": {
                        "logic": "OR",
                        "conditions": [
                            {
                                "condition": {
                                    "logic": "None",
                                    "fields": [{
                                        "table": "user_basic_info",
                                        "field": "user_level", 
                                        "operator": "belongs_to",
                                        "value": ["VIP3", "VIP4"],
                                        "type": "enum"
                                    }]
                                }
                            },
                            {
                                "condition": {
                                    "logic": "None",
                                    "fields": [{
                                        "table": "user_activity_summary",
                                        "field": "trade_count_30d",
                                        "operator": ">",
                                        "value": "20", 
                                        "type": "number"
                                    }]
                                }
                            }
                        ]
                    }
                }
            ]
        })
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = [
            # user_id, age, total_asset_value, user_level, trade_count_30d
            ("user001", 30, 80000, "VIP3", 15),    # âœ… age+assets âœ…, VIP3 âœ…
            ("user002", 28, 60000, "VIP2", 25),    # âœ… age+assets âœ…, trade_count âœ…
            ("user003", 20, 100000, "VIP4", 5),    # âŒ ageä¸å¤Ÿ
            ("user004", 35, 30000, "VIP3", 10),    # âŒ assetsä¸å¤Ÿ
            ("user005", 40, 120000, "VIP2", 15),   # âŒ åŸºç¡€æ¡ä»¶æ»¡è¶³ï¼Œä½†VIPçº§åˆ«å’Œäº¤æ˜“éƒ½ä¸å¤Ÿ
            ("user006", 25, 50000, "VIP4", 21),    # âœ… å…¨éƒ¨æ»¡è¶³(è¾¹ç•Œå€¼)
            ("user007", 35, 200000, "VIP1", 50)    # âœ… age+assets âœ…, trade_count âœ…
        ]
        
        schema = StructType([
            StructField("user_id", StringType(), False),
            StructField("age", IntegerType(), False),
            StructField("total_asset_value", IntegerType(), False),
            StructField("user_level", StringType(), False), 
            StructField("trade_count_30d", IntegerType(), False)
        ])
        
        testDF = spark.createDataFrame(test_data, schema)
        
        # è§£æè§„åˆ™å¹¶åº”ç”¨
        parser = TagRuleParser()
        sql_condition = parser.parseRuleToSql(rule_json, ["user_basic_info", "user_asset_summary", "user_activity_summary"])
        print(f"   SQLæ¡ä»¶: {sql_condition}")
        
        # æ„å»ºæ ‡ç­¾è¡¨è¾¾å¼ - ç§»é™¤è¡¨å‰ç¼€
        simple_condition = sql_condition.replace("user_basic_info.", "").replace("user_asset_summary.", "").replace("user_activity_summary.", "")
        tag_conditions = [{'tag_id': 5, 'condition': simple_condition}]
        tag_expr = buildParallelTagExpression(tag_conditions)
        
        resultDF = testDF.withColumn("tag_ids_array", tag_expr)
        results = resultDF.collect()
        
        # éªŒè¯ç»“æœ
        for row in results:
            user_id = row.user_id
            actual_tags = row.tag_ids_array
            
            # åµŒå¥—é€»è¾‘éªŒè¯
            # ç¬¬ä¸€å±‚ANDï¼šåŸºç¡€æ¡ä»¶
            age_ok = row.age >= 25
            asset_ok = row.total_asset_value >= 50000
            basic_ok = age_ok and asset_ok
            
            # ç¬¬äºŒå±‚ORï¼šVIPæˆ–æ´»è·ƒ
            vip_ok = row.user_level in ["VIP3", "VIP4"]
            trade_ok = row.trade_count_30d > 20
            advanced_ok = vip_ok or trade_ok
            
            # æœ€ç»ˆç»“æœ
            should_have_tag = basic_ok and advanced_ok
            expected_tags = [5] if should_have_tag else []
            
            print(f"   {user_id}: åŸºç¡€({age_ok}&{asset_ok}={basic_ok}) AND é«˜çº§({vip_ok}|{trade_ok}={advanced_ok}) = {should_have_tag} â†’ é¢„æœŸ{expected_tags}, å®é™…{actual_tags}")
            
            assert actual_tags == expected_tags, f"ç”¨æˆ·{user_id}æ ‡ç­¾ä¸åŒ¹é…"
            
        print("   âœ… å¤æ‚åµŒå¥—é€»è¾‘æ ‡ç­¾æµ‹è¯•é€šè¿‡")
    
    def test_multi_tag_parallel_quality(self, spark):
        """æµ‹è¯•å¤šæ ‡ç­¾å¹¶è¡Œè®¡ç®—è´¨é‡"""
        print("\nğŸ§ª æµ‹è¯•å¤šæ ‡ç­¾å¹¶è¡Œè®¡ç®—è´¨é‡")
        print("   åŒæ—¶è®¡ç®—ï¼šæˆå¹´æ ‡ç­¾(1) + VIPæ ‡ç­¾(2) + é«˜å‡€å€¼æ ‡ç­¾(3)")
        
        # å‡†å¤‡å¤šä¸ªæ ‡ç­¾çš„æ¡ä»¶
        tag_conditions = [
            # æ ‡ç­¾1ï¼šæˆå¹´ç”¨æˆ· (age >= 18)  
            {'tag_id': 1, 'condition': 'age >= 18'},
            # æ ‡ç­¾2ï¼šVIPç”¨æˆ· (user_level IN ('VIP2','VIP3','VIP4'))
            {'tag_id': 2, 'condition': "user_level IN ('VIP2','VIP3','VIP4')"},
            # æ ‡ç­¾3ï¼šé«˜èµ„äº§ç”¨æˆ· (total_asset_value >= 100000)
            {'tag_id': 3, 'condition': 'total_asset_value >= 100000'}
        ]
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = [
            # user_id, age, user_level, total_asset_value
            ("user001", 25, "VIP3", 150000),   # âœ… åº”è¯¥æœ‰æ ‡ç­¾ [1,2,3]
            ("user002", 17, "VIP2", 200000),   # âœ… åº”è¯¥æœ‰æ ‡ç­¾ [2,3] (æœªæˆå¹´)
            ("user003", 30, "VIP1", 50000),    # âœ… åº”è¯¥æœ‰æ ‡ç­¾ [1] (æ™®é€šVIP,ä½èµ„äº§)
            ("user004", 16, "NORMAL", 20000),  # âŒ åº”è¯¥æœ‰æ ‡ç­¾ [] (å…¨éƒ¨ä¸æ»¡è¶³)
            ("user005", 35, "VIP4", 500000),   # âœ… åº”è¯¥æœ‰æ ‡ç­¾ [1,2,3] (å…¨éƒ¨æ»¡è¶³)
            ("user006", 18, "VIP2", 100000)    # âœ… åº”è¯¥æœ‰æ ‡ç­¾ [1,2,3] (è¾¹ç•Œå€¼å…¨éƒ¨æ»¡è¶³)
        ]
        
        schema = StructType([
            StructField("user_id", StringType(), False),
            StructField("age", IntegerType(), False),
            StructField("user_level", StringType(), False),
            StructField("total_asset_value", IntegerType(), False)
        ])
        
        testDF = spark.createDataFrame(test_data, schema)
        
        # æ„å»ºå¹¶è¡Œæ ‡ç­¾è¡¨è¾¾å¼
        tag_expr = buildParallelTagExpression(tag_conditions)
        resultDF = testDF.withColumn("tag_ids_array", tag_expr)
        results = resultDF.collect()
        
        # éªŒè¯ç»“æœ
        expected_results = {
            "user001": [1, 2, 3],  # 25>=18 âœ…, VIP3âˆˆ[VIP2,VIP3,VIP4] âœ…, 150000>=100000 âœ…
            "user002": [2, 3],     # 17>=18 âŒ, VIP2âˆˆ[VIP2,VIP3,VIP4] âœ…, 200000>=100000 âœ…  
            "user003": [1],        # 30>=18 âœ…, VIP1âˆˆ[VIP2,VIP3,VIP4] âŒ, 50000>=100000 âŒ
            "user004": [],         # 16>=18 âŒ, NORMALâˆˆ[VIP2,VIP3,VIP4] âŒ, 20000>=100000 âŒ
            "user005": [1, 2, 3],  # 35>=18 âœ…, VIP4âˆˆ[VIP2,VIP3,VIP4] âœ…, 500000>=100000 âœ…
            "user006": [1, 2, 3]   # 18>=18 âœ…, VIP2âˆˆ[VIP2,VIP3,VIP4] âœ…, 100000>=100000 âœ…
        }
        
        for row in results:
            user_id = row.user_id
            actual_tags = sorted(row.tag_ids_array) if row.tag_ids_array else []
            expected_tags = expected_results[user_id]
            
            # é€ä¸ªéªŒè¯æ¡ä»¶
            age_ok = row.age >= 18
            vip_ok = row.user_level in ["VIP2", "VIP3", "VIP4"] 
            asset_ok = row.total_asset_value >= 100000
            
            print(f"   {user_id}: age={row.age}({age_ok}), level={row.user_level}({vip_ok}), assets={row.total_asset_value}({asset_ok}) â†’ é¢„æœŸ{expected_tags}, å®é™…{actual_tags}")
            
            assert actual_tags == expected_tags, f"ç”¨æˆ·{user_id}å¤šæ ‡ç­¾å¹¶è¡Œè®¡ç®—ç»“æœä¸åŒ¹é…"
            
        print("   âœ… å¤šæ ‡ç­¾å¹¶è¡Œè®¡ç®—è´¨é‡æµ‹è¯•é€šè¿‡")
    
    def test_edge_cases_tag_quality(self, spark):
        """æµ‹è¯•è¾¹ç•Œæƒ…å†µæ ‡ç­¾è´¨é‡"""
        print("\nğŸ§ª æµ‹è¯•è¾¹ç•Œæƒ…å†µæ ‡ç­¾è´¨é‡")
        
        # æµ‹è¯•NULLå€¼å¤„ç†
        test_data = [
            ("user001", None, "VIP2", 100000),    # ageä¸ºNULL
            ("user002", 25, None, 150000),        # user_levelä¸ºNULL
            ("user003", 30, "VIP3", None),        # total_asset_valueä¸ºNULL
            ("user004", 25, "VIP2", 100000)       # æ­£å¸¸æ•°æ®
        ]
        
        schema = StructType([
            StructField("user_id", StringType(), False),
            StructField("age", IntegerType(), True),      # å…è®¸NULL
            StructField("user_level", StringType(), True), # å…è®¸NULL
            StructField("total_asset_value", IntegerType(), True) # å…è®¸NULL
        ])
        
        testDF = spark.createDataFrame(test_data, schema)
        
        # æµ‹è¯•æ¡ä»¶ï¼šage >= 25 AND user_level = 'VIP2' AND total_asset_value >= 100000
        tag_conditions = [
            {'tag_id': 99, 'condition': "age >= 25 AND user_level = 'VIP2' AND total_asset_value >= 100000"}
        ]
        
        tag_expr = buildParallelTagExpression(tag_conditions)  
        resultDF = testDF.withColumn("tag_ids_array", tag_expr)
        results = resultDF.collect()
        
        print("   æµ‹è¯•NULLå€¼å¤„ç†:")
        for row in results:
            user_id = row.user_id
            actual_tags = row.tag_ids_array
            
            print(f"   {user_id}: age={row.age}, level={row.user_level}, assets={row.total_asset_value} â†’ æ ‡ç­¾{actual_tags}")
            
            # åªæœ‰user004åº”è¯¥æœ‰æ ‡ç­¾ï¼Œå…¶ä»–å› ä¸ºæœ‰NULLå€¼éƒ½ä¸åº”è¯¥æœ‰æ ‡ç­¾
            if user_id == "user004":
                assert actual_tags == [99], f"ç”¨æˆ·{user_id}åº”è¯¥æœ‰æ ‡ç­¾"
            else:
                assert actual_tags == [], f"ç”¨æˆ·{user_id}å› NULLå€¼ä¸åº”è¯¥æœ‰æ ‡ç­¾"
                
        print("   âœ… è¾¹ç•Œæƒ…å†µæ ‡ç­¾è´¨é‡æµ‹è¯•é€šè¿‡")


if __name__ == "__main__":
    pytest.main([__file__, "-v"])