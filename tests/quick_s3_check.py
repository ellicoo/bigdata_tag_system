#!/usr/bin/env python3
"""
S3æ•°æ®å¿«é€Ÿæ£€æŸ¥è„šæœ¬
å¿«é€ŸæŸ¥çœ‹S3ä¸­å„è¡¨çš„åŸºæœ¬ä¿¡æ¯å’Œæ•°æ®æ ·æœ¬
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import logging
from pyspark.sql import SparkSession

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def create_spark_session():
    """åˆ›å»ºSparkä¼šè¯"""
    # è·å–JARæ–‡ä»¶è·¯å¾„
    jars_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "environments/local/jars")
    jar_files = [
        os.path.join(jars_dir, "mysql-connector-j-8.0.33.jar"),
        os.path.join(jars_dir, "hadoop-aws-3.3.4.jar"),
        os.path.join(jars_dir, "aws-java-sdk-bundle-1.11.1034.jar")
    ]
    existing_jars = [jar for jar in jar_files if os.path.exists(jar)]
    
    spark = SparkSession.builder \
        .appName("QuickS3Check") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.jars", ",".join(existing_jars) if existing_jars else "") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("ERROR")
    return spark


def quick_check_table(spark, table_name: str, s3_path: str):
    """å¿«é€Ÿæ£€æŸ¥è¡¨"""
    logger.info(f"\nğŸ“‹ {table_name}")
    logger.info("-" * 50)
    
    try:
        df = spark.read.parquet(s3_path)
        count = df.count()
        cols = len(df.columns)
        
        logger.info(f"ğŸ“Š è®°å½•æ•°: {count:,}")
        logger.info(f"ğŸ“Š å­—æ®µæ•°: {cols}")
        logger.info(f"ğŸ“‹ å­—æ®µå: {', '.join(df.columns)}")
        
        # æ˜¾ç¤ºå‰3è¡Œæ•°æ®
        logger.info("ğŸ“ æ•°æ®æ ·æœ¬:")
        df.show(3, truncate=False)
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ£€æŸ¥å¤±è´¥: {str(e)}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ S3æ•°æ®å¿«é€Ÿæ£€æŸ¥")
    logger.info("=" * 60)
    
    spark = create_spark_session()
    
    try:
        # å®šä¹‰è¦æ£€æŸ¥çš„è¡¨
        tables = [
            ("ç”¨æˆ·åŸºç¡€ä¿¡æ¯", "s3a://test-data-lake/warehouse/user_basic_info"),
            ("ç”¨æˆ·èµ„äº§æ±‡æ€»", "s3a://test-data-lake/warehouse/user_asset_summary"),
            ("ç”¨æˆ·æ´»åŠ¨æ±‡æ€»", "s3a://test-data-lake/warehouse/user_activity_summary"),
            ("ç”¨æˆ·äº¤æ˜“æ˜ç»†", "s3a://test-data-lake/warehouse/user_transaction_detail")
        ]
        
        success_count = 0
        for table_name, s3_path in tables:
            if quick_check_table(spark, table_name, s3_path):
                success_count += 1
        
        logger.info(f"\nğŸ‰ æ£€æŸ¥å®Œæˆ: {success_count}/{len(tables)} ä¸ªè¡¨æˆåŠŸ")
        
    except Exception as e:
        logger.error(f"âŒ æ£€æŸ¥è¿‡ç¨‹å‡ºé”™: {str(e)}")
    
    finally:
        spark.stop()


if __name__ == "__main__":
    main()