# 大数据侧标签系统实施方案

## 1. 职责边界明确

### 1.1 职责范围（大数据侧）
✅ **数据计算引擎**：
- 从S3读取用户业务数据（Hive表）
- 从MySQL读取标签规则配置
- 使用Spark执行标签计算逻辑
- 合并用户标签结果
- 将最终标签结果写回MySQL

❌ **不在你的职责范围**：
- 标签管理后台开发
- 用户界面和API开发
- 业务系统集成
- 前端标签配置界面

### 1.2 技术架构图（数据侧）
```
┌─────────────┐    ┌──────────────┐    ┌─────────────┐    ┌─────────────┐
│  S3 Hive表   │    │    MySQL     │    │    Spark    │    │   MySQL     │
│  用户业务数据 │───▶│  标签规则表   │───▶│   计算引擎   │───▶│  标签结果表  │
│             │    │              │    │             │    │             │
└─────────────┘    └──────────────┘    └─────────────┘    └─────────────┘
```

## 2. 数据侧技术方案

### 2.1 核心数据流
```python
# 完整的数据处理流程
def main_tag_processing_pipeline():
    """标签计算主流程"""
    
    # 1. 初始化Spark环境
    spark = init_spark_session()
    
    # 2. 读取所有待执行的标签规则
    active_rules = read_active_tag_rules(spark)
    
    # 3. 按数据表分组，批量读取业务数据
    data_groups = group_rules_by_data_source(active_rules)
    
    # 4. 执行标签计算
    all_tag_results = []
    for data_source, rules in data_groups.items():
        # 读取业务数据
        business_data = read_hive_data_from_s3(spark, data_source)
        
        # 批量计算多个标签
        for rule in rules:
            tag_result = compute_single_tag(business_data, rule)
            all_tag_results.append(tag_result)
    
    # 5. 合并所有标签结果
    final_result = merge_all_tag_results(spark, all_tag_results)
    
    # 6. 写入MySQL结果表
    write_tag_results_to_mysql(final_result)
    
    # 7. 清理资源
    spark.stop()
```

### 2.2 关键模块设计

#### 2.2.1 规则读取模块
```python
class TagRuleReader:
    def __init__(self, spark, mysql_config):
        self.spark = spark
        self.mysql_config = mysql_config
    
    def read_active_rules(self):
        """读取所有启用的标签规则"""
        rules_df = self.spark.read.jdbc(
            url=self.mysql_config['url'],
            table="""
            (SELECT tr.*, td.tag_name, td.tag_code 
             FROM tag_rules tr 
             JOIN tag_definition td ON tr.tag_id = td.id 
             WHERE tr.status = 1 AND td.status = 1) as active_rules
            """,
            properties=self.mysql_config['properties']
        )
        return rules_df.collect()
    
    def parse_rule_condition(self, rule_json):
        """解析JSON规则条件为Spark SQL"""
        import json
        rule = json.loads(rule_json)
        
        conditions = []
        for condition in rule.get('conditions', []):
            field = condition['field']
            operator = condition['operator']
            value = condition['value']
            
            if operator == '>=':
                conditions.append(f"{field} >= {value}")
            elif operator == '<=':
                conditions.append(f"{field} <= {value}")
            elif operator == 'in_range':
                conditions.append(f"{field} BETWEEN {value[0]} AND {value[1]}")
            # 添加更多操作符支持
        
        logic = rule.get('logic', 'AND')
        return f" {logic} ".join(conditions)
```

#### 2.2.2 业务数据读取模块
```python
class HiveDataReader:
    def __init__(self, spark, s3_config):
        self.spark = spark
        self.s3_config = s3_config
    
    def read_table_data(self, table_name, required_fields):
        """从S3读取Hive表数据"""
        # S3路径构建
        s3_path = f"s3a://{self.s3_config['bucket']}/warehouse/{table_name}/"
        
        # 读取parquet文件
        df = self.spark.read.parquet(s3_path)
        
        # 字段裁剪
        if required_fields:
            field_list = required_fields.split(',')
            df = df.select(*field_list)
        
        return df
    
    def optimize_data_reading(self, table_name, partition_filter=None):
        """优化数据读取性能"""
        s3_path = f"s3a://{self.s3_config['bucket']}/warehouse/{table_name}/"
        
        df = self.spark.read.parquet(s3_path)
        
        # 添加分区过滤
        if partition_filter:
            df = df.filter(partition_filter)
        
        # 缓存热点数据
        if table_name in ['user_basic_info', 'user_asset_summary']:
            df = df.cache()
        
        return df
```

#### 2.2.3 标签计算引擎
```python
class TagComputeEngine:
    def __init__(self, spark):
        self.spark = spark
    
    def compute_tag(self, data_df, rule):
        """计算单个标签"""
        # 解析规则条件
        condition_sql = self.parse_rule_condition(rule['rule_conditions'])
        
        # 执行标签计算
        tagged_users = data_df.filter(condition_sql).select(
            'user_id',
            *self.get_hit_value_fields(rule['rule_conditions'])
        )
        
        # 生成标签详细信息
        result_df = tagged_users.withColumn('tag_id', F.lit(rule['tag_id'])) \
                               .withColumn('tag_detail', self.generate_tag_detail(rule))
        
        return result_df.select('user_id', 'tag_id', 'tag_detail')
    
    def batch_compute_tags(self, data_df, rules):
        """批量计算多个标签"""
        results = []
        for rule in rules:
            try:
                result = self.compute_tag(data_df, rule)
                results.append(result)
                print(f"✅ 标签 {rule['tag_name']} 计算完成")
            except Exception as e:
                print(f"❌ 标签 {rule['tag_name']} 计算失败: {str(e)}")
        
        return results
```

#### 2.2.4 标签合并模块
```python
class TagMerger:
    def __init__(self, spark, mysql_config):
        self.spark = spark
        self.mysql_config = mysql_config
    
    def merge_user_tags(self, new_tag_results):
        """合并用户标签"""
        # 1. 读取现有标签
        existing_tags = self.read_existing_tags()
        
        # 2. 将新标签按用户聚合
        new_tags_aggregated = self.aggregate_new_tags(new_tag_results)
        
        # 3. 合并新旧标签
        merged_tags = self.merge_tags_logic(existing_tags, new_tags_aggregated)
        
        return merged_tags
    
    def aggregate_new_tags(self, tag_results):
        """聚合新计算的标签"""
        if not tag_results:
            return None
        
        # 合并所有标签结果
        all_tags = reduce(lambda df1, df2: df1.union(df2), tag_results)
        
        # 按用户聚合
        aggregated = all_tags.groupBy('user_id').agg(
            F.collect_list('tag_id').alias('new_tag_ids'),
            F.collect_list('tag_detail').alias('new_tag_details')
        )
        
        return aggregated
    
    @F.udf(returnType=StringType())
    def merge_tag_arrays_udf(existing_tags, new_tag_ids):
        """UDF: 合并标签数组"""
        import json
        
        # 处理现有标签
        if existing_tags:
            try:
                existing_list = json.loads(existing_tags)
            except:
                existing_list = []
        else:
            existing_list = []
        
        # 合并新标签
        if new_tag_ids:
            tag_set = set(existing_list + new_tag_ids)
            return json.dumps(list(tag_set))
        else:
            return json.dumps(existing_list)
```

#### 2.2.5 结果写入模块
```python
class TagResultWriter:
    def __init__(self, spark, mysql_config):
        self.spark = spark
        self.mysql_config = mysql_config
    
    def write_to_mysql(self, result_df):
        """写入MySQL结果表"""
        try:
            # 写入模式：覆盖整表
            result_df.write.jdbc(
                url=self.mysql_config['url'],
                table='user_tags',
                mode='overwrite',
                properties={
                    **self.mysql_config['properties'],
                    'batchsize': '10000',
                    'isolationLevel': 'READ_COMMITTED'
                }
            )
            
            print(f"✅ 成功写入 {result_df.count()} 条标签记录")
            
        except Exception as e:
            print(f"❌ 写入MySQL失败: {str(e)}")
            raise e
    
    def write_with_backup(self, result_df):
        """带备份的安全写入"""
        # 1. 备份当前数据
        backup_table = f"user_tags_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.backup_current_data(backup_table)
        
        # 2. 写入新数据
        try:
            self.write_to_mysql(result_df)
        except Exception as e:
            # 3. 如果失败，恢复备份
            self.restore_from_backup(backup_table)
            raise e
```

## 3. 开发排期和里程碑

### 3.1 第一期：基础框架搭建（2周）

#### Week 1: 环境搭建和数据读取
**目标**: 完成基础数据读写功能

**任务清单**:
- [ ] 配置Spark环境和S3连接
- [ ] 实现MySQL标签规则读取模块
- [ ] 实现S3 Hive表数据读取模块
- [ ] 编写基础的配置管理模块
- [ ] 单元测试：数据读取功能

**交付物**:
```python
# 核心模块
├── config/
│   ├── spark_config.py      # Spark配置
│   ├── mysql_config.py      # MySQL配置  
│   └── s3_config.py         # S3配置
├── readers/
│   ├── rule_reader.py       # 规则读取
│   └── hive_reader.py       # Hive数据读取
└── tests/
    └── test_data_readers.py # 单元测试
```

#### Week 2: 标签计算引擎
**目标**: 完成标签计算核心逻辑

**任务清单**:
- [ ] 实现规则条件解析器
- [ ] 开发标签计算引擎
- [ ] 实现标签详细信息生成
- [ ] 批量计算优化
- [ ] 单元测试：标签计算逻辑

**交付物**:
```python
├── engine/
│   ├── rule_parser.py       # 规则解析
│   ├── tag_computer.py      # 标签计算
│   └── detail_generator.py  # 详情生成
└── tests/
    └── test_tag_engine.py   # 单元测试
```

### 3.2 第二期：标签合并和写入（1.5周）

#### Week 3-4: 标签合并逻辑
**目标**: 完成标签合并和数据写入

**任务清单**:
- [ ] 实现标签合并算法
- [ ] 开发MySQL写入模块
- [ ] 实现数据备份和恢复
- [ ] 性能优化和调试
- [ ] 集成测试

**交付物**:
```python
├── merger/
│   ├── tag_merger.py        # 标签合并
│   └── merge_strategy.py    # 合并策略
├── writers/
│   ├── mysql_writer.py      # MySQL写入
│   └── backup_manager.py    # 备份管理
└── tests/
    └── test_integration.py  # 集成测试
```

### 3.3 第三期：调度和监控（1周）

#### Week 5: 任务调度和监控
**目标**: 完成生产环境部署

**任务清单**:
- [ ] 开发主任务调度器
- [ ] 实现错误处理和重试机制
- [ ] 添加日志和监控
- [ ] 性能调优
- [ ] 生产环境测试

**交付物**:
```python
├── scheduler/
│   ├── main_scheduler.py    # 主调度器
│   ├── error_handler.py     # 错误处理
│   └── retry_manager.py     # 重试机制
├── monitoring/
│   ├── logger.py           # 日志管理
│   └── metrics.py          # 性能监控
└── deploy/
    ├── run_tag_job.py      # 生产运行脚本
    └── config_prod.py     # 生产配置
```

## 4. 技术难点和风险评估

### 4.1 技术难点

#### 4.1.1 数据量和性能问题
**挑战**: S3数据读取和Spark计算性能
**解决方案**:
- 分区过滤减少数据扫描
- 数据缓存策略
- Spark参数调优
- 分批处理大表

#### 4.1.2 标签合并复杂度
**挑战**: 大用户量下的标签合并性能
**解决方案**:
- 使用Spark SQL优化合并逻辑
- 采用UDF处理复杂合并场景
- 分片处理减少内存压力

#### 4.1.3 数据一致性
**挑战**: 确保标签计算结果的准确性
**解决方案**:
- 事务性写入MySQL
- 数据校验和对账机制
- 备份恢复策略

### 4.2 风险评估

| 风险类型 | 风险等级 | 影响 | 缓解措施 |
|---------|---------|------|----------|
| S3连接不稳定 | 中 | 任务失败 | 重试机制+多AZ部署 |
| 数据量增长 | 高 | 性能下降 | 分区策略+弹性扩容 |
| 规则配置错误 | 中 | 标签错误 | 规则验证+灰度发布 |
| MySQL写入冲突 | 低 | 数据丢失 | 事务机制+备份策略 |

## 5. 资源需求评估

### 5.1 技术资源
- **开发人员**: 1人（你）
- **开发周期**: 4.5周
- **维护成本**: 每周2-4小时

### 5.2 基础设施
- **Spark集群**: 建议4-8核，16-32GB内存
- **S3存储**: 无额外成本
- **MySQL**: 需要标签规则表和结果表

### 5.3 数据量预估
- **用户量**: 假设100万用户
- **标签数**: 初期20-50个标签
- **计算频率**: 每日执行
- **存储需求**: 结果表约1-5GB

## 6. 成功标准

### 6.1 功能指标
- ✅ 支持20+标签规则类型
- ✅ 日处理100万用户数据
- ✅ 标签计算准确率99.9%+
- ✅ 支持增量和全量计算

### 6.2 性能指标
- ✅ 全量计算完成时间 < 2小时
- ✅ 增量计算完成时间 < 30分钟
- ✅ 系统可用率 > 99.5%
- ✅ 数据延迟 < 4小时

### 6.3 运维指标
- ✅ 自动错误恢复率 > 95%
- ✅ 监控覆盖率 100%
- ✅ 文档完整度 > 90%

## 7. 下一步行动

### 7.1 立即启动（本周）
1. **环境准备**: 确认Spark、S3、MySQL连通性
2. **需求确认**: 与后端同学确认数据表结构
3. **代码框架**: 搭建基础项目结构

### 7.2 第一周目标
1. **完成数据读取**: S3 Hive表 + MySQL规则表
2. **验证数据质量**: 确保读取的数据正确性
3. **建立测试数据**: 准备小规模测试数据集

### 7.3 风险控制
1. **每周Review**: 每周五检查进度和风险点
2. **及时沟通**: 发现问题立即与相关同学沟通
3. **备用方案**: 准备简化版方案应对突发情况

---

**总结**: 4.5周完成数据侧标签系统，重点关注性能优化和数据质量。