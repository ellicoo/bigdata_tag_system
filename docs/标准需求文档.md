# 用户标签管理系统需求规格说明书

## 1. 项目概述

### 1.1 项目背景
为支持精细化运营、用户画像构建、精准营销与风控预警，平台需构建一套标签管理系统，实现对用户的结构化标注管理，标签可灵活配置、自动更新、统一调用，支撑多个业务系统联动。

### 1.2 项目目标
1. 统一标签创建与分类管理，形成标准化标签体系
2. 支持标签的手动及自动配置与更新
3. 提供标签的可视化使用能力和跨系统调用能力
4. 支持标签组合筛选、用户分群、活动触达、风控识别等场景

### 1.3 技术架构
基于PySpark分布式计算的三环境标签系统：
- **数据源**：S3 Hive表（用户数据）+ MySQL规则表（标签配置）
- **计算引擎**：PySpark分布式计算引擎，支持6种并行计算场景
- **部署环境**：本地Docker + AWS Glue开发/生产环境
- **结果存储**：MySQL数据库（一个用户一条记录，包含标签ID数组）
- **标签层级**：一级标签（大类）+ 二级标签（具体标签）

## 2. 标签体系设计

### 2.1 标签分类标准

| 一级标签分类 | 二级标签示例 | 说明 |
|-------------|-------------|------|
| 基础属性 | 注册渠道、国家、设备类型 | 用户基本信息属性 |
| 行为属性 | 最近登录、交易频率、交易额 | 用户行为数据 |
| 生命周期 | 新手期、成长期、衰退期、流失期 | 用户生命周期阶段 |
| 风险属性 | 风控等级、KYC状态 | 风险控制相关 |
| 用户价值 | 高价值用户、VIP、活跃用户 | 价值分层 |
| 行为特征 | 套利者、囤币党、高频交易者 | 行为偏好特征 |
| 客户偏好 | 合约偏好、理财偏好、杠杆偏好 | 产品偏好 |

### 2.2 标签定义数据模型

#### 一级标签表 (tag_category)
```sql
CREATE TABLE tag_category (
    id INT PRIMARY KEY AUTO_INCREMENT,
    category_name VARCHAR(50) NOT NULL COMMENT '大类名称',
    category_code VARCHAR(50) NOT NULL UNIQUE COMMENT '大类编码',
    description TEXT COMMENT '描述',
    status TINYINT DEFAULT 1 COMMENT '状态：1启用 0禁用',
    created_by VARCHAR(50) COMMENT '创建人',
    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
);
```

#### 二级标签表 (tag_definition)
```sql
CREATE TABLE tag_definition (
    id INT PRIMARY KEY AUTO_INCREMENT,
    tag_name VARCHAR(100) NOT NULL COMMENT '标签名称',
    tag_code VARCHAR(100) NOT NULL UNIQUE COMMENT '标签标识',
    category_id INT NOT NULL COMMENT '所属一级标签ID',
    description TEXT COMMENT '标签描述',
    tag_type ENUM('AUTO', 'MANUAL') NOT NULL COMMENT '标签类型：AUTO自动 MANUAL手动',
    trigger_type ENUM('REALTIME', 'SCHEDULE') COMMENT '触发方式：REALTIME实时 SCHEDULE定时',
    schedule_cycle ENUM('HOUR', 'DAY', 'WEEK') COMMENT '执行周期',
    status TINYINT DEFAULT 1 COMMENT '状态：1启用 0禁用',
    created_by VARCHAR(50) COMMENT '创建人',
    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    FOREIGN KEY (category_id) REFERENCES tag_category(id)
);
```

#### 标签规则表 (tag_rules)
```sql
CREATE TABLE tag_rules (
    id INT PRIMARY KEY AUTO_INCREMENT,
    tag_id INT NOT NULL COMMENT '标签ID',
    rule_name VARCHAR(100) COMMENT '规则名称', 
    rule_description TEXT COMMENT '规则说明',
    condition_logic ENUM('AND', 'OR', 'NOT') DEFAULT 'AND' COMMENT '条件关系',
    rule_conditions JSON COMMENT '规则条件配置（JSON格式）',
    target_table VARCHAR(100) COMMENT '目标Hive表名',
    target_fields TEXT COMMENT '需要的字段列表',
    status TINYINT DEFAULT 1 COMMENT '状态：1启用 0禁用',
    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    FOREIGN KEY (tag_id) REFERENCES tag_definition(id)
);
```

#### 用户标签结果表 (user_tags)

根据MySQL版本选择合适的方案：

**当前采用方案：JSON数组存储（MySQL 5.7+）- 已实现**
```sql
CREATE TABLE user_tags (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    user_id VARCHAR(100) NOT NULL COMMENT '用户ID',
    tag_ids JSON NOT NULL COMMENT '用户的所有标签ID数组 [1,2,3,5]',
    tag_details JSON COMMENT '标签详细信息（key-value形式）',
    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间（永远不变）',
    updated_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP COMMENT '更新时间（由UPSERT逻辑控制）',
    INDEX idx_user_id (user_id),
    INDEX idx_created_time (created_time),
    INDEX idx_updated_time (updated_time),
    UNIQUE KEY uk_user_id (user_id)
);
```

**方案二：字符串存储（MySQL 5.6及以下兼容）**
```sql
CREATE TABLE user_tags (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    user_id VARCHAR(50) NOT NULL UNIQUE COMMENT '用户ID',
    tag_ids TEXT COMMENT '用户标签ID，逗号分隔，如"1,3,5,8"',
    tag_details TEXT COMMENT '标签详细信息和命中原因JSON字符串',
    updated_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_user_id (user_id)
);
```

**方案三：传统关系表（最大兼容性）**
```sql
CREATE TABLE user_tags (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    user_id VARCHAR(50) NOT NULL COMMENT '用户ID',
    tag_id INT NOT NULL COMMENT '标签ID',
    tag_value VARCHAR(500) COMMENT '标签值（针对动态标签）',
    source ENUM('AUTO', 'MANUAL') NOT NULL COMMENT '标签来源',
    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    UNIQUE KEY uk_user_tag (user_id, tag_id),
    INDEX idx_user_id (user_id),
    INDEX idx_tag_id (tag_id)
);
```

## 版本选择建议

| MySQL版本 | 推荐方案 | 理由 |
|----------|---------|------|
| MySQL 8.0+ | 方案一（JSON） | 性能最佳，功能最强 |
| MySQL 5.7 | 方案一（JSON） | 基础JSON支持 |
| MySQL 5.6及以下 | 方案二（TEXT）或方案三（关系表） | 兼容性最好 |

**MySQL版本检测方法：**
```python
def detect_mysql_version(spark, jdbc_url, properties):
    """检测MySQL版本并返回合适的存储方案"""
    try:
        version_df = spark.read.jdbc(
            url=jdbc_url,
            table="(SELECT VERSION() as version) as tmp",
            properties=properties
        )
        version_str = version_df.first()['version']
        version_num = float(version_str.split('.')[0] + '.' + version_str.split('.')[1])
        
        if version_num >= 5.7:
            return "json", "方案一：JSON存储"
        else:
            return "text", "方案二：TEXT存储"
    except:
        return "text", "方案二：TEXT存储（兼容模式）"

# 使用示例
storage_type, description = detect_mysql_version(
    spark, 
    'jdbc:mysql://localhost:3306/tag_system',
    {"user": "root", "password": "password"}
)
print(f"检测到MySQL版本，推荐使用：{description}")
```

**不同方案的查询对比：**

## tag_details字段详细说明

`tag_details` 字段记录用户命中每个标签的**具体原因和详细信息**：

### 字段结构设计
```json
{
  "标签ID": {
    "value": "命中时的具体数值",
    "reason": "命中原因的文字描述", 
    "source": "标签来源(AUTO/MANUAL)",
    "hit_time": "命中时间",
    "rule_version": "规则版本号",
    "confidence": "置信度(可选)"
  }
}
```

### 实际数据示例
```sql
-- 用户user_1001的完整标签记录
INSERT INTO user_tags VALUES (
    1, 
    'user_1001', 
    '[1, 3, 5, 8]',  -- 标签ID数组
    '{
      "1": {
        "value": "150000", 
        "reason": "总资产150000 USDT ≥ 10万美元阈值", 
        "source": "AUTO", 
        "hit_time": "2024-01-01 10:30:00",
        "rule_version": "v1.0"
      },
      "3": {
        "value": "25", 
        "reason": "近7日登录25次 ≥ 20次阈值", 
        "source": "AUTO", 
        "hit_time": "2024-01-01 10:30:00",
        "rule_version": "v1.0"
      },
      "5": {
        "value": "合约交易占比80%", 
        "reason": "合约交易次数 > 现货交易次数", 
        "source": "AUTO", 
        "hit_time": "2024-01-01 10:30:00",
        "rule_version": "v1.2"
      },
      "8": {
        "value": "VIP3", 
        "reason": "运营手动标记为重要客户", 
        "source": "MANUAL", 
        "hit_time": "2024-01-01 15:00:00",
        "operator": "admin_001"
      }
    }',
    '2024-01-01 10:30:00',
    '2024-01-01 08:00:00'
);
```

### 业务价值

1. **审计追踪**: 记录用户为什么被打上某个标签
2. **业务解释**: 客服/运营可以向用户解释标签来源  
3. **规则验证**: 验证标签规则是否按预期工作
4. **合规要求**: 满足金融行业的可解释性要求
5. **调试优化**: 分析标签质量和规则效果

### 查询示例

**方案一（JSON）查询：**
```sql
-- 查询拥有特定标签的用户
SELECT user_id FROM user_tags WHERE JSON_CONTAINS(tag_ids, '1');

-- 查询用户标签及命中原因
SELECT user_id, 
       JSON_EXTRACT(tag_details, '$.1.reason') as high_value_reason,
       JSON_EXTRACT(tag_details, '$.1.value') as asset_value
FROM user_tags 
WHERE JSON_CONTAINS(tag_ids, '1');

-- 查询用户的所有标签详情
SELECT u.user_id, u.tag_ids, u.tag_details,
       GROUP_CONCAT(t.tag_name) as tag_names
FROM user_tags u
JOIN tag_definition t ON JSON_CONTAINS(u.tag_ids, CAST(t.id AS JSON))
WHERE u.user_id = 'user_1001';
```

**方案二（TEXT）查询：**
```sql
-- 查询拥有特定标签的用户
SELECT user_id FROM user_tags WHERE FIND_IN_SET('1', tag_ids);

-- 查询用户标签及命中原因（需要解析JSON字符串）
SELECT user_id, tag_details
FROM user_tags 
WHERE FIND_IN_SET('1', tag_ids);

-- 查询用户的所有标签
SELECT u.user_id, u.tag_ids, u.tag_details,
       GROUP_CONCAT(t.tag_name) as tag_names  
FROM user_tags u
JOIN tag_definition t ON FIND_IN_SET(t.id, u.tag_ids)
WHERE u.user_id = 'user_1001';
```

## 3. 标签字段规范

### 3.1 数值型字段

| 字段名称 | 数据类型 | 字段说明 | 支持操作符 |
|---------|---------|----------|------------|
| 累计充值金额 | DECIMAL(20,8) | 总充值入金金额(USDT) | =、≠、>、<、≥、≤、在范围内、不在范围内、为空、不为空 |
| 累计提现金额 | DECIMAL(20,8) | 总提现金额(USDT) | =、≠、>、<、≥、≤、在范围内、不在范围内、为空、不为空 |
| 累计净入金金额 | DECIMAL(20,8) | 充值-提现(USDT) | =、≠、>、<、≥、≤、在范围内、不在范围内、为空、不为空 |
| 总交易额 | DECIMAL(20,8) | 历史累计成交额(USDT) | =、≠、>、<、≥、≤、在范围内、不在范围内、为空、不为空 |
| 最近30日交易额 | DECIMAL(20,8) | 近30天成交额(USDT) | =、≠、>、<、≥、≤、在范围内、不在范围内、为空、不为空 |
| 总持仓市值 | DECIMAL(20,8) | 当前账户总价值(USDT) | =、≠、>、<、≥、≤、在范围内、不在范围内、为空、不为空 |
| 可用余额 | DECIMAL(20,8) | 当前资产余额(USDT) | =、≠、>、<、≥、≤、在范围内、不在范围内、为空、不为空 |
| 锁仓金额 | DECIMAL(20,8) | 当前锁仓金额(USDT) | =、≠、>、<、≥、≤、在范围内、不在范围内、为空、不为空 |
| 注册至今天数 | INT | 系统自动计算注册天数 | =、≠、>、<、≥、≤、在范围内、不在范围内、为空、不为空 |
| 最近登录距今天数 | INT | 最近一次登录至今天数 | =、≠、>、<、≥、≤、在范围内、不在范围内、为空、不为空 |
| 登录次数(近7日) | INT | 判断是否频繁活跃 | =、≠、>、<、≥、≤、在范围内、不在范围内、为空、不为空 |
| 提现次数(近30日) | INT | 判断是否频繁提现 | =、≠、>、<、≥、≤、在范围内、不在范围内、为空、不为空 |
| 充值失败次数 | INT | 判断支付问题 | =、≠、>、<、≥、≤、在范围内、不在范围内、为空、不为空 |
| 红包领取次数 | INT | 活动参与度 | =、≠、>、<、≥、≤、在范围内、不在范围内、为空、不为空 |
| 好友邀请成功数 | INT | 邀请能力 | =、≠、>、<、≥、≤、在范围内、不在范围内、为空、不为空 |
| 返佣比例 | DECIMAL(5,4) | 邀请佣金比例(%) | =、≠、>、<、≥、≤、在范围内、不在范围内、为空、不为空 |
| 返佣金额(累计) | DECIMAL(20,8) | 累计返佣金额(USDT) | =、≠、>、<、≥、≤、在范围内、不在范围内、为空、不为空 |
| 链上账户持仓金额 | DECIMAL(20,8) | 链上资产余额(USDT) | =、≠、>、<、≥、≤、在范围内、不在范围内、为空、不为空 |
| 链上交易次数 | INT | 链上交易记录 | =、≠、>、<、≥、≤、在范围内、不在范围内、为空、不为空 |
| 理财账户持仓金额 | DECIMAL(20,8) | 理财产品资产(USDT) | =、≠、>、<、≥、≤、在范围内、不在范围内、为空、不为空 |
| 理财次数 | INT | 理财参与记录 | =、≠、>、<、≥、≤、在范围内、不在范围内、为空、不为空 |

### 3.2 字符串型字段

| 字段名称 | 数据类型 | 字段说明 | 支持操作符 |
|---------|---------|----------|------------|
| 登录IP地址 | VARCHAR(45) | 登录时的IP | =、≠、包含、不包含、开头是、结尾是、为空、不为空 |
| 国家地区编码 | VARCHAR(10) | 区号(如+86、+81) | =、≠、包含、不包含、开头是、结尾是、为空、不为空 |
| 邮箱后缀 | VARCHAR(100) | 企业邮箱判断 | =、≠、包含、不包含、开头是、结尾是、为空、不为空 |
| 注册来源渠道 | VARCHAR(100) | 渠道编号、页面、活动标识 | =、≠、包含、不包含、开头是、结尾是、为空、不为空 |
| 操作系统 | VARCHAR(50) | 设备类型判断 | =、≠、包含、不包含、开头是、结尾是、为空、不为空 |

### 3.3 日期型字段

| 字段名称 | 数据类型 | 字段说明 | 支持操作符 |
|---------|---------|----------|------------|
| 注册时间 | TIMESTAMP | 账户创建时间 | =、≠、>、<、在区间内、不在区间内、最近N天、为空、不为空 |
| 最近登录时间 | TIMESTAMP | 最后登录时间 | =、≠、>、<、在区间内、不在区间内、最近N天、为空、不为空 |
| 最近一次充值时间 | TIMESTAMP | 上次充值时间 | =、≠、>、<、在区间内、不在区间内、最近N天、为空、不为空 |
| 最近一次提现时间 | TIMESTAMP | 上次提现时间 | =、≠、>、<、在区间内、不在区间内、最近N天、为空、不为空 |
| 首次交易时间 | TIMESTAMP | 第一次交易时间 | =、≠、>、<、在区间内、不在区间内、最近N天、为空、不为空 |
| 最后交易时间 | TIMESTAMP | 最后交易时间 | =、≠、>、<、在区间内、不在区间内、最近N天、为空、不为空 |
| 最后活动时间 | TIMESTAMP | 最后活跃时间 | =、≠、>、<、在区间内、不在区间内、最近N天、为空、不为空 |

### 3.4 布尔型字段

| 字段名称 | 数据类型 | 字段说明 | 支持操作符 |
|---------|---------|----------|------------|
| 是否完成KYC | BOOLEAN | 实名认证状态 | 是/否 |
| 是否绑定二次验证 | BOOLEAN | 安全验证状态 | 是/否 |
| 是否为黑名单用户 | BOOLEAN | 风控记录 | 是/否 |
| 是否为高风险IP | BOOLEAN | IP风险判断 | 是/否 |
| 是否有合约交易 | BOOLEAN | 合约参与状态 | 是/否 |
| 是否开通过理财 | BOOLEAN | 理财参与状态 | 是/否 |
| 是否有挂单行为 | BOOLEAN | 委托单记录 | 是/否 |
| 是否为代理 | BOOLEAN | 合伙人状态 | 是/否 |

### 3.5 枚举型字段

| 字段名称 | 数据类型 | 字段说明 | 可选值 | 支持操作符 |
|---------|---------|----------|--------|------------|
| 用户等级 | ENUM | VIP等级 | VIP0～VIP5 | =、≠、属于、不属于、为空、不为空 |
| 渠道来源 | ENUM | 注册渠道 | 自然流量、广告投放、邀请好友 | =、≠、属于、不属于、为空、不为空 |
| 注册方式 | ENUM | 注册类型 | 邮箱、手机号、Google | =、≠、属于、不属于、为空、不为空 |
| 合约交易风格 | ENUM | 交易偏好 | 高杠杆、低频、套保 | =、≠、属于、不属于、为空、不为空 |
| 注册国家 | ENUM | 注册地 | 系统国家列表 | =、≠、属于、不属于、为空、不为空 |
| KYC国家 | ENUM | 认证国家 | 系统国家列表 | =、≠、属于、不属于、为空、不为空 |

### 3.6 数组型字段

| 字段名称 | 数据类型 | 字段说明 | 支持操作符 |
|---------|---------|----------|------------|
| 当前持仓币种列表 | JSON | 当前持有币种 | 包含任意、包含全部、不包含、相交、无交集、为空、不为空 |
| 交易过的币种列表 | JSON | 历史交易币种 | 包含任意、包含全部、不包含、相交、无交集、为空、不为空 |
| 登录设备指纹列表 | JSON | 设备记录 | 包含任意、包含全部、不包含、相交、无交集、为空、不为空 |
| 参与过的活动ID列表 | JSON | 活动参与记录 | 包含任意、包含全部、不包含、相交、无交集、为空、不为空 |
| 奖励领取历史 | JSON | 奖品类型记录 | 包含任意、包含全部、不包含、相交、无交集、为空、不为空 |
| 使用过的卡券类型 | JSON | 卡券使用记录 | 包含任意、包含全部、不包含、相交、无交集、为空、不为空 |

## 4. 技术实现方案

### 4.1 数据架构图
```
┌─────────────┐    ┌──────────────┐    ┌─────────────┐    ┌─────────────┐
│  Hive Tables │    │    MySQL     │    │    Spark    │    │   MySQL     │
│   (S3存储)   │───▶│  标签规则表   │───▶│   计算引擎   │───▶│  标签结果表  │
│  用户业务数据 │    │              │    │             │    │             │
└─────────────┘    └──────────────┘    └─────────────┘    └─────────────┘
```

### 4.2 核心处理流程

#### 4.2.1 简化的标签计算基类
```python
class SimplifiedTagModel:
    def __init__(self, tag_id, mysql_version="5.7"):
        self.tag_id = tag_id
        self.mysql_version = mysql_version
        self.storage_type = "json" if float(mysql_version) >= 5.7 else "text"
        self.spark = SparkSession.builder.master("local[2]").getOrCreate()
    
    def read_tag_rule(self):
        """从MySQL读取标签规则"""
        rule_df = self.spark.read.jdbc(
            url='jdbc:mysql://localhost:3306/tag_system',
            table='tag_rules',
            properties={"user": "root", "password": "password"}
        ).filter(f"tag_id = {self.tag_id} AND status = 1")
        return rule_df.first()
    
    def read_hive_data(self, table_name, fields):
        """从S3的Hive表读取业务数据"""
        hive_df = self.spark.read.parquet(f"s3a://data-lake/{table_name}")
        if fields:
            field_list = fields.split(",")
            hive_df = hive_df.select(*field_list)
        return hive_df
    
    def compute_tag(self, data_df, rule_conditions):
        """根据规则计算标签，包含命中原因"""
        # 解析JSON规则条件，生成Spark SQL条件
        condition_sql = self.parse_rule_conditions(rule_conditions)
        
        # 执行标签计算，同时记录命中原因
        tagged_users = data_df.filter(condition_sql).select(
            "user_id",
            *self.get_hit_value_fields(rule_conditions)  # 获取命中时的具体数值
        )
        
        # 添加标签详细信息
        @F.udf(returnType=StringType())
        def generate_tag_detail(user_id, *hit_values):
            """生成标签详细信息"""
            import json
            from datetime import datetime
            
            # 根据规则条件生成命中原因说明
            reason = self.generate_hit_reason(rule_conditions, hit_values)
            
            detail = {
                "value": str(hit_values[0]) if hit_values else "",
                "reason": reason,
                "source": "AUTO",
                "hit_time": datetime.now().isoformat(),
                "rule_version": "v1.0"
            }
            
            return json.dumps({str(self.tag_id): detail})
        
        # 应用UDF生成详细信息
        hit_value_cols = [F.col(field) for field in self.get_hit_value_fields(rule_conditions)]
        
        result_df = tagged_users.withColumn("tag_id", F.lit(self.tag_id)) \
                                .withColumn("tag_detail", generate_tag_detail(F.col("user_id"), *hit_value_cols))
        
        return result_df.select("user_id", "tag_id", "tag_detail")
    
    def generate_hit_reason(self, rule_conditions, hit_values):
        """根据规则条件和命中值生成原因说明"""
        import json
        rules = json.loads(rule_conditions)
        
        reasons = []
        for i, condition in enumerate(rules.get("conditions", [])):
            field = condition.get("field", "")
            operator = condition.get("operator", "")
            threshold = condition.get("value", "")
            hit_value = hit_values[i] if i < len(hit_values) else ""
            
            if operator == ">=":
                reasons.append(f"{field}={hit_value} ≥ {threshold}")
            elif operator == ">":
                reasons.append(f"{field}={hit_value} > {threshold}")
            # 可以根据需要添加更多操作符的处理
        
        return " AND ".join(reasons)
    
    def get_hit_value_fields(self, rule_conditions):
        """获取需要记录的命中数值字段"""
        import json
        rules = json.loads(rule_conditions)
        
        fields = []
        for condition in rules.get("conditions", []):
            field_name = condition.get("field", "")
            if field_name:
                fields.append(field_name)
        
        return fields
    
    def write_tag_result(self, result_df):
        """将标签结果写入MySQL（合并模式）"""
        # 读取现有用户标签
        existing_tags_df = self.spark.read.jdbc(
            url='jdbc:mysql://localhost:3306/tag_system',
            table='user_tags',
            properties={"user": "root", "password": "password"}
        )
        
        # 合并新标签到现有标签数组中
        merged_df = self.merge_user_tags(existing_tags_df, result_df)
        
        # 写入结果（覆盖模式）
        merged_df.write.jdbc(
            url='jdbc:mysql://localhost:3306/tag_system',
            table='user_tags',
            mode='overwrite',
            properties={"user": "root", "password": "password"}
        )
    
    def merge_user_tags(self, existing_df, new_df):
        """合并用户标签数组"""
        @F.udf(returnType=StringType())
        def merge_tag_arrays(existing_tags, new_tag_id, storage_type="json"):
            """合并标签数组的UDF函数 - 支持JSON和TEXT两种存储格式"""
            if storage_type.lower() == "json":
                # JSON格式处理（MySQL 5.7+）
                import json
                if existing_tags is None:
                    return json.dumps([new_tag_id])
                try:
                    tag_list = json.loads(existing_tags)
                    if new_tag_id not in tag_list:
                        tag_list.append(new_tag_id)
                    return json.dumps(tag_list)
                except:
                    return json.dumps([new_tag_id])
            else:
                # TEXT格式处理（MySQL 5.6及以下）
                if existing_tags is None or existing_tags == "":
                    return str(new_tag_id)
                
                tag_list = existing_tags.split(",")
                if str(new_tag_id) not in tag_list:
                    tag_list.append(str(new_tag_id))
                return ",".join(tag_list)
        
        # 左连接现有标签和新标签
        merged_df = new_df.alias("new").join(
            existing_df.alias("existing"), 
            F.col("new.user_id") == F.col("existing.user_id"), 
            "left"
        ).select(
            F.col("new.user_id"),
            merge_tag_arrays(
                F.col("existing.tag_ids"), 
                F.col("new.tag_id"),
                F.lit(self.storage_type)
            ).alias("tag_ids"),
            F.current_timestamp().alias("updated_time")
        )
        
        return merged_df
    
    def execute(self):
        """执行完整的标签计算流程"""
        # 1. 读取标签规则
        tag_rule = self.read_tag_rule()
        
        # 2. 读取Hive业务数据
        data_df = self.read_hive_data(
            tag_rule['target_table'], 
            tag_rule['target_fields']
        )
        
        # 3. 计算标签
        result_df = self.compute_tag(data_df, tag_rule['rule_conditions'])
        
        # 4. 写入结果
        self.write_tag_result(result_df)
```

#### 4.2.2 规则条件JSON格式
```json
{
  "logic": "AND",
  "conditions": [
    {
      "field": "total_deposit_amount",
      "operator": ">=",
      "value": 100000,
      "type": "number"
    },
    {
      "field": "register_time",
      "operator": "recent_days",
      "value": 30,
      "type": "date"
    }
  ]
}
```

### 4.3 批量标签计算优化
```python
class BatchTagProcessor:
    def __init__(self, tag_ids):
        self.tag_ids = tag_ids
        self.spark = SparkSession.builder.master("local[4]").getOrCreate()
    
    def execute_batch_tags(self):
        """批量执行多个标签计算"""
        # 1. 批量读取所有标签规则
        all_rules = self.read_all_tag_rules()
        
        # 2. 按数据表分组，减少数据读取次数
        table_groups = self.group_rules_by_table(all_rules)
        
        # 3. 分表批量处理
        all_results = []
        for table_name, rules in table_groups.items():
            # 读取一次数据，计算多个标签
            data_df = self.read_hive_data(table_name, self.get_all_fields(rules))
            
            for rule in rules:
                result_df = self.compute_single_tag(data_df, rule)
                all_results.append(result_df)
        
        # 4. 批量合并所有用户的标签数组
        final_result = self.merge_batch_user_tags(all_results)
        self.write_batch_results(final_result)
    
    def merge_batch_user_tags(self, result_list):
        """批量合并多个标签结果到用户标签数组"""
        # 读取现有用户标签
        existing_tags_df = self.spark.read.jdbc(
            url='jdbc:mysql://localhost:3306/tag_system',
            table='user_tags',
            properties={"user": "root", "password": "password"}
        )
        
        # 将所有新标签按用户聚合
        all_new_tags = reduce(lambda df1, df2: df1.union(df2), result_list)
        
        # 按用户ID聚合新标签
        aggregated_new_tags = all_new_tags.groupBy("user_id").agg(
            F.collect_list("tag_id").alias("new_tag_list")
        )
        
        @F.udf(returnType=StringType())
        def merge_all_tags(existing_tags, new_tag_list):
            """合并所有新标签到现有标签数组"""
            import json
            
            # 处理现有标签
            if existing_tags is None:
                tag_set = set()
            else:
                try:
                    tag_set = set(json.loads(existing_tags))
                except:
                    tag_set = set()
            
            # 添加新标签
            if new_tag_list:
                tag_set.update(new_tag_list)
            
            return json.dumps(list(tag_set))
        
        # 合并标签
        final_df = aggregated_new_tags.alias("new").join(
            existing_tags_df.alias("existing"),
            F.col("new.user_id") == F.col("existing.user_id"),
            "full_outer"
        ).select(
            F.coalesce(F.col("new.user_id"), F.col("existing.user_id")).alias("user_id"),
            merge_all_tags(
                F.col("existing.tag_ids"),
                F.col("new.new_tag_list")
            ).alias("tag_ids"),
            F.current_timestamp().alias("updated_time")
        )
        
        return final_df
```

## 5. 典型标签配置示例

### 5.1 高净值用户标签
```json
{
  "tag_name": "高净值用户",
  "tag_code": "high_value_user",
  "category_id": 5,
  "tag_type": "AUTO",
  "trigger_type": "SCHEDULE",
  "schedule_cycle": "DAY",
  "rule_conditions": {
    "logic": "OR",
    "conditions": [
      {
        "field": "total_asset_value",
        "operator": ">=",
        "value": 100000,
        "type": "number"
      },
      {
        "field": "last_30d_trading_volume",
        "operator": ">=",
        "value": 500000,
        "type": "number"
      }
    ]
  },
  "target_table": "user_asset_summary",
  "target_fields": "user_id,total_asset_value,last_30d_trading_volume"
}
```

### 5.2 新注册3天未转化用户
```json
{
  "tag_name": "新注册3天未转化用户",
  "tag_code": "new_user_3d_unconverted", 
  "category_id": 3,
  "tag_type": "AUTO",
  "trigger_type": "SCHEDULE",
  "schedule_cycle": "DAY",
  "rule_conditions": {
    "logic": "AND",
    "conditions": [
      {
        "field": "register_time",
        "operator": "days_ago_between",
        "value": [3, 999],
        "type": "date"
      },
      {
        "field": "first_deposit_time",
        "operator": "is_null",
        "value": null,
        "type": "date"
      },
      {
        "field": "first_trade_time", 
        "operator": "is_null",
        "value": null,
        "type": "date"
      }
    ]
  },
  "target_table": "user_basic_info",
  "target_fields": "user_id,register_time,first_deposit_time,first_trade_time"
}
```

### 5.3 沉默用户标签
```json
{
  "tag_name": "沉默用户",
  "tag_code": "silent_user",
  "category_id": 3,
  "tag_type": "AUTO", 
  "trigger_type": "SCHEDULE",
  "schedule_cycle": "DAY",
  "rule_conditions": {
    "logic": "AND",
    "conditions": [
      {
        "field": "last_login_time",
        "operator": "days_ago",
        "value": 30,
        "type": "date"
      },
      {
        "field": "historical_trade_count",
        "operator": ">",
        "value": 0,
        "type": "number"
      }
    ]
  },
  "target_table": "user_activity_summary",
  "target_fields": "user_id,last_login_time,historical_trade_count"
}
```

## 6. 系统功能模块

### 6.1 标签管理后台功能

#### 6.1.1 标签大类管理
- **功能**：创建、编辑、删除一级标签分类
- **字段**：大类名称（最多10个中文字符）
- **操作**：编辑名称、删除（需验证无关联标签）、查看关联标签

#### 6.1.2 标签定义管理  
- **筛选**：标签名称、标签标识、标签大类、标签类型、状态
- **列表字段**：标签名称、标签标识、标签大类、标签类型、标签状态、创建人、创建时间
- **操作**：查看、编辑、查看关联用户数量

#### 6.1.3 新增标签配置
**基础信息（必填）**：
- 标签名称：最多20字符
- 标签标识：英文、数字、下划线组成
- 标签大类：下拉选择
- 标签描述：最多1000字符
- 标签类型：自动标签/人工标签

**规则配置**：
- **自动标签**：
  - 触发方式：实时/定时
  - 执行周期：小时/天/周
  - 条件关系：AND/OR/NOT
  - 条件设置：字段+操作符+值+单位
- **人工标签**：无规则配置

### 6.2 用户人群管理
- **功能**：基于标签组合创建用户人群
- **规则**：满足选择的所有标签对应的用户
- **展示**：人群名称、关联用户数、关联标签、状态、创建时间

### 6.3 批量人工标签处理
- **功能**：批量绑定/去除人工标签
- **操作方式**：上传用户UID模板文件
- **记录**：记录操作历史和状态跟踪

## 7. 业务系统集成

### 7.1 用户管理系统
- **用户列表**：增加标签大类+标签筛选和展示
- **用户详情**：展示自动标签和人工标签，支持人工标签编辑

### 7.2 营销触达系统
**邮件发送**：
- 固定用户：通过UID发送
- 标签用户：选择标签对应的用户
- 人群用户：选择人群对应的用户

**Push推送**：增加标签用户和人群用户发送方式

**站内信**：增加标签用户和人群用户发送方式

**首页弹框**：支持全部用户、固定用户、标签用户、人群用户

### 7.3 风控审核系统
- **提现审核**：用户信息展开时增加用户标签展示

## 8. 实施计划

### 8.1 第一阶段：核心标签计算引擎（2周）
- [ ] 设计并创建MySQL标签系统数据表
- [ ] 开发简化的标签计算基类
- [ ] 实现基础标签规则解析和执行
- [ ] 完成Hive数据读取和MySQL结果写入

### 8.2 第二阶段：标签管理后台（3周）
- [ ] 开发标签大类管理功能
- [ ] 开发标签定义管理功能
- [ ] 实现新增标签配置界面
- [ ] 完成用户人群管理功能

### 8.3 第三阶段：批量处理和优化（2周）
- [ ] 实现批量标签计算优化
- [ ] 开发人工标签批量处理功能
- [ ] 性能优化和监控

### 8.4 第四阶段：业务系统集成（2周）
- [ ] 用户管理系统标签集成
- [ ] 营销触达系统改造
- [ ] 风控系统标签展示

## 9. 技术风险和注意事项

### 9.1 数据一致性
- 采用事务机制确保标签计算和存储的一致性
- 实现标签计算失败回滚机制

### 9.2 性能优化
- 批量标签计算，减少数据读取次数
- 合理设置Spark作业参数
- 对高频查询字段建立数据库索引

### 9.3 扩展性设计
- 预留实时标签计算接口
- 设计可插拔的规则引擎
- 支持后续接入更多数据源

### 9.4 监控和告警
- 标签计算任务执行状态监控
- 数据质量监控（标签覆盖率、准确性）
- 系统性能监控（计算耗时、资源使用）

---

**文档版本**：v1.0  
**编写时间**：2024年  
**适用范围**：用户标签管理系统MVP版本