#!/usr/bin/env python3
"""
æµ·è±šè°ƒåº¦å™¨å›¾å½¢ç•Œé¢éƒ¨ç½²åŒ…ç”Ÿæˆå™¨
åŸºäºç°æœ‰S3 Hiveèƒ½åŠ›ï¼Œä¸ºæµ·è±šè°ƒåº¦å™¨å›¾å½¢ç•Œé¢ç”Ÿæˆéƒ¨ç½²åŒ…
"""

import os
import zipfile
import tempfile
from pathlib import Path


class DolphinGUIDeployPackager:
    """æµ·è±šè°ƒåº¦å™¨å›¾å½¢ç•Œé¢éƒ¨ç½²åŒ…ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.output_dir = self.project_root / "dolphin_gui_deploy"
        self.output_dir.mkdir(exist_ok=True)
    
    
    def create_hive_test_tables(self) -> str:
        """åˆ›å»ºHiveæµ‹è¯•è¡¨SQL - åŸºäºç°æœ‰å»ºè¡¨è¯­å¥"""
        sql_content = '''-- æ ‡ç­¾ç³»ç»Ÿæµ‹è¯•è¡¨
-- åŸºäºç°æœ‰ crate_table_demo.sql æ ¼å¼

-- ç”¨æˆ·åŸºæœ¬ä¿¡æ¯è¡¨
CREATE EXTERNAL TABLE IF NOT EXISTS tag_system.user_basic_info (
    user_id string COMMENT 'ç”¨æˆ·ID',
    age int COMMENT 'å¹´é¾„',
    user_level string COMMENT 'ç”¨æˆ·ç­‰çº§',
    kyc_status string COMMENT 'KYCçŠ¶æ€',
    registration_date string COMMENT 'æ³¨å†Œæ—¥æœŸ',
    risk_score double COMMENT 'é£é™©è¯„åˆ†'
) COMMENT 'ç”¨æˆ·åŸºæœ¬ä¿¡æ¯è¡¨'
PARTITIONED BY (`dt` string)
ROW FORMAT SERDE
    'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
STORED AS INPUTFORMAT
    'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
OUTPUTFORMAT
    'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
LOCATION
    's3://exchanges-flink-test/batch/data/tag_system/user_basic_info/';

-- ç”¨æˆ·èµ„äº§æ±‡æ€»è¡¨
CREATE EXTERNAL TABLE IF NOT EXISTS tag_system.user_asset_summary (
    user_id string COMMENT 'ç”¨æˆ·ID',
    total_asset_value double COMMENT 'æ€»èµ„äº§ä»·å€¼',
    cash_balance double COMMENT 'ç°é‡‘ä½™é¢'
) COMMENT 'ç”¨æˆ·èµ„äº§æ±‡æ€»è¡¨'
PARTITIONED BY (`dt` string)
ROW FORMAT SERDE
    'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
STORED AS INPUTFORMAT
    'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
OUTPUTFORMAT
    'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
LOCATION
    's3://exchanges-flink-test/batch/data/tag_system/user_asset_summary/';

-- ç”¨æˆ·æ´»åŠ¨æ±‡æ€»è¡¨
CREATE EXTERNAL TABLE IF NOT EXISTS tag_system.user_activity_summary (
    user_id string COMMENT 'ç”¨æˆ·ID',
    trade_count_30d int COMMENT '30å¤©äº¤æ˜“æ¬¡æ•°',
    last_login_date string COMMENT 'æœ€åç™»å½•æ—¥æœŸ'
) COMMENT 'ç”¨æˆ·æ´»åŠ¨æ±‡æ€»è¡¨'
PARTITIONED BY (`dt` string)
ROW FORMAT SERDE
    'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
STORED AS INPUTFORMAT
    'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
OUTPUTFORMAT
    'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
LOCATION
    's3://exchanges-flink-test/batch/data/tag_system/user_activity_summary/';
'''
        return sql_content
    
    def create_test_data_generator(self) -> str:
        """åˆ›å»ºæµ‹è¯•æ•°æ®ç”Ÿæˆå™¨"""
        generator_content = '''#!/usr/bin/env python3
"""
æµ·è±šè°ƒåº¦å™¨æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨
ç›´æ¥å†™å…¥Hiveè¡¨ï¼ŒåŸºäºç°æœ‰Sparkèƒ½åŠ›
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import random
from datetime import datetime, timedelta

def create_spark_session():
    """åˆ›å»ºSparkä¼šè¯ - åŸºäºç°æœ‰HiveToKafka.pyæ¨¡å¼"""
    spark = SparkSession.builder \\
        .appName("TagSystemTestDataGenerator") \\
        .enableHiveSupport() \\
        .getOrCreate()
    return spark

def generate_test_data(spark, dt='2025-01-20'):
    """ç”Ÿæˆæµ‹è¯•æ•°æ®å¹¶å†™å…¥Hiveè¡¨"""
    
    print(f"ğŸš€ ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼Œæ—¥æœŸ: {dt}")
    
    # ç”Ÿæˆç”¨æˆ·åŸºæœ¬ä¿¡æ¯æµ‹è¯•æ•°æ®
    user_basic_data = []
    for i in range(1000):
        user_id = f"user_{i:06d}"
        age = random.randint(18, 65)
        user_level = random.choice(['VIP1', 'VIP2', 'VIP3', 'NORMAL'])
        kyc_status = random.choice(['verified', 'pending', 'rejected'])
        registration_date = (datetime.now() - timedelta(days=random.randint(1, 1000))).strftime('%Y-%m-%d')
        risk_score = random.uniform(0, 100)
        
        user_basic_data.append((user_id, age, user_level, kyc_status, registration_date, risk_score))
    
    # åˆ›å»ºDataFrameå¹¶å†™å…¥Hive
    user_basic_schema = StructType([
        StructField("user_id", StringType(), True),
        StructField("age", IntegerType(), True), 
        StructField("user_level", StringType(), True),
        StructField("kyc_status", StringType(), True),
        StructField("registration_date", StringType(), True),
        StructField("risk_score", DoubleType(), True)
    ])
    
    user_basic_df = spark.createDataFrame(user_basic_data, user_basic_schema)
    user_basic_df = user_basic_df.withColumn("dt", lit(dt))
    
    # å†™å…¥Hiveè¡¨
    user_basic_df.write \\
        .mode("overwrite") \\
        .partitionBy("dt") \\
        .saveAsTable("tag_system.user_basic_info")
    
    print("âœ… ç”¨æˆ·åŸºæœ¬ä¿¡æ¯æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆ")
    
    # ç”Ÿæˆç”¨æˆ·èµ„äº§æ•°æ®
    user_asset_data = []
    for i in range(1000):
        user_id = f"user_{i:06d}"
        total_asset = random.uniform(1000, 1000000)
        cash_balance = random.uniform(100, total_asset * 0.5)
        
        user_asset_data.append((user_id, total_asset, cash_balance))
    
    user_asset_schema = StructType([
        StructField("user_id", StringType(), True),
        StructField("total_asset_value", DoubleType(), True),
        StructField("cash_balance", DoubleType(), True)
    ])
    
    user_asset_df = spark.createDataFrame(user_asset_data, user_asset_schema)
    user_asset_df = user_asset_df.withColumn("dt", lit(dt))
    
    user_asset_df.write \\
        .mode("overwrite") \\
        .partitionBy("dt") \\
        .saveAsTable("tag_system.user_asset_summary")
    
    print("âœ… ç”¨æˆ·èµ„äº§æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆ")
    
    # ç”Ÿæˆç”¨æˆ·æ´»åŠ¨æ•°æ®
    user_activity_data = []
    for i in range(1000):
        user_id = f"user_{i:06d}"
        trade_count = random.randint(0, 100)
        last_login = (datetime.now() - timedelta(days=random.randint(0, 30))).strftime('%Y-%m-%d')
        
        user_activity_data.append((user_id, trade_count, last_login))
    
    user_activity_schema = StructType([
        StructField("user_id", StringType(), True),
        StructField("trade_count_30d", IntegerType(), True),
        StructField("last_login_date", StringType(), True)
    ])
    
    user_activity_df = spark.createDataFrame(user_activity_data, user_activity_schema)
    user_activity_df = user_activity_df.withColumn("dt", lit(dt))
    
    user_activity_df.write \\
        .mode("overwrite") \\
        .partitionBy("dt") \\
        .saveAsTable("tag_system.user_activity_summary")
    
    print("âœ… ç”¨æˆ·æ´»åŠ¨æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆ")
    
    # éªŒè¯æ•°æ®
    print("\\nğŸ“Š æ•°æ®éªŒè¯:")
    print(f"ç”¨æˆ·åŸºæœ¬ä¿¡æ¯è¡¨è®°å½•æ•°: {spark.table('tag_system.user_basic_info').count()}")
    print(f"ç”¨æˆ·èµ„äº§è¡¨è®°å½•æ•°: {spark.table('tag_system.user_asset_summary').count()}")
    print(f"ç”¨æˆ·æ´»åŠ¨è¡¨è®°å½•æ•°: {spark.table('tag_system.user_activity_summary').count()}")

if __name__ == "__main__":
    spark = create_spark_session()
    
    try:
        # åˆ›å»ºæ•°æ®åº“
        spark.sql("CREATE DATABASE IF NOT EXISTS tag_system")
        print("âœ… æ•°æ®åº“ tag_system åˆ›å»ºæˆåŠŸ")
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        generate_test_data(spark)
        
        print("ğŸ‰ æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆï¼")
        
    finally:
        spark.stop()
'''
        return generator_content
    
    def create_main_entry(self) -> str:
        """åˆ›å»ºä¸»ç¨‹åºå…¥å£ - ç”¨äºæµ·è±šè°ƒåº¦å™¨ä¸»ç¨‹åºå‚æ•°"""
        main_content = '''#!/usr/bin/env python3
"""
æµ·è±šè°ƒåº¦å™¨ä¸»ç¨‹åºå…¥å£
æ”¯æŒé€šè¿‡æµ·è±šè°ƒåº¦å™¨å›¾å½¢ç•Œé¢çš„ä¸»ç¨‹åºå‚æ•°æ‰§è¡Œ
"""

import sys
import os
import argparse
from pyspark.sql import SparkSession

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

def create_spark_session():
    """åˆ›å»ºSparkä¼šè¯ - åŸºäºç°æœ‰HiveToKafka.pyæ¨¡å¼"""
    spark = SparkSession.builder \\
        .appName("BigDataTagSystem-Dolphin") \\
        .enableHiveSupport() \\
        .getOrCreate()
    return spark

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = argparse.ArgumentParser(description="æµ·è±šè°ƒåº¦å™¨æ ‡ç­¾ç³»ç»Ÿ")
    parser.add_argument("--mode", required=True, choices=[
        "health", "task-all", "task-tags", "task-users", "list-tasks", "generate-test-data"
    ], help="æ‰§è¡Œæ¨¡å¼")
    parser.add_argument("--tag-ids", help="æ ‡ç­¾IDåˆ—è¡¨ï¼Œé€—å·åˆ†éš”")
    parser.add_argument("--user-ids", help="ç”¨æˆ·IDåˆ—è¡¨ï¼Œé€—å·åˆ†éš”")
    parser.add_argument("--dt", default="2025-01-20", help="æ•°æ®æ—¥æœŸ")
    
    args = parser.parse_args()
    
    print(f"ğŸš€ æµ·è±šè°ƒåº¦å™¨æ ‡ç­¾ç³»ç»Ÿå¯åŠ¨")
    print(f"ğŸ“‹ æ‰§è¡Œæ¨¡å¼: {args.mode}")
    
    # åˆ›å»ºSparkä¼šè¯
    spark = create_spark_session()
    
    try:
        if args.mode == "generate-test-data":
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            from generate_test_data import generate_test_data
            generate_test_data(spark, args.dt)
            
        elif args.mode == "health":
            # å¥åº·æ£€æŸ¥
            print("ğŸ” æ‰§è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥...")
            
            # æ£€æŸ¥Hiveè¡¨è®¿é—®
            try:
                spark.sql("SHOW DATABASES").show()
                print("âœ… Hiveè®¿é—®æ­£å¸¸")
            except Exception as e:
                print(f"âŒ Hiveè®¿é—®å¤±è´¥: {e}")
                return 1
            
            # æ£€æŸ¥MySQLè¿æ¥
            try:
                from mysql_config import DolphinMySQLConfig
                config = DolphinMySQLConfig()
                
                # æµ‹è¯•MySQLè¿æ¥
                mysql_df = spark.read \\
                    .format("jdbc") \\
                    .option("url", config.get_connection_url()) \\
                    .option("driver", "com.mysql.cj.jdbc.Driver") \\
                    .option("user", config.MYSQL_USERNAME) \\
                    .option("password", config.MYSQL_PASSWORD) \\
                    .option("query", "SELECT 1 as test") \\
                    .load()
                
                mysql_df.show()
                print("âœ… MySQLè¿æ¥æ­£å¸¸")
                
            except Exception as e:
                print(f"âŒ MySQLè¿æ¥å¤±è´¥: {e}")
                return 1
            
            print("ğŸ‰ ç³»ç»Ÿå¥åº·æ£€æŸ¥é€šè¿‡")
            
        elif args.mode == "task-all":
            # å…¨é‡æ ‡ç­¾è®¡ç®—
            from src.entry.tag_system_api import TagSystemAPI
            
            with TagSystemAPI(environment='dolphinscheduler') as api:
                success = api.run_task_all_users_all_tags()
                if not success:
                    return 1
                    
        elif args.mode == "task-tags":
            # æŒ‡å®šæ ‡ç­¾è®¡ç®—
            if not args.tag_ids:
                print("âŒ æŒ‡å®šæ ‡ç­¾æ¨¡å¼éœ€è¦æä¾› --tag-ids å‚æ•°")
                return 1
                
            tag_ids = [int(x.strip()) for x in args.tag_ids.split(',')]
            
            from src.entry.tag_system_api import TagSystemAPI
            with TagSystemAPI(environment='dolphinscheduler') as api:
                success = api.run_task_specific_tags(tag_ids)
                if not success:
                    return 1
                    
        elif args.mode == "list-tasks":
            # åˆ—å‡ºå¯ç”¨ä»»åŠ¡
            from src.tasks.task_registry import TagTaskFactory
            TagTaskFactory.register_all_tasks()
            tasks = TagTaskFactory.get_all_available_tasks()
            
            print("ğŸ“‹ å¯ç”¨æ ‡ç­¾ä»»åŠ¡:")
            for task_id, task_class in tasks.items():
                print(f"  {task_id}: {task_class.__name__}")
                
        else:
            print(f"âŒ ä¸æ”¯æŒçš„æ¨¡å¼: {args.mode}")
            return 1
            
        print("âœ… ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ")
        return 0
        
    except Exception as e:
        print(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
        
    finally:
        spark.stop()

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
'''
        return main_content
    
    def create_optimized_main_entry(self) -> str:
        """åˆ›å»ºä¼˜åŒ–çš„ä¸»ç¨‹åºå…¥å£ - ç›´æ¥è°ƒç”¨src/tag_engine/main.py"""
        main_content = '''#!/usr/bin/env python3
"""
æµ·è±šè°ƒåº¦å™¨ä¸»ç¨‹åºå…¥å£ - ç»Ÿä¸€è°ƒç”¨å…¥å£
ç›´æ¥è°ƒç”¨src/tag_engine/main.pyï¼Œé¿å…é‡å¤ä»£ç 
"""

import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

def main():
    """ä¸»ç¨‹åºå…¥å£ - ç›´æ¥è°ƒç”¨æ ¸å¿ƒmain.py"""
    print("ğŸ¬ æµ·è±šè°ƒåº¦å™¨æ ‡ç­¾ç³»ç»Ÿ - ç»Ÿä¸€å…¥å£")
    print("ğŸ“¡ è°ƒç”¨æ ¸å¿ƒæ ‡ç­¾å¼•æ“...")
    
    try:
        # ç›´æ¥è°ƒç”¨src/tag_engine/main.py
        from src.tag_engine.main import main as core_main
        
        # è®¾ç½®é»˜è®¤åº”ç”¨åç§°ä¸ºæµ·è±šè°ƒåº¦å™¨ç‰ˆæœ¬
        if '--app-name' not in sys.argv:
            sys.argv.extend(['--app-name', 'BigDataTagSystem-Dolphin'])
            
        # è°ƒç”¨æ ¸å¿ƒmainå‡½æ•°
        core_main()
        
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥æ ¸å¿ƒæ¨¡å—: {e}")
        print("è¯·ç¡®ä¿src/tag_engineç›®å½•å­˜åœ¨å¹¶åŒ…å«æ‰€éœ€æ¨¡å—")
        sys.exit(1)
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
'''
        return main_content
    
    def create_optimized_deploy_guide(self, custom_extract_path: str = None) -> str:
        """åˆ›å»ºä¼˜åŒ–çš„éƒ¨ç½²æŒ‡å—"""
        extract_path = custom_extract_path or "/dolphinscheduler/default/resources/"
        
        guide = f'''# ğŸ¬ æµ·è±šè°ƒåº¦å™¨å›¾å½¢ç•Œé¢éƒ¨ç½²æŒ‡å—

## ğŸ“¦ éƒ¨ç½²åŒ…å†…å®¹ï¼ˆç²¾ç®€ç‰ˆï¼‰
- `main.py` - ä¸»ç¨‹åºå…¥å£ï¼ˆä½¿ç”¨src.config.base.MySQLConfigç»Ÿä¸€é…ç½®ï¼‰
- `src/` - é¡¹ç›®æºç ï¼ˆç›´æ¥è¯»å–Hiveè¡¨ï¼Œç§»é™¤S3ä¾èµ–ï¼‰
- `generate_test_data.py` - æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨
- `create_test_tables.sql` - æµ‹è¯•è¡¨åˆ›å»ºSQL
- `requirements.txt` - Pythonä¾èµ–

## ğŸš€ UIç•Œé¢éƒ¨ç½²æ­¥éª¤ï¼ˆæ¨èï¼‰

### 1. ä¸Šä¼ ZIPåŒ…åˆ°èµ„æºä¸­å¿ƒ
1. ç™»å½•æµ·è±šè°ƒåº¦å™¨Webç•Œé¢
2. è¿›å…¥ **èµ„æºä¸­å¿ƒ** â†’ **æ–‡ä»¶ç®¡ç†**
3. ä¸Šä¼  `tag_system_dolphin.zip`

### 2. ç›´æ¥åœ¨èµ„æºä¸­å¿ƒè§£å‹
1. åœ¨èµ„æºä¸­å¿ƒä¸­å³é”®ç‚¹å‡»ä¸Šä¼ çš„`tag_system_dolphin.zip`
2. é€‰æ‹©è§£å‹ï¼Œæˆ–è€…åˆ›å»ºShellä»»åŠ¡è§£å‹ï¼š
```bash
#!/bin/bash
cd {extract_path}
unzip -o tag_system_dolphin.zip
echo "âœ… æ ‡ç­¾ç³»ç»Ÿéƒ¨ç½²åŒ…è§£å‹å®Œæˆåˆ°: {extract_path}"
```

### 3. åˆ›å»ºæ ‡ç­¾è®¡ç®—å·¥ä½œæµ
1. åˆ›å»ºæ–°å·¥ä½œæµï¼š"æ ‡ç­¾è®¡ç®—ä»»åŠ¡"
2. æ·»åŠ SparkèŠ‚ç‚¹ï¼š
   - **ä¸»ç¨‹åº**: `{extract_path}main.py`
   - **ä¸»ç¨‹åºå‚æ•°**: `--mode health`
   - **Sparkä»»åŠ¡åç§°**: BigDataTagSystem-Dolphin

### 4. Sparkä»»åŠ¡é…ç½®
**åŸºç¡€é…ç½®**:
- Driveræ ¸å¿ƒæ•°: 2
- Driverå†…å­˜: 2g
- Executoræ•°é‡: 3
- Executorå†…å­˜: 4g
- Executoræ ¸å¿ƒæ•°: 2

**é«˜çº§é…ç½®**:
- YARNé˜Ÿåˆ—: default
- ä¸»ç¨‹åºå‚æ•°ç¤ºä¾‹:
  - å¥åº·æ£€æŸ¥: `--mode health`
  - å…¨é‡æ ‡ç­¾: `--mode task-all`
  - æŒ‡å®šæ ‡ç­¾: `--mode task-tags --tag-ids 1,2,3`

## ğŸ¯ æ”¯æŒçš„æ‰§è¡Œæ¨¡å¼

### å¥åº·æ£€æŸ¥æ¨¡å¼
```bash
--mode health
```
éªŒè¯Hiveå’ŒMySQLè¿æ¥

### æµ‹è¯•æ•°æ®ç”Ÿæˆ
```bash
--mode generate-test-data --dt 2025-01-20
```
ç”Ÿæˆæµ‹è¯•æ•°æ®åˆ°Hiveè¡¨

### å…¨é‡æ ‡ç­¾è®¡ç®—
```bash
--mode task-all
```
è®¡ç®—æ‰€æœ‰ç”¨æˆ·çš„æ‰€æœ‰æ ‡ç­¾

### æŒ‡å®šæ ‡ç­¾è®¡ç®—
```bash
--mode task-tags --tag-ids 1,2,3
```
è®¡ç®—æŒ‡å®šæ ‡ç­¾IDçš„æ ‡ç­¾

### ä»»åŠ¡åˆ—è¡¨æŸ¥çœ‹
```bash
--mode list-tasks
```
æŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„æ ‡ç­¾ä»»åŠ¡

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. å‚æ•°åŒ–å·¥ä½œæµ
åœ¨å·¥ä½œæµä¸­ä½¿ç”¨å…¨å±€å‚æ•°:
- `tag_ids`: æ ‡ç­¾IDåˆ—è¡¨
- `data_date`: æ•°æ®æ—¥æœŸ
- `mode`: æ‰§è¡Œæ¨¡å¼

### 2. ä¾èµ–ç®¡ç†
æ— éœ€é¢å¤–å®‰è£…Pythonä¾èµ–ï¼Œä½¿ç”¨é›†ç¾¤é¢„è£…çš„PySparkç¯å¢ƒ

### 3. é”™è¯¯å¤„ç†
- è®¾ç½®ä»»åŠ¡å¤±è´¥é‡è¯•æ¬¡æ•°: 2
- è®¾ç½®ä»»åŠ¡è¶…æ—¶æ—¶é—´: 30åˆ†é’Ÿ
- é…ç½®å‘Šè­¦é€šçŸ¥

### 4. ç›‘æ§å»ºè®®
- å®šæœŸæ‰§è¡Œå¥åº·æ£€æŸ¥ä»»åŠ¡
- ç›‘æ§MySQLæ ‡ç­¾æ•°æ®å¢é•¿
- æŸ¥çœ‹Spark UIèµ„æºä½¿ç”¨æƒ…å†µ

## ğŸ”§ æ•…éšœæ’é™¤

### MySQLè¿æ¥é—®é¢˜
- æ£€æŸ¥ç½‘ç»œè¿é€šæ€§
- éªŒè¯ç”¨æˆ·åå¯†ç 
- ç¡®è®¤æ•°æ®åº“æƒé™

### Hiveè¡¨è®¿é—®é—®é¢˜
- éªŒè¯è¡¨æ˜¯å¦å­˜åœ¨
- æ£€æŸ¥åˆ†åŒºæ•°æ®
- ç¡®è®¤æƒé™é…ç½®

### æ€§èƒ½ä¼˜åŒ–
- æ ¹æ®æ•°æ®é‡è°ƒæ•´Executoré…ç½®
- è€ƒè™‘å¢åŠ å¹¶è¡Œåº¦
- ä¼˜åŒ–SQLæŸ¥è¯¢é€»è¾‘
'''
        return guide
    
    def create_requirements(self) -> str:
        """åˆ›å»ºrequirements.txtï¼ˆæµ·è±šè°ƒåº¦å™¨ç¯å¢ƒå¯é€‰ä¾èµ–ï¼‰"""
        requirements = '''# æµ·è±šè°ƒåº¦å™¨æ ‡ç­¾ç³»ç»Ÿä¾èµ–
# æ³¨æ„ï¼šPySparké€šå¸¸å·²åœ¨é›†ç¾¤ç¯å¢ƒä¸­é¢„è£…ï¼Œæ— éœ€å®‰è£…

# å¿…éœ€ä¾èµ–ï¼ˆå¦‚æœé›†ç¾¤ç¯å¢ƒç¼ºå°‘ï¼‰
pymysql>=1.0.0       # MySQLè¿æ¥å™¨

# å¯é€‰ä¾èµ–ï¼ˆé€šå¸¸é›†ç¾¤å·²æœ‰ï¼‰
# pyspark>=3.2.0     # Sparkåˆ†å¸ƒå¼è®¡ç®—å¼•æ“ï¼ˆé›†ç¾¤é¢„è£…ï¼‰
# pandas>=1.3.0      # æ•°æ®åˆ†æåº“ï¼ˆé›†ç¾¤é¢„è£…ï¼‰

# å®‰è£…å‘½ä»¤ï¼ˆä»…åœ¨éœ€è¦æ—¶æ‰§è¡Œï¼‰:
# pip3 install pymysql
'''
        return requirements
    
    def create_deploy_guide(self) -> str:
        """åˆ›å»ºéƒ¨ç½²æŒ‡å—"""
        guide = '''# ğŸ¬ æµ·è±šè°ƒåº¦å™¨å›¾å½¢ç•Œé¢éƒ¨ç½²æŒ‡å—

## ğŸ“¦ éƒ¨ç½²åŒ…å†…å®¹
- `main.py` - ä¸»ç¨‹åºå…¥å£
- `src/` - é¡¹ç›®æºç 
- `generate_test_data.py` - æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨
- `create_test_tables.sql` - æµ‹è¯•è¡¨åˆ›å»ºSQL
- `requirements.txt` - Pythonä¾èµ–

## ğŸš€ éƒ¨ç½²æ­¥éª¤

### 1. ä¸Šä¼ åˆ°èµ„æºä¸­å¿ƒ
1. ç™»å½•æµ·è±šè°ƒåº¦å™¨Webç•Œé¢
2. è¿›å…¥ **èµ„æºä¸­å¿ƒ** â†’ **æ–‡ä»¶ç®¡ç†**
3. ä¸Šä¼  `tag_system_dolphin.zip`
4. ç›´æ¥åœ¨èµ„æºä¸­å¿ƒè§£å‹åˆ° `/dolphinscheduler/default/resources/`

### 2. ä¾èµ–ç®¡ç†
é€šå¸¸æ— éœ€å®‰è£…é¢å¤–ä¾èµ–ï¼Œé›†ç¾¤ç¯å¢ƒå·²é¢„è£…PySparkã€‚
å¦‚éœ€è¦ï¼Œå¯åˆ›å»ºShellä»»åŠ¡ï¼š
```bash
#!/bin/bash
pip3 install pymysql
echo "âœ… å®‰è£…MySQLè¿æ¥å™¨å®Œæˆ"
```

### 3. åˆ›å»ºæµ‹è¯•è¡¨å’Œæ•°æ®
åˆ›å»ºSparkä»»åŠ¡æ‰§è¡Œï¼š
```bash
# ä¸»ç¨‹åºå‚æ•°
--mode generate-test-data --dt 2025-01-20

# æˆ–è€…å…ˆåˆ›å»ºè¡¨
spark-sql -f create_test_tables.sql
```

### 4. å¥åº·æ£€æŸ¥
åˆ›å»ºSparkä»»åŠ¡æµ‹è¯•ï¼š
```bash
# ä¸»ç¨‹åºå‚æ•°  
--mode health
```

## ğŸ¯ ä»»åŠ¡é…ç½®

### Sparkä»»åŠ¡é…ç½®ï¼ˆåœ¨æµ·è±šå›¾å½¢ç•Œé¢ä¸­ï¼‰
- **ä¸»ç¨‹åº**: `/dolphinscheduler/default/resources/main.py`
- **ä¸»ç¨‹åºå‚æ•°**: `--mode health` (æ ¹æ®éœ€è¦è°ƒæ•´)
- **Driveræ ¸å¿ƒæ•°**: 2
- **Driverå†…å­˜**: 2g
- **Executoræ•°é‡**: 5
- **Executorå†…å­˜**: 4g
- **Executoræ ¸å¿ƒæ•°**: 2
- **YARNé˜Ÿåˆ—**: default

### å¸¸ç”¨ä¸»ç¨‹åºå‚æ•°
```bash
# å¥åº·æ£€æŸ¥
--mode health

# ç”Ÿæˆæµ‹è¯•æ•°æ®
--mode generate-test-data --dt 2025-01-20

# å…¨é‡æ ‡ç­¾è®¡ç®—
--mode task-all

# æŒ‡å®šæ ‡ç­¾è®¡ç®—
--mode task-tags --tag-ids 1,2,3

# åˆ—å‡ºå¯ç”¨ä»»åŠ¡
--mode list-tasks
```

## ğŸ”§ Javaæ¥å£é›†æˆï¼ˆåç»­ï¼‰
é¡¹ç›®æ”¯æŒé€šè¿‡Javaæ¥å£è§¦å‘ï¼š
```java
// é€šè¿‡æµ·è±šè°ƒåº¦å™¨APIè§¦å‘Sparkä»»åŠ¡
DolphinSchedulerClient client = new DolphinSchedulerClient();
client.triggerWorkflow("tag_system_compute", Map.of(
    "mode", "task-tags",
    "tag_ids", "1,2,3"
));
```

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—
- é€šè¿‡æµ·è±šè°ƒåº¦å™¨UIæŸ¥çœ‹ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€
- Spark UIç›‘æ§èµ„æºä½¿ç”¨æƒ…å†µ
- ä»»åŠ¡æ—¥å¿—åœ¨æµ·è±šè°ƒåº¦å™¨ä¸­æŸ¥çœ‹
'''
        return guide
    
    def create_zip_package(self, custom_extract_path: str = None):
        """åˆ›å»ºZIPéƒ¨ç½²åŒ…ï¼Œæ”¯æŒè‡ªå®šä¹‰è§£å‹è·¯å¾„"""
        zip_path = self.output_dir / "tag_system_dolphin.zip"
        
        print("ğŸ“¦ åˆ›å»ºæµ·è±šè°ƒåº¦å™¨éƒ¨ç½²åŒ…...")
        
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            
            # æ·»åŠ srcç›®å½•
            src_dir = self.project_root / "src"
            if src_dir.exists():
                for file_path in src_dir.rglob("*.py"):
                    arc_name = f"src/{file_path.relative_to(src_dir)}"
                    zip_file.write(file_path, arc_name)
                    print(f"  âœ… æ·»åŠ æºç : {arc_name}")
            
            # æ·»åŠ ä¼˜åŒ–åçš„ä¸»ç¨‹åºå…¥å£ï¼ˆä½¿ç”¨ç»Ÿä¸€MySQLé…ç½®ï¼‰
            main_content = self.create_optimized_main_entry()
            zip_file.writestr("main.py", main_content)
            print("  âœ… æ·»åŠ ä¸»ç¨‹åº: main.py")
            
            # æ·»åŠ æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨
            test_generator = self.create_test_data_generator()
            zip_file.writestr("generate_test_data.py", test_generator)
            print("  âœ… æ·»åŠ æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨: generate_test_data.py")
            
            # æ·»åŠ å»ºè¡¨SQL
            create_tables_sql = self.create_hive_test_tables()
            zip_file.writestr("create_test_tables.sql", create_tables_sql)
            print("  âœ… æ·»åŠ å»ºè¡¨SQL: create_test_tables.sql")
            
            # æ·»åŠ ä¾èµ–æ–‡ä»¶
            requirements = self.create_requirements()
            zip_file.writestr("requirements.txt", requirements)
            print("  âœ… æ·»åŠ ä¾èµ–: requirements.txt")
        
        print(f"\nğŸ‰ éƒ¨ç½²åŒ…åˆ›å»ºå®Œæˆ!")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ“¦ ZIPåŒ…: {zip_path}")
        print(f"ğŸ“‹ å¤§å°: {zip_path.stat().st_size / 1024:.1f} KB")
        
        return zip_path

def main():
    """ä¸»å‡½æ•°"""
    packager = DolphinGUIDeployPackager()
    zip_path = packager.create_zip_package()
    
    print(f"\nğŸ“‹ åç»­æ­¥éª¤:")
    print(f"1. ä¸Šä¼  {zip_path} åˆ°æµ·è±šè°ƒåº¦å™¨èµ„æºä¸­å¿ƒ")
    print(f"2. ç›´æ¥åœ¨èµ„æºä¸­å¿ƒè§£å‹åˆ° /dolphinscheduler/default/resources/")
    print(f"3. æŒ‰ç…§ dolphin_gui_deploy/éƒ¨ç½²è¯´æ˜.md è¿›è¡Œé…ç½®")
    print(f"4. åˆ›å»ºSparkä»»åŠ¡ï¼Œä¸»ç¨‹åºè·¯å¾„ï¼š/dolphinscheduler/default/resources/main.py")

if __name__ == "__main__":
    main()