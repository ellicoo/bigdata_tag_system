import logging
from typing import Optional, Dict, Any
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import col, from_json, array_union, array_distinct, when, lit
from pyspark.sql.types import ArrayType, IntegerType
from datetime import date

from src.config.base import MySQLConfig

logger = logging.getLogger(__name__)


class AdvancedTagMerger:
    """é«˜çº§æ ‡ç­¾åˆå¹¶å™¨ - æ”¯æŒå†…å­˜åˆå¹¶å’Œæ•°æ®åº“åˆå¹¶çš„å…¬å…±ç»„ä»¶"""
    
    def __init__(self, spark: SparkSession, mysql_config: MySQLConfig):
        self.spark = spark
        self.mysql_config = mysql_config
    
    def merge_with_existing_tags(self, new_tags_df: DataFrame, cached_existing_tags: DataFrame = None) -> Optional[DataFrame]:
        """
        ä¸MySQLä¸­ç°æœ‰æ ‡ç­¾åˆå¹¶
        
        Args:
            new_tags_df: æ–°è®¡ç®—çš„æ ‡ç­¾DataFrame (user_id, tag_ids, tag_details, computed_date)
            
        Returns:
            åˆå¹¶åçš„DataFrame
        """
        try:
            logger.info("å¼€å§‹ä¸MySQLä¸­ç°æœ‰æ ‡ç­¾åˆå¹¶...")
            
            # 1. ä½¿ç”¨é¢„ç¼“å­˜çš„ç°æœ‰æ ‡ç­¾æ•°æ®
            if cached_existing_tags is not None:
                existing_tags = cached_existing_tags
                logger.info("ä½¿ç”¨é¢„ç¼“å­˜çš„ç°æœ‰æ ‡ç­¾æ•°æ®")
            else:
                # å…œåº•ï¼šè¯»å–ç°æœ‰æ ‡ç­¾å¹¶ä½¿ç”¨å†…å­˜+ç£ç›˜æŒä¹…åŒ–
                existing_tags = self._read_existing_user_tags()
                if existing_tags is not None:
                    from pyspark import StorageLevel
                    existing_tags = existing_tags.persist(StorageLevel.MEMORY_AND_DISK)
            
            if existing_tags is None or existing_tags.count() == 0:
                logger.info("æ•°æ®åº“ä¸­æ²¡æœ‰ç°æœ‰æ ‡ç­¾ï¼Œç›´æ¥è¿”å›æ–°æ ‡ç­¾")
                return new_tags_df
            
            existing_count = existing_tags.count()
            logger.info(f"ç°æœ‰æ ‡ç­¾æ•°æ®: {existing_count} æ¡ç”¨æˆ·æ ‡ç­¾")
            
            # 3. å·¦è¿æ¥åˆå¹¶ - ä¿®å¤åˆ—åå†²çªé—®é¢˜
            merged_df = new_tags_df.alias("new").join(
                existing_tags.select("user_id", "tag_ids").alias("existing"),
                "user_id",
                "left"
            )
            
            # 4. åˆå¹¶æ ‡ç­¾æ•°ç»„ - ä¿®å¤åˆ—å¼•ç”¨é—®é¢˜
            final_merged = merged_df.select(
                col("user_id"),
                self._merge_tag_arrays(
                    col("existing.tag_ids"), 
                    col("new.tag_ids")
                ).alias("tag_ids"),
                col("new.tag_details"),
                col("new.computed_date")
            )
            
            # 5. è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºåˆå¹¶å‰åçš„æ•°æ®
            logger.info("åˆå¹¶å‰æ–°æ ‡ç­¾æ ·ä¾‹:")
            new_tags_df.show(3, truncate=False)
            
            logger.info("åˆå¹¶å‰ç°æœ‰æ ‡ç­¾æ ·ä¾‹:")
            existing_tags.show(3, truncate=False)
            
            logger.info("åˆå¹¶åæ ‡ç­¾æ ·ä¾‹:")
            final_merged.show(3, truncate=False)
            
            # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œæ¸…ç†é¢„ç¼“å­˜æ•°æ®ï¼Œç”±åœºæ™¯è°ƒåº¦å™¨ç»Ÿä¸€ç®¡ç†
            if cached_existing_tags is None and existing_tags is not None:
                # åªæœ‰éé¢„ç¼“å­˜æ•°æ®æ‰éœ€è¦åœ¨è¿™é‡Œæ¸…ç†
                existing_tags.unpersist()
            
            merge_count = final_merged.count()
            logger.info(f"âœ… ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶å®Œæˆï¼Œå½±å“ {merge_count} ä¸ªç”¨æˆ·")
            
            return final_merged
            
        except Exception as e:
            logger.error(f"ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶å¤±è´¥: {str(e)}")
            # å¤±è´¥æ—¶è¿”å›åŸå§‹æ•°æ®
            return new_tags_df
    
    def _read_existing_user_tags(self) -> Optional[DataFrame]:
        """ä»MySQLè¯»å–ç°æœ‰ç”¨æˆ·æ ‡ç­¾å¹¶ç¼“å­˜åˆ°å†…å­˜/ç£ç›˜"""
        try:
            logger.info("ğŸ“– ä»MySQLè¯»å–ç°æœ‰ç”¨æˆ·æ ‡ç­¾...")
            
            existing_df = self.spark.read.jdbc(
                url=self.mysql_config.jdbc_url,
                table="user_tags",
                properties=self.mysql_config.connection_properties
            )
            
            if existing_df.count() == 0:
                logger.info("MySQLä¸­æ²¡æœ‰ç°æœ‰æ ‡ç­¾æ•°æ®")
                return None
            
            # å°†JSONå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°ç»„ç±»å‹
            processed_df = existing_df.select(
                "user_id",
                from_json(col("tag_ids"), ArrayType(IntegerType())).alias("tag_ids"),
                "tag_details"
            )
            
            # æŒä¹…åŒ–åˆ°å†…å­˜å’Œç£ç›˜
            processed_df = processed_df.persist()
            
            logger.info(f"æˆåŠŸè¯»å–å¹¶ç¼“å­˜ç°æœ‰æ ‡ç­¾æ•°æ®")
            return processed_df
            
        except Exception as e:
            logger.info(f"è¯»å–ç°æœ‰æ ‡ç­¾å¤±è´¥ï¼ˆå¯èƒ½æ˜¯é¦–æ¬¡è¿è¡Œï¼‰: {str(e)}")
            return None
    
    def _merge_tag_arrays(self, existing_tags_col, new_tags_col):
        """åˆå¹¶ä¸¤ä¸ªæ ‡ç­¾æ•°ç»„å¹¶å»é‡"""
        from pyspark.sql.functions import udf
        from pyspark.sql.types import ArrayType, IntegerType
        
        @udf(returnType=ArrayType(IntegerType()))
        def merge_arrays(existing_tags, new_tags):
            if existing_tags is None:
                existing_tags = []
            if new_tags is None:
                new_tags = []
            
            # åˆå¹¶å¹¶å»é‡ï¼Œä¿æŒæ’åº
            merged = list(set(existing_tags + new_tags))
            return sorted(merged)
        
        return merge_arrays(existing_tags_col, new_tags_col)
    
    def cleanup_cache(self):
        """æ¸…ç†ç¼“å­˜èµ„æº"""
        try:
            self.spark.catalog.clearCache()
            logger.info("âœ… æ¸…ç†æ ‡ç­¾åˆå¹¶ç¼“å­˜å®Œæˆ")
        except Exception as e:
            logger.warning(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {str(e)}")


class TagMergeStrategy:
    """æ ‡ç­¾åˆå¹¶ç­–ç•¥æšä¸¾"""
    
    # ä¸ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶ï¼Œç›´æ¥å†…å­˜åˆå¹¶ç»“æœ
    MEMORY_ONLY = "memory_only"
    
    # ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶ï¼Œå†…å­˜åˆå¹¶åå†ä¸MySQLæ ‡ç­¾åˆå¹¶
    MEMORY_THEN_DATABASE = "memory_then_database"


class UnifiedTagMerger:
    """ç»Ÿä¸€æ ‡ç­¾åˆå¹¶å™¨ - æ ¹æ®åœºæ™¯é€‰æ‹©åˆå¹¶ç­–ç•¥"""
    
    def __init__(self, spark: SparkSession, mysql_config: MySQLConfig):
        self.spark = spark
        self.mysql_config = mysql_config
        self.advanced_merger = AdvancedTagMerger(spark, mysql_config)
    
    def merge_tags(self, tag_results: list, strategy: str) -> Optional[DataFrame]:
        """
        æ ¹æ®ç­–ç•¥åˆå¹¶æ ‡ç­¾
        
        Args:
            tag_results: æ ‡ç­¾è®¡ç®—ç»“æœåˆ—è¡¨
            strategy: åˆå¹¶ç­–ç•¥ (MEMORY_ONLY | MEMORY_THEN_DATABASE)
            
        Returns:
            åˆå¹¶åçš„DataFrame
        """
        try:
            if not tag_results:
                logger.warning("æ²¡æœ‰æ ‡ç­¾ç»“æœéœ€è¦åˆå¹¶")
                return None
            
            logger.info(f"ä½¿ç”¨ç­–ç•¥ {strategy} åˆå¹¶æ ‡ç­¾")
            
            # ç¬¬ä¸€æ­¥ï¼šå†…å­˜åˆå¹¶ï¼ˆæ‰€æœ‰ç­–ç•¥éƒ½éœ€è¦ï¼‰
            memory_merged = self._memory_merge(tag_results)
            if memory_merged is None:
                return None
            
            # ç¬¬äºŒæ­¥ï¼šæ ¹æ®ç­–ç•¥å†³å®šæ˜¯å¦ä¸æ•°æ®åº“åˆå¹¶
            if strategy == TagMergeStrategy.MEMORY_ONLY:
                logger.info("ä»…å†…å­˜åˆå¹¶ï¼Œä¸ä¸æ•°æ®åº“ç°æœ‰æ ‡ç­¾åˆå¹¶")
                return memory_merged
            
            elif strategy == TagMergeStrategy.MEMORY_THEN_DATABASE:
                logger.info("å†…å­˜åˆå¹¶åï¼Œå†ä¸æ•°æ®åº“ç°æœ‰æ ‡ç­¾åˆå¹¶")
                return self.advanced_merger.merge_with_existing_tags(memory_merged)
            
            else:
                logger.error(f"æœªçŸ¥çš„åˆå¹¶ç­–ç•¥: {strategy}")
                return memory_merged
                
        except Exception as e:
            logger.error(f"æ ‡ç­¾åˆå¹¶å¤±è´¥: {str(e)}")
            return None
    
    def _memory_merge(self, tag_results: list) -> Optional[DataFrame]:
        """å†…å­˜åˆå¹¶ï¼šå°†åŒä¸€ç”¨æˆ·çš„å¤šä¸ªæ ‡ç­¾åˆå¹¶"""
        try:
            from functools import reduce
            from pyspark.sql.functions import collect_list, array_distinct, struct
            
            # åˆå¹¶æ‰€æœ‰æ ‡ç­¾ç»“æœ
            all_tags = reduce(lambda df1, df2: df1.union(df2), tag_results)
            
            if all_tags.count() == 0:
                return None
            
            # å»é‡
            deduplicated = all_tags.dropDuplicates(["user_id", "tag_id"])
            
            # ä¸°å¯Œæ ‡ç­¾ä¿¡æ¯
            enriched = self._enrich_with_tag_info(deduplicated)
            
            # æŒ‰ç”¨æˆ·èšåˆ
            aggregated = enriched.groupBy("user_id").agg(
                collect_list("tag_id").alias("tag_ids_raw"),
                collect_list(struct("tag_id", "tag_name", "tag_category")).alias("tag_info_list")
            )
            
            # å»é‡å¹¶æ ¼å¼åŒ–
            final_result = aggregated.select(
                "user_id",
                array_distinct("tag_ids_raw").alias("tag_ids"),
                "tag_info_list"
            )
            
            return self._format_output(final_result)
            
        except Exception as e:
            logger.error(f"å†…å­˜åˆå¹¶å¤±è´¥: {str(e)}")
            return None
    
    def _enrich_with_tag_info(self, tags_df: DataFrame) -> DataFrame:
        """ä¸°å¯Œæ ‡ç­¾ä¿¡æ¯"""
        try:
            tag_definitions = self.spark.read.jdbc(
                url=self.mysql_config.jdbc_url,
                table="tag_definition",
                properties=self.mysql_config.connection_properties
            ).select("tag_id", "tag_name", "tag_category")
            
            return tags_df.join(
                tag_definitions, "tag_id", "left"
            ).select(
                "user_id", "tag_id", 
                col("tag_name"), col("tag_category"), "tag_detail"
            )
            
        except Exception as e:
            logger.error(f"ä¸°å¯Œæ ‡ç­¾ä¿¡æ¯å¤±è´¥: {str(e)}")
            return tags_df.select(
                "user_id", "tag_id",
                lit("unknown").alias("tag_name"),
                lit("unknown").alias("tag_category"),
                "tag_detail"
            )
    
    def _format_output(self, user_tags_df: DataFrame) -> DataFrame:
        """æ ¼å¼åŒ–è¾“å‡º"""
        from pyspark.sql.functions import udf
        from pyspark.sql.types import StringType
        import json
        
        @udf(returnType=StringType())
        def build_tag_details(tag_info_list):
            if not tag_info_list:
                return "{}"
            
            tag_details = {}
            for tag_info in tag_info_list:
                tag_id = str(tag_info['tag_id'])
                tag_details[tag_id] = {
                    'tag_name': tag_info['tag_name'],
                    'tag_category': tag_info['tag_category']
                }
            return json.dumps(tag_details, ensure_ascii=False)
        
        return user_tags_df.select(
            col("user_id"),
            col("tag_ids"),
            build_tag_details(col("tag_info_list")).alias("tag_details"),
            lit(date.today()).alias("computed_date")
        )
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.advanced_merger.cleanup_cache()