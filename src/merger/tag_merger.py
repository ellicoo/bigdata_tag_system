import logging
from typing import List, Optional
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import col, collect_list, array_distinct, array_union, to_json, struct, map_from_arrays, lit, when, expr, size
from pyspark.sql.types import ArrayType, IntegerType
from functools import reduce
from datetime import date

from src.config.base import MySQLConfig

logger = logging.getLogger(__name__)


class TagMerger:
    """æ ‡ç­¾åˆå¹¶å™¨ - å¤„ç†ç”¨æˆ·æ ‡ç­¾çš„åˆå¹¶å’ŒåŽ»é‡ï¼ˆæ­£ç¡®å®žçŽ°ï¼šä¸€ä¸ªç”¨æˆ·ä¸€æ¡è®°å½•ï¼ŒåŒ…å«æ ‡ç­¾IDæ•°ç»„ï¼‰"""
    
    def __init__(self, spark: SparkSession, mysql_config: MySQLConfig):
        self.spark = spark
        self.mysql_config = mysql_config
    
    def merge_user_tags(self, new_tag_results: List[DataFrame]) -> Optional[DataFrame]:
        """åˆå¹¶å¤šä¸ªæ ‡ç­¾è®¡ç®—ç»“æžœåˆ°ç»Ÿä¸€çš„ç”¨æˆ·æ ‡ç­¾è¡¨ç»“æž„ï¼ˆä¸€ä¸ªç”¨æˆ·ä¸€æ¡è®°å½•ï¼ŒåŒ…å«æ ‡ç­¾IDæ•°ç»„ï¼‰"""
        try:
            if not new_tag_results:
                logger.warning("æ²¡æœ‰æ ‡ç­¾ç»“æžœéœ€è¦åˆå¹¶")
                return None
            
            logger.info(f"å¼€å§‹åˆå¹¶ {len(new_tag_results)} ä¸ªæ ‡ç­¾è®¡ç®—ç»“æžœ...")
            
            # 1. åˆå¹¶æ‰€æœ‰æ–°è®¡ç®—çš„æ ‡ç­¾ç»“æžœ
            all_new_tags = reduce(lambda df1, df2: df1.union(df2), new_tag_results)
            
            if all_new_tags.count() == 0:
                logger.warning("åˆå¹¶åŽæ²¡æœ‰æ ‡ç­¾æ•°æ®")
                return None
            
            # 2. é¦–å…ˆä»Žè§„åˆ™ä¸­èŽ·å–æ ‡ç­¾åç§°å’Œåˆ†ç±»ä¿¡æ¯
            enriched_tags = self._enrich_with_tag_info(all_new_tags)
            
            # 3. å…ˆåŽ»é‡ï¼Œå†æŒ‰ç”¨æˆ·èšåˆï¼ˆå…³é”®ä¿®å¤ï¼šé¿å…æ ‡ç­¾é‡å¤ï¼‰
            # å…ˆåŽ»é™¤æ¯ä¸ªç”¨æˆ·çš„é‡å¤æ ‡ç­¾
            deduplicated_tags = enriched_tags.dropDuplicates(["user_id", "tag_id"])
            
            # ç„¶åŽèšåˆæˆæ•°ç»„
            user_new_tags = deduplicated_tags.groupBy("user_id").agg(
                collect_list("tag_id").alias("new_tag_ids_raw"),
                collect_list(struct("tag_id", "tag_name", "tag_category")).alias("tag_info_list")
            )
            
            # å¯¹æ ‡ç­¾IDæ•°ç»„è¿›è¡ŒåŽ»é‡
            from pyspark.sql.functions import array_distinct
            user_new_tags = user_new_tags.select(
                "user_id",
                array_distinct("new_tag_ids_raw").alias("new_tag_ids"),
                "tag_info_list"
            )
            
            # 4. è¯»å–çŽ°æœ‰ç”¨æˆ·æ ‡ç­¾ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
            existing_tags = self._read_existing_user_tags()
            
            # 5. åˆå¹¶æ–°è€æ ‡ç­¾
            merged_result = self._merge_new_and_existing_tags(user_new_tags, existing_tags)
            
            logger.info(f"âœ… æ ‡ç­¾åˆå¹¶å®Œæˆï¼Œå½±å“ {merged_result.count()} ä¸ªç”¨æˆ·")
            
            # è°ƒè¯•ï¼šæ£€æŸ¥tag_mergerè¾“å‡ºçš„æœ€ç»ˆç»“æžœæ˜¯å¦æœ‰é‡å¤
            logger.info("ðŸ” æ£€æŸ¥tag_mergerè¾“å‡ºç»“æžœæ˜¯å¦æœ‰é‡å¤...")
            merged_result.select("user_id", "tag_ids").show(5, truncate=False)
            
            return merged_result
            
        except Exception as e:
            logger.error(f"æ ‡ç­¾åˆå¹¶å¤±è´¥: {str(e)}")
            return None
    
    def _read_existing_user_tags(self) -> Optional[DataFrame]:
        """è¯»å–çŽ°æœ‰çš„ç”¨æˆ·æ ‡ç­¾æ•°æ®"""
        try:
            existing_df = self.spark.read.jdbc(
                url=self.mysql_config.jdbc_url,
                table="user_tags",
                properties=self.mysql_config.connection_properties
            )
            
            if existing_df.count() == 0:
                logger.info("å½“å‰æ²¡æœ‰å­˜å‚¨çš„ç”¨æˆ·æ ‡ç­¾")
                return None
            
            # å°†JSONå­—ç¬¦ä¸²è½¬æ¢å›žæ•°ç»„ï¼Œä»¥ä¾¿åœ¨Sparkä¸­è¿›è¡Œæ•°ç»„æ“ä½œ
            from pyspark.sql.functions import from_json
            from pyspark.sql.types import ArrayType, IntegerType
            
            processed_df = existing_df.select(
                "user_id",
                from_json(col("tag_ids"), ArrayType(IntegerType())).alias("tag_ids"),
                "tag_details"
            )
            
            logger.info(f"è¯»å–åˆ° {existing_df.count()} æ¡çŽ°æœ‰ç”¨æˆ·æ ‡ç­¾è®°å½•")
            return processed_df
            
        except Exception as e:
            logger.info(f"è¯»å–çŽ°æœ‰æ ‡ç­¾å¤±è´¥ï¼ˆå¯èƒ½æ˜¯é¦–æ¬¡è¿è¡Œï¼‰: {str(e)}")
            return None
    
    def _merge_new_and_existing_tags(self, new_tags_df: DataFrame, existing_tags_df: Optional[DataFrame]) -> DataFrame:
        """åˆå¹¶æ–°æ ‡ç­¾å’Œå·²æœ‰æ ‡ç­¾"""
        try:
            if existing_tags_df is None:
                # é¦–æ¬¡è¿è¡Œï¼Œç›´æŽ¥ä½¿ç”¨æ–°æ ‡ç­¾
                logger.info("é¦–æ¬¡è¿è¡Œï¼Œç›´æŽ¥ä½¿ç”¨æ–°è®¡ç®—çš„æ ‡ç­¾")
                return self._format_final_output(new_tags_df)
            
            # åˆå¹¶æ–°è€æ ‡ç­¾
            logger.info("åˆå¹¶æ–°æ ‡ç­¾å’Œå·²æœ‰æ ‡ç­¾...")
            
            # å·¦è¿žæŽ¥ï¼šä»¥æ–°æ ‡ç­¾ä¸ºä¸»ï¼Œå…³è”å·²æœ‰æ ‡ç­¾
            merged_df = new_tags_df.join(
                existing_tags_df, 
                "user_id", 
                "left"
            )
            
            # åˆå¹¶æ ‡ç­¾IDæ•°ç»„ï¼ˆåŽ»é‡ï¼‰
            final_df = merged_df.select(
                col("user_id"),
                # åˆå¹¶æ ‡ç­¾IDï¼šæ–°æ ‡ç­¾ + çŽ°æœ‰æ ‡ç­¾ï¼Œç„¶åŽåŽ»é‡
                # åˆå¹¶æ ‡ç­¾IDæ•°ç»„å¹¶åŽ»é‡
                self._merge_tag_arrays(col("tag_ids"), col("new_tag_ids")).alias("merged_tag_ids"),
                col("tag_info_list")
            )
            
            return self._format_final_output_with_merged_ids(final_df)
            
        except Exception as e:
            logger.error(f"åˆå¹¶æ–°è€æ ‡ç­¾å¤±è´¥: {str(e)}")
            raise
    
    def _format_final_output(self, user_tags_df: DataFrame) -> DataFrame:
        """æ ¼å¼åŒ–æœ€ç»ˆè¾“å‡ºï¼ˆé¦–æ¬¡è¿è¡Œï¼‰"""
        from pyspark.sql.functions import udf
        from pyspark.sql.types import StringType
        import json
        
        # ä½¿ç”¨UDFç®€åŒ–å¤„ç†
        @udf(returnType=StringType())
        def build_tag_details(tag_info_list):
            if not tag_info_list:
                return "{}"
            
            tag_details = {}
            for tag_info in tag_info_list:
                tag_id = str(tag_info['tag_id'])
                tag_details[tag_id] = {
                    'tag_name': tag_info['tag_name'],
                    'tag_category': tag_info['tag_category']
                }
            return json.dumps(tag_details, ensure_ascii=False)
        
        formatted_df = user_tags_df.select(
            col("user_id"),
            col("new_tag_ids").alias("tag_ids"),
            build_tag_details(col("tag_info_list")).alias("tag_details"),
            lit(date.today()).alias("computed_date")
        )
        
        return formatted_df
    
    def _format_final_output_with_merged_ids(self, merged_df: DataFrame) -> DataFrame:
        """æ ¼å¼åŒ–æœ€ç»ˆè¾“å‡ºï¼ˆåŒ…å«åˆå¹¶çš„æ ‡ç­¾IDï¼‰"""
        from pyspark.sql.functions import udf
        from pyspark.sql.types import StringType
        import json
        
        # ä½¿ç”¨UDFç®€åŒ–å¤„ç†
        @udf(returnType=StringType())
        def build_tag_details(tag_info_list):
            if not tag_info_list:
                return "{}"
            
            tag_details = {}
            for tag_info in tag_info_list:
                tag_id = str(tag_info['tag_id'])
                tag_details[tag_id] = {
                    'tag_name': tag_info['tag_name'],
                    'tag_category': tag_info['tag_category']
                }
            return json.dumps(tag_details, ensure_ascii=False)
        
        formatted_df = merged_df.select(
            col("user_id"),
            col("merged_tag_ids").alias("tag_ids"),
            build_tag_details(col("tag_info_list")).alias("tag_details"),
            lit(date.today()).alias("computed_date")
        )
        
        return formatted_df
    
    def _merge_tag_arrays(self, existing_tags_col, new_tags_col):
        """åˆå¹¶ä¸¤ä¸ªæ ‡ç­¾æ•°ç»„å¹¶åŽ»é‡"""
        from pyspark.sql.functions import udf, array, flatten, array_distinct
        from pyspark.sql.types import ArrayType, IntegerType
        
        @udf(returnType=ArrayType(IntegerType()))
        def merge_arrays(existing_tags, new_tags):
            if existing_tags is None:
                existing_tags = []
            if new_tags is None:
                new_tags = []
            
            # åˆå¹¶å¹¶åŽ»é‡
            merged = list(set(existing_tags + new_tags))
            return sorted(merged)
        
        return merge_arrays(existing_tags_col, new_tags_col)
    
    def _enrich_with_tag_info(self, tags_df: DataFrame) -> DataFrame:
        """ç”¨æ ‡ç­¾å®šä¹‰ä¿¡æ¯ä¸°å¯Œæ ‡ç­¾æ•°æ®"""
        try:
            # è¯»å–æ ‡ç­¾å®šä¹‰
            tag_definitions = self.spark.read.jdbc(
                url=self.mysql_config.jdbc_url,
                table="tag_definition",
                properties=self.mysql_config.connection_properties
            ).select("tag_id", "tag_name", "tag_category")
            
            # å…³è”æ ‡ç­¾å®šä¹‰ä¿¡æ¯
            enriched_df = tags_df.join(
                tag_definitions,
                "tag_id",
                "left"
            ).select(
                "user_id",
                "tag_id", 
                col("tag_name").alias("tag_name"),
                col("tag_category").alias("tag_category"),
                "tag_detail"
            )
            
            return enriched_df
            
        except Exception as e:
            logger.error(f"ä¸°å¯Œæ ‡ç­¾ä¿¡æ¯å¤±è´¥: {str(e)}")
            # é™çº§å¤„ç†ï¼šä½¿ç”¨é»˜è®¤å€¼
            return tags_df.select(
                "user_id",
                "tag_id",
                lit("unknown").alias("tag_name"),
                lit("unknown").alias("tag_category"),
                "tag_detail"
            )
    
    def validate_merge_result(self, merged_df: DataFrame) -> bool:
        """éªŒè¯åˆå¹¶ç»“æžœçš„æœ‰æ•ˆæ€§"""
        try:
            if merged_df.count() == 0:
                logger.error("åˆå¹¶ç»“æžœä¸ºç©º")
                return False
            
            # æ£€æŸ¥å¿…è¦å­—æ®µ
            required_fields = ["user_id", "tag_ids", "computed_date"]
            missing_fields = [field for field in required_fields if field not in merged_df.columns]
            
            if missing_fields:
                logger.error(f"åˆå¹¶ç»“æžœç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}")
                return False
            
            # æ£€æŸ¥ç”¨æˆ·IDä¸ä¸ºç©º
            null_user_count = merged_df.filter(col("user_id").isNull()).count()
            if null_user_count > 0:
                logger.error(f"å­˜åœ¨ {null_user_count} ä¸ªç©ºçš„ç”¨æˆ·ID")
                return False
            
            # æ£€æŸ¥æ ‡ç­¾æ•°ç»„ä¸ä¸ºç©º
            empty_tags_count = merged_df.filter(
                col("tag_ids").isNull() | (expr("size(tag_ids)") == 0)
            ).count()
            
            if empty_tags_count > 0:
                logger.warning(f"å­˜åœ¨ {empty_tags_count} ä¸ªç”¨æˆ·æ²¡æœ‰æ ‡ç­¾")
            
            logger.info("âœ… åˆå¹¶ç»“æžœéªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"åˆå¹¶ç»“æžœéªŒè¯å¤±è´¥: {str(e)}")
            return False
    
    def get_merge_statistics(self, merged_df: DataFrame) -> dict:
        """èŽ·å–åˆå¹¶ç»Ÿè®¡ä¿¡æ¯ï¼ˆé€‚é…æ–°çš„æ•°æ®æ¨¡åž‹ï¼‰"""
        try:
            total_users = merged_df.count()
            
            if total_users == 0:
                return {
                    "total_users": 0,
                    "total_tag_assignments": 0,
                    "avg_tags_per_user": 0,
                    "max_tags_per_user": 0,
                    "min_tags_per_user": 0
                }
            
            # ç»Ÿè®¡æ¯ä¸ªç”¨æˆ·çš„æ ‡ç­¾æ•°
            stats_df = merged_df.select(
                col("user_id"),
                size(col("tag_ids")).alias("tag_count")
            )
            
            tag_counts = stats_df.select("tag_count").collect()
            tag_count_values = [row['tag_count'] for row in tag_counts]
            
            total_assignments = sum(tag_count_values)
            
            stats = {
                "total_users": total_users,
                "total_tag_assignments": total_assignments,
                "avg_tags_per_user": round(total_assignments / total_users, 2) if total_users > 0 else 0,
                "max_tags_per_user": max(tag_count_values) if tag_count_values else 0,
                "min_tags_per_user": min(tag_count_values) if tag_count_values else 0
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"èŽ·å–åˆå¹¶ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {}
    
    def optimize_merge_performance(self, df: DataFrame) -> DataFrame:
        """ä¼˜åŒ–åˆå¹¶æ€§èƒ½"""
        # ç¼“å­˜ä¸­é—´ç»“æžœ
        df = df.cache()
        
        # é‡åˆ†åŒºä¼˜åŒ–
        if df.rdd.getNumPartitions() > 50:
            df = df.coalesce(50)
        
        return df