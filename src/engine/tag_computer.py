import json
import logging
from datetime import datetime
from typing import Dict, Any, List, Optional
from pyspark.sql import DataFrame, SparkSession
import pyspark.sql.functions as F
from pyspark.sql.types import StringType

from .rule_parser import RuleConditionParser

logger = logging.getLogger(__name__)


class TagComputeEngine:
    """æ ‡ç­¾è®¡ç®—å¼•æ“ - æ ¸å¿ƒæ ‡ç­¾è®¡ç®—é€»è¾‘"""
    
    def __init__(self, spark: SparkSession, max_workers: int = 4):
        self.spark = spark
        self.max_workers = max_workers
        self.rule_parser = RuleConditionParser()
    
    def compute_single_tag(self, data_df: DataFrame, rule: Dict[str, Any]) -> Optional[DataFrame]:
        """
        è®¡ç®—å•ä¸ªæ ‡ç­¾
        
        Args:
            data_df: ä¸šåŠ¡æ•°æ®DataFrame
            rule: æ ‡ç­¾è§„åˆ™å­—å…¸
            
        Returns:
            åŒ…å«user_id, tag_id, tag_detailçš„DataFrame
        """
        try:
            tag_id = rule['tag_id']
            tag_name = rule['tag_name']
            rule_conditions = rule['rule_conditions']
            
            logger.info(f"å¼€å§‹è®¡ç®—æ ‡ç­¾: {tag_name} (ID: {tag_id})")
            
            # è§£æè§„åˆ™æ¡ä»¶
            condition_sql = self.rule_parser.parse_rule_conditions(rule_conditions)
            logger.debug(f"ç”Ÿæˆçš„SQLæ¡ä»¶: {condition_sql}")
            
            # è·å–å‘½ä¸­æ¡ä»¶éœ€è¦çš„å­—æ®µ
            hit_fields = self.rule_parser.get_condition_fields(rule_conditions)
            
            # æ‰§è¡Œæ ‡ç­¾è®¡ç®— - ç­›é€‰ç¬¦åˆæ¡ä»¶çš„ç”¨æˆ·
            tagged_users = data_df.filter(condition_sql)
            
            if tagged_users.count() == 0:
                logger.info(f"æ ‡ç­¾ {tag_name} æ²¡æœ‰å‘½ä¸­ä»»ä½•ç”¨æˆ·")
                return None
            
            # é€‰æ‹©éœ€è¦çš„å­—æ®µ
            select_fields = ['user_id'] + [f for f in hit_fields if f in data_df.columns]
            result_df = tagged_users.select(*select_fields)
            
            # æ·»åŠ æ ‡ç­¾ID
            result_df = result_df.withColumn('tag_id', F.lit(tag_id))
            
            # ç”Ÿæˆæ ‡ç­¾è¯¦ç»†ä¿¡æ¯
            result_df = self._add_tag_details(result_df, rule, hit_fields)
            
            hit_count = result_df.count()
            logger.info(f"âœ… æ ‡ç­¾ {tag_name} è®¡ç®—å®Œæˆï¼Œå‘½ä¸­ç”¨æˆ·æ•°: {hit_count}")
            
            return result_df.select('user_id', 'tag_id', 'tag_detail')
            
        except Exception as e:
            logger.error(f"âŒ è®¡ç®—æ ‡ç­¾å¤±è´¥: {rule.get('tag_name', 'Unknown')}, é”™è¯¯: {str(e)}")
            return None
    
    def compute_batch_tags(self, data_df: DataFrame, rules: List[Dict[str, Any]]) -> List[DataFrame]:
        """
        æ‰¹é‡è®¡ç®—å¤šä¸ªæ ‡ç­¾
        
        Args:
            data_df: ä¸šåŠ¡æ•°æ®DataFrame
            rules: æ ‡ç­¾è§„åˆ™åˆ—è¡¨
            
        Returns:
            æ ‡ç­¾ç»“æœDataFrameåˆ—è¡¨
        """
        results = []
        
        for rule in rules:
            try:
                result_df = self.compute_single_tag(data_df, rule)
                if result_df is not None:
                    results.append(result_df)
                    
            except Exception as e:
                logger.error(f"æ‰¹é‡è®¡ç®—ä¸­å•ä¸ªæ ‡ç­¾å¤±è´¥: {rule.get('tag_name', 'Unknown')}, é”™è¯¯: {str(e)}")
                continue
        
        logger.info(f"æ‰¹é‡è®¡ç®—å®Œæˆï¼ŒæˆåŠŸè®¡ç®— {len(results)}/{len(rules)} ä¸ªæ ‡ç­¾")
        return results
    
    def compute_tags_parallel(self, data_df: DataFrame, rules: List[Dict[str, Any]]) -> List[DataFrame]:
        """
        å¹¶è¡Œè®¡ç®—å¤šä¸ªæ ‡ç­¾ - åˆ©ç”¨SparkåŸç”Ÿå¹¶è¡Œèƒ½åŠ›
        
        Args:
            data_df: ä¸šåŠ¡æ•°æ®DataFrame
            rules: æ ‡ç­¾è§„åˆ™åˆ—è¡¨
            
        Returns:
            æ ‡ç­¾ç»“æœDataFrameåˆ—è¡¨
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import threading
        
        logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œè®¡ç®— {len(rules)} ä¸ªæ ‡ç­¾")
        
        # ç¼“å­˜æ•°æ®æå‡å¹¶è¡Œæ€§èƒ½
        cached_data = data_df.cache()
        
        results = []
        failed_tags = []
        lock = threading.Lock()
        
        def compute_single_tag_threadsafe(rule):
            """çº¿ç¨‹å®‰å…¨çš„å•æ ‡ç­¾è®¡ç®—"""
            try:
                # Sparkæ“ä½œæœ¬èº«æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œä½†æˆ‘ä»¬åŠ é”ç¡®ä¿ç¨³å®šæ€§
                with lock:
                    result_df = self.compute_single_tag(cached_data, rule)
                    return rule, result_df
            except Exception as e:
                logger.error(f"å¹¶è¡Œè®¡ç®—æ ‡ç­¾å¤±è´¥: {rule.get('tag_name', 'Unknown')}, é”™è¯¯: {str(e)}")
                return rule, None
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        max_workers = min(self.max_workers, len(rules))  # ä½¿ç”¨é…ç½®çš„æœ€å¤§çº¿ç¨‹æ•°
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰è®¡ç®—ä»»åŠ¡
            future_to_rule = {
                executor.submit(compute_single_tag_threadsafe, rule): rule 
                for rule in rules
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_rule):
                rule = future_to_rule[future]
                try:
                    rule_returned, result_df = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                    if result_df is not None:
                        results.append(result_df)
                        logger.info(f"âœ… æ ‡ç­¾ {rule['tag_name']} å¹¶è¡Œè®¡ç®—å®Œæˆ")
                    else:
                        failed_tags.append(rule['tag_name'])
                        
                except Exception as e:
                    logger.error(f"âŒ æ ‡ç­¾ {rule['tag_name']} è®¡ç®—è¶…æ—¶æˆ–å¼‚å¸¸: {str(e)}")
                    failed_tags.append(rule['tag_name'])
        
        # æ¸…ç†ç¼“å­˜
        cached_data.unpersist()
        
        logger.info(f"ğŸ‰ å¹¶è¡Œè®¡ç®—å®Œæˆ - æˆåŠŸ: {len(results)}, å¤±è´¥: {len(failed_tags)}")
        if failed_tags:
            logger.warning(f"å¤±è´¥çš„æ ‡ç­¾: {failed_tags}")
        
        return results
    
    def _add_tag_details(self, result_df: DataFrame, rule: Dict[str, Any], hit_fields: List[str]) -> DataFrame:
        """ä¸ºæ ‡ç­¾ç»“æœæ·»åŠ è¯¦ç»†ä¿¡æ¯"""
        
        # å¤åˆ¶éœ€è¦çš„æ•°æ®é¿å…åºåˆ—åŒ–æ•´ä¸ªå¯¹è±¡
        tag_name = rule['tag_name']
        rule_conditions = rule['rule_conditions']
        
        @F.udf(returnType=StringType())
        def generate_tag_detail(*hit_values):
            """ç”Ÿæˆæ ‡ç­¾è¯¦ç»†ä¿¡æ¯çš„UDF"""
            try:
                # ç®€åŒ–çš„å‘½ä¸­åŸå› ç”Ÿæˆ
                reason = f"æ»¡è¶³æ ‡ç­¾è§„åˆ™: {tag_name}"
                
                # æ„å»ºæ ‡ç­¾è¯¦ç»†ä¿¡æ¯
                detail = {
                    'value': str(hit_values[0]) if hit_values and hit_values[0] is not None else "",
                    'reason': reason,
                    'source': 'AUTO',
                    'hit_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'rule_version': '1.0',
                    'tag_name': tag_name
                }
                
                return json.dumps(detail, ensure_ascii=False)
                
            except Exception as e:
                return json.dumps({'error': str(e)})
        
        # è·å–ç”¨äºç”Ÿæˆè¯¦æƒ…çš„å­—æ®µåˆ—
        detail_columns = []
        for field in hit_fields:
            if field in result_df.columns:
                detail_columns.append(F.col(field))
        
        # å¦‚æœæ²¡æœ‰å¯ç”¨å­—æ®µï¼Œä½¿ç”¨ç©ºå€¼
        if not detail_columns:
            detail_columns = [F.lit(None)]
        
        # æ·»åŠ æ ‡ç­¾è¯¦æƒ…åˆ—
        result_df = result_df.withColumn('tag_detail', generate_tag_detail(*detail_columns))
        
        return result_df
    
    def _generate_hit_reason(self, rule_conditions: Dict[str, Any], hit_fields: List[str], hit_values: tuple) -> str:
        """ç”Ÿæˆå‘½ä¸­åŸå› è¯´æ˜"""
        try:
            conditions = rule_conditions.get('conditions', [])
            logic = rule_conditions.get('logic', 'AND')
            
            reasons = []
            
            for i, condition in enumerate(conditions):
                field = condition.get('field', '')
                operator = condition.get('operator', '')
                threshold = condition.get('value', '')
                
                # è·å–å¯¹åº”çš„å‘½ä¸­å€¼
                hit_value = ""
                if i < len(hit_values) and hit_values[i] is not None:
                    hit_value = str(hit_values[i])
                elif field in hit_fields:
                    field_index = hit_fields.index(field)
                    if field_index < len(hit_values) and hit_values[field_index] is not None:
                        hit_value = str(hit_values[field_index])
                
                # ç”Ÿæˆå•ä¸ªæ¡ä»¶çš„åŸå› 
                reason = self._format_single_reason(field, operator, threshold, hit_value)
                if reason:
                    reasons.append(reason)
            
            # ç»„åˆåŸå› 
            if logic.upper() == 'OR':
                return " æˆ– ".join(reasons)
            elif logic.upper() == 'NOT':
                return f"ä¸æ»¡è¶³: {' ä¸” '.join(reasons)}"
            else:
                return " ä¸” ".join(reasons)
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆå‘½ä¸­åŸå› å¤±è´¥: {str(e)}")
            return "ç³»ç»Ÿè‡ªåŠ¨è®¡ç®—"
    
    def _format_single_reason(self, field: str, operator: str, threshold: Any, hit_value: str) -> str:
        """æ ¼å¼åŒ–å•ä¸ªæ¡ä»¶çš„åŸå› """
        try:
            # å­—æ®µåæ˜ å°„ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•ï¼‰
            field_map = {
                'total_asset_value': 'æ€»èµ„äº§',
                'last_30d_trading_volume': 'è¿‘30æ—¥äº¤æ˜“é¢',
                'login_count_7d': 'è¿‘7æ—¥ç™»å½•æ¬¡æ•°',
                'register_days': 'æ³¨å†Œå¤©æ•°',
                'total_deposit_amount': 'ç´¯è®¡å……å€¼é‡‘é¢',
                'kyc_status': 'KYCçŠ¶æ€',
                'user_level': 'ç”¨æˆ·ç­‰çº§'
            }
            
            field_name = field_map.get(field, field)
            
            if operator == '>=':
                return f"{field_name}{hit_value} â‰¥ {threshold}"
            elif operator == '>':
                return f"{field_name}{hit_value} > {threshold}"
            elif operator == '<=':
                return f"{field_name}{hit_value} â‰¤ {threshold}"
            elif operator == '<':
                return f"{field_name}{hit_value} < {threshold}"
            elif operator == '=':
                return f"{field_name}={hit_value}"
            elif operator == '!=':
                return f"{field_name}â‰ {threshold}"
            elif operator == 'in':
                return f"{field_name}å±äº{threshold}"
            elif operator == 'not_in':
                return f"{field_name}ä¸å±äº{threshold}"
            elif operator == 'in_range':
                if isinstance(threshold, list) and len(threshold) == 2:
                    return f"{field_name}{hit_value}åœ¨{threshold[0]}-{threshold[1]}èŒƒå›´å†…"
            elif operator == 'recent_days':
                return f"{field_name}åœ¨æœ€è¿‘{threshold}å¤©å†…"
            elif operator == 'contains':
                return f"{field_name}åŒ…å«{threshold}"
            elif operator == 'is_not_null':
                return f"{field_name}ä¸ä¸ºç©º"
            elif operator == 'is_null':
                return f"{field_name}ä¸ºç©º"
            else:
                return f"{field_name} {operator} {threshold}"
                
        except Exception as e:
            logger.error(f"æ ¼å¼åŒ–åŸå› å¤±è´¥: {str(e)}")
            return f"{field} {operator} {threshold}"
    
    def validate_data_for_rule(self, data_df: DataFrame, rule: Dict[str, Any]) -> bool:
        """éªŒè¯æ•°æ®æ˜¯å¦æ»¡è¶³è§„åˆ™è®¡ç®—è¦æ±‚"""
        try:
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if 'user_id' not in data_df.columns:
                logger.error("æ•°æ®ç¼ºå°‘user_idå­—æ®µ")
                return False
            
            # æ£€æŸ¥è§„åˆ™éœ€è¦çš„å­—æ®µ
            required_fields = self.rule_parser.get_condition_fields(rule['rule_conditions'])
            missing_fields = [f for f in required_fields if f not in data_df.columns]
            
            if missing_fields:
                logger.warning(f"æ•°æ®ç¼ºå°‘æ ‡ç­¾è®¡ç®—æ‰€éœ€å­—æ®µ: {missing_fields}")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"æ•°æ®éªŒè¯å¤±è´¥: {str(e)}")
            return False