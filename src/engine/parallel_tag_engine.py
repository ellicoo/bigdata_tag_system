import logging
from typing import Dict, Any, List, Optional
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import col, collect_list, array_distinct, struct, lit
import pyspark.sql.functions as F
from pyspark.sql.types import StringType
from datetime import date, datetime
import json

from .rule_parser import RuleConditionParser

logger = logging.getLogger(__name__)


class ParallelTagEngine:
    """å¹¶è¡Œæ ‡ç­¾è®¡ç®—å¼•æ“ - æ”¯æŒå¤šæ ‡ç­¾å¹¶è¡Œè®¡ç®—å’Œå†…å­˜åˆå¹¶"""
    
    def __init__(self, spark: SparkSession, max_workers: int = 4, mysql_config=None):
        self.spark = spark
        self.max_workers = max_workers
        self.mysql_config = mysql_config
        self.rule_parser = RuleConditionParser()
    
    def compute_tags_with_memory_merge(self, data_df: DataFrame, rules: List[Dict[str, Any]]) -> Optional[DataFrame]:
        """
        å¤šæ ‡ç­¾å¹¶è¡Œè®¡ç®— + å†…å­˜åˆå¹¶
        
        Args:
            data_df: ç”¨æˆ·æ•°æ®
            rules: æ ‡ç­¾è§„åˆ™åˆ—è¡¨
            
        Returns:
            å†…å­˜åˆå¹¶åçš„ç”¨æˆ·æ ‡ç­¾DataFrame (user_id, tag_ids, tag_details, computed_date)
        """
        try:
            logger.info(f"ğŸš€ å¼€å§‹å¤šæ ‡ç­¾å¹¶è¡Œè®¡ç®—ï¼Œå…± {len(rules)} ä¸ªæ ‡ç­¾")
            
            # 1. å¹¶è¡Œè®¡ç®—æ‰€æœ‰æ ‡ç­¾
            tag_results = self._compute_tags_parallel(data_df, rules)
            
            if not tag_results:
                logger.warning("æ²¡æœ‰ä»»ä½•æ ‡ç­¾è®¡ç®—å‡ºç»“æœ")
                return None
            
            # 2. å†…å­˜åˆå¹¶ï¼šåŒä¸€ç”¨æˆ·çš„å¤šä¸ªæ ‡ç­¾åˆå¹¶ä¸ºä¸€æ¡è®°å½•
            merged_result = self._merge_user_tags_in_memory(tag_results)
            
            logger.info(f"âœ… å¤šæ ‡ç­¾å¹¶è¡Œè®¡ç®—å’Œå†…å­˜åˆå¹¶å®Œæˆ")
            return merged_result
            
        except Exception as e:
            logger.error(f"å¤šæ ‡ç­¾å¹¶è¡Œè®¡ç®—å¤±è´¥: {str(e)}")
            return None
    
    def _merge_user_tags_in_memory(self, tag_results: List[DataFrame]) -> Optional[DataFrame]:
        """å†…å­˜åˆå¹¶ï¼šå°†åŒä¸€ç”¨æˆ·çš„å¤šä¸ªæ ‡ç­¾åˆå¹¶ä¸ºä¸€æ¡è®°å½•"""
        try:
            if not tag_results:
                return None
                
            logger.info(f"å¼€å§‹å†…å­˜åˆå¹¶ {len(tag_results)} ä¸ªæ ‡ç­¾ç»“æœ...")
            
            from functools import reduce
            
            # 1. åˆå¹¶æ‰€æœ‰æ ‡ç­¾ç»“æœ
            all_tags = reduce(lambda df1, df2: df1.union(df2), tag_results)
            
            if all_tags.count() == 0:
                logger.warning("åˆå¹¶åæ²¡æœ‰æ ‡ç­¾æ•°æ®")
                return None
            
            # 2. å»é‡ï¼šç§»é™¤åŒä¸€ç”¨æˆ·çš„é‡å¤æ ‡ç­¾
            deduplicated_tags = all_tags.dropDuplicates(["user_id", "tag_id"])
            
            # 3. ä¸°å¯Œæ ‡ç­¾ä¿¡æ¯
            enriched_tags = self._enrich_with_tag_info(deduplicated_tags)
            
            # 4. æŒ‰ç”¨æˆ·èšåˆï¼šå°†ç”¨æˆ·çš„å¤šä¸ªæ ‡ç­¾åˆå¹¶ä¸ºæ•°ç»„
            user_aggregated = enriched_tags.groupBy("user_id").agg(
                collect_list("tag_id").alias("tag_ids_raw"),
                collect_list(struct("tag_id", "tag_name", "tag_category")).alias("tag_info_list")
            )
            
            # 5. ç¡®ä¿æ ‡ç­¾æ•°ç»„å»é‡
            user_aggregated = user_aggregated.select(
                "user_id",
                array_distinct("tag_ids_raw").alias("tag_ids"),
                "tag_info_list"
            )
            
            # 6. æ ¼å¼åŒ–è¾“å‡º
            final_result = self._format_memory_merge_output(user_aggregated)
            
            logger.info(f"âœ… å†…å­˜åˆå¹¶å®Œæˆï¼Œå½±å“ {final_result.count()} ä¸ªç”¨æˆ·")
            return final_result
            
        except Exception as e:
            logger.error(f"å†…å­˜åˆå¹¶å¤±è´¥: {str(e)}")
            return None
    
    def _enrich_with_tag_info(self, tags_df: DataFrame) -> DataFrame:
        """ä¸°å¯Œæ ‡ç­¾ä¿¡æ¯"""
        try:
            # ä½¿ç”¨ä¼ é€’çš„é…ç½®æˆ–ä»é…ç½®ç®¡ç†å™¨è·å–
            if self.mysql_config:
                mysql_config = self.mysql_config
            else:
                from src.config.manager import ConfigManager
                config = ConfigManager.load_config('local')
                mysql_config = config.mysql
            
            # è¯»å–æ ‡ç­¾å®šä¹‰
            tag_definitions = self.spark.read.jdbc(
                url=mysql_config.jdbc_url,
                table="tag_definition",
                properties=mysql_config.connection_properties
            ).select("tag_id", "tag_name", "tag_category")
            
            # å…³è”æ ‡ç­¾å®šä¹‰ä¿¡æ¯
            enriched_df = tags_df.join(
                tag_definitions,
                "tag_id",
                "left"
            ).select(
                "user_id",
                "tag_id", 
                col("tag_name").alias("tag_name"),
                col("tag_category").alias("tag_category"),
                "tag_detail"
            )
            
            return enriched_df
            
        except Exception as e:
            logger.error(f"ä¸°å¯Œæ ‡ç­¾ä¿¡æ¯å¤±è´¥: {str(e)}")
            # é™çº§å¤„ç†
            return tags_df.select(
                "user_id",
                "tag_id",
                lit("unknown").alias("tag_name"),
                lit("unknown").alias("tag_category"),
                "tag_detail"
            )
    
    def _format_memory_merge_output(self, user_tags_df: DataFrame) -> DataFrame:
        """æ ¼å¼åŒ–å†…å­˜åˆå¹¶è¾“å‡º"""
        from pyspark.sql.functions import udf
        from pyspark.sql.types import StringType
        
        @udf(returnType=StringType())
        def build_tag_details(tag_info_list):
            if not tag_info_list:
                return "{}"
            
            tag_details = {}
            for tag_info in tag_info_list:
                tag_id = str(tag_info['tag_id'])
                tag_details[tag_id] = {
                    'tag_name': tag_info['tag_name'],
                    'tag_category': tag_info['tag_category']
                }
            return json.dumps(tag_details, ensure_ascii=False)
        
        formatted_df = user_tags_df.select(
            col("user_id"),
            col("tag_ids"),
            build_tag_details(col("tag_info_list")).alias("tag_details"),
            lit(date.today()).alias("computed_date")
        )
        
        return formatted_df
    
    def _compute_tags_parallel(self, data_df: DataFrame, rules: List[Dict[str, Any]]) -> List[DataFrame]:
        """
        å¹¶è¡Œè®¡ç®—å¤šä¸ªæ ‡ç­¾ - åˆ©ç”¨SparkåŸç”Ÿåˆ†å¸ƒå¼å¹¶è¡Œèƒ½åŠ›
        
        Args:
            data_df: ä¸šåŠ¡æ•°æ®DataFrame
            rules: æ ‡ç­¾è§„åˆ™åˆ—è¡¨
            
        Returns:
            æ ‡ç­¾ç»“æœDataFrameåˆ—è¡¨
        """
        logger.info(f"ğŸš€ å¼€å§‹Sparkåˆ†å¸ƒå¼å¹¶è¡Œè®¡ç®— {len(rules)} ä¸ªæ ‡ç­¾")
        
        # æ™ºèƒ½ç¼“å­˜ç­–ç•¥ï¼šåªæœ‰è§„åˆ™æ•°é‡è¾ƒå¤šæ—¶æ‰ç¼“å­˜
        should_cache = len(rules) > 3
        if should_cache:
            logger.info("ğŸ“¦ ç¼“å­˜æ•°æ®ä»¥æå‡å¤šæ ‡ç­¾è®¡ç®—æ€§èƒ½")
            cached_data = data_df.cache()
            # è§¦å‘ç¼“å­˜ - ä½¿ç”¨è½»é‡çº§æ“ä½œ
            _ = cached_data.count()
        else:
            cached_data = data_df
        
        results = []
        failed_tags = []
        
        # ç›´æ¥ä½¿ç”¨Sparkçš„åˆ†å¸ƒå¼è®¡ç®—ï¼Œæ— éœ€Pythonçº¿ç¨‹æ± 
        for rule in rules:
            try:
                logger.info(f"ğŸ”„ è®¡ç®—æ ‡ç­¾: {rule['tag_name']}")
                result_df = self._compute_single_tag(cached_data, rule)
                
                if result_df is not None:
                    results.append(result_df)
                    logger.info(f"âœ… æ ‡ç­¾ {rule['tag_name']} è®¡ç®—å®Œæˆ")
                else:
                    failed_tags.append(rule['tag_name'])
                    logger.warning(f"âš ï¸ æ ‡ç­¾ {rule['tag_name']} æ— å‘½ä¸­ç”¨æˆ·")
                    
            except Exception as e:
                logger.error(f"âŒ æ ‡ç­¾ {rule['tag_name']} è®¡ç®—å¤±è´¥: {str(e)}")
                failed_tags.append(rule['tag_name'])
        
        # æ¸…ç†ç¼“å­˜
        if should_cache:
            logger.info("ğŸ§¹ æ¸…ç†æ•°æ®ç¼“å­˜")
            cached_data.unpersist()
        
        logger.info(f"ğŸ‰ Sparkåˆ†å¸ƒå¼è®¡ç®—å®Œæˆ - æˆåŠŸ: {len(results)}, å¤±è´¥: {len(failed_tags)}")
        if failed_tags:
            logger.warning(f"å¤±è´¥çš„æ ‡ç­¾: {failed_tags}")
        
        return results
    
    def _compute_single_tag(self, data_df: DataFrame, rule: Dict[str, Any]) -> Optional[DataFrame]:
        """
        è®¡ç®—å•ä¸ªæ ‡ç­¾
        
        Args:
            data_df: ä¸šåŠ¡æ•°æ®DataFrame
            rule: æ ‡ç­¾è§„åˆ™å­—å…¸
            
        Returns:
            åŒ…å«user_id, tag_id, tag_detailçš„DataFrame
        """
        try:
            tag_id = rule['tag_id']
            tag_name = rule['tag_name']
            rule_conditions = rule['rule_conditions']
            
            logger.debug(f"å¼€å§‹è®¡ç®—æ ‡ç­¾: {tag_name} (ID: {tag_id})")
            
            # è§£æè§„åˆ™æ¡ä»¶
            condition_sql = self.rule_parser.parse_rule_conditions(rule_conditions)
            logger.debug(f"ç”Ÿæˆçš„SQLæ¡ä»¶: {condition_sql}")
            
            # è·å–å‘½ä¸­æ¡ä»¶éœ€è¦çš„å­—æ®µ
            hit_fields = self.rule_parser.get_condition_fields(rule_conditions)
            
            # æ‰§è¡Œæ ‡ç­¾è®¡ç®— - ç­›é€‰ç¬¦åˆæ¡ä»¶çš„ç”¨æˆ·
            tagged_users = data_df.filter(condition_sql)
            
            if tagged_users.count() == 0:
                logger.debug(f"æ ‡ç­¾ {tag_name} æ²¡æœ‰å‘½ä¸­ä»»ä½•ç”¨æˆ·")
                return None
            
            # é€‰æ‹©éœ€è¦çš„å­—æ®µ
            select_fields = ['user_id'] + [f for f in hit_fields if f in data_df.columns]
            result_df = tagged_users.select(*select_fields)
            
            # æ·»åŠ æ ‡ç­¾ID
            result_df = result_df.withColumn('tag_id', F.lit(tag_id))
            
            # ç”Ÿæˆæ ‡ç­¾è¯¦ç»†ä¿¡æ¯
            result_df = self._add_tag_details(result_df, rule, hit_fields)
            
            hit_count = result_df.count()
            logger.debug(f"âœ… æ ‡ç­¾ {tag_name} è®¡ç®—å®Œæˆï¼Œå‘½ä¸­ç”¨æˆ·æ•°: {hit_count}")
            
            return result_df.select('user_id', 'tag_id', 'tag_detail')
            
        except Exception as e:
            logger.error(f"âŒ è®¡ç®—æ ‡ç­¾å¤±è´¥: {rule.get('tag_name', 'Unknown')}, é”™è¯¯: {str(e)}")
            return None
    
    def _add_tag_details(self, result_df: DataFrame, rule: Dict[str, Any], hit_fields: List[str]) -> DataFrame:
        """ä¸ºæ ‡ç­¾ç»“æœæ·»åŠ è¯¦ç»†ä¿¡æ¯"""
        
        # å¤åˆ¶éœ€è¦çš„æ•°æ®é¿å…åºåˆ—åŒ–æ•´ä¸ªå¯¹è±¡
        tag_name = rule['tag_name']
        
        @F.udf(returnType=StringType())
        def generate_tag_detail(*hit_values):
            """ç”Ÿæˆæ ‡ç­¾è¯¦ç»†ä¿¡æ¯çš„UDF"""
            try:
                # ç®€åŒ–çš„å‘½ä¸­åŸå› ç”Ÿæˆ
                reason = f"æ»¡è¶³æ ‡ç­¾è§„åˆ™: {tag_name}"
                
                # æ„å»ºæ ‡ç­¾è¯¦ç»†ä¿¡æ¯
                detail = {
                    'value': str(hit_values[0]) if hit_values and hit_values[0] is not None else "",
                    'reason': reason,
                    'source': 'AUTO',
                    'hit_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'rule_version': '1.0',
                    'tag_name': tag_name
                }
                
                return json.dumps(detail, ensure_ascii=False)
                
            except Exception as e:
                return json.dumps({'error': str(e)})
        
        # è·å–ç”¨äºç”Ÿæˆè¯¦æƒ…çš„å­—æ®µåˆ—
        detail_columns = []
        for field in hit_fields:
            if field in result_df.columns:
                detail_columns.append(F.col(field))
        
        # å¦‚æœæ²¡æœ‰å¯ç”¨å­—æ®µï¼Œä½¿ç”¨ç©ºå€¼
        if not detail_columns:
            detail_columns = [F.lit(None)]
        
        # æ·»åŠ æ ‡ç­¾è¯¦æƒ…åˆ—
        result_df = result_df.withColumn('tag_detail', generate_tag_detail(*detail_columns))
        
        return result_df