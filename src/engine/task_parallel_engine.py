"""
åŸºäºä»»åŠ¡æŠ½è±¡çš„å¹¶è¡Œæ ‡ç­¾è®¡ç®—å¼•æ“
"""

import logging
from typing import Dict, List, Any, Optional, Set
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import col, lit
import pyspark.sql.functions as F
from pyspark.sql.types import StringType
import json

from src.tasks.base_tag_task import BaseTagTask
from src.tasks.task_factory import TagTaskFactory
from src.tasks.task_registry import register_all_tasks
from src.engine.rule_parser import RuleConditionParser

logger = logging.getLogger(__name__)


class TaskBasedParallelEngine:
    """åŸºäºä»»åŠ¡æŠ½è±¡çš„å¹¶è¡Œæ ‡ç­¾è®¡ç®—å¼•æ“"""
    
    def __init__(self, spark: SparkSession, config, max_workers: int = 4, rule_reader=None):
        self.spark = spark
        self.config = config
        self.max_workers = max_workers
        self.rule_parser = RuleConditionParser()
        self.data_cache = {}  # æ•°æ®æºç¼“å­˜
        self.rule_reader = rule_reader  # æ¥å—å¤–éƒ¨ä¼ å…¥çš„rule_readerï¼ˆå·²åŒ…å«persistç¼“å­˜ï¼‰
        
        # ç¡®ä¿ä»»åŠ¡å·²æ³¨å†Œ
        register_all_tasks()
        
        # åˆå§‹åŒ–å¹¶é¢„ç¼“å­˜æ‰€æœ‰æ ‡ç­¾è§„åˆ™
        self._preload_all_rules()
    
    def execute_all_tasks(self, user_filter: Optional[List[str]] = None) -> Optional[DataFrame]:
        """
        æ‰§è¡Œæ‰€æœ‰å·²æ³¨å†Œçš„æ ‡ç­¾ä»»åŠ¡ï¼ˆå…¨é‡æ ‡ç­¾ï¼‰
        
        Args:
            user_filter: å¯é€‰çš„ç”¨æˆ·IDè¿‡æ»¤åˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„æ ‡ç­¾ç»“æœDataFrame
        """
        try:
            # è·å–æ‰€æœ‰å·²æ³¨å†Œçš„ä»»åŠ¡ID
            all_task_ids = list(TagTaskFactory.get_all_available_tasks().keys())
            
            if not all_task_ids:
                logger.warning("æ²¡æœ‰å·²æ³¨å†Œçš„ä»»åŠ¡ï¼Œæ— æ³•æ‰§è¡Œå…¨é‡æ ‡ç­¾è®¡ç®—")
                return None
            
            logger.info(f"ğŸ¯ å¼€å§‹å…¨é‡æ ‡ç­¾ä»»åŠ¡åŒ–è®¡ç®—")
            logger.info(f"ğŸ“‹ å°†æ‰§è¡Œæ‰€æœ‰å·²æ³¨å†Œä»»åŠ¡: {all_task_ids} ({len(all_task_ids)} ä¸ªä»»åŠ¡)")
            logger.info(f"ğŸ‘¥ ç”¨æˆ·è¿‡æ»¤: {user_filter if user_filter else 'å…¨é‡ç”¨æˆ·'}")
            
            # è°ƒç”¨ç°æœ‰çš„æ‰§è¡Œé€»è¾‘
            return self._execute_tasks_internal(all_task_ids, user_filter)
            
        except Exception as e:
            logger.error(f"âŒ å…¨é‡æ ‡ç­¾ä»»åŠ¡åŒ–è®¡ç®—å¤±è´¥: {str(e)}")
            return None
    
    def execute_tasks(self, task_ids: List[int], user_filter: Optional[List[str]] = None) -> Optional[DataFrame]:
        """
        æ‰§è¡ŒæŒ‡å®šçš„æ ‡ç­¾ä»»åŠ¡ï¼ˆåªæ‰§è¡Œtask_idsä¸­çš„ä»»åŠ¡ï¼‰
        
        Args:
            task_ids: è¦æ‰§è¡Œçš„æ ‡ç­¾ä»»åŠ¡IDåˆ—è¡¨
            user_filter: å¯é€‰çš„ç”¨æˆ·IDè¿‡æ»¤åˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„æ ‡ç­¾ç»“æœDataFrame
        """
        try:
            logger.info(f"ğŸ¯ å¼€å§‹æŒ‡å®šæ ‡ç­¾ä»»åŠ¡åŒ–è®¡ç®—")
            logger.info(f"ğŸ“‹ æŒ‡å®šæ ‡ç­¾ä»»åŠ¡: {task_ids} ({len(task_ids)} ä¸ªä»»åŠ¡)")
            logger.info(f"ğŸ‘¥ ç”¨æˆ·è¿‡æ»¤: {user_filter if user_filter else 'å…¨é‡ç”¨æˆ·'}")
            
            # è°ƒç”¨å†…éƒ¨æ‰§è¡Œé€»è¾‘
            return self._execute_tasks_internal(task_ids, user_filter)
                
        except Exception as e:
            logger.error(f"âŒ æŒ‡å®šæ ‡ç­¾ä»»åŠ¡åŒ–è®¡ç®—å¤±è´¥: {str(e)}")
            return None
    
    def _execute_tasks_internal(self, task_ids: List[int], user_filter: Optional[List[str]] = None) -> Optional[DataFrame]:
        """
        å†…éƒ¨ä»»åŠ¡æ‰§è¡Œé€»è¾‘ - è¢«execute_taskså’Œexecute_all_tasksè°ƒç”¨
        """
        try:
            # 1. éªŒè¯æŒ‡å®šçš„æ ‡ç­¾IDæ˜¯å¦å­˜åœ¨å¹¶æ˜¾ç¤ºæ ‡ç­¾IDåˆ°ä»»åŠ¡ç±»çš„æ˜ å°„
            valid_task_ids = []
            logger.info("ğŸ” æ ‡ç­¾IDåˆ°ä»»åŠ¡ç±»æ˜ å°„éªŒè¯:")
            
            # ä»DataFrameè·å–è§„åˆ™ï¼ˆå·²persistç¼“å­˜ï¼‰
            rules_df = self.rule_reader.get_active_rules_df()
            
            for tag_id in task_ids:
                rule_rows = rules_df.filter(rules_df.tag_id == tag_id).collect()
                
                if rule_rows:
                    rule = self._convert_rule_row_to_config(rule_rows[0])
                    # è·å–å¯¹åº”çš„ä»»åŠ¡ç±»
                    try:
                        task_class = TagTaskFactory.get_task_class(tag_id)
                        task_class_name = task_class.__name__ if task_class else "æœªæ³¨å†Œä»»åŠ¡ç±»"
                    except:
                        task_class_name = "æœªæ³¨å†Œä»»åŠ¡ç±»"
                    
                    valid_task_ids.append(tag_id)
                    logger.info(f"âœ… æ ‡ç­¾ID {tag_id} â†’ ä»»åŠ¡ç±»: {task_class_name}")
                    logger.info(f"   ğŸ“‹ æ ‡ç­¾åç§°: {rule['tag_name']}")
                    logger.info(f"   ğŸ·ï¸  æ ‡ç­¾åˆ†ç±»: {rule.get('tag_category', 'unknown')}")
                    logger.info(f"   ğŸ“œ è§„åˆ™æ¡ä»¶: {json.dumps(rule.get('rule_conditions', {}), ensure_ascii=False, indent=2)}")
                    logger.info(f"   â”€" * 60)
                else:
                    logger.warning(f"âš ï¸ æ ‡ç­¾ID {tag_id} ä¸å­˜åœ¨æˆ–æœªæ¿€æ´»ï¼Œè·³è¿‡")
            
            if not valid_task_ids:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ ‡ç­¾ä»»åŠ¡å¯æ‰§è¡Œ")
                return None
            
            # 2. åªä¸ºæœ‰æ•ˆçš„æ ‡ç­¾IDåˆ›å»ºä»»åŠ¡å®ä¾‹
            tasks = self._create_tasks(valid_task_ids)
            if not tasks:
                logger.warning("åˆ›å»ºä»»åŠ¡å®ä¾‹å¤±è´¥")
                return None
            
            logger.info(f"ğŸ“Š æˆåŠŸåˆ›å»º {len(tasks)} ä¸ªä»»åŠ¡å®ä¾‹ï¼Œå°†å¹¶è¡Œæ‰§è¡Œ")
            
            # 3. åˆ†ææ•°æ®ä¾èµ–ï¼Œä¼˜åŒ–æ•°æ®è¯»å–
            data_requirements = self._analyze_data_requirements(tasks)
            logger.info(f"ğŸ“‚ æ•°æ®éœ€æ±‚åˆ†æå®Œæˆ - æ•°æ®æº: {list(data_requirements.keys())}")
            
            # 4. æ‰¹é‡è¯»å–æ•°æ®
            loaded_data = self._batch_load_data(data_requirements, user_filter)
            if not loaded_data:
                logger.warning("æ— æ³•åŠ è½½å¿…éœ€çš„æ•°æ®æº")
                return None
            
            # 5. å¹¶è¡Œæ‰§è¡Œä»»åŠ¡
            task_results = self._execute_tasks_parallel(tasks, loaded_data)
            
            # 6. åˆå¹¶ç»“æœ
            if task_results:
                merged_result = self._merge_task_results(task_results)
                result_count = merged_result.count() if merged_result else 0
                logger.info(f"âœ… ä»»åŠ¡åŒ–æ ‡ç­¾è®¡ç®—å®Œæˆ - å½±å“ç”¨æˆ·: {result_count}")
                return merged_result
            else:
                logger.info("ğŸ“Š ä»»åŠ¡åŒ–æ ‡ç­¾è®¡ç®—å®Œæˆ - æ‰€æœ‰ä»»åŠ¡å‡æ— ç¬¦åˆæ¡ä»¶çš„ç”¨æˆ· (è¿™æ˜¯æ­£å¸¸æƒ…å†µ)")
                return None
                
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡åŒ–æ ‡ç­¾è®¡ç®—å¤±è´¥: {str(e)}")
            return None
    
    def _preload_all_rules(self):
        """é¢„åŠ è½½æ ‡ç­¾è§„åˆ™ - ä½¿ç”¨RuleReaderçš„DataFrameç¼“å­˜ï¼Œé¿å…å†—ä½™"""
        try:
            # å¦‚æœæ²¡æœ‰ä¼ å…¥rule_readerï¼Œåˆ™åˆ›å»ºæ–°å®ä¾‹
            if self.rule_reader is None:
                logger.info("ğŸ”„ åˆ›å»ºæ–°çš„è§„åˆ™è¯»å–å™¨å¹¶åŠ è½½è§„åˆ™...")
                from src.readers.rule_reader import RuleReader
                self.rule_reader = RuleReader(self.spark, self.config.mysql)
                self.rule_reader.initialize()
            else:
                logger.info("ğŸ”„ ä½¿ç”¨å·²åˆå§‹åŒ–çš„è§„åˆ™è¯»å–å™¨ï¼ˆå·²persistç¼“å­˜ï¼‰...")
            
            # è·å–DataFrameçº§åˆ«çš„è§„åˆ™ç¼“å­˜ï¼ˆå·²persistï¼‰ï¼Œæ— éœ€é¢å¤–å­—å…¸ç¼“å­˜
            rules_df = self.rule_reader.get_active_rules_df()
            rule_count = rules_df.count()
            
            logger.info(f"âœ… ä»»åŠ¡å¼•æ“ä½¿ç”¨RuleReaderçš„DataFrameç¼“å­˜ï¼Œ{rule_count} ä¸ªæ ‡ç­¾è§„åˆ™å¯ç”¨")
            
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡å¼•æ“é¢„åŠ è½½æ ‡ç­¾è§„åˆ™å¤±è´¥: {str(e)}")
            # ä¸å†éœ€è¦self.cached_ruleså­—å…¸
    
    def _create_tasks(self, task_ids: List[int]) -> List[BaseTagTask]:
        """åˆ›å»ºä»»åŠ¡å®ä¾‹ - ç›´æ¥ä½¿ç”¨RuleReaderçš„DataFrameç¼“å­˜"""
        tasks = []
        
        # ä»RuleReaderè·å–è§„åˆ™DataFrameï¼ˆå·²persistç¼“å­˜ï¼‰
        rules_df = self.rule_reader.get_active_rules_df()
        
        for tag_id in task_ids:
            try:
                # ä»DataFrameä¸­è¿‡æ»¤æŒ‡å®štag_idçš„è§„åˆ™
                rule_rows = rules_df.filter(rules_df.tag_id == tag_id).collect()
                
                if rule_rows:
                    rule_row = rule_rows[0]
                    # è½¬æ¢ä¸ºä»»åŠ¡é…ç½®å­—å…¸
                    tag_config = self._convert_rule_row_to_config(rule_row)
                    task = TagTaskFactory.create_task(tag_id, tag_config)
                    tasks.append(task)
                    logger.debug(f"âœ… åˆ›å»ºä»»åŠ¡: {task.tag_name} (ID: {tag_id})")
                else:
                    logger.warning(f"âš ï¸ æ ‡ç­¾ID {tag_id} ä¸å­˜åœ¨æˆ–æœªæ¿€æ´»")
                    
            except Exception as e:
                logger.error(f"âŒ åˆ›å»ºä»»åŠ¡å¤±è´¥ {tag_id}: {str(e)}")
        
        return tasks
    
    def _convert_rule_row_to_config(self, rule_row) -> Dict[str, Any]:
        """å°†DataFrameè¡Œè½¬æ¢ä¸ºä»»åŠ¡é…ç½®å­—å…¸"""
        import json
        
        rule_dict = rule_row.asDict()
        # è§£æJSONè§„åˆ™æ¡ä»¶
        try:
            rule_dict['rule_conditions'] = json.loads(rule_dict['rule_conditions'])
        except (json.JSONDecodeError, TypeError):
            logger.warning(f"è§„åˆ™ {rule_dict['rule_id']} çš„æ¡ä»¶æ ¼å¼é”™è¯¯")
            rule_dict['rule_conditions'] = {}
        
        return rule_dict
    
    def _get_task_rule(self, tag_id: int) -> Optional[Dict[str, Any]]:
        """ä»DataFrameç¼“å­˜ä¸­è·å–ä»»åŠ¡å¯¹åº”çš„æ ‡ç­¾è§„åˆ™"""
        try:
            rules_df = self.rule_reader.get_active_rules_df()
            rule_rows = rules_df.filter(rules_df.tag_id == tag_id).collect()
            
            if rule_rows:
                return self._convert_rule_row_to_config(rule_rows[0])
            else:
                return None
                
        except Exception as e:
            logger.error(f"è·å–æ ‡ç­¾è§„åˆ™å¤±è´¥ {tag_id}: {str(e)}")
            return None
    
    def _analyze_data_requirements(self, tasks: List[BaseTagTask]) -> Dict[str, Set[str]]:
        """
        åˆ†ææ•°æ®éœ€æ±‚ï¼Œä¼˜åŒ–è¯»å–
        
        Returns:
            Dict[æ•°æ®æºåç§°, éœ€è¦çš„å­—æ®µé›†åˆ]
        """
        requirements = {}
        
        for task in tasks:
            try:
                sources = task.get_data_sources()
                fields = set(task.get_required_fields())
                
                for source_name, source_path in sources.items():
                    if source_path:  # è¿‡æ»¤æ‰Noneå€¼
                        if source_path not in requirements:
                            requirements[source_path] = set()
                        requirements[source_path].update(fields)
                        
            except Exception as e:
                logger.warning(f"åˆ†æä»»åŠ¡ {task.tag_name} æ•°æ®éœ€æ±‚å¤±è´¥: {str(e)}")
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        return {source: list(fields) for source, fields in requirements.items()}
    
    def _batch_load_data(self, data_requirements: Dict[str, List[str]], user_filter: Optional[List[str]] = None) -> Dict[str, DataFrame]:
        """
        æ‰¹é‡è¯»å–æ•°æ®ï¼Œæ”¯æŒç¼“å­˜å’Œå­—æ®µä¼˜åŒ–
        
        Args:
            data_requirements: æ•°æ®éœ€æ±‚æ˜ å°„
            user_filter: ç”¨æˆ·è¿‡æ»¤åˆ—è¡¨
            
        Returns:
            Dict[æ•°æ®æºåç§°, DataFrame]
        """
        loaded_data = {}
        
        for source_name, required_fields in data_requirements.items():
            try:
                # æ£€æŸ¥ç¼“å­˜
                cache_key = f"{source_name}_{hash(tuple(sorted(required_fields)))}"
                if cache_key in self.data_cache:
                    logger.debug(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜æ•°æ®: {source_name}")
                    loaded_data[source_name] = self.data_cache[cache_key]
                    continue
                
                # ç¡®ä¿user_idå­—æ®µåŒ…å«åœ¨å†…
                if 'user_id' not in required_fields:
                    required_fields = ['user_id'] + required_fields
                
                logger.info(f"ğŸ“– åŠ è½½æ•°æ®æº: {source_name}, å­—æ®µ: {required_fields}")
                
                # ä»æ•°æ®æºè¯»å–æ•°æ®
                data_df = self._load_single_data_source(source_name, required_fields)
                
                if data_df is None:
                    logger.warning(f"âš ï¸ æ•°æ®æº {source_name} åŠ è½½å¤±è´¥")
                    continue
                
                # ç”¨æˆ·è¿‡æ»¤
                if user_filter:
                    data_df = data_df.filter(col('user_id').isin(user_filter))
                    logger.debug(f"ğŸ” ç”¨æˆ·è¿‡æ»¤åè®°å½•æ•°: {data_df.count()}")
                
                # ç¼“å­˜æ•°æ®
                data_df.cache()
                self.data_cache[cache_key] = data_df
                loaded_data[source_name] = data_df
                
                logger.info(f"âœ… æ•°æ®æº {source_name} åŠ è½½å®Œæˆ")
                
            except Exception as e:
                logger.error(f"âŒ åŠ è½½æ•°æ®æºå¤±è´¥ {source_name}: {str(e)}")
        
        return loaded_data
    
    def _load_single_data_source(self, source_name: str, required_fields: List[str]) -> Optional[DataFrame]:
        """åŠ è½½å•ä¸ªæ•°æ®æº"""
        try:
            # æ ¹æ®ç¯å¢ƒé…ç½®é€‰æ‹©æ•°æ®è¯»å–æ–¹å¼
            if self.config.environment == 'local':
                # æœ¬åœ°ç¯å¢ƒï¼šä½¿ç”¨å†…ç½®æ•°æ®ç”Ÿæˆå™¨æˆ–MinIO
                return self._load_local_data_source(source_name, required_fields)
            else:
                # Glueç¯å¢ƒï¼šä»S3è¯»å–
                return self._load_s3_data_source(source_name, required_fields)
                
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®æºå¤±è´¥ {source_name}: {str(e)}")
            return None
    
    def _load_local_data_source(self, source_name: str, required_fields: List[str]) -> Optional[DataFrame]:
        """æœ¬åœ°ç¯å¢ƒæ•°æ®åŠ è½½"""
        # è¿™é‡Œå¯ä»¥é›†æˆç°æœ‰çš„æœ¬åœ°æ•°æ®ç”Ÿæˆé€»è¾‘
        from src.scheduler.tag_scheduler import TagScheduler
        
        # åˆ›å»ºä¸´æ—¶è°ƒåº¦å™¨å®ä¾‹æ¥ç”Ÿæˆæµ‹è¯•æ•°æ®
        temp_scheduler = TagScheduler(self.config)
        temp_scheduler.spark = self.spark
        
        if source_name in ['user_basic_info', 'user_asset_summary', 'user_activity_summary']:
            return temp_scheduler._generate_production_like_data(source_name)
        else:
            logger.warning(f"æœªçŸ¥çš„æœ¬åœ°æ•°æ®æº: {source_name}")
            return None
    
    def _load_s3_data_source(self, source_name: str, required_fields: List[str]) -> Optional[DataFrame]:
        """S3ç¯å¢ƒæ•°æ®åŠ è½½"""
        from src.readers.hive_reader import HiveDataReader
        
        hive_reader = HiveDataReader(self.spark, self.config.s3)
        
        # æ ¹æ®æ•°æ®æºåç§°é€‰æ‹©å¯¹åº”çš„S3è·¯å¾„
        source_mapping = {
            'user_basic_info': 'user_basic_info/',
            'user_asset_summary': 'user_asset_summary/',
            'user_activity_summary': 'user_activity_summary/'
        }
        
        if source_name in source_mapping:
            s3_path = source_mapping[source_name]
            return hive_reader.read_hive_table(s3_path, selected_columns=required_fields)
        else:
            logger.warning(f"æœªçŸ¥çš„S3æ•°æ®æº: {source_name}")
            return None
    
    def _execute_tasks_parallel(self, tasks: List[BaseTagTask], loaded_data: Dict[str, DataFrame]) -> List[DataFrame]:
        """å¹¶è¡Œæ‰§è¡Œä»»åŠ¡"""
        results = []
        failed_tasks = []
        
        logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œæ‰§è¡Œ {len(tasks)} ä¸ªæ ‡ç­¾ä»»åŠ¡")
        
        for task in tasks:
            try:
                logger.info(f"ğŸ”„ æ‰§è¡Œä»»åŠ¡ç±»: {task.__class__.__name__} (æ ‡ç­¾ID: {task.tag_id})")
                logger.info(f"   ğŸ“‹ æ ‡ç­¾åç§°: {task.tag_name}")
                
                # ä»ç¼“å­˜ä¸­è·å–å¹¶æ˜¾ç¤ºMySQLè§„åˆ™
                mysql_rule = self._get_task_rule(task.tag_id)
                if not mysql_rule or 'rule_conditions' not in mysql_rule:
                    logger.warning(f"âš ï¸ ä»»åŠ¡ç±» {task.__class__.__name__} æ— æ³•ä»ç¼“å­˜è·å–MySQLè§„åˆ™")
                    failed_tasks.append(task.__class__.__name__)
                    continue
                
                logger.info(f"   ğŸ“œ ä»ç¼“å­˜è·å–æ ‡ç­¾è§„åˆ™ (ID: {task.tag_id}):")
                logger.info(f"   ğŸ“‹ è§„åˆ™JSON: {json.dumps(mysql_rule['rule_conditions'], ensure_ascii=False, indent=6)}")
                
                # è·å–ä»»åŠ¡æ‰€éœ€æ•°æ®
                task_data = self._prepare_task_data(task, loaded_data)
                if task_data is None:
                    logger.warning(f"âš ï¸ ä»»åŠ¡ç±» {task.__class__.__name__} æ— æ³•è·å–æ‰€éœ€æ•°æ®")
                    failed_tasks.append(task.__class__.__name__)
                    continue
                
                # éªŒè¯æ•°æ®
                if not task.validate_data(task_data):
                    logger.warning(f"âš ï¸ ä»»åŠ¡ç±» {task.__class__.__name__} æ•°æ®éªŒè¯å¤±è´¥")
                    failed_tasks.append(task.__class__.__name__)
                    continue
                
                # é¢„å¤„ç†æ•°æ®
                processed_data = task.preprocess_data(task_data)
                logger.info(f"   ğŸ“Š æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œè®°å½•æ•°: {processed_data.count()}")
                
                # æ‰§è¡Œæ ‡ç­¾è®¡ç®—
                tagged_users = self._compute_single_task(processed_data, task, mysql_rule['rule_conditions'])
                
                # åå¤„ç†
                if tagged_users and tagged_users.count() > 0:
                    final_result = task.post_process_result(tagged_users)
                    results.append(final_result)
                    logger.info(f"âœ… ä»»åŠ¡ç±» {task.__class__.__name__} æ‰§è¡ŒæˆåŠŸï¼Œå‘½ä¸­ç”¨æˆ·: {final_result.count()}")
                else:
                    # è¿™ä¸ªæ—¥å¿—å·²ç»åœ¨_compute_single_taskä¸­è¾“å‡ºäº†ï¼Œä¸éœ€è¦é‡å¤
                    pass
                    
            except Exception as e:
                logger.error(f"âŒ ä»»åŠ¡ç±» {task.__class__.__name__} æ‰§è¡Œå¤±è´¥: {str(e)}")
                failed_tasks.append(task.__class__.__name__)
        
        logger.info(f"ğŸ‰ å¹¶è¡Œæ‰§è¡Œå®Œæˆ - æˆåŠŸ: {len(results)}, å¤±è´¥: {len(failed_tasks)}")
        if failed_tasks:
            logger.warning(f"å¤±è´¥çš„ä»»åŠ¡: {failed_tasks}")
        
        return results
    
    def _prepare_task_data(self, task: BaseTagTask, loaded_data: Dict[str, DataFrame]) -> Optional[DataFrame]:
        """ä¸ºä»»åŠ¡å‡†å¤‡æ•°æ®"""
        try:
            data_sources = task.get_data_sources()
            primary_source = data_sources.get('primary')
            
            if not primary_source or primary_source not in loaded_data:
                logger.error(f"ä»»åŠ¡ {task.tag_name} çš„ä¸»æ•°æ®æº {primary_source} ä¸å¯ç”¨")
                return None
            
            # è·å–ä¸»æ•°æ®æº
            task_data = loaded_data[primary_source]
            
            # å¦‚æœæœ‰è¾…åŠ©æ•°æ®æºï¼Œè¿›è¡ŒJOIN
            secondary_source = data_sources.get('secondary')
            if secondary_source and secondary_source in loaded_data:
                secondary_data = loaded_data[secondary_source]
                task_data = task_data.join(secondary_data, 'user_id', 'left')
                logger.debug(f"ä»»åŠ¡ {task.tag_name} åˆå¹¶äº†è¾…åŠ©æ•°æ®æº")
            
            return task_data
            
        except Exception as e:
            logger.error(f"å‡†å¤‡ä»»åŠ¡æ•°æ®å¤±è´¥ {task.tag_name}: {str(e)}")
            return None
    
    def _compute_single_task(self, data_df: DataFrame, task: BaseTagTask, rules: Dict[str, Any]) -> Optional[DataFrame]:
        """è®¡ç®—å•ä¸ªä»»åŠ¡"""
        try:
            logger.info(f"   ğŸ” å¼€å§‹è®¡ç®—ä»»åŠ¡ç±»: {task.__class__.__name__} (æ ‡ç­¾ID: {task.tag_id})")
            
            # è§£æè§„åˆ™æ¡ä»¶
            condition_sql = self.rule_parser.parse_rule_conditions(rules)
            logger.info(f"   ğŸ“ ç”ŸæˆSQLæ¡ä»¶: {condition_sql}")
            
            # æ‰§è¡Œæ ‡ç­¾è®¡ç®—
            tagged_users = data_df.filter(condition_sql)
            
            hit_count = tagged_users.count()
            total_users = data_df.count()
            
            logger.info(f"   ğŸ“Š æ ‡ç­¾è®¡ç®—ç»“æœ:")
            logger.info(f"      ğŸ‘¥ æ€»ç”¨æˆ·æ•°: {total_users}")
            logger.info(f"      âœ… ç¬¦åˆæ¡ä»¶: {hit_count}")
            logger.info(f"      ğŸ“ˆ å‘½ä¸­ç‡: {(hit_count/total_users*100):.2f}%" if total_users > 0 else "      ğŸ“ˆ å‘½ä¸­ç‡: 0%")
            
            if hit_count == 0:
                logger.info(f"   ğŸ’¡ ä»»åŠ¡ç±» {task.__class__.__name__}: æ— ç”¨æˆ·æ»¡è¶³æ ‡ç­¾æ¡ä»¶ (æ­£å¸¸ä¸šåŠ¡ç»“æœ)")
                return None
            
            # æ·»åŠ æ ‡ç­¾ä¿¡æ¯
            result_df = tagged_users.select('user_id').withColumn('tag_id', lit(task.tag_id))
            
            # ç”Ÿæˆæ ‡ç­¾è¯¦æƒ…
            result_df = self._add_task_details(result_df, task)
            
            logger.info(f"   âœ… ä»»åŠ¡ç±» {task.__class__.__name__} è®¡ç®—å®Œæˆï¼Œå‘½ä¸­ç”¨æˆ·: {hit_count}")
            return result_df.select('user_id', 'tag_id', 'tag_detail')
            
        except Exception as e:
            logger.error(f"   âŒ ä»»åŠ¡ç±» {task.__class__.__name__} è®¡ç®—å¤±è´¥: {str(e)}")
            return None
    
    def _add_task_details(self, result_df: DataFrame, task: BaseTagTask) -> DataFrame:
        """ä¸ºä»»åŠ¡ç»“æœæ·»åŠ è¯¦ç»†ä¿¡æ¯"""
        @F.udf(returnType=StringType())
        def generate_task_detail():
            """ç”Ÿæˆä»»åŠ¡è¯¦ç»†ä¿¡æ¯çš„UDF"""
            detail = {
                'tag_name': task.tag_name,
                'tag_category': task.tag_category,
                'source': 'TASK_ENGINE'
            }
            return json.dumps(detail, ensure_ascii=False)
        
        return result_df.withColumn('tag_detail', generate_task_detail())
    
    def _merge_task_results(self, task_results: List[DataFrame]) -> Optional[DataFrame]:
        """åˆå¹¶å¤šä¸ªå¹¶è¡Œä»»åŠ¡çš„å†…å­˜æ ‡ç­¾ç»“æœ"""
        try:
            if not task_results:
                return None
            
            logger.info(f"ğŸ”„ å¼€å§‹åˆå¹¶ {len(task_results)} ä¸ªå¹¶è¡Œä»»åŠ¡çš„å†…å­˜æ ‡ç­¾ç»“æœ...")
            
            # 1. ä½¿ç”¨å†…ç½®çš„å†…å­˜åˆå¹¶é€»è¾‘
            merged_result = self._merge_user_tags_in_memory(task_results)
            
            if merged_result:
                logger.info(f"âœ… å¤šä¸ªå¹¶è¡Œä»»åŠ¡çš„å†…å­˜æ ‡ç­¾åˆå¹¶å®Œæˆï¼Œå½±å“ {merged_result.count()} ä¸ªç”¨æˆ·")
                return merged_result
            else:
                logger.info("ğŸ“Š å¤šä¸ªå¹¶è¡Œä»»åŠ¡çš„å†…å­˜æ ‡ç­¾åˆå¹¶å®Œæˆï¼Œæ— ç”¨æˆ·æ•°æ®")
                return None
            
        except Exception as e:
            logger.error(f"âŒ åˆå¹¶å¤šä¸ªå¹¶è¡Œä»»åŠ¡çš„å†…å­˜æ ‡ç­¾å¤±è´¥: {str(e)}")
            return None
    
    def _merge_user_tags_in_memory(self, tag_results: List[DataFrame]) -> Optional[DataFrame]:
        """å†…å­˜åˆå¹¶ï¼šå°†åŒä¸€ç”¨æˆ·çš„å¤šä¸ªæ ‡ç­¾åˆå¹¶ä¸ºä¸€æ¡è®°å½•"""
        try:
            if not tag_results:
                return None
                
            logger.info(f"å¼€å§‹å†…å­˜åˆå¹¶ {len(tag_results)} ä¸ªæ ‡ç­¾ç»“æœ...")
            
            # è®°å½•æ¯ä¸ªä»»åŠ¡çš„æ ‡ç­¾ç»“æœç”¨äºè¿½è¸ª
            for i, task_df in enumerate(tag_results):
                task_count = task_df.count()
                if task_count > 0:
                    # æ˜¾ç¤ºæ¯ä¸ªä»»åŠ¡çš„æ ‡ç­¾æ‰“ä¸­æƒ…å†µ
                    sample_task_users = task_df.limit(3).collect()
                    logger.info(f"   ğŸ“‹ ä»»åŠ¡ {i+1} ç»“æœ: {task_count} ä¸ªç”¨æˆ·")
                    for user_row in sample_task_users:
                        logger.info(f"      ğŸ‘¤ {user_row.user_id} â†’ æ ‡ç­¾ {user_row.tag_id}")
            
            from functools import reduce
            from pyspark.sql.functions import collect_list, array_distinct, struct, lit, col, udf
            from pyspark.sql.types import StringType
            import json
            from datetime import date
            
            # 1. åˆå¹¶æ‰€æœ‰æ ‡ç­¾ç»“æœ
            all_tags = reduce(lambda df1, df2: df1.union(df2), tag_results)
            
            if all_tags.count() == 0:
                logger.warning("åˆå¹¶åæ²¡æœ‰æ ‡ç­¾æ•°æ®")
                return None
            
            # 2. å»é‡ï¼šç§»é™¤åŒä¸€ç”¨æˆ·çš„é‡å¤æ ‡ç­¾
            deduplicated_tags = all_tags.dropDuplicates(["user_id", "tag_id"])
            
            # è®°å½•å»é‡å‰åçš„å¯¹æ¯”
            original_count = all_tags.count()
            deduplicated_count = deduplicated_tags.count()
            if original_count != deduplicated_count:
                logger.info(f"   ğŸ”„ å»é‡å¤„ç†: {original_count} â†’ {deduplicated_count} æ¡è®°å½• (å»é™¤ {original_count - deduplicated_count} é‡å¤)")
            else:
                logger.info(f"   âœ… æ— é‡å¤æ ‡ç­¾: {deduplicated_count} æ¡è®°å½•")
            
            # 3. ä¸°å¯Œæ ‡ç­¾ä¿¡æ¯
            enriched_tags = self._enrich_with_tag_info(deduplicated_tags)
            
            # 4. æŒ‰ç”¨æˆ·èšåˆï¼šå°†ç”¨æˆ·çš„å¤šä¸ªæ ‡ç­¾åˆå¹¶ä¸ºæ•°ç»„
            user_aggregated = enriched_tags.groupBy("user_id").agg(
                collect_list("tag_id").alias("tag_ids_raw"),
                collect_list(struct("tag_id", "tag_name", "tag_category")).alias("tag_info_list")
            )
            
            # 5. ç¡®ä¿æ ‡ç­¾æ•°ç»„å»é‡
            user_aggregated = user_aggregated.select(
                "user_id",
                array_distinct("tag_ids_raw").alias("tag_ids"),
                "tag_info_list"
            )
            
            # 6. å†…å­˜åˆå¹¶è¿‡ç¨‹è¯¦ç»†è¿½è¸ª
            logger.info("ğŸ“Š å†…å­˜åˆå¹¶è¯¦ç»†è¿‡ç¨‹ï¼ˆå‰3ä¸ªç”¨æˆ·ï¼‰:")
            sample_memory_merged = user_aggregated.limit(3).collect()
            
            for user_row in sample_memory_merged:
                user_id = user_row.user_id
                final_tag_ids = user_row.tag_ids
                
                # æ˜¾ç¤ºè¯¥ç”¨æˆ·å„ä¸ªä»»åŠ¡çš„åŸå§‹æ ‡ç­¾
                logger.info(f"   ğŸ‘¤ ç”¨æˆ· {user_id} å†…å­˜åˆå¹¶è¿‡ç¨‹:")
                
                # æŸ¥æ‰¾è¯¥ç”¨æˆ·åœ¨å„ä¸ªä»»åŠ¡ä¸­çš„æ ‡ç­¾
                user_task_tags = []
                for i, task_df in enumerate(tag_results):
                    user_tags_in_task = task_df.filter(col("user_id") == user_id).collect()
                    if user_tags_in_task:
                        task_tag_ids = [row.tag_id for row in user_tags_in_task]
                        user_task_tags.extend(task_tag_ids)
                        logger.info(f"      ğŸ“‹ ä»»åŠ¡ {i+1} åŸå§‹æ ‡ç­¾: {task_tag_ids}")
                    else:
                        logger.info(f"      ğŸ“‹ ä»»åŠ¡ {i+1} åŸå§‹æ ‡ç­¾: æ— ")
                
                logger.info(f"      ğŸ”„ åˆå¹¶å‰æ‰€æœ‰æ ‡ç­¾: {user_task_tags}")
                logger.info(f"      âœ… å†…å­˜åˆå¹¶åæ ‡ç­¾: {final_tag_ids}")
                
                # éªŒè¯åˆå¹¶é€»è¾‘
                expected_merged = sorted(list(set(user_task_tags)))
                actual_merged = sorted(final_tag_ids)
                if expected_merged == actual_merged:
                    logger.info(f"      âœ… å†…å­˜åˆå¹¶é€»è¾‘æ­£ç¡®")
                else:
                    logger.info(f"      âŒ å†…å­˜åˆå¹¶é€»è¾‘å¼‚å¸¸ - æœŸæœ›: {expected_merged}, å®é™…: {actual_merged}")
                
                logger.info(f"      â”€" * 50)
            
            # 7. æ ¼å¼åŒ–è¾“å‡º
            final_result = self._format_memory_merge_output(user_aggregated)
            
            return final_result
            
        except Exception as e:
            logger.error(f"å†…å­˜åˆå¹¶å¤±è´¥: {str(e)}")
            return None
    
    def _enrich_with_tag_info(self, tags_df: DataFrame) -> DataFrame:
        """ä¸°å¯Œæ ‡ç­¾ä¿¡æ¯"""
        try:
            # è¯»å–æ ‡ç­¾å®šä¹‰
            tag_definitions = self.spark.read.jdbc(
                url=self.config.mysql.jdbc_url,
                table="tag_definition",
                properties=self.config.mysql.connection_properties
            ).select("tag_id", "tag_name", "tag_category")
            
            # å…³è”æ ‡ç­¾å®šä¹‰ä¿¡æ¯
            enriched_df = tags_df.join(
                tag_definitions,
                "tag_id",
                "left"
            ).select(
                "user_id",
                "tag_id", 
                col("tag_name").alias("tag_name"),
                col("tag_category").alias("tag_category"),
                "tag_detail"
            )
            
            return enriched_df
            
        except Exception as e:
            logger.error(f"ä¸°å¯Œæ ‡ç­¾ä¿¡æ¯å¤±è´¥: {str(e)}")
            # é™çº§å¤„ç†
            from pyspark.sql.functions import lit
            return tags_df.select(
                "user_id",
                "tag_id",
                lit("unknown").alias("tag_name"),
                lit("unknown").alias("tag_category"),
                "tag_detail"
            )
    
    def _format_memory_merge_output(self, user_tags_df: DataFrame) -> DataFrame:
        """æ ¼å¼åŒ–å†…å­˜åˆå¹¶è¾“å‡º"""
        from pyspark.sql.functions import udf, col, lit
        from pyspark.sql.types import StringType
        import json
        from datetime import date
        
        @udf(returnType=StringType())
        def build_tag_details(tag_info_list):
            if not tag_info_list:
                return "{}"
            
            tag_details = {}
            for tag_info in tag_info_list:
                tag_id = str(tag_info['tag_id'])
                tag_details[tag_id] = {
                    'tag_name': tag_info['tag_name'],
                    'tag_category': tag_info['tag_category']
                }
            return json.dumps(tag_details, ensure_ascii=False)
        
        formatted_df = user_tags_df.select(
            col("user_id"),
            col("tag_ids"),
            build_tag_details(col("tag_info_list")).alias("tag_details"),
            lit(date.today()).alias("computed_date")
        )
        
        return formatted_df
    
    def execute_specific_tag_tasks(self, tag_ids: List[int], user_filter: Optional[List[str]] = None) -> Optional[DataFrame]:
        """
        æ‰§è¡ŒæŒ‡å®šæ ‡ç­¾IDå¯¹åº”çš„ä»»åŠ¡ç±»
        
        Args:
            tag_ids: æ ‡ç­¾IDåˆ—è¡¨
            user_filter: ç”¨æˆ·è¿‡æ»¤åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            åˆå¹¶åçš„ç»“æœDataFrame
        """
        try:
            logger.info(f"ğŸ¯ æ‰§è¡ŒæŒ‡å®šæ ‡ç­¾ä»»åŠ¡: {tag_ids}")
            
            # è·å–æ ‡ç­¾IDåˆ°ä»»åŠ¡ç±»çš„æ˜ å°„
            tag_to_task_mapping = self._get_tag_to_task_mapping()
            
            # æ‰¾åˆ°å¯¹åº”çš„ä»»åŠ¡ç±»å¹¶åˆ›å»ºå®ä¾‹
            target_tasks = []
            rules_df = self.rule_reader.get_active_rules_df()
            
            for tag_id in tag_ids:
                rule_rows = rules_df.filter(rules_df.tag_id == tag_id).collect()
                
                if rule_rows:
                    tag_config = self._convert_rule_row_to_config(rule_rows[0])
                    task = TagTaskFactory.create_task(tag_id, tag_config)
                    target_tasks.append(task)
                    logger.debug(f"âœ… åˆ›å»ºæŒ‡å®šä»»åŠ¡: {task.tag_name} (ID: {tag_id})")
                else:
                    logger.warning(f"æ ‡ç­¾ID {tag_id} æ²¡æœ‰å¯¹åº”çš„ä»»åŠ¡ç±»æˆ–è§„åˆ™")
            
            if not target_tasks:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯¹åº”çš„ä»»åŠ¡ç±»")
                return None
            
            # æ‰§è¡ŒæŒ‡å®šä»»åŠ¡ï¼ˆè§„åˆ™å·²åœ¨åˆå§‹åŒ–æ—¶é¢„åŠ è½½ï¼‰
            # åˆ†ææ•°æ®éœ€æ±‚
            data_requirements = self._analyze_data_requirements(target_tasks)
            logger.info(f"ğŸ“‚ æ•°æ®éœ€æ±‚åˆ†æå®Œæˆ - æ•°æ®æº: {list(data_requirements.keys())}")
            
            # æ‰¹é‡è¯»å–æ•°æ®
            loaded_data = self._batch_load_data(data_requirements, user_filter)
            if not loaded_data:
                logger.warning("æ— æ³•åŠ è½½å¿…éœ€çš„æ•°æ®æº")
                return None
            
            # æ‰§è¡Œä»»åŠ¡
            results = self._execute_tasks_parallel(target_tasks, loaded_data)
            
            # åˆå¹¶ç»“æœ
            if results:
                return self._merge_task_results(results)
            else:
                logger.info("æŒ‡å®šæ ‡ç­¾ä»»åŠ¡æ²¡æœ‰äº§ç”Ÿç»“æœ")
                return None
                
        except Exception as e:
            logger.error(f"æ‰§è¡ŒæŒ‡å®šæ ‡ç­¾ä»»åŠ¡å¤±è´¥: {str(e)}")
            return None
    
    def _get_tag_to_task_mapping(self) -> Dict[int, type]:
        """è·å–æ ‡ç­¾IDåˆ°ä»»åŠ¡ç±»çš„æ˜ å°„"""
        from src.tasks.wealth.high_net_worth_task import HighNetWorthUserTask
        from src.tasks.behavior.active_trader_task import ActiveTraderTask
        from src.tasks.risk.low_risk_task import LowRiskUserTask
        from src.tasks.lifecycle.new_user_task import NewUserTask
        from src.tasks.lifecycle.vip_user_task import VIPUserTask
        from src.tasks.wealth.cash_rich_task import CashRichUserTask
        from src.tasks.demographic.young_user_task import YoungUserTask
        from src.tasks.behavior.recent_active_task import RecentActiveUserTask
        
        return {
            1: HighNetWorthUserTask,
            2: ActiveTraderTask,
            3: LowRiskUserTask,
            4: NewUserTask,
            5: VIPUserTask,
            6: CashRichUserTask,
            7: YoungUserTask,
            8: RecentActiveUserTask
        }
    
    def cleanup_cache(self):
        """æ¸…ç†æ•°æ®ç¼“å­˜"""
        for cache_key, cached_df in self.data_cache.items():
            try:
                cached_df.unpersist()
            except:
                pass
        
        self.data_cache.clear()
        logger.info("ğŸ§¹ ä»»åŠ¡å¼•æ“æ•°æ®ç¼“å­˜å·²æ¸…ç†")