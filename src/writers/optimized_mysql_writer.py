import logging
from typing import Optional
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import to_json, col, when

from src.config.base import MySQLConfig

logger = logging.getLogger(__name__)


class OptimizedMySQLWriter:
    """ä¼˜åŒ–çš„MySQLå†™å…¥å™¨ - ç»Ÿä¸€ä½¿ç”¨UPSERTç­–ç•¥é¿å…å…¨è¡¨è¦†ç›–"""
    
    def __init__(self, spark: SparkSession, mysql_config: MySQLConfig):
        self.spark = spark
        self.mysql_config = mysql_config
    
    def write_tag_results(self, result_df: DataFrame) -> bool:
        """
        å†™å…¥æ ‡ç­¾ç»“æœ - ç»Ÿä¸€ä½¿ç”¨UPSERTç­–ç•¥
        
        Args:
            result_df: æ ‡ç­¾ç»“æœDataFrame (user_id, tag_ids, tag_details, computed_date)
            
        Returns:
            å†™å…¥æ˜¯å¦æˆåŠŸ
        """
        try:
            if result_df.count() == 0:
                logger.info("æ²¡æœ‰æ•°æ®éœ€è¦å†™å…¥")
                return True
            
            logger.info("ğŸš€ å¼€å§‹UPSERTå†™å…¥æ ‡ç­¾ç»“æœ...")
            
            # è½¬æ¢ä¸ºMySQLå…¼å®¹æ ¼å¼
            mysql_ready_df = self._prepare_for_mysql(result_df)
            
            # æ‰§è¡ŒUPSERTå†™å…¥
            success = self._upsert_to_mysql(mysql_ready_df)
            
            if success:
                # éªŒè¯å†™å…¥ç»“æœ
                return self._validate_write_result(result_df)
            
            return False
            
        except Exception as e:
            logger.error(f"å†™å…¥æ ‡ç­¾ç»“æœå¤±è´¥: {str(e)}")
            return False
    
    def _prepare_for_mysql(self, result_df: DataFrame) -> DataFrame:
        """å‡†å¤‡MySQLå†™å…¥æ ¼å¼"""
        try:
            # è½¬æ¢tag_idsä¸ºJSONå­—ç¬¦ä¸²ï¼Œç§»é™¤computed_dateå­—æ®µ
            mysql_ready_df = result_df.select(
                col("user_id"),
                when(col("tag_ids").isNotNull(), to_json(col("tag_ids")))
                .otherwise("[]").alias("tag_ids"),
                col("tag_details")
            )
            
            # æ•°æ®é¢„è§ˆ
            logger.info("MySQLå†™å…¥æ•°æ®æ ·ä¾‹:")
            mysql_ready_df.show(3, truncate=False)
            
            return mysql_ready_df
            
        except Exception as e:
            logger.error(f"MySQLæ•°æ®å‡†å¤‡å¤±è´¥: {str(e)}")
            raise
    
    def _upsert_to_mysql(self, df: DataFrame) -> bool:
        """ç»Ÿä¸€UPSERTç­–ç•¥ - INSERT ON DUPLICATE KEY UPDATE"""
        import pymysql
        
        # é…ç½®å‚æ•°
        host = self.mysql_config.host
        port = self.mysql_config.port
        username = self.mysql_config.username
        password = self.mysql_config.password
        database = self.mysql_config.database
        
        def upsert_partition(partition_data):
            """åˆ†åŒºUPSERTé€»è¾‘"""
            import pymysql
            
            rows = list(partition_data)
            if not rows:
                return
            
            connection = None
            try:
                connection = pymysql.connect(
                    host=host,
                    port=port,
                    user=username,
                    password=password,
                    database=database,
                    charset='utf8mb4',
                    autocommit=False,
                    connect_timeout=30,
                    read_timeout=60,
                    write_timeout=60
                )
                
                cursor = connection.cursor()
                
                # UPSERT SQL - åªæœ‰å½“æ ‡ç­¾æ•°æ®çœŸæ­£å˜åŒ–æ—¶æ‰æ›´æ–°updated_time
                # created_timeæ°¸è¿œä¸å˜ï¼Œä¿æŒç¬¬ä¸€æ¬¡æ’å…¥çš„æ—¶é—´
                # ä¿®å¤æ‰§è¡Œé¡ºåºé—®é¢˜ï¼šå…ˆæ¯”è¾ƒåæ›´æ–°ï¼Œç¡®ä¿æ¯”è¾ƒçš„æ˜¯æ—§å€¼å’Œæ–°å€¼
                upsert_sql = """
                INSERT INTO user_tags (user_id, tag_ids, tag_details) 
                VALUES (%s, %s, %s)
                ON DUPLICATE KEY UPDATE
                    updated_time = CASE 
                        WHEN JSON_EXTRACT(tag_ids, '$') <> JSON_EXTRACT(VALUES(tag_ids), '$')
                        THEN CURRENT_TIMESTAMP 
                        ELSE updated_time 
                    END,
                    tag_ids = VALUES(tag_ids),
                    tag_details = VALUES(tag_details)
                """
                
                # æ‰¹é‡å¤„ç†
                batch_size = 1000
                for i in range(0, len(rows), batch_size):
                    batch = rows[i:i + batch_size]
                    batch_data = []
                    
                    for row in batch:
                        batch_data.append((
                            str(row.user_id),
                            str(row.tag_ids) if row.tag_ids else '[]',
                            str(row.tag_details) if row.tag_details else '{}'
                        ))
                    
                    cursor.executemany(upsert_sql, batch_data)
                
                connection.commit()
                
            except Exception as e:
                if connection:
                    connection.rollback()
                raise
            finally:
                if connection:
                    connection.close()
        
        try:
            # ä¼˜åŒ–åˆ†åŒºæ•°é‡
            total_count = df.count()
            optimal_partitions = min(8, max(1, total_count // 5000))
            
            logger.info(f"ğŸ“Š UPSERTå†™å…¥ï¼š{total_count} æ¡è®°å½•ï¼Œ{optimal_partitions} ä¸ªåˆ†åŒº")
            
            # é‡åˆ†åŒºå¹¶æ‰§è¡Œ
            repartitioned_df = df.repartition(optimal_partitions, "user_id")
            
            # åŠ å¼ºé”™è¯¯å¤„ç†çš„åˆ†åŒºå‡½æ•°
            def upsert_partition_with_logging(partition_data):
                try:
                    rows = list(partition_data)
                    if not rows:
                        return
                    
                    print(f"ğŸ”„ åˆ†åŒºå¤„ç† {len(rows)} æ¡è®°å½•...")
                    upsert_partition(rows)  # ä¼ é€’rowsè€Œä¸æ˜¯partition_data
                    print(f"âœ… åˆ†åŒºUPSERTå®Œæˆï¼Œå¤„ç†äº† {len(rows)} æ¡è®°å½•")
                except Exception as e:
                    print(f"âŒ åˆ†åŒºUPSERTå¤±è´¥: {str(e)}")
                    import traceback
                    traceback.print_exc()
                    raise
            
            repartitioned_df.foreachPartition(upsert_partition_with_logging)
            
            logger.info("âœ… UPSERTå†™å…¥å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"UPSERTå†™å…¥å¤±è´¥: {str(e)}")
            return False
    
    def _validate_write_result(self, original_df: DataFrame) -> bool:
        """éªŒè¯å†™å…¥ç»“æœ - åªéªŒè¯ä»»åŠ¡ç±»æ‰“åˆ°æ ‡ç­¾çš„ç”¨æˆ·æ˜¯å¦æˆåŠŸå†™å…¥"""
        try:
            # è·å–åº”è¯¥å†™å…¥çš„ç”¨æˆ·
            original_users = original_df.select("user_id").distinct().collect()
            original_user_set = {row["user_id"] for row in original_users}
            expected_count = len(original_user_set)
            
            if expected_count == 0:
                logger.info("âœ… æ— æ•°æ®å†™å…¥ï¼ŒéªŒè¯é€šè¿‡")
                return True
            
            # åªæ£€æŸ¥å½“å‰ä»»åŠ¡ç±»æ‰“åˆ°æ ‡ç­¾çš„ç”¨æˆ·æ˜¯å¦æˆåŠŸå†™å…¥
            written_df = self.spark.read.jdbc(
                url=self.mysql_config.jdbc_url,
                table="user_tags",
                properties=self.mysql_config.connection_properties
            )
            
            if written_df.count() == 0:
                logger.error(f"âŒ æ•°æ®åº“ä¸­æ²¡æœ‰è®°å½•ï¼Œä½†æœŸæœ›å†™å…¥ {expected_count} ä¸ªç”¨æˆ·")
                return False
            
            written_users = written_df.select("user_id").distinct().collect()
            written_user_set = {row["user_id"] for row in written_users}
            
            # æ£€æŸ¥å½“å‰ä»»åŠ¡ç±»æ‰“åˆ°æ ‡ç­¾çš„ç”¨æˆ·æ˜¯å¦éƒ½å·²å†™å…¥
            missing_users = original_user_set - written_user_set
            if missing_users:
                logger.error(f"âŒ ä»»åŠ¡ç±»æ‰“åˆ°æ ‡ç­¾çš„ç”¨æˆ·æœªæˆåŠŸå†™å…¥: {list(missing_users)[:5]}...")
                return False
            
            logger.info(f"âœ… ä»»åŠ¡ç±»æ‰“åˆ°æ ‡ç­¾çš„ç”¨æˆ·å†™å…¥éªŒè¯é€šè¿‡ï¼š{expected_count} ä¸ªç”¨æˆ·æˆåŠŸå†™å…¥")
            return True
            
        except Exception as e:
            logger.error(f"å†™å…¥éªŒè¯å¤±è´¥: {str(e)}")
            return False
    
    def get_write_statistics(self) -> dict:
        """è·å–å†™å…¥ç»Ÿè®¡"""
        try:
            from pyspark.sql.functions import from_json, size, expr, explode
            from pyspark.sql.types import ArrayType, IntegerType
            
            stats_df = self.spark.read.jdbc(
                url=self.mysql_config.jdbc_url,
                table="user_tags",
                properties=self.mysql_config.connection_properties
            )
            
            total_users = stats_df.count()
            if total_users == 0:
                return {"total_users": 0, "total_tag_assignments": 0}
            
            # è§£æJSONç»Ÿè®¡
            parsed_df = stats_df.select(
                "user_id",
                from_json("tag_ids", ArrayType(IntegerType())).alias("tag_ids_array")
            )
            
            tag_counts = parsed_df.select(
                size("tag_ids_array").alias("tag_count")
            ).collect()
            
            tag_count_values = [row['tag_count'] for row in tag_counts]
            total_assignments = sum(tag_count_values)
            
            return {
                "total_users": total_users,
                "total_tag_assignments": total_assignments,
                "average_tags_per_user": round(total_assignments / total_users, 2) if total_users > 0 else 0,
                "max_tags_per_user": max(tag_count_values) if tag_count_values else 0
            }
            
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {}