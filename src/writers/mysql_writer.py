import logging
from datetime import datetime
from typing import Optional
from pyspark.sql import DataFrame, SparkSession

from src.config.base import MySQLConfig

logger = logging.getLogger(__name__)


class MySQLTagWriter:
    """MySQLæ ‡ç­¾ç»“æœå†™å…¥å™¨"""
    
    def __init__(self, spark: SparkSession, mysql_config: MySQLConfig):
        self.spark = spark
        self.mysql_config = mysql_config
    
    def write_tag_results(self, result_df: DataFrame, mode: str = "overwrite", 
                         enable_backup: bool = False, merge_with_existing: bool = True) -> bool:
        """
        å†™å…¥æ ‡ç­¾ç»“æœåˆ°MySQL
        
        Args:
            result_df: æ ‡ç­¾ç»“æœDataFrame
            mode: å†™å…¥æ¨¡å¼ (overwrite/append)
            enable_backup: æ˜¯å¦å¯ç”¨å¤‡ä»½
            merge_with_existing: æ˜¯å¦ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶
            
        Returns:
            å†™å…¥æ˜¯å¦æˆåŠŸ
        """
        try:
            if enable_backup and mode == "overwrite":
                # å¤‡ä»½ç°æœ‰æ•°æ®
                backup_success = self._backup_current_data()
                if not backup_success:
                    logger.warning("å¤‡ä»½å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œå†™å…¥æ“ä½œ")
            
            # æ‰§è¡Œå†™å…¥
            success = self._write_to_mysql(result_df, mode, merge_with_existing)
            
            if success:
                logger.info(f"âœ… æ ‡ç­¾ç»“æœå†™å…¥æˆåŠŸï¼Œæ¨¡å¼: {mode}, è®°å½•æ•°: {result_df.count()}")
                
                # éªŒè¯å†™å…¥ç»“æœ
                if self._validate_write_result(result_df, mode):
                    return True
                else:
                    logger.error("å†™å…¥ç»“æœéªŒè¯å¤±è´¥")
                    return False
            else:
                logger.error("æ ‡ç­¾ç»“æœå†™å…¥å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"å†™å…¥æ ‡ç­¾ç»“æœå¼‚å¸¸: {str(e)}")
            
            # å¦‚æœå¯ç”¨äº†å¤‡ä»½ä¸”å†™å…¥å¤±è´¥ï¼Œå°è¯•æ¢å¤
            if enable_backup and mode == "overwrite":
                logger.info("å°è¯•ä»å¤‡ä»½æ¢å¤æ•°æ®...")
                self._restore_from_backup()
            
            return False
    
    def _write_to_mysql(self, result_df: DataFrame, mode: str, merge_with_existing: bool = True) -> bool:
        """æ‰§è¡ŒMySQLå†™å…¥æ“ä½œ - ç»Ÿä¸€UPSERTé€»è¾‘"""
        try:
            # æ ¹æ®æ˜¯å¦éœ€è¦åˆå¹¶å†³å®šå¤„ç†é€»è¾‘
            if merge_with_existing:
                final_df = self._merge_with_existing_tags(result_df)
            else:
                final_df = result_df
            
            # å°†Sparkæ•°ç»„è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ï¼Œä¿æŒæ•°ç»„ç»“æ„
            from pyspark.sql.functions import to_json, col, when
            
            mysql_ready_df = final_df.select(
                col("user_id"),
                # ç¡®ä¿tag_idsæ˜¯JSONæ•°ç»„å­—ç¬¦ä¸²æ ¼å¼
                when(col("tag_ids").isNotNull(), to_json(col("tag_ids")))
                .otherwise("[]").alias("tag_ids"),
                # tag_detailså·²ç»æ˜¯JSONå­—ç¬¦ä¸²æ ¼å¼
                col("tag_details"),
                col("computed_date")
            )
            
            # æ˜¾ç¤ºæ•°æ®æ ·ä¾‹ç”¨äºè°ƒè¯•
            logger.info("å‡†å¤‡å†™å…¥MySQLçš„æ•°æ®æ ·ä¾‹:")
            mysql_ready_df.show(3, truncate=False)
            
            total_count = mysql_ready_df.count()
            logger.info(f"å‡†å¤‡UPSERT {total_count} æ¡ç”¨æˆ·æ ‡ç­¾æ•°æ®")
            
            # ç»Ÿä¸€ä½¿ç”¨UPSERTç­–ç•¥
            logger.info(f"ä½¿ç”¨ UPSERT ç­–ç•¥å†™å…¥ {total_count} æ¡æ•°æ®")
            return self._write_with_upsert(mysql_ready_df)
            
        except Exception as e:
            logger.error(f"MySQLå†™å…¥æ“ä½œå¤±è´¥: {str(e)}")
            return False
    
    def _delete_user_tags_for_date(self, computed_date):
        """åˆ é™¤æŒ‡å®šæ—¥æœŸçš„ç”¨æˆ·æ ‡ç­¾ - ä½¿ç”¨è¡Œçº§é”ï¼Œæ”¯æŒå¹¶å‘"""
        import pymysql
        from datetime import date
        
        connection = None
        try:
            connection = pymysql.connect(
                host=self.mysql_config.host,
                port=self.mysql_config.port,
                user=self.mysql_config.username,
                password=self.mysql_config.password,
                database=self.mysql_config.database,
                charset='utf8mb4',
                autocommit=True
            )
            
            cursor = connection.cursor()
            # ä½¿ç”¨DELETEä»£æ›¿TRUNCATEï¼Œæ”¯æŒå¹¶å‘
            delete_sql = "DELETE FROM user_tags WHERE computed_date = %s"
            cursor.execute(delete_sql, (computed_date,))
            deleted_count = cursor.rowcount
            logger.info(f"âœ… åˆ é™¤ {computed_date} çš„ç”¨æˆ·æ ‡ç­¾æ•°æ®ï¼Œå…± {deleted_count} æ¡")
            
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤ç”¨æˆ·æ ‡ç­¾æ•°æ®å¤±è´¥: {str(e)}")
            raise
        finally:
            if connection:
                connection.close()
    
    # ç§»é™¤äº†ä¼˜åŒ–JDBCå†™å…¥æ–¹æ³•ï¼Œç»Ÿä¸€ä½¿ç”¨foreachPartition
    
    def _merge_with_existing_tags(self, result_df: DataFrame) -> DataFrame:
        """ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶"""
        try:
            logger.info("å¼€å§‹ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶...")
            
            # è¯»å–ç°æœ‰æ ‡ç­¾
            try:
                existing_df = self.spark.read.jdbc(
                    url=self.mysql_config.jdbc_url,
                    table="user_tags",
                    properties=self.mysql_config.connection_properties
                )
                
                if existing_df.count() == 0:
                    logger.info("æ•°æ®åº“ä¸­æ²¡æœ‰ç°æœ‰æ ‡ç­¾ï¼Œç›´æ¥ä½¿ç”¨æ–°è®¡ç®—çš„æ ‡ç­¾")
                    return result_df
                    
            except Exception as e:
                logger.info(f"è¯»å–ç°æœ‰æ ‡ç­¾å¤±è´¥ï¼ˆå¯èƒ½æ˜¯é¦–æ¬¡è¿è¡Œï¼‰: {str(e)}")
                return result_df
            
            # å°†JSONå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°ç»„è¿›è¡Œåˆå¹¶
            from pyspark.sql.functions import from_json, col, array_union, array_distinct
            from pyspark.sql.types import ArrayType, IntegerType
            
            existing_with_arrays = existing_df.select(
                col("user_id"),
                from_json(col("tag_ids"), ArrayType(IntegerType())).alias("existing_tag_ids")
            )
            
            # å·¦è¿æ¥åˆå¹¶æ ‡ç­¾
            merged_df = result_df.join(
                existing_with_arrays,
                "user_id",
                "left"
            )
            
            # åˆå¹¶æ ‡ç­¾æ•°ç»„å¹¶å»é‡
            from pyspark.sql.functions import when, array_distinct, array_union
            
            final_merged_df = merged_df.select(
                col("user_id"),
                when(col("existing_tag_ids").isNull(), col("tag_ids"))
                .otherwise(array_distinct(array_union(col("existing_tag_ids"), col("tag_ids"))))
                .alias("tag_ids"),
                col("tag_details"),
                col("computed_date")
            )
            
            logger.info("âœ… æ ‡ç­¾åˆå¹¶å®Œæˆ")
            return final_merged_df
            
        except Exception as e:
            logger.error(f"æ ‡ç­¾åˆå¹¶å¤±è´¥: {str(e)}")
            return result_df
    
    def _write_with_upsert(self, df: DataFrame) -> bool:
        """ç»Ÿä¸€çš„UPSERTå†™å…¥ - ä½¿ç”¨INSERT ... ON DUPLICATE KEY UPDATE"""
        import pymysql
        
        # æå–é…ç½®å‚æ•°é¿å…é—­åŒ…åºåˆ—åŒ–é—®é¢˜
        host = self.mysql_config.host
        port = self.mysql_config.port
        username = self.mysql_config.username
        password = self.mysql_config.password
        database = self.mysql_config.database
        
        def upsert_partition_to_mysql(partition_data):
            """æ¯ä¸ªåˆ†åŒºçš„UPSERTé€»è¾‘"""
            import pymysql
            
            rows = list(partition_data)
            if not rows:
                return
                
            partition_size = len(rows)
            batch_size = 2000
            
            connection = None
            try:
                connection = pymysql.connect(
                    host=host,
                    port=port,
                    user=username,
                    password=password,
                    database=database,
                    charset='utf8mb4',
                    autocommit=False,
                    connect_timeout=30,
                    read_timeout=60,
                    write_timeout=60,
                    init_command="SET NAMES utf8mb4 COLLATE utf8mb4_unicode_ci"
                )
                
                cursor = connection.cursor()
                
                # UPSERT SQL: ç”¨æˆ·å­˜åœ¨åˆ™æ›´æ–°ï¼Œä¸å­˜åœ¨åˆ™æ’å…¥
                upsert_sql = """
                INSERT INTO user_tags (user_id, tag_ids, tag_details, computed_date) 
                VALUES (%s, %s, %s, %s)
                ON DUPLICATE KEY UPDATE
                    tag_ids = VALUES(tag_ids),
                    tag_details = VALUES(tag_details),
                    computed_date = VALUES(computed_date)
                """
                
                # åˆ†æ‰¹å¤„ç†æ•°æ®
                for i in range(0, partition_size, batch_size):
                    batch_rows = rows[i:i + batch_size]
                    batch_data = []
                    
                    for row in batch_rows:
                        user_id = str(row.user_id)
                        tag_ids = str(row.tag_ids) if row.tag_ids else '[]'
                        tag_details = str(row.tag_details) if row.tag_details else '{}'
                        computed_date = row.computed_date
                        
                        batch_data.append((user_id, tag_ids, tag_details, computed_date))
                    
                    cursor.executemany(upsert_sql, batch_data)
                
                # æäº¤äº‹åŠ¡
                connection.commit()
                
            except Exception as e:
                if connection:
                    connection.rollback()
                raise
            finally:
                if connection:
                    connection.close()
        
        try:
            # åˆç†åˆ†åŒºé¿å…è¿‡å¤šè¿æ¥
            optimal_partitions = min(8, max(1, df.count() // 8000))
            logger.info(f"ğŸ” MySQL UPSERTåˆ†åŒºè®¾ç½®ï¼š{optimal_partitions} ä¸ªåˆ†åŒº")
            repartitioned_df = df.repartition(optimal_partitions, "user_id")
            
            # è°ƒè¯•ï¼šæ£€æŸ¥é‡åˆ†åŒºåæ˜¯å¦æœ‰é‡å¤
            logger.info("ğŸ” æ£€æŸ¥é‡åˆ†åŒºåæ˜¯å¦æœ‰é‡å¤ç”¨æˆ·...")
            user_counts = repartitioned_df.groupBy("user_id").count()
            duplicates = user_counts.filter(user_counts["count"] > 1)
            duplicate_count = duplicates.count()
            if duplicate_count > 0:
                logger.error(f"âŒ å‘ç°MySQLå†™å…¥å‰æœ‰é‡å¤ç”¨æˆ·ï¼é‡å¤æ•°: {duplicate_count}")
                duplicates.show(10, truncate=False)
                return False
            else:
                logger.info("âœ… MySQLå†™å…¥å‰æ— é‡å¤ç”¨æˆ·")
            
            repartitioned_df.foreachPartition(upsert_partition_to_mysql)
            return True
            
        except Exception as e:
            logger.error(f"UPSERTå†™å…¥å¤±è´¥: {str(e)}")
            return False
    
    def _backup_current_data(self) -> bool:
        """å¤‡ä»½å½“å‰æ•°æ®"""
        try:
            backup_table_name = f"user_tags_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            # è¯»å–å½“å‰æ•°æ®
            current_df = self.spark.read.jdbc(
                url=self.mysql_config.jdbc_url,
                table="user_tags",
                properties=self.mysql_config.connection_properties
            )
            
            if current_df.count() == 0:
                logger.info("å½“å‰è¡¨ä¸ºç©ºï¼Œæ— éœ€å¤‡ä»½")
                return True
            
            # å†™å…¥å¤‡ä»½è¡¨
            current_df.write.jdbc(
                url=self.mysql_config.jdbc_url,
                table=backup_table_name,
                mode="overwrite",
                properties=self.mysql_config.connection_properties
            )
            
            logger.info(f"æ•°æ®å¤‡ä»½æˆåŠŸï¼Œå¤‡ä»½è¡¨: {backup_table_name}")
            return True
            
        except Exception as e:
            logger.error(f"æ•°æ®å¤‡ä»½å¤±è´¥: {str(e)}")
            return False
    
    def _restore_from_backup(self) -> bool:
        """ä»æœ€æ–°å¤‡ä»½æ¢å¤æ•°æ®"""
        try:
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æŸ¥è¯¢æœ€æ–°çš„å¤‡ä»½è¡¨
            # å¯ä»¥é€šè¿‡information_schema.tablesæŸ¥è¯¢å¤‡ä»½è¡¨
            logger.warning("è‡ªåŠ¨æ¢å¤åŠŸèƒ½éœ€è¦æ‰‹åŠ¨å®ç°ï¼Œè¯·æ£€æŸ¥å¤‡ä»½è¡¨")
            return False
            
        except Exception as e:
            logger.error(f"æ•°æ®æ¢å¤å¤±è´¥: {str(e)}")
            return False
    
    def _validate_write_result(self, original_df: DataFrame, mode: str = "overwrite") -> bool:
        """éªŒè¯å†™å…¥ç»“æœ - ç»Ÿä¸€ä»¥æ‰“åˆ°æ ‡ç­¾çš„ç”¨æˆ·æ•°ä¸ºæ ¸å¯¹æ ‡å‡†"""
        try:
            # è·å–éœ€è¦å†™å…¥çš„ç”¨æˆ·IDåˆ—è¡¨ï¼ˆè¿™äº›æ˜¯è®¡ç®—å‡ºæ ‡ç­¾çš„ç”¨æˆ·ï¼‰
            original_user_ids = original_df.select("user_id").distinct().collect()
            original_user_id_set = {row["user_id"] for row in original_user_ids}
            tagged_user_count = len(original_user_id_set)
            
            logger.info(f"å†™å…¥éªŒè¯ - æœ¬æ¬¡æ‰“åˆ°æ ‡ç­¾çš„ç”¨æˆ·æ•°: {tagged_user_count}, æ¨¡å¼: {mode}")
            
            if tagged_user_count == 0:
                logger.info("âœ… æ— ç”¨æˆ·æ‰“åˆ°æ ‡ç­¾ï¼ŒéªŒè¯é€šè¿‡")
                return True
            
            # è¯»å–å†™å…¥åçš„æ•°æ®ï¼Œåªæ£€æŸ¥éœ€è¦å†™å…¥çš„ç”¨æˆ·
            written_df = self.spark.read.jdbc(
                url=self.mysql_config.jdbc_url,
                table="user_tags",
                properties=self.mysql_config.connection_properties
            )
            
            # æ£€æŸ¥æ‰“åˆ°æ ‡ç­¾çš„ç”¨æˆ·æ˜¯å¦éƒ½å·²æˆåŠŸå†™å…¥
            written_user_ids = written_df.select("user_id").distinct().collect()
            written_user_id_set = {row["user_id"] for row in written_user_ids}
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ‰“åˆ°æ ‡ç­¾çš„ç”¨æˆ·éƒ½å·²æˆåŠŸå†™å…¥
            missing_users = original_user_id_set - written_user_id_set
            if missing_users:
                logger.error(f"âŒ å†™å…¥éªŒè¯å¤±è´¥ï¼šä»¥ä¸‹æ‰“åˆ°æ ‡ç­¾çš„ç”¨æˆ·æœªæˆåŠŸå†™å…¥ {list(missing_users)[:5]}...")
                return False
            
            # æ£€æŸ¥å†™å…¥çš„ç”¨æˆ·æ˜¯å¦éƒ½æœ‰æœ‰æ•ˆçš„æ ‡ç­¾æ•°æ®
            from pyspark.sql.functions import col, from_json, size
            from pyspark.sql.types import ArrayType, IntegerType
            
            # åªæ£€æŸ¥æœ¬æ¬¡æ‰“åˆ°æ ‡ç­¾çš„ç”¨æˆ·
            target_users_df = written_df.filter(col("user_id").isin(list(original_user_id_set)))
            
            # æ£€æŸ¥æ ‡ç­¾æ•°ç»„å­—æ®µï¼ˆJSONæ ¼å¼ï¼‰
            parsed_for_validation = target_users_df.select(
                "user_id",
                from_json("tag_ids", ArrayType(IntegerType())).alias("tag_ids_array")
            )
            
            # æ£€æŸ¥å…³é”®å­—æ®µ
            user_id_count = target_users_df.filter(target_users_df.user_id.isNotNull()).count()
            if user_id_count != tagged_user_count:
                logger.error("âŒ å­˜åœ¨ç©ºçš„user_id")
                return False
            
            # æ£€æŸ¥æ ‡ç­¾æ•°ç»„æ˜¯å¦æœ‰æ•ˆ
            null_tag_ids_count = parsed_for_validation.filter(
                col("tag_ids_array").isNull() | (size("tag_ids_array") == 0)
            ).count()
            
            if null_tag_ids_count > 0:
                logger.warning(f"âš ï¸ å­˜åœ¨ {null_tag_ids_count} ä¸ªç”¨æˆ·æ²¡æœ‰æ ‡ç­¾ï¼ˆå¯èƒ½æ˜¯æ­£å¸¸æƒ…å†µï¼‰")
            
            successfully_written = tagged_user_count - len(missing_users)
            logger.info(f"âœ… å†™å…¥éªŒè¯é€šè¿‡ï¼šæˆåŠŸå†™å…¥ {successfully_written}/{tagged_user_count} ä¸ªæ‰“åˆ°æ ‡ç­¾çš„ç”¨æˆ·")
            
            return True
            
        except Exception as e:
            logger.error(f"å†™å…¥ç»“æœéªŒè¯å¤±è´¥: {str(e)}")
            return False
    
    def write_incremental_tags(self, new_tags_df: DataFrame) -> bool:
        """å¢é‡å†™å…¥æ ‡ç­¾ï¼ˆæ›´æ–°å·²å­˜åœ¨çš„ç”¨æˆ·ï¼Œæ’å…¥æ–°ç”¨æˆ·ï¼‰- é€‚é…æ–°æ•°æ®æ¨¡å‹"""
        try:
            # è¯»å–ç°æœ‰æ•°æ®
            try:
                existing_df = self.spark.read.jdbc(
                    url=self.mysql_config.jdbc_url,
                    table="user_tags",
                    properties=self.mysql_config.connection_properties
                )
            except Exception:
                logger.info("è¯»å–ç°æœ‰æ•°æ®å¤±è´¥ï¼Œå¯èƒ½æ˜¯é¦–æ¬¡è¿è¡Œ")
                return self.write_tag_results(new_tags_df, mode="overwrite")
            
            if existing_df.count() == 0:
                logger.info("ç°æœ‰æ•°æ®ä¸ºç©ºï¼Œç›´æ¥å†™å…¥æ–°æ•°æ®")
                return self.write_tag_results(new_tags_df, mode="overwrite")
            
            # æ‰¾å‡ºéœ€è¦æ›´æ–°çš„ç”¨æˆ·ï¼ˆå·²å­˜åœ¨çš„ç”¨æˆ·ï¼‰
            update_users = new_tags_df.join(
                existing_df.select("user_id"), 
                "user_id", 
                "inner"
            )
            
            # æ‰¾å‡ºéœ€è¦æ’å…¥çš„æ–°ç”¨æˆ·
            insert_users = new_tags_df.join(
                existing_df.select("user_id"), 
                "user_id", 
                "left_anti"
            )
            
            update_count = update_users.count()
            insert_count = insert_users.count()
            
            logger.info(f"å¢é‡å†™å…¥ - æ›´æ–°ç”¨æˆ·æ•°: {update_count}, æ–°å¢ç”¨æˆ·æ•°: {insert_count}")
            
            # å¯¹äºæ–°æ•°æ®æ¨¡å‹ï¼Œç”±äºæ¯ä¸ªç”¨æˆ·åªæœ‰ä¸€æ¡è®°å½•ï¼Œä¸”æ ‡ç­¾æ˜¯æ•°ç»„å½¢å¼
            # æˆ‘ä»¬éœ€è¦å…ˆåˆ é™¤ç°æœ‰çš„ç”¨æˆ·è®°å½•ï¼Œç„¶åæ’å…¥æ›´æ–°çš„è®°å½•
            
            if update_count > 0:
                # æ„å»ºåˆ é™¤è¯­å¥ï¼ˆåˆ é™¤éœ€è¦æ›´æ–°çš„ç”¨æˆ·ï¼‰
                user_ids_to_update = update_users.select("user_id").distinct().collect()
                user_id_list = [f"'{row['user_id']}'" for row in user_ids_to_update]
                
                if user_id_list:
                    delete_sql = f"DELETE FROM user_tags WHERE user_id IN ({','.join(user_id_list)})"
                    
                    # æ‰§è¡Œåˆ é™¤ï¼ˆé€šè¿‡ä¸´æ—¶è¿æ¥ï¼‰
                    connection_props = {
                        "user": self.mysql_config.connection_properties["user"],
                        "password": self.mysql_config.connection_properties["password"],
                        "driver": self.mysql_config.connection_properties["driver"]
                    }
                    
                    # ä½¿ç”¨PyMySQLç›´æ¥æ‰§è¡Œåˆ é™¤ï¼Œé¿å…ä¸´æ—¶è¡¨åˆ›å»º
                    import pymysql
                    
                    connection = None
                    try:
                        connection = pymysql.connect(
                            host=self.mysql_config.host,
                            port=self.mysql_config.port,
                            user=self.mysql_config.username,
                            password=self.mysql_config.password,
                            database=self.mysql_config.database,
                            charset='utf8mb4',
                            autocommit=True
                        )
                        cursor = connection.cursor()
                        delete_sql = f"DELETE FROM user_tags WHERE user_id IN ({','.join(user_id_list)})"
                        cursor.execute(delete_sql)
                        deleted_count = cursor.rowcount
                        logger.info(f"åˆ é™¤ {deleted_count} ä¸ªç”¨æˆ·çš„æ—§æ ‡ç­¾è®°å½•")
                    except Exception as e:
                        logger.error(f"åˆ é™¤æ—§æ ‡ç­¾è®°å½•å¤±è´¥: {str(e)}")
                        raise
                    finally:
                        if connection:
                            connection.close()
                    
                    logger.info(f"åˆ é™¤ {len(user_id_list)} ä¸ªç”¨æˆ·çš„æ—§æ ‡ç­¾è®°å½•")
            
            # å†™å…¥æ‰€æœ‰æ–°æ•°æ®ï¼ˆåŒ…æ‹¬æ›´æ–°å’Œæ–°å¢çš„ç”¨æˆ·ï¼‰
            return self.write_tag_results(new_tags_df, mode="append")
            
        except Exception as e:
            logger.error(f"å¢é‡å†™å…¥å¤±è´¥: {str(e)}")
            # å¤±è´¥æ—¶å›é€€åˆ°è¦†ç›–æ¨¡å¼
            logger.info("å¢é‡å†™å…¥å¤±è´¥ï¼Œå›é€€åˆ°è¦†ç›–æ¨¡å¼")
            return self.write_tag_results(new_tags_df, mode="overwrite")
    
    def get_write_statistics(self) -> dict:
        """è·å–å†™å…¥ç»Ÿè®¡ä¿¡æ¯ - é€‚é…æ–°çš„æ•°æ®æ¨¡å‹ï¼ˆä¸€ä¸ªç”¨æˆ·ä¸€æ¡è®°å½•ï¼ŒåŒ…å«æ ‡ç­¾IDæ•°ç»„ï¼‰"""
        try:
            stats_df = self.spark.read.jdbc(
                url=self.mysql_config.jdbc_url,
                table="user_tags",
                properties=self.mysql_config.connection_properties
            )
            
            total_users = stats_df.count()
            if total_users == 0:
                return {
                    "total_users": 0,
                    "total_tag_assignments": 0,
                    "average_tags_per_user": 0,
                    "max_tags_per_user": 0,
                    "min_tags_per_user": 0,
                    "unique_tags": 0
                }
            
            # ç»Ÿè®¡æ¯ä¸ªç”¨æˆ·çš„æ ‡ç­¾æ•°ï¼ˆé€šè¿‡JSONæ•°ç»„é•¿åº¦ï¼‰
            from pyspark.sql.functions import from_json, size, expr, explode
            from pyspark.sql.types import ArrayType, IntegerType
            
            # è§£æJSONæ•°ç»„
            parsed_df = stats_df.select(
                "user_id",
                from_json("tag_ids", ArrayType(IntegerType())).alias("tag_ids_array")
            )
            
            user_tag_stats = parsed_df.select(
                "user_id",
                size("tag_ids_array").alias("tag_count")
            )
            
            tag_counts = user_tag_stats.select("tag_count").collect()
            tag_count_values = [row['tag_count'] for row in tag_counts]
            
            total_assignments = sum(tag_count_values)
            
            # ç»Ÿè®¡å”¯ä¸€æ ‡ç­¾æ•°ï¼ˆéœ€è¦å±•å¼€æ•°ç»„ï¼‰
            unique_tags_df = parsed_df.select(explode("tag_ids_array").alias("tag_id"))
            unique_tags = unique_tags_df.select("tag_id").distinct().count()
            
            return {
                "total_users": total_users,
                "total_tag_assignments": total_assignments,
                "average_tags_per_user": round(total_assignments / total_users, 2) if total_users > 0 else 0,
                "max_tags_per_user": max(tag_count_values) if tag_count_values else 0,
                "min_tags_per_user": min(tag_count_values) if tag_count_values else 0,
                "unique_tags": unique_tags
            }
            
        except Exception as e:
            logger.error(f"è·å–å†™å…¥ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {}
    
    def cleanup_old_backups(self, keep_days: int = 7) -> bool:
        """æ¸…ç†æ—§çš„å¤‡ä»½è¡¨"""
        try:
            # è¿™é‡Œéœ€è¦å®ç°å¤‡ä»½è¡¨çš„æ¸…ç†é€»è¾‘
            # æŸ¥è¯¢information_schema.tablesæ‰¾å‡ºæ—§çš„å¤‡ä»½è¡¨å¹¶åˆ é™¤
            logger.info(f"å¤‡ä»½æ¸…ç†åŠŸèƒ½å¾…å®ç°ï¼Œä¿ç•™{keep_days}å¤©å†…çš„å¤‡ä»½")
            return True
            
        except Exception as e:
            logger.error(f"æ¸…ç†å¤‡ä»½å¤±è´¥: {str(e)}")
            return False