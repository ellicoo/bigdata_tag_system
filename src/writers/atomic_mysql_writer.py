import logging
from typing import List, Dict, Any
from pyspark.sql import DataFrame, SparkSession
from concurrent.futures import ThreadPoolExecutor, as_completed
import pymysql
import json
import threading

from src.config.base import MySQLConfig

logger = logging.getLogger(__name__)


class AtomicMySQLTagWriter:
    """åŸå­åŒ–MySQLæ ‡ç­¾å†™å…¥å™¨ - æ”¯æŒå¹¶å‘å†™å…¥æ—¶çš„æ ‡ç­¾åˆå¹¶"""
    
    def __init__(self, spark: SparkSession, mysql_config: MySQLConfig):
        self.spark = spark
        self.mysql_config = mysql_config
        self._lock = threading.Lock()
    
    def write_tags_parallel_atomic(self, tag_results: List[DataFrame]) -> bool:
        """
        å¹¶è¡ŒåŸå­å†™å…¥å¤šä¸ªæ ‡ç­¾ç»“æœ
        
        æ¯ä¸ªæ ‡ç­¾ç‹¬ç«‹å†™å…¥ï¼Œé€šè¿‡MySQLåŸå­æ“ä½œè§£å†³å¹¶å‘å†²çª
        """
        logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡ŒåŸå­å†™å…¥ {len(tag_results)} ä¸ªæ ‡ç­¾ç»“æœ")
        
        success_count = 0
        failed_count = 0
        
        def write_single_tag_atomic(tag_df: DataFrame):
            """åŸå­å†™å…¥å•ä¸ªæ ‡ç­¾çš„ç»“æœ"""
            try:
                # æ”¶é›†è¯¥æ ‡ç­¾çš„æ‰€æœ‰ç”¨æˆ·ç»“æœ
                tag_data = tag_df.collect()
                if not tag_data:
                    return True, "ç©ºç»“æœ"
                
                tag_id = tag_data[0]['tag_id']
                logger.info(f"å¼€å§‹åŸå­å†™å…¥æ ‡ç­¾ {tag_id}")
                
                # ä¸ºæ¯ä¸ªç”¨æˆ·åŸå­æ›´æ–°æ ‡ç­¾
                success_users = 0
                for row in tag_data:
                    user_id = row['user_id']
                    if self._atomic_merge_user_tag(user_id, tag_id, row):
                        success_users += 1
                
                logger.info(f"âœ… æ ‡ç­¾ {tag_id} å†™å…¥å®Œæˆï¼ŒæˆåŠŸç”¨æˆ·æ•°: {success_users}/{len(tag_data)}")
                return True, f"æˆåŠŸç”¨æˆ·æ•°: {success_users}/{len(tag_data)}"
                
            except Exception as e:
                logger.error(f"âŒ æ ‡ç­¾åŸå­å†™å…¥å¤±è´¥: {str(e)}")
                return False, str(e)
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå†™å…¥ä¸åŒæ ‡ç­¾
        max_workers = min(4, len(tag_results))
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰å†™å…¥ä»»åŠ¡
            future_to_tag = {
                executor.submit(write_single_tag_atomic, tag_df): i 
                for i, tag_df in enumerate(tag_results)
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_tag):
                tag_index = future_to_tag[future]
                try:
                    success, message = future.result(timeout=180)  # 3åˆ†é’Ÿè¶…æ—¶
                    if success:
                        success_count += 1
                        logger.info(f"æ ‡ç­¾ {tag_index} å†™å…¥æˆåŠŸ: {message}")
                    else:
                        failed_count += 1
                        logger.error(f"æ ‡ç­¾ {tag_index} å†™å…¥å¤±è´¥: {message}")
                        
                except Exception as e:
                    failed_count += 1
                    logger.error(f"æ ‡ç­¾ {tag_index} å†™å…¥è¶…æ—¶æˆ–å¼‚å¸¸: {str(e)}")
        
        logger.info(f"ğŸ‰ å¹¶è¡ŒåŸå­å†™å…¥å®Œæˆ - æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}")
        return failed_count == 0
    
    def _atomic_merge_user_tag(self, user_id: str, tag_id: int, tag_row) -> bool:
        """
        åŸå­åˆå¹¶å•ä¸ªç”¨æˆ·çš„æ ‡ç­¾ - ä½¿ç”¨MySQLçš„JSONå‡½æ•°å®ç°åŸå­æ“ä½œ
        """
        connection = None
        try:
            # åˆ›å»ºæ•°æ®åº“è¿æ¥
            connection = pymysql.connect(
                host=self.mysql_config.host,
                port=self.mysql_config.port,
                user=self.mysql_config.username,
                password=self.mysql_config.password,
                database=self.mysql_config.database,
                charset='utf8mb4'
            )
            
            with connection.cursor() as cursor:
                # æ„å»ºæ ‡ç­¾è¯¦æƒ…
                tag_detail = {
                    str(tag_id): {
                        'tag_name': getattr(tag_row, 'tag_name', ''),
                        'tag_category': getattr(tag_row, 'tag_category', ''),
                        'computed_time': getattr(tag_row, 'computed_date', '').strftime('%Y-%m-%d') if hasattr(tag_row, 'computed_date') else '',
                        'tag_detail': getattr(tag_row, 'tag_detail', '{}')
                    }
                }
                
                # ä½¿ç”¨MySQLçš„åŸå­æ“ä½œè¿›è¡Œæ ‡ç­¾åˆå¹¶
                merge_sql = """
                INSERT INTO user_tags (user_id, tag_ids, tag_details, computed_date) 
                VALUES (%s, JSON_ARRAY(%s), %s, CURDATE())
                ON DUPLICATE KEY UPDATE 
                    tag_ids = JSON_MERGE_PRESERVE(
                        tag_ids, 
                        JSON_ARRAY(%s)
                    ),
                    tag_details = JSON_MERGE_PATCH(
                        tag_details,
                        %s
                    )
                """
                
                tag_detail_json = json.dumps(tag_detail, ensure_ascii=False)
                
                cursor.execute(merge_sql, (
                    user_id,           # INSERTçš„tag_ids
                    tag_id,
                    tag_detail_json,   # INSERTçš„tag_details
                    tag_id,           # UPDATEçš„tag_idsï¼ˆè¦åˆå¹¶çš„æ–°æ ‡ç­¾ï¼‰
                    tag_detail_json    # UPDATEçš„tag_detailsï¼ˆè¦åˆå¹¶çš„æ–°è¯¦æƒ…ï¼‰
                ))
                
                connection.commit()
                return True
                
        except Exception as e:
            logger.error(f"ç”¨æˆ· {user_id} æ ‡ç­¾ {tag_id} åŸå­åˆå¹¶å¤±è´¥: {str(e)}")
            if connection:
                connection.rollback()
            return False
            
        finally:
            if connection:
                connection.close()
    
    def write_tags_batch_atomic(self, tag_results: List[DataFrame]) -> bool:
        """
        æ‰¹é‡åŸå­å†™å…¥ - å…ˆåˆå¹¶æ‰€æœ‰æ ‡ç­¾ï¼Œå†æ‰¹é‡åŸå­å†™å…¥
        """
        logger.info(f"å¼€å§‹æ‰¹é‡åŸå­å†™å…¥ {len(tag_results)} ä¸ªæ ‡ç­¾ç»“æœ")
        
        try:
            # 1. åˆå¹¶æ‰€æœ‰æ ‡ç­¾ç»“æœåˆ°ç”¨æˆ·ç»´åº¦
            user_tag_map = self._merge_all_tag_results(tag_results)
            
            # 2. æ‰¹é‡åŸå­å†™å…¥
            return self._batch_atomic_write(user_tag_map)
            
        except Exception as e:
            logger.error(f"æ‰¹é‡åŸå­å†™å…¥å¤±è´¥: {str(e)}")
            return False
    
    def _merge_all_tag_results(self, tag_results: List[DataFrame]) -> Dict[str, Dict]:
        """åˆå¹¶æ‰€æœ‰æ ‡ç­¾ç»“æœåˆ°ç”¨æˆ·ç»´åº¦"""
        from functools import reduce
        
        # åˆå¹¶æ‰€æœ‰æ ‡ç­¾ç»“æœ
        all_tags = reduce(lambda df1, df2: df1.union(df2), tag_results)
        
        # æ”¶é›†æ•°æ®å¹¶æŒ‰ç”¨æˆ·åˆ†ç»„
        all_data = all_tags.collect()
        user_tag_map = {}
        
        for row in all_data:
            user_id = row['user_id']
            tag_id = row['tag_id']
            
            if user_id not in user_tag_map:
                user_tag_map[user_id] = {
                    'tag_ids': [],
                    'tag_details': {}
                }
            
            user_tag_map[user_id]['tag_ids'].append(tag_id)
            user_tag_map[user_id]['tag_details'][str(tag_id)] = {
                'tag_name': getattr(row, 'tag_name', ''),
                'tag_category': getattr(row, 'tag_category', ''),
                'tag_detail': getattr(row, 'tag_detail', '{}')
            }
        
        return user_tag_map
    
    def _batch_atomic_write(self, user_tag_map: Dict[str, Dict]) -> bool:
        """æ‰¹é‡åŸå­å†™å…¥ç”¨æˆ·æ ‡ç­¾"""
        connection = None
        try:
            connection = pymysql.connect(
                host=self.mysql_config.host,
                port=self.mysql_config.port,
                user=self.mysql_config.username,
                password=self.mysql_config.password,
                database=self.mysql_config.database,
                charset='utf8mb4'
            )
            
            success_count = 0
            
            with connection.cursor() as cursor:
                for user_id, user_data in user_tag_map.items():
                    try:
                        tag_ids_json = json.dumps(user_data['tag_ids'])
                        tag_details_json = json.dumps(user_data['tag_details'], ensure_ascii=False)
                        
                        # åŸå­åˆå¹¶SQL
                        merge_sql = """
                        INSERT INTO user_tags (user_id, tag_ids, tag_details, computed_date) 
                        VALUES (%s, %s, %s, CURDATE())
                        ON DUPLICATE KEY UPDATE 
                            tag_ids = JSON_MERGE_PRESERVE(tag_ids, VALUES(tag_ids)),
                            tag_details = JSON_MERGE_PATCH(tag_details, VALUES(tag_details))
                        """
                        
                        cursor.execute(merge_sql, (user_id, tag_ids_json, tag_details_json))
                        success_count += 1
                        
                    except Exception as e:
                        logger.error(f"ç”¨æˆ· {user_id} æ‰¹é‡å†™å…¥å¤±è´¥: {str(e)}")
                        continue
                
                connection.commit()
                logger.info(f"âœ… æ‰¹é‡åŸå­å†™å…¥å®Œæˆï¼ŒæˆåŠŸç”¨æˆ·æ•°: {success_count}/{len(user_tag_map)}")
                return success_count > 0
                
        except Exception as e:
            logger.error(f"æ‰¹é‡åŸå­å†™å…¥å¤±è´¥: {str(e)}")
            if connection:
                connection.rollback()
            return False
            
        finally:
            if connection:
                connection.close()
    
    def get_atomic_write_statistics(self) -> dict:
        """è·å–åŸå­å†™å…¥åçš„ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats_df = self.spark.read.jdbc(
                url=self.mysql_config.jdbc_url,
                table="user_tags",
                properties=self.mysql_config.connection_properties
            )
            
            total_users = stats_df.count()
            if total_users == 0:
                return {"total_users": 0, "total_tag_assignments": 0}
            
            # ç»Ÿè®¡æ ‡ç­¾åˆ†é…ï¼ˆéœ€è¦è§£æJSONæ•°ç»„ï¼‰
            from pyspark.sql.functions import from_json, size
            from pyspark.sql.types import ArrayType, IntegerType
            
            parsed_df = stats_df.select(
                "user_id",
                from_json("tag_ids", ArrayType(IntegerType())).alias("tag_ids_array")
            )
            
            tag_stats = parsed_df.select("user_id", size("tag_ids_array").alias("tag_count"))
            total_assignments = tag_stats.agg({"tag_count": "sum"}).collect()[0][0]
            
            return {
                "total_users": total_users,
                "total_tag_assignments": total_assignments,
                "avg_tags_per_user": round(total_assignments / total_users, 2) if total_users > 0 else 0
            }
            
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}