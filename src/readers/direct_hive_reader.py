"""
ç›´æ¥Hiveè¡¨è¯»å–å™¨
åŸºäºæ‚¨æä¾›çš„è¯»è¡¨æ–¹å¼ï¼šspark.sql("MSCK REPAIR TABLE xxx") + spark.table("xxx").where("dt = 'xxx'")
"""

import logging
from typing import Dict, List, Optional
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col

from src.config.base import BaseConfig


class DirectHiveReader:
    """ç›´æ¥Hiveè¡¨è¯»å–å™¨ - åŸºäºæ‚¨çš„HiveToKafka.pyæ–¹å¼"""
    
    def __init__(self, spark: SparkSession, config: BaseConfig):
        self.spark = spark
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # è·å–Hiveè¡¨åé…ç½®
        self.table_names = config.get_hive_table_names()
        
    def repair_and_read_table(self, table_name: str, dt: str = None) -> DataFrame:
        """
        ä¿®å¤åˆ†åŒºå¹¶è¯»å–Hiveè¡¨ - åŸºäºæ‚¨çš„æ–¹å¼
        
        Args:
            table_name: Hiveè¡¨åï¼ˆå¦‚ tag_test.user_basic_infoï¼‰
            dt: æ•°æ®æ—¥æœŸåˆ†åŒº
            
        Returns:
            DataFrame: è¯»å–çš„æ•°æ®
        """
        try:
            # 1. ä¿®å¤åˆ†åŒºï¼ˆåŸºäºæ‚¨çš„æ–¹å¼ï¼‰
            repair_sql = f"MSCK REPAIR TABLE {table_name}"
            self.logger.info(f"ğŸ”§ ä¿®å¤åˆ†åŒº: {repair_sql}")
            self.spark.sql(repair_sql)
            
            # 2. è¯»å–è¡¨æ•°æ®ï¼ˆåŸºäºæ‚¨çš„æ–¹å¼ï¼‰
            if dt:
                df = self.spark.table(table_name).where(f"dt = '{dt}'")
                self.logger.info(f"ğŸ“Š è¯»å–è¡¨ {table_name}ï¼Œæ—¥æœŸåˆ†åŒº: {dt}")
            else:
                df = self.spark.table(table_name)
                self.logger.info(f"ğŸ“Š è¯»å–è¡¨ {table_name}ï¼Œå…¨é‡æ•°æ®")
            
            # 3. éªŒè¯æ•°æ®
            count = df.count()
            self.logger.info(f"âœ… è¡¨ {table_name} è¯»å–æˆåŠŸï¼Œè®°å½•æ•°: {count}")
            
            if count == 0:
                self.logger.warning(f"âš ï¸ è¡¨ {table_name} æ— æ•°æ®")
            
            return df
            
        except Exception as e:
            self.logger.error(f"âŒ è¯»å–è¡¨ {table_name} å¤±è´¥: {str(e)}")
            raise
    
    def read_user_basic_info(self, dt: str = None) -> DataFrame:
        """è¯»å–ç”¨æˆ·åŸºæœ¬ä¿¡æ¯è¡¨"""
        dt = dt or self.config.data_date
        table_name = self.table_names["user_basic_info"]
        
        df = self.repair_and_read_table(table_name, dt)
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        required_fields = ['user_id', 'age', 'user_level', 'kyc_status', 'registration_date', 'risk_score']
        missing_fields = [field for field in required_fields if field not in df.columns]
        
        if missing_fields:
            self.logger.warning(f"âš ï¸ ç”¨æˆ·åŸºæœ¬ä¿¡æ¯è¡¨ç¼ºå°‘å­—æ®µ: {missing_fields}")
        
        return df
    
    def read_user_asset_summary(self, dt: str = None) -> DataFrame:
        """è¯»å–ç”¨æˆ·èµ„äº§æ±‡æ€»è¡¨"""
        dt = dt or self.config.data_date
        table_name = self.table_names["user_asset_summary"]
        
        df = self.repair_and_read_table(table_name, dt)
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        required_fields = ['user_id', 'total_asset_value', 'cash_balance']
        missing_fields = [field for field in required_fields if field not in df.columns]
        
        if missing_fields:
            self.logger.warning(f"âš ï¸ ç”¨æˆ·èµ„äº§è¡¨ç¼ºå°‘å­—æ®µ: {missing_fields}")
        
        return df
    
    def read_user_activity_summary(self, dt: str = None) -> DataFrame:
        """è¯»å–ç”¨æˆ·æ´»åŠ¨æ±‡æ€»è¡¨"""
        dt = dt or self.config.data_date
        table_name = self.table_names["user_activity_summary"]
        
        df = self.repair_and_read_table(table_name, dt)
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        required_fields = ['user_id', 'trade_count_30d', 'last_login_date']
        missing_fields = [field for field in required_fields if field not in df.columns]
        
        if missing_fields:
            self.logger.warning(f"âš ï¸ ç”¨æˆ·æ´»åŠ¨è¡¨ç¼ºå°‘å­—æ®µ: {missing_fields}")
        
        return df
    
    def read_all_user_data(self, dt: str = None) -> Dict[str, DataFrame]:
        """
        è¯»å–æ‰€æœ‰ç”¨æˆ·æ•°æ®è¡¨
        
        Returns:
            Dict[str, DataFrame]: åŒ…å«æ‰€æœ‰ç”¨æˆ·æ•°æ®çš„å­—å…¸
        """
        dt = dt or self.config.data_date
        self.logger.info(f"ğŸš€ å¼€å§‹è¯»å–æ‰€æœ‰ç”¨æˆ·æ•°æ®ï¼Œæ—¥æœŸ: {dt}")
        
        try:
            # å¹¶è¡Œè¯»å–æ‰€æœ‰è¡¨
            user_basic = self.read_user_basic_info(dt)
            user_asset = self.read_user_asset_summary(dt)
            user_activity = self.read_user_activity_summary(dt)
            
            # ç¼“å­˜æ•°æ®ä»¥æé«˜æ€§èƒ½
            if self.config.enable_cache:
                user_basic.cache()
                user_asset.cache()
                user_activity.cache()
                self.logger.info("ğŸ“¦ æ•°æ®å·²ç¼“å­˜åˆ°å†…å­˜")
            
            result = {
                "user_basic_info": user_basic,
                "user_asset_summary": user_asset,
                "user_activity_summary": user_activity
            }
            
            self.logger.info("âœ… æ‰€æœ‰ç”¨æˆ·æ•°æ®è¯»å–å®Œæˆ")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ è¯»å–ç”¨æˆ·æ•°æ®å¤±è´¥: {str(e)}")
            raise
    
    def get_available_partitions(self, table_name: str) -> List[str]:
        """
        è·å–è¡¨çš„å¯ç”¨åˆ†åŒº
        
        Args:
            table_name: è¡¨å
            
        Returns:
            List[str]: åˆ†åŒºåˆ—è¡¨
        """
        try:
            partitions_df = self.spark.sql(f"SHOW PARTITIONS {table_name}")
            partitions = [row[0] for row in partitions_df.collect()]
            
            # æå–dtåˆ†åŒºå€¼
            dt_partitions = []
            for partition in partitions:
                if partition.startswith('dt='):
                    dt_value = partition.replace('dt=', '')
                    dt_partitions.append(dt_value)
            
            self.logger.info(f"ğŸ“… è¡¨ {table_name} å¯ç”¨åˆ†åŒº: {dt_partitions}")
            return dt_partitions
            
        except Exception as e:
            self.logger.error(f"âŒ è·å–åˆ†åŒºå¤±è´¥: {str(e)}")
            return []
    
    def validate_table_access(self) -> bool:
        """éªŒè¯Hiveè¡¨è®¿é—®æƒé™"""
        try:
            self.logger.info("ğŸ” éªŒè¯Hiveè¡¨è®¿é—®æƒé™...")
            
            # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨
            databases = self.spark.sql("SHOW DATABASES").collect()
            db_names = [row[0] for row in databases]
            
            if self.config.hive_database not in db_names:
                self.logger.error(f"âŒ æ•°æ®åº“ {self.config.hive_database} ä¸å­˜åœ¨")
                self.logger.info(f"ğŸ“‹ å¯ç”¨æ•°æ®åº“: {db_names}")
                return False
            
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            tables = self.spark.sql(f"SHOW TABLES IN {self.config.hive_database}").collect()
            table_names = [row[1] for row in tables]  # row[1]æ˜¯è¡¨å
            
            required_tables = ['user_basic_info', 'user_asset_summary', 'user_activity_summary']
            missing_tables = [table for table in required_tables if table not in table_names]
            
            if missing_tables:
                self.logger.error(f"âŒ ç¼ºå°‘å¿…éœ€çš„è¡¨: {missing_tables}")
                self.logger.info(f"ğŸ“‹ å¯ç”¨è¡¨: {table_names}")
                return False
            
            self.logger.info("âœ… Hiveè¡¨è®¿é—®æƒé™éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Hiveè¡¨è®¿é—®éªŒè¯å¤±è´¥: {str(e)}")
            return False
    
    def show_table_schema(self, table_name: str = None):
        """æ˜¾ç¤ºè¡¨ç»“æ„"""
        tables_to_show = [table_name] if table_name else list(self.table_names.values())
        
        for table in tables_to_show:
            try:
                self.logger.info(f"ğŸ“‹ è¡¨ {table} ç»“æ„:")
                schema_df = self.spark.sql(f"DESCRIBE {table}")
                schema_df.show(truncate=False)
                
                # æ˜¾ç¤ºæ ·ä¾‹æ•°æ®
                sample_df = self.spark.table(table).limit(3)
                self.logger.info(f"ğŸ“Š è¡¨ {table} æ ·ä¾‹æ•°æ®:")
                sample_df.show(truncate=False)
                
            except Exception as e:
                self.logger.error(f"âŒ æ— æ³•æ˜¾ç¤ºè¡¨ {table} ç»“æ„: {str(e)}")