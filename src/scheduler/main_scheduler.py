import logging
import time
import json
from typing import List, Dict, Any, Optional
from pyspark.sql import SparkSession

from src.config.base import BaseConfig
from src.readers.hive_reader import HiveDataReader
from src.readers.rule_reader import RuleReader
from src.engine.tag_computer import TagComputeEngine
from src.writers.optimized_mysql_writer import OptimizedMySQLWriter
from src.merger.tag_merger import TagMerger
from src.scheduler.scenario_scheduler import ScenarioScheduler

logger = logging.getLogger(__name__)


class TagComputeScheduler:
    """æ ‡ç­¾è®¡ç®—ä¸»è°ƒåº¦å™¨"""
    
    def __init__(self, config: BaseConfig, parallel_mode=False, atomic_mode=False, max_workers=4):
        self.config = config
        self.spark = None
        self.parallel_mode = parallel_mode
        self.atomic_mode = atomic_mode
        self.max_workers = max_workers
        
        # ç»„ä»¶åˆå§‹åŒ– - æ¢å¤æ¨¡å—åŒ–æ¶æ„
        self.rule_reader = None
        self.hive_reader = None
        self.tag_engine = None
        self.mysql_writer = None
        self.tag_merger = None
        
        # æ–°å¢åœºæ™¯è°ƒåº¦å™¨
        self.scenario_scheduler = None
    
    def initialize(self):
        """åˆå§‹åŒ–Sparkå’Œå„ä¸ªç»„ä»¶"""
        try:
            logger.info("å¼€å§‹åˆå§‹åŒ–æ ‡ç­¾è®¡ç®—ç³»ç»Ÿ...")
            
            # åˆå§‹åŒ–Spark
            self.spark = self._create_spark_session()
            
            # åˆå§‹åŒ–å„ä¸ªç»„ä»¶ - æ¢å¤æ¨¡å—åŒ–æ¶æ„
            self.rule_reader = RuleReader(self.spark, self.config.mysql)
            self.hive_reader = HiveDataReader(self.spark, self.config.s3)
            self.tag_engine = TagComputeEngine(self.spark, self.max_workers)
            self.mysql_writer = OptimizedMySQLWriter(self.spark, self.config.mysql)
            self.tag_merger = TagMerger(self.spark, self.config.mysql)
            
            # åˆå§‹åŒ–åœºæ™¯è°ƒåº¦å™¨
            self.scenario_scheduler = ScenarioScheduler(self.config, self.max_workers)
            self.scenario_scheduler.initialize()
            
            # ä¸€æ¬¡æ€§åˆå§‹åŒ–è§„åˆ™æ•°æ®ï¼Œé¿å…é‡å¤è¿æ¥
            self.rule_reader.initialize()
            
            logger.info("âœ… æ ‡ç­¾è®¡ç®—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _create_spark_session(self) -> SparkSession:
        """åˆ›å»ºSparkä¼šè¯"""
        builder = SparkSession.builder
        
        # åº”ç”¨Sparké…ç½®
        for key, value in self.config.spark.to_dict().items():
            builder = builder.config(key, value)
        
        # åº”ç”¨S3é…ç½®
        for key, value in self.config.s3.to_spark_config().items():
            builder = builder.config(key, value)
        
        spark = builder.getOrCreate()
        spark.sparkContext.setLogLevel("WARN")  # å‡å°‘æ—¥å¿—è¾“å‡º
        
        logger.info(f"Sparkä¼šè¯åˆ›å»ºæˆåŠŸ: {spark.sparkContext.applicationId}")
        return spark
    
    def run_full_tag_compute(self) -> bool:
        """è¿è¡Œå®Œæ•´çš„æ ‡ç­¾è®¡ç®—æµç¨‹"""
        try:
            start_time = time.time()
            logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œå®Œæ•´æ ‡ç­¾è®¡ç®—...")
            
            # 1. ä»è§„åˆ™è¯»å–å™¨è·å–æ‰€æœ‰æ´»è·ƒçš„æ ‡ç­¾è§„åˆ™ï¼ˆä½¿ç”¨persistç¼“å­˜ï¼‰
            rules = self.rule_reader.read_active_rules()
            if not rules:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„æ ‡ç­¾è§„åˆ™")
                return False
            
            logger.info(f"å…±æ‰¾åˆ° {len(rules)} ä¸ªæ´»è·ƒæ ‡ç­¾è§„åˆ™")
            
            # 2. ç®€åŒ–å¤„ç†ï¼šç›´æ¥å¤„ç†æ‰€æœ‰æ ‡ç­¾è§„åˆ™
            all_tag_results = []
            
            try:
                # ä½¿ç”¨æœ¬åœ°ç”Ÿæˆçš„æµ‹è¯•æ•°æ®ï¼ˆæ¨¡æ‹Ÿç”Ÿäº§åœºæ™¯æ•°æ®ç»“æ„ï¼‰
                logger.info("ç”Ÿæˆç”Ÿäº§çº§æ¨¡æ‹Ÿç”¨æˆ·æ•°æ®...")
                test_data = self._generate_production_like_data()
                
                # è§„åˆ™å·²ç»æ˜¯å­—å…¸æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                logger.info(f"å¼€å§‹å¹¶è¡Œè®¡ç®— {len(rules)} ä¸ªæ ‡ç­¾...")
                tag_results = self.tag_engine.compute_tags_parallel(test_data, rules)
                
                all_tag_results.extend(tag_results)
                
                logger.info(f"âœ… æ ‡ç­¾è®¡ç®—å®Œæˆï¼ŒæˆåŠŸè®¡ç®— {len(tag_results)} ä¸ªæ ‡ç­¾")
                
            except Exception as e:
                logger.error(f"âŒ æ ‡ç­¾è®¡ç®—å¤±è´¥: {str(e)}")
                raise
            
            if not all_tag_results:
                logger.warning("æ²¡æœ‰æˆåŠŸè®¡ç®—å‡ºä»»ä½•æ ‡ç­¾")
                return False
            
            # 4. å…¨é‡æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨tag_mergerè¿›è¡Œå®Œæ•´çš„æ ‡ç­¾åˆå¹¶ï¼ˆåŒ…å«å†…å­˜åˆå¹¶+æ•°æ®åº“åˆå¹¶ï¼‰
            logger.info("å…¨é‡æ¨¡å¼ï¼šä½¿ç”¨tag_mergerè¿›è¡Œå®Œæ•´æ ‡ç­¾åˆå¹¶...")
            final_merged_result = self.tag_merger.merge_user_tags(all_tag_results)
            
            if final_merged_result is None:
                logger.error("æ ‡ç­¾åˆå¹¶å¤±è´¥")
                return False
            
            # 5. å†™å…¥åˆå¹¶åçš„æ ‡ç­¾ç»“æœï¼ˆä½¿ç”¨MySQLå†™å…¥å™¨ï¼‰
            logger.info("å¼€å§‹å†™å…¥æ ‡ç­¾ç»“æœ...")
            # å…¨é‡æ¨¡å¼ï¼šä¸éœ€è¦ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶
            write_success = self.mysql_writer.write_tag_results(final_merged_result, mode="overwrite", merge_with_existing=False)
            
            if not write_success:
                logger.error("æ ‡ç­¾ç»“æœå†™å…¥å¤±è´¥")
                return False
            
            # 7. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            end_time = time.time()
            execution_time = end_time - start_time
            
            stats = self.mysql_writer.get_write_statistics()
            
            logger.info(f"""
ğŸ‰ æ ‡ç­¾è®¡ç®—å®Œæˆï¼
â±ï¸  æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’
ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
   - æ€»ç”¨æˆ·æ•°: {stats.get('total_users', 'N/A')}
   - å¹³å‡æ ‡ç­¾æ•°/ç”¨æˆ·: {stats.get('average_tags_per_user', 'N/A')}
   - æœ€å¤§æ ‡ç­¾æ•°/ç”¨æˆ·: {stats.get('max_tags_per_user', 'N/A')}
   - å¤„ç†çš„æ ‡ç­¾è§„åˆ™æ•°: {len(rules)}
   - æˆåŠŸè®¡ç®—çš„æ ‡ç­¾æ•°: {len(all_tag_results)}
            """)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ ‡ç­¾è®¡ç®—æµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}")
            return False
    
    def run_incremental_compute(self, days_back: int = 1) -> bool:
        """è¿è¡Œå¢é‡æ ‡ç­¾è®¡ç®— - æ­£ç¡®é€»è¾‘ï¼šè¯†åˆ«çœŸæ­£çš„æ–°å¢ç”¨æˆ·"""
        try:
            logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œå¢é‡æ ‡ç­¾è®¡ç®—ï¼Œå›æº¯ {days_back} å¤©...")
            
            # 1. è¯»å–æ‰€æœ‰æ´»è·ƒçš„æ ‡ç­¾è§„åˆ™ï¼ˆæ–°å¢ç”¨æˆ·éœ€è¦è®¡ç®—æ‰€æœ‰æ ‡ç­¾ï¼‰
            rules = self.rule_reader.read_active_rules()
            if not rules:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„æ ‡ç­¾è§„åˆ™")
                return False
            
            logger.info(f"å¢é‡æ¨¡å¼ï¼šå¯¹æ–°å¢ç”¨æˆ·è®¡ç®—æ‰€æœ‰ {len(rules)} ä¸ªæ ‡ç­¾è§„åˆ™")
            
            # 2. è¯†åˆ«çœŸæ­£çš„æ–°å¢ç”¨æˆ·
            new_users_data = self._identify_truly_new_users(days_back)
            
            if new_users_data.count() == 0:
                logger.info("æ²¡æœ‰å‘ç°æ–°å¢ç”¨æˆ·")
                return True
            
            logger.info(f"å‘ç° {new_users_data.count()} ä¸ªçœŸæ­£çš„æ–°å¢ç”¨æˆ·")
            
            # 3. å¯¹æ–°å¢ç”¨æˆ·è®¡ç®—æ‰€æœ‰æ ‡ç­¾è§„åˆ™
            all_tag_results = []
            try:
                # è®¡ç®—æ‰€æœ‰æ ‡ç­¾ï¼ˆæ–°ç”¨æˆ·éœ€è¦å…¨é‡æ‰“æ ‡ç­¾ï¼‰
                tag_results = self.tag_engine.compute_tags_parallel(new_users_data, rules)
                all_tag_results.extend(tag_results)
                
                logger.info(f"âœ… ä¸ºæ–°å¢ç”¨æˆ·è®¡ç®—å®Œæˆ {len(tag_results)} ä¸ªæ ‡ç­¾")
                
                # è°ƒè¯•ï¼šæ£€æŸ¥æ ‡ç­¾è®¡ç®—ç»“æœ
                total_tag_records = sum(df.count() for df in tag_results)
                logger.info(f"ğŸ” æ ‡ç­¾è®¡ç®—äº§ç”Ÿ {total_tag_records} æ¡æ ‡ç­¾è®°å½•")
                if total_tag_records > 0 and tag_results:
                    logger.info("æ ‡ç­¾è®¡ç®—ç»“æœç¤ºä¾‹:")
                    tag_results[0].select("user_id", "tag_id").show(5, truncate=False)
                
            except Exception as e:
                logger.error(f"âŒ æ–°å¢ç”¨æˆ·æ ‡ç­¾è®¡ç®—å¤±è´¥: {str(e)}")
                raise
            
            if not all_tag_results:
                logger.info("æ–°å¢ç”¨æˆ·æ²¡æœ‰å‘½ä¸­ä»»ä½•æ ‡ç­¾")
                return True
            
            # 4. å†…å­˜åˆå¹¶æ–°ç”¨æˆ·çš„å¤šä¸ªæ ‡ç­¾
            logger.info("å†…å­˜åˆå¹¶æ–°å¢ç”¨æˆ·å¤šæ ‡ç­¾...")
            merged_result = self._merge_user_multi_tags_in_memory(all_tag_results)
            
            if merged_result is None:
                logger.error("æ–°å¢ç”¨æˆ·æ ‡ç­¾å†…å­˜åˆå¹¶å¤±è´¥")
                return False
            
            # è°ƒè¯•ï¼šæ£€æŸ¥åˆå¹¶ç»“æœ
            merged_count = merged_result.count()
            logger.info(f"ğŸ” å†…å­˜åˆå¹¶åå¾—åˆ° {merged_count} ä¸ªæœ‰æ ‡ç­¾çš„æ–°å¢ç”¨æˆ·")
            if merged_count > 0:
                logger.info("æ–°å¢ç”¨æˆ·æ ‡ç­¾ç¤ºä¾‹:")
                merged_result.select("user_id", "tag_ids").show(5, truncate=False)
            
            # 5. ç›´æ¥è¿½åŠ å†™å…¥ï¼ˆæ–°ç”¨æˆ·ä¸å­˜åœ¨äºæ•°æ®åº“ä¸­ï¼‰
            logger.info("æ–°å¢ç”¨æˆ·æ ‡ç­¾ç›´æ¥è¿½åŠ åˆ°æ•°æ®åº“...")
            # å¢é‡æ¨¡å¼ï¼šä¸éœ€è¦ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶ï¼ˆæ–°ç”¨æˆ·ï¼‰
            return self.mysql_writer.write_tag_results(merged_result, mode="append", merge_with_existing=False)
            
        except Exception as e:
            logger.error(f"å¢é‡è®¡ç®—å¤±è´¥: {str(e)}")
            return False
    
    def run_specific_tags(self, tag_ids: List[int]) -> bool:
        """è¿è¡ŒæŒ‡å®šæ ‡ç­¾çš„è®¡ç®—"""
        try:
            logger.info(f"ğŸ¯ å¼€å§‹è®¡ç®—æŒ‡å®šæ ‡ç­¾: {tag_ids}")
            
            # è¯»å–æŒ‡å®šæ ‡ç­¾çš„è§„åˆ™
            all_rules = self.rule_reader.read_active_rules()
            target_rules = [rule for rule in all_rules if rule['tag_id'] in tag_ids]
            
            if not target_rules:
                logger.warning(f"æ²¡æœ‰æ‰¾åˆ°æŒ‡å®šæ ‡ç­¾çš„è§„åˆ™: {tag_ids}")
                return False
            
            # ç®€åŒ–å¤„ç†ï¼šç›´æ¥è®¡ç®—æŒ‡å®šæ ‡ç­¾
            all_tag_results = []
            
            try:
                # ç”Ÿæˆæµ‹è¯•ç”¨æˆ·æ•°æ®
                test_data = self._generate_test_user_data()
                
                # è®¡ç®—æŒ‡å®šæ ‡ç­¾
                tag_results = self.tag_engine.compute_batch_tags(test_data, target_rules)
                all_tag_results.extend(tag_results)
                
            except Exception as e:
                logger.error(f"è®¡ç®—æŒ‡å®šæ ‡ç­¾å¤±è´¥: {str(e)}")
                raise
            
            if not all_tag_results:
                logger.warning("æŒ‡å®šæ ‡ç­¾æ²¡æœ‰è®¡ç®—å‡ºç»“æœ")
                return False
            
            # åˆå¹¶å’Œå†™å…¥
            merged_result = self.tag_merger.merge_user_tags(all_tag_results)
            if merged_result is None:
                return False
            
            # æŒ‡å®šæ ‡ç­¾æ¨¡å¼ï¼šéœ€è¦ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶
            return self.mysql_writer.write_tag_results(merged_result, mode="overwrite", merge_with_existing=True)
            
        except Exception as e:
            logger.error(f"æŒ‡å®šæ ‡ç­¾è®¡ç®—å¤±è´¥: {str(e)}")
            return False
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # æ¸…ç†è§„åˆ™è¯»å–å™¨çš„persistç¼“å­˜
            if self.rule_reader:
                self.rule_reader.cleanup()
            
            if self.spark:
                self.spark.stop()
                
            logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"èµ„æºæ¸…ç†å¤±è´¥: {str(e)}")
    
    def _generate_production_like_data(self):
        """ç”Ÿæˆç”Ÿäº§çº§æ¨¡æ‹Ÿç”¨æˆ·æ•°æ®ï¼ˆç¬¦åˆç”Ÿäº§åœºæ™¯çš„æ•°æ®åˆ†å¸ƒï¼‰"""
        from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType
        from pyspark.sql import Row
        from datetime import date, timedelta
        import random
        
        # å®šä¹‰å®Œæ•´çš„ä¸šåŠ¡æ•°æ®schema
        schema = StructType([
            StructField("user_id", StringType(), True),
            StructField("age", IntegerType(), True),
            StructField("total_asset_value", DoubleType(), True),
            StructField("trade_count_30d", IntegerType(), True),
            StructField("risk_score", DoubleType(), True),
            StructField("registration_date", DateType(), True),
            StructField("user_level", StringType(), True),
            StructField("kyc_status", StringType(), True),
            StructField("cash_balance", DoubleType(), True),
            StructField("last_login_date", DateType(), True)
        ])
        
        # ç”Ÿæˆ100ä¸ªç”¨æˆ·ï¼Œç¡®ä¿èƒ½å‘½ä¸­æ‰€æœ‰æ ‡ç­¾è§„åˆ™
        test_users = []
        for i in range(100):
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„é«˜å‡€å€¼ç”¨æˆ·
            if i < 50:  # å‰50ä¸ªç”¨æˆ·éƒ½æ˜¯é«˜å‡€å€¼
                total_asset = random.uniform(150000, 500000)  # ç¡®ä¿ >= 100000
                cash_balance = random.uniform(60000, 150000)  # ç¡®ä¿ >= 50000
            else:
                total_asset = random.uniform(1000, 80000)
                cash_balance = random.uniform(1000, 40000)
            
            # ç¡®ä¿æœ‰VIPç”¨æˆ·ä¸”KYCå·²éªŒè¯
            if i < 20:  # å‰20ä¸ªæ˜¯VIPå®¢æˆ·
                user_level = random.choice(["VIP2", "VIP3"])
                kyc_status = "verified"
            else:
                user_level = random.choice(["BRONZE", "SILVER", "GOLD", "VIP1"])
                kyc_status = random.choice(["verified", "pending", "rejected"])
            
            # ç¡®ä¿èƒ½å‘½ä¸­å¹´è½»ç”¨æˆ·æ ‡ç­¾
            if i < 30:
                age = random.randint(18, 30)  # å¹´è½»ç”¨æˆ·
            else:
                age = random.randint(31, 65)
            
            # ç¡®ä¿æœ‰æ´»è·ƒäº¤æ˜“è€…
            if i < 80:  # 80%æ˜¯æ´»è·ƒäº¤æ˜“è€…
                trade_count = random.randint(15, 50)  # > 10
            else:
                trade_count = random.randint(0, 8)
            
            # ç¡®ä¿æœ‰ä½é£é™©ç”¨æˆ·
            if i < 25:
                risk_score = random.uniform(10, 28)  # <= 30
            else:
                risk_score = random.uniform(35, 80)
            
            # ç¡®ä¿æœ‰æ–°æ³¨å†Œå’Œæœ€è¿‘æ´»è·ƒç”¨æˆ·
            if i < 15:
                registration_date = date.today() - timedelta(days=random.randint(1, 25))  # æœ€è¿‘30å¤©
                last_login_date = date.today() - timedelta(days=random.randint(0, 5))    # æœ€è¿‘7å¤©
            else:
                registration_date = date.today() - timedelta(days=random.randint(40, 300))
                last_login_date = date.today() - timedelta(days=random.randint(10, 25))
                
            user_data = Row(
                user_id=f"user_{i+1:06d}",
                age=age,
                total_asset_value=total_asset,
                trade_count_30d=trade_count,
                risk_score=risk_score,
                registration_date=registration_date,
                user_level=user_level,
                kyc_status=kyc_status,
                cash_balance=cash_balance,
                last_login_date=last_login_date
            )
            test_users.append(user_data)
        
        # åˆ›å»ºDataFrame
        test_df = self.spark.createDataFrame(test_users, schema)
        logger.info(f"ç”Ÿæˆäº† {test_df.count()} æ¡ç”Ÿäº§çº§æ¨¡æ‹Ÿæ•°æ®")
        return test_df

    def _generate_test_user_data(self):
        """ç”Ÿæˆæµ‹è¯•ç”¨æˆ·æ•°æ®"""
        from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType
        from pyspark.sql import Row
        from datetime import date, timedelta
        import random
        
        # å®šä¹‰ç”¨æˆ·æ•°æ®schema
        schema = StructType([
            StructField("user_id", StringType(), True),
            StructField("age", IntegerType(), True),
            StructField("total_asset_value", DoubleType(), True),
            StructField("trade_count_30d", IntegerType(), True),
            StructField("risk_score", DoubleType(), True),
            StructField("registration_date", DateType(), True),
            StructField("user_level", StringType(), True),
            StructField("kyc_status", StringType(), True),
            StructField("cash_balance", DoubleType(), True),
            StructField("last_login_date", DateType(), True)
        ])
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        test_users = []
        for i in range(100):
            user_data = Row(
                user_id=f"user_{i:04d}",
                age=random.randint(18, 65),
                total_asset_value=random.uniform(1000, 500000),
                trade_count_30d=random.randint(0, 50),
                risk_score=random.uniform(10, 80),
                registration_date=date.today() - timedelta(days=random.randint(1, 365)),
                user_level=random.choice(["VIP1", "VIP2", "VIP3", "NORMAL"]),
                kyc_status=random.choice(["verified", "unverified"]),
                cash_balance=random.uniform(100, 100000),
                last_login_date=date.today() - timedelta(days=random.randint(0, 30))
            )
            test_users.append(user_data)
        
        # åˆ›å»ºDataFrame
        test_df = self.spark.createDataFrame(test_users, schema)
        logger.info(f"ç”Ÿæˆäº† {test_df.count()} æ¡æµ‹è¯•ç”¨æˆ·æ•°æ®")
        return test_df
    
    def _identify_truly_new_users(self, days_back: int):
        """è¯†åˆ«çœŸæ­£çš„æ–°å¢ç”¨æˆ·ï¼šHiveä¸­æœ‰ä½†MySQLä¸­æ²¡æœ‰çš„ç”¨æˆ·"""
        try:
            logger.info(f"ğŸ” å¼€å§‹è¯†åˆ«æœ€è¿‘ {days_back} å¤©çš„çœŸæ­£æ–°å¢ç”¨æˆ·...")
            
            # 1. è·å–æ¨¡æ‹Ÿçš„å…¨é‡Hiveæ•°æ®ï¼ˆåŒ…å«æ–°å¢ç”¨æˆ·ï¼‰
            hive_all_users = self._generate_hive_data_with_new_users(days_back)
            
            # 2. è¯»å–MySQLä¸­å·²æœ‰çš„ç”¨æˆ·åˆ—è¡¨
            mysql_existing_users = self._read_existing_users_from_mysql()
            
            # 3. å¯¹æ¯”æ‰¾å‡ºæ–°å¢ç”¨æˆ·ï¼ˆleft_anti joinï¼‰
            new_users = hive_all_users.join(
                mysql_existing_users,
                "user_id",
                "left_anti"  # æ‰¾å‡ºHiveä¸­æœ‰ä½†MySQLä¸­æ²¡æœ‰çš„ç”¨æˆ·
            )
            
            new_user_count = new_users.count()
            logger.info(f"âœ… è¯†åˆ«å‡º {new_user_count} ä¸ªçœŸæ­£çš„æ–°å¢ç”¨æˆ·")
            
            if new_user_count > 0:
                logger.info("æ–°å¢ç”¨æˆ·ç¤ºä¾‹:")
                new_users.select("user_id", "registration_date").show(5, truncate=False)
            
            return new_users
            
        except Exception as e:
            logger.error(f"è¯†åˆ«æ–°å¢ç”¨æˆ·å¤±è´¥: {str(e)}")
            raise
    
    def _generate_hive_data_with_new_users(self, days_back: int):
        """ç”ŸæˆåŒ…å«æ–°å¢ç”¨æˆ·çš„Hiveæ¨¡æ‹Ÿæ•°æ®"""
        from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType
        from pyspark.sql import Row
        from datetime import date, timedelta
        import random
        
        logger.info(f"ç”ŸæˆåŒ…å«æ–°å¢ç”¨æˆ·çš„Hiveæ¨¡æ‹Ÿæ•°æ®...")
        
        # å®šä¹‰ç”¨æˆ·æ•°æ®schema
        schema = StructType([
            StructField("user_id", StringType(), True),
            StructField("age", IntegerType(), True),
            StructField("total_asset_value", DoubleType(), True),
            StructField("trade_count_30d", IntegerType(), True),
            StructField("risk_score", DoubleType(), True),
            StructField("registration_date", DateType(), True),
            StructField("user_level", StringType(), True),
            StructField("kyc_status", StringType(), True),
            StructField("cash_balance", DoubleType(), True),
            StructField("last_login_date", DateType(), True)
        ])
        
        all_users = []
        
        # 1. æ·»åŠ ä¸€äº›ç°æœ‰ç”¨æˆ·ï¼ˆä¸MySQLä¸­é‡å¤ï¼Œæ¨¡æ‹Ÿè€ç”¨æˆ·ï¼‰
        for i in range(1, 6):  # user_000001 åˆ° user_000005
            user_data = Row(
                user_id=f"user_{i:06d}",
                age=random.randint(25, 50),
                total_asset_value=random.uniform(50000, 300000),
                trade_count_30d=random.randint(10, 30),
                risk_score=random.uniform(20, 70),
                registration_date=date.today() - timedelta(days=random.randint(30, 365)),
                user_level=random.choice(["SILVER", "GOLD", "VIP1"]),
                kyc_status="verified",
                cash_balance=random.uniform(10000, 100000),
                last_login_date=date.today() - timedelta(days=random.randint(0, 7))
            )
            all_users.append(user_data)
        
        # 2. æ·»åŠ æ–°å¢ç”¨æˆ·ï¼ˆMySQLä¸­ä¸å­˜åœ¨ï¼‰
        for i in range(10):  # 10ä¸ªæ–°ç”¨æˆ·
            # æ–°ç”¨æˆ·æ³¨å†Œæ—¶é—´åœ¨æŒ‡å®šå¤©æ•°å†…
            reg_date = date.today() - timedelta(days=random.randint(1, days_back))
            
            user_data = Row(
                user_id=f"new_user_{i+1:04d}",
                age=random.randint(18, 45),
                total_asset_value=random.uniform(10000, 200000),
                trade_count_30d=random.randint(5, 25),
                risk_score=random.uniform(15, 60),
                registration_date=reg_date,
                user_level=random.choice(["BRONZE", "SILVER", "GOLD", "VIP1"]),
                kyc_status=random.choice(["verified", "pending"]),
                cash_balance=random.uniform(5000, 80000),
                last_login_date=date.today() - timedelta(days=random.randint(0, 3))
            )
            all_users.append(user_data)
        
        hive_df = self.spark.createDataFrame(all_users, schema)
        logger.info(f"ç”Ÿæˆäº† {hive_df.count()} æ¡Hiveæ¨¡æ‹Ÿæ•°æ®ï¼ˆåŒ…å«æ–°è€ç”¨æˆ·ï¼‰")
        return hive_df
    
    def _read_existing_users_from_mysql(self):
        """ä»MySQLè¯»å–å·²æœ‰çš„ç”¨æˆ·åˆ—è¡¨"""
        try:
            logger.info("ğŸ“– ä»MySQLè¯»å–å·²æœ‰ç”¨æˆ·åˆ—è¡¨...")
            
            existing_users_df = self.spark.read.jdbc(
                url=self.config.mysql.jdbc_url,
                table="user_tags",
                properties=self.config.mysql.connection_properties
            ).select("user_id").distinct()
            
            existing_count = existing_users_df.count()
            logger.info(f"MySQLä¸­å·²æœ‰ {existing_count} ä¸ªç”¨æˆ·")
            
            return existing_users_df
            
        except Exception as e:
            logger.warning(f"è¯»å–MySQLå·²æœ‰ç”¨æˆ·å¤±è´¥ï¼ˆå¯èƒ½æ˜¯é¦–æ¬¡è¿è¡Œï¼‰: {str(e)}")
            # è¿”å›ç©ºDataFrame
            from pyspark.sql.types import StructType, StructField, StringType
            empty_schema = StructType([StructField("user_id", StringType(), True)])
            return self.spark.createDataFrame([], empty_schema)
    
    def _generate_new_users_data(self, days_back: int):
        """ç”Ÿæˆæ–°å¢ç”¨æˆ·æ•°æ®ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰"""
        logger.warning("è¯¥æ–¹æ³•å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ _identify_truly_new_users")
        return self._identify_truly_new_users(days_back)
    
    def _merge_user_multi_tags_in_memory(self, tag_results_list: List):
        """ç»Ÿä¸€çš„ç”¨æˆ·å¤šæ ‡ç­¾å†…å­˜åˆå¹¶ - å°†ä¸€ä¸ªç”¨æˆ·çš„å¤šä¸ªå¹¶è¡Œæ ‡ç­¾åœ¨å†…å­˜ä¸­åˆå¹¶"""
        try:
            if not tag_results_list:
                logger.warning("æ²¡æœ‰æ ‡ç­¾ç»“æœéœ€è¦åˆå¹¶")
                return None
            
            logger.info(f"å¼€å§‹å†…å­˜åˆå¹¶ {len(tag_results_list)} ä¸ªæ ‡ç­¾ç»“æœ...")
            
            from pyspark.sql.functions import col, collect_list, struct, lit
            from pyspark.sql.types import ArrayType, IntegerType
            from datetime import date
            from functools import reduce
            
            # 1. åˆå¹¶æ‰€æœ‰æ ‡ç­¾è®¡ç®—ç»“æœ
            all_tags = reduce(lambda df1, df2: df1.union(df2), tag_results_list)
            
            if all_tags.count() == 0:
                logger.warning("åˆå¹¶åæ²¡æœ‰æ ‡ç­¾æ•°æ®")
                return None
            
            logger.info(f"åˆå¹¶å‰æ€»è®°å½•æ•°: {all_tags.count()}")
            
            # è°ƒè¯•ï¼šæ£€æŸ¥æ˜¯å¦åœ¨è®¡ç®—é˜¶æ®µå°±æœ‰é‡å¤
            logger.info("ğŸ” æ£€æŸ¥æ ‡ç­¾è®¡ç®—é˜¶æ®µæ˜¯å¦æœ‰é‡å¤...")
            user_tag_counts = all_tags.groupBy("user_id", "tag_id").count()
            duplicates = user_tag_counts.filter(user_tag_counts["count"] > 1)
            duplicate_count = duplicates.count()
            if duplicate_count > 0:
                logger.warning(f"âš ï¸ å‘ç°æ ‡ç­¾è®¡ç®—é˜¶æ®µå°±æœ‰é‡å¤ï¼é‡å¤è®°å½•æ•°: {duplicate_count}")
                duplicates.show(10, truncate=False)
            else:
                logger.info("âœ… æ ‡ç­¾è®¡ç®—é˜¶æ®µæ— é‡å¤")
            
            # 2. è·å–æ ‡ç­¾å®šä¹‰ä¿¡æ¯
            enriched_tags = self._enrich_tags_with_definition_info(all_tags)
            
            # 3. å…ˆå»é‡ï¼Œå†æŒ‰ç”¨æˆ·èšåˆï¼ˆå…³é”®ä¿®å¤ï¼šé¿å…æ ‡ç­¾é‡å¤ï¼‰
            from pyspark.sql.functions import array_distinct, collect_set
            
            # å…ˆå»é™¤æ¯ä¸ªç”¨æˆ·çš„é‡å¤æ ‡ç­¾
            deduplicated_tags = enriched_tags.dropDuplicates(["user_id", "tag_id"])
            
            # ç„¶åèšåˆæˆæ•°ç»„
            user_aggregated_tags = deduplicated_tags.groupBy("user_id").agg(
                collect_list("tag_id").alias("tag_ids_raw"),
                collect_list(struct("tag_id", "tag_name", "tag_category")).alias("tag_info_list")
            )
            
            # å¯¹æ ‡ç­¾IDæ•°ç»„è¿›è¡Œå»é‡å’Œæ’åº
            user_aggregated_tags = user_aggregated_tags.select(
                "user_id",
                array_distinct("tag_ids_raw").alias("tag_ids"),
                "tag_info_list"
            )
            
            # 4. æ ¼å¼åŒ–ä¸ºæ ‡å‡†çš„ç”¨æˆ·æ ‡ç­¾æ ¼å¼
            formatted_result = self._format_user_tags_output(user_aggregated_tags)
            
            logger.info(f"âœ… ç”¨æˆ·å¤šæ ‡ç­¾å†…å­˜åˆå¹¶å®Œæˆï¼Œå½±å“ {formatted_result.count()} ä¸ªç”¨æˆ·")
            return formatted_result
            
        except Exception as e:
            logger.error(f"ç”¨æˆ·å¤šæ ‡ç­¾å†…å­˜åˆå¹¶å¤±è´¥: {str(e)}")
            return None
    
    def _merge_new_user_tags_in_memory(self, new_tag_results: List):
        """å¢é‡æ¨¡å¼ä¸“ç”¨ï¼šæ–°ç”¨æˆ·æ ‡ç­¾å†…å­˜åˆå¹¶ï¼ˆå¤ç”¨ç»Ÿä¸€é€»è¾‘ï¼‰"""
        return self._merge_user_multi_tags_in_memory(new_tag_results)
    
    def _enrich_tags_with_definition_info(self, tags_df):
        """ç»Ÿä¸€çš„æ ‡ç­¾å®šä¹‰ä¿¡æ¯è¡¥å……æ–¹æ³•"""
        try:
            from pyspark.sql.functions import col, lit
            
            # è¯»å–æ ‡ç­¾å®šä¹‰
            tag_definitions = self.spark.read.jdbc(
                url=self.config.mysql.jdbc_url,
                table="tag_definition",
                properties=self.config.mysql.connection_properties
            ).select("tag_id", "tag_name", "tag_category")
            
            # å…³è”æ ‡ç­¾å®šä¹‰ä¿¡æ¯
            enriched_df = tags_df.join(
                tag_definitions,
                "tag_id",
                "left"
            ).select(
                "user_id",
                "tag_id", 
                col("tag_name").alias("tag_name"),
                col("tag_category").alias("tag_category"),
                "tag_detail"
            )
            
            return enriched_df
            
        except Exception as e:
            logger.error(f"ä¸°å¯Œæ ‡ç­¾ä¿¡æ¯å¤±è´¥: {str(e)}")
            # é™çº§å¤„ç†ï¼šä½¿ç”¨é»˜è®¤å€¼
            from pyspark.sql.functions import lit
            return tags_df.select(
                "user_id",
                "tag_id",
                lit("unknown").alias("tag_name"),
                lit("unknown").alias("tag_category"),
                "tag_detail"
            )
    
    def _format_user_tags_output(self, user_tags_df):
        """ç»Ÿä¸€çš„ç”¨æˆ·æ ‡ç­¾è¾“å‡ºæ ¼å¼åŒ–æ–¹æ³•"""
        from pyspark.sql.functions import udf, col, lit
        from pyspark.sql.types import StringType
        from datetime import date
        import json
        
        # ä½¿ç”¨UDFæ„å»ºæ ‡ç­¾è¯¦æƒ…
        @udf(returnType=StringType())
        def build_tag_details(tag_info_list):
            if not tag_info_list:
                return "{}"
            
            tag_details = {}
            for tag_info in tag_info_list:
                tag_id = str(tag_info['tag_id'])
                tag_details[tag_id] = {
                    'tag_name': tag_info['tag_name'],
                    'tag_category': tag_info['tag_category']
                }
            return json.dumps(tag_details, ensure_ascii=False)
        
        formatted_df = user_tags_df.select(
            col("user_id"),
            col("tag_ids"),
            build_tag_details(col("tag_info_list")).alias("tag_details"),
            lit(date.today()).alias("computed_date")
        )
        
        return formatted_df
    
    def _enrich_new_user_tags_with_info(self, tags_df):
        """ä¸ºæ–°ç”¨æˆ·æ ‡ç­¾è¡¥å……æ ‡ç­¾å®šä¹‰ä¿¡æ¯"""
        try:
            from pyspark.sql.functions import col, lit
            
            # è¯»å–æ ‡ç­¾å®šä¹‰
            tag_definitions = self.spark.read.jdbc(
                url=self.config.mysql.jdbc_url,
                table="tag_definition",
                properties=self.config.mysql.connection_properties
            ).select("tag_id", "tag_name", "tag_category")
            
            # å…³è”æ ‡ç­¾å®šä¹‰ä¿¡æ¯
            enriched_df = tags_df.join(
                tag_definitions,
                "tag_id",
                "left"
            ).select(
                "user_id",
                "tag_id", 
                col("tag_name").alias("tag_name"),
                col("tag_category").alias("tag_category"),
                "tag_detail"
            )
            
            return enriched_df
            
        except Exception as e:
            logger.error(f"ä¸°å¯Œæ–°ç”¨æˆ·æ ‡ç­¾ä¿¡æ¯å¤±è´¥: {str(e)}")
            # é™çº§å¤„ç†ï¼šä½¿ç”¨é»˜è®¤å€¼
            from pyspark.sql.functions import lit
            return tags_df.select(
                "user_id",
                "tag_id",
                lit("unknown").alias("tag_name"),
                lit("unknown").alias("tag_category"),
                "tag_detail"
            )
    
    def _format_new_user_final_output(self, user_tags_df):
        """æ ¼å¼åŒ–æ–°ç”¨æˆ·æœ€ç»ˆè¾“å‡º"""
        from pyspark.sql.functions import udf, col, lit
        from pyspark.sql.types import StringType
        from datetime import date
        import json
        
        # ä½¿ç”¨UDFæ„å»ºæ ‡ç­¾è¯¦æƒ…
        @udf(returnType=StringType())
        def build_tag_details(tag_info_list):
            if not tag_info_list:
                return "{}"
            
            tag_details = {}
            for tag_info in tag_info_list:
                tag_id = str(tag_info['tag_id'])
                tag_details[tag_id] = {
                    'tag_name': tag_info['tag_name'],
                    'tag_category': tag_info['tag_category']
                }
            return json.dumps(tag_details, ensure_ascii=False)
        
        formatted_df = user_tags_df.select(
            col("user_id"),
            col("tag_ids"),
            build_tag_details(col("tag_info_list")).alias("tag_details"),
            lit(date.today()).alias("computed_date")
        )
        
        return formatted_df
    
    def _generate_incremental_data_for_table(self, table_name: str, days_back: int):
        """ä¸ºæŒ‡å®šè¡¨ç”Ÿæˆå¢é‡æ•°æ® - æœ¬åœ°ç¯å¢ƒä¸“ç”¨ï¼ˆé—ç•™æ–¹æ³•ï¼Œä¿æŒå…¼å®¹æ€§ï¼‰"""
        # ä¸ºäº†ç®€åŒ–ï¼Œæ‰€æœ‰è¡¨éƒ½ä½¿ç”¨ç›¸åŒçš„ç”¨æˆ·æ•°æ®ç»“æ„
        # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œä¸åŒè¡¨ä¼šæœ‰ä¸åŒçš„schema
        logger.info(f"ä¸ºè¡¨ {table_name} ç”Ÿæˆæœ€è¿‘ {days_back} å¤©çš„å¢é‡æ•°æ®")
        
        # ç”Ÿæˆè¾ƒå°‘çš„ç”¨æˆ·æ•°æ®æ¨¡æ‹Ÿå¢é‡ï¼ˆæ¯”å¦‚20%çš„ç”¨æˆ·æœ‰å˜åŒ–ï¼‰
        incremental_data = self._generate_production_like_data()
        
        # æ¨¡æ‹Ÿå¢é‡ï¼šåªå–éƒ¨åˆ†ç”¨æˆ·ï¼Œæ¨¡æ‹Ÿæœ€è¿‘æœ‰å˜åŒ–çš„ç”¨æˆ·
        sample_ratio = min(0.3, 1.0)  # æœ€å¤š30%çš„ç”¨æˆ·æœ‰å¢é‡å˜åŒ–
        incremental_sample = incremental_data.sample(fraction=sample_ratio, seed=42)
        
        logger.info(f"è¡¨ {table_name} å¢é‡æ•°æ®åŒ…å« {incremental_sample.count()} ä¸ªç”¨æˆ·")
        return incremental_sample
    
    # ç§»é™¤äº†é—ç•™çš„data_managerç›¸å…³æ–¹æ³•
    # ç°åœ¨ä½¿ç”¨tag_merger.merge_user_tags()æ›¿ä»£
    def health_check(self) -> bool:
        """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
        try:
            logger.info("å¼€å§‹ç³»ç»Ÿå¥åº·æ£€æŸ¥...")
            
            # æ£€æŸ¥Sparkè¿æ¥
            if not self.spark or self.spark._sc._jsc is None:
                logger.error("Sparkè¿æ¥å¼‚å¸¸")
                return False
            
            # æ£€æŸ¥MySQLè¿æ¥
            test_df = self.spark.read.jdbc(
                url=self.config.mysql.jdbc_url,
                table="(SELECT 1 as test) as tmp",
                properties=self.config.mysql.connection_properties
            )
            
            if test_df.count() != 1:
                logger.error("MySQLè¿æ¥å¼‚å¸¸")
                return False
            
            # æ£€æŸ¥S3è¿æ¥ï¼ˆä»…åœ¨éæœ¬åœ°ç¯å¢ƒï¼‰
            if self.config.environment != 'local':
                try:
                    test_schemas = self.hive_reader.get_table_schema("user_basic_info")
                    if not test_schemas:
                        logger.warning("S3è¿æ¥æˆ–æ•°æ®è®¿é—®å¯èƒ½æœ‰é—®é¢˜")
                except:
                    logger.warning("S3è¿æ¥æ£€æŸ¥å¤±è´¥")
            else:
                logger.info("ğŸ’¡ æœ¬åœ°ç¯å¢ƒè·³è¿‡S3è¿æ¥æ£€æŸ¥ï¼Œä½¿ç”¨å†…å­˜æ•°æ®ç”Ÿæˆ")
            
            logger.info("âœ… ç³»ç»Ÿå¥åº·æ£€æŸ¥é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç³»ç»Ÿå¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            return False
    
    def cleanup(self):
        """æ¸…ç†æ‰€æœ‰èµ„æº"""
        try:
            logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†ç³»ç»Ÿèµ„æº...")
            
            # æ¸…ç†Sparkç¼“å­˜
            if self.spark:
                try:
                    self.spark.catalog.clearCache()
                    logger.info("âœ… Sparkç¼“å­˜å·²æ¸…ç†")
                except Exception as e:
                    logger.warning(f"âš ï¸ Sparkç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
                
                # åœæ­¢Spark Session
                try:
                    self.spark.stop()
                    logger.info("âœ… Spark Sessionå·²åœæ­¢")
                except Exception as e:
                    logger.warning(f"âš ï¸ Spark Sessionåœæ­¢å¤±è´¥: {e}")
            
            # æ¸…ç†ç»„ä»¶å¼•ç”¨
            if self.scenario_scheduler:
                self.scenario_scheduler.cleanup()
            
            self.data_manager = None
            self.hive_reader = None
            self.tag_engine = None
            self.scenario_scheduler = None
            self.spark = None
            
            logger.info("âœ… ç³»ç»Ÿèµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ èµ„æºæ¸…ç†å¼‚å¸¸: {str(e)}")
            # å¼ºåˆ¶æ¸…ç†
            try:
                if self.spark:
                    self.spark.stop()
            except:
                pass
    
    def run_specific_users(self, user_ids: List[str]) -> bool:
        """è¿è¡ŒæŒ‡å®šç”¨æˆ·çš„å…¨é‡æ ‡ç­¾è®¡ç®—"""
        try:
            logger.info(f"ğŸ¯ å¼€å§‹è®¡ç®—æŒ‡å®šç”¨æˆ·çš„å…¨é‡æ ‡ç­¾: {user_ids}")
            
            # è¯»å–æ‰€æœ‰æ¿€æ´»çš„æ ‡ç­¾è§„åˆ™
            all_rules = self.rule_reader.read_active_rules()
            if not all_rules:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°æ¿€æ´»çš„æ ‡ç­¾è§„åˆ™")
                return False
            
            # ç”Ÿæˆæµ‹è¯•ç”¨æˆ·æ•°æ®
            test_data = self._generate_test_user_data()
            
            # è¿‡æ»¤å‡ºæŒ‡å®šçš„ç”¨æˆ·
            from pyspark.sql.functions import col
            filtered_data = test_data.filter(col("user_id").isin(user_ids))
            
            filtered_count = filtered_data.count()
            if filtered_count == 0:
                logger.warning(f"æ²¡æœ‰æ‰¾åˆ°æŒ‡å®šçš„ç”¨æˆ·: {user_ids}")
                return False
            
            logger.info(f"æ‰¾åˆ° {filtered_count} ä¸ªæŒ‡å®šç”¨æˆ·")
            
            # è®¡ç®—æ‰€æœ‰æ ‡ç­¾
            all_tag_results = []
            tag_results = self.tag_engine.compute_batch_tags(filtered_data, all_rules)
            all_tag_results.extend(tag_results)
            
            if not all_tag_results:
                logger.warning("æŒ‡å®šç”¨æˆ·æ²¡æœ‰è®¡ç®—å‡ºä»»ä½•æ ‡ç­¾")
                return False
            
            # åˆå¹¶å’Œå†™å…¥
            merged_result = self.tag_merger.merge_user_tags(all_tag_results)
            if merged_result is None:
                return False
            
            # æŒ‡å®šç”¨æˆ·æ¨¡å¼ï¼šéœ€è¦ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶
            return self.mysql_writer.write_tag_results(merged_result, mode="overwrite", merge_with_existing=True)
            
        except Exception as e:
            logger.error(f"æŒ‡å®šç”¨æˆ·æ ‡ç­¾è®¡ç®—å¤±è´¥: {str(e)}")
            return False
    
    def run_specific_user_tags(self, user_ids: List[str], tag_ids: List[int]) -> bool:
        """è¿è¡ŒæŒ‡å®šç”¨æˆ·çš„æŒ‡å®šæ ‡ç­¾è®¡ç®—"""
        try:
            logger.info(f"ğŸ¯ å¼€å§‹è®¡ç®—æŒ‡å®šç”¨æˆ·çš„æŒ‡å®šæ ‡ç­¾: ç”¨æˆ·{user_ids}, æ ‡ç­¾{tag_ids}")
            
            # è¯»å–æŒ‡å®šæ ‡ç­¾çš„è§„åˆ™
            all_rules = self.rule_reader.read_active_rules()
            target_rules = [rule for rule in all_rules if rule['tag_id'] in tag_ids]
            
            if not target_rules:
                logger.warning(f"æ²¡æœ‰æ‰¾åˆ°æŒ‡å®šæ ‡ç­¾çš„è§„åˆ™: {tag_ids}")
                return False
            
            # ç”Ÿæˆæµ‹è¯•ç”¨æˆ·æ•°æ®
            test_data = self._generate_test_user_data()
            
            # è¿‡æ»¤å‡ºæŒ‡å®šçš„ç”¨æˆ·
            from pyspark.sql.functions import col
            filtered_data = test_data.filter(col("user_id").isin(user_ids))
            
            filtered_count = filtered_data.count()
            if filtered_count == 0:
                logger.warning(f"æ²¡æœ‰æ‰¾åˆ°æŒ‡å®šçš„ç”¨æˆ·: {user_ids}")
                return False
            
            logger.info(f"æ‰¾åˆ° {filtered_count} ä¸ªæŒ‡å®šç”¨æˆ·")
            
            # è®¡ç®—æŒ‡å®šæ ‡ç­¾
            all_tag_results = []
            tag_results = self.tag_engine.compute_batch_tags(filtered_data, target_rules)
            all_tag_results.extend(tag_results)
            
            if not all_tag_results:
                logger.warning("æŒ‡å®šç”¨æˆ·å’Œæ ‡ç­¾æ²¡æœ‰è®¡ç®—å‡ºç»“æœ")
                return False
            
            # åˆå¹¶å’Œå†™å…¥
            merged_result = self.tag_merger.merge_user_tags(all_tag_results)
            if merged_result is None:
                return False
            
            # æŒ‡å®šç”¨æˆ·æŒ‡å®šæ ‡ç­¾æ¨¡å¼ï¼šéœ€è¦ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶
            return self.mysql_writer.write_tag_results(merged_result, mode="overwrite", merge_with_existing=True)
            
        except Exception as e:
            logger.error(f"æŒ‡å®šç”¨æˆ·æŒ‡å®šæ ‡ç­¾è®¡ç®—å¤±è´¥: {str(e)}")
            return False
    
    def run_incremental_specific_tags(self, days_back: int, tag_ids: List[int]) -> bool:
        """è¿è¡Œå¢é‡æŒ‡å®šæ ‡ç­¾è®¡ç®—ï¼ˆæ–°å¢ç”¨æˆ·ï¼ŒæŒ‡å®šæ ‡ç­¾ï¼‰"""
        try:
            logger.info(f"ğŸ¯ å¼€å§‹å¢é‡æŒ‡å®šæ ‡ç­¾è®¡ç®—ï¼Œå›æº¯{days_back}å¤©ï¼Œæ ‡ç­¾: {tag_ids}")
            
            # è¯»å–æŒ‡å®šæ ‡ç­¾çš„è§„åˆ™
            all_rules = self.rule_reader.read_active_rules()
            target_rules = [rule for rule in all_rules if rule['tag_id'] in tag_ids]
            
            if not target_rules:
                logger.warning(f"æ²¡æœ‰æ‰¾åˆ°æŒ‡å®šæ ‡ç­¾çš„è§„åˆ™: {tag_ids}")
                return False
            
            # è¯†åˆ«æ–°å¢ç”¨æˆ·
            new_users = self._identify_truly_new_users(days_back)
            new_user_count = new_users.count()
            
            if new_user_count == 0:
                logger.info("æ²¡æœ‰æ‰¾åˆ°æ–°å¢ç”¨æˆ·ï¼Œè·³è¿‡è®¡ç®—")
                return True
            
            logger.info(f"æ‰¾åˆ° {new_user_count} ä¸ªæ–°å¢ç”¨æˆ·")
            
            # è®¡ç®—æŒ‡å®šæ ‡ç­¾
            all_tag_results = []
            tag_results = self.tag_engine.compute_batch_tags(new_users, target_rules)
            all_tag_results.extend(tag_results)
            
            if not all_tag_results:
                logger.warning("æ–°å¢ç”¨æˆ·æ²¡æœ‰è®¡ç®—å‡ºæŒ‡å®šæ ‡ç­¾")
                return True  # æ²¡æœ‰ç»“æœä¹Ÿç®—æˆåŠŸ
            
            # æ–¹æ¡ˆ2ï¼šç‹¬ç«‹å†…å­˜å¤„ç† - åˆå¹¶å¤šä¸ªæ ‡ç­¾ç»“æœ
            merged_result = self._merge_user_multi_tags_in_memory(all_tag_results)
            if merged_result is None:
                return False
            
            # å†™å…¥æ•°æ®åº“ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
            # å¢é‡æŒ‡å®šæ ‡ç­¾æ¨¡å¼ï¼šä¸éœ€è¦ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶ï¼ˆæ–°ç”¨æˆ·ï¼‰
            return self.mysql_writer.write_tag_results(merged_result, mode="append", merge_with_existing=False)
            
        except Exception as e:
            logger.error(f"å¢é‡æŒ‡å®šæ ‡ç­¾è®¡ç®—å¤±è´¥: {str(e)}")
            return False
    
    # ==================== æ–°å¢6ä¸ªåŠŸèƒ½åœºæ™¯æ–¹æ³• ====================
    
    def run_scenario_1_full_users_full_tags(self) -> bool:
        """åœºæ™¯1: å…¨é‡ç”¨æˆ·æ‰“å…¨é‡æ ‡ç­¾"""
        try:
            logger.info("ğŸ¯ æ‰§è¡Œåœºæ™¯1: å…¨é‡ç”¨æˆ·æ‰“å…¨é‡æ ‡ç­¾")
            return self.scenario_scheduler.scenario_1_full_users_full_tags()
        except Exception as e:
            logger.error(f"åœºæ™¯1æ‰§è¡Œå¤±è´¥: {str(e)}")
            return False
    
    def run_scenario_2_full_users_specific_tags(self, tag_ids: List[int]) -> bool:
        """åœºæ™¯2: å…¨é‡ç”¨æˆ·æ‰“æŒ‡å®šæ ‡ç­¾"""
        try:
            logger.info(f"ğŸ¯ æ‰§è¡Œåœºæ™¯2: å…¨é‡ç”¨æˆ·æ‰“æŒ‡å®šæ ‡ç­¾ {tag_ids}")
            return self.scenario_scheduler.scenario_2_full_users_specific_tags(tag_ids)
        except Exception as e:
            logger.error(f"åœºæ™¯2æ‰§è¡Œå¤±è´¥: {str(e)}")
            return False
    
    def run_scenario_3_incremental_users_full_tags(self, days_back: int = 1) -> bool:
        """åœºæ™¯3: å¢é‡ç”¨æˆ·æ‰“å…¨é‡æ ‡ç­¾"""
        try:
            logger.info(f"ğŸ¯ æ‰§è¡Œåœºæ™¯3: å¢é‡ç”¨æˆ·æ‰“å…¨é‡æ ‡ç­¾ï¼ˆå›æº¯{days_back}å¤©ï¼‰")
            return self.scenario_scheduler.scenario_3_incremental_users_full_tags(days_back)
        except Exception as e:
            logger.error(f"åœºæ™¯3æ‰§è¡Œå¤±è´¥: {str(e)}")
            return False
    
    def run_scenario_4_incremental_users_specific_tags(self, days_back: int, tag_ids: List[int]) -> bool:
        """åœºæ™¯4: å¢é‡ç”¨æˆ·æ‰“æŒ‡å®šæ ‡ç­¾"""
        try:
            logger.info(f"ğŸ¯ æ‰§è¡Œåœºæ™¯4: å¢é‡ç”¨æˆ·æ‰“æŒ‡å®šæ ‡ç­¾ï¼ˆå›æº¯{days_back}å¤©ï¼Œæ ‡ç­¾{tag_ids}ï¼‰")
            return self.scenario_scheduler.scenario_4_incremental_users_specific_tags(days_back, tag_ids)
        except Exception as e:
            logger.error(f"åœºæ™¯4æ‰§è¡Œå¤±è´¥: {str(e)}")
            return False
    
    def run_scenario_5_specific_users_full_tags(self, user_ids: List[str]) -> bool:
        """åœºæ™¯5: æŒ‡å®šç”¨æˆ·æ‰“å…¨é‡æ ‡ç­¾"""
        try:
            logger.info(f"ğŸ¯ æ‰§è¡Œåœºæ™¯5: æŒ‡å®šç”¨æˆ·æ‰“å…¨é‡æ ‡ç­¾ {user_ids}")
            return self.scenario_scheduler.scenario_5_specific_users_full_tags(user_ids)
        except Exception as e:
            logger.error(f"åœºæ™¯5æ‰§è¡Œå¤±è´¥: {str(e)}")
            return False
    
    def run_scenario_6_specific_users_specific_tags(self, user_ids: List[str], tag_ids: List[int]) -> bool:
        """åœºæ™¯6: æŒ‡å®šç”¨æˆ·æ‰“æŒ‡å®šæ ‡ç­¾"""
        try:
            logger.info(f"ğŸ¯ æ‰§è¡Œåœºæ™¯6: æŒ‡å®šç”¨æˆ·æ‰“æŒ‡å®šæ ‡ç­¾ï¼ˆç”¨æˆ·{user_ids}ï¼Œæ ‡ç­¾{tag_ids}ï¼‰")
            return self.scenario_scheduler.scenario_6_specific_users_specific_tags(user_ids, tag_ids)
        except Exception as e:
            logger.error(f"åœºæ™¯6æ‰§è¡Œå¤±è´¥: {str(e)}")
            return False