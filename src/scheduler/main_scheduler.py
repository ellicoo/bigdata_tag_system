import logging
import time
from typing import List, Dict, Any
from pyspark.sql import SparkSession

from ..config.base_config import TagSystemConfig
from ..readers.rule_reader import TagRuleReader
from ..readers.hive_reader import HiveDataReader
from ..engine.tag_computer import TagComputeEngine
from ..merger.tag_merger import TagMerger
from ..writers.mysql_writer import MySQLTagWriter

logger = logging.getLogger(__name__)


class TagComputeScheduler:
    """æ ‡ç­¾è®¡ç®—ä¸»è°ƒåº¦å™¨"""
    
    def __init__(self, config: TagSystemConfig):
        self.config = config
        self.spark = None
        
        # ç»„ä»¶åˆå§‹åŒ–
        self.rule_reader = None
        self.hive_reader = None
        self.tag_engine = None
        self.tag_merger = None
        self.mysql_writer = None
    
    def initialize(self):
        """åˆå§‹åŒ–Sparkå’Œå„ä¸ªç»„ä»¶"""
        try:
            logger.info("å¼€å§‹åˆå§‹åŒ–æ ‡ç­¾è®¡ç®—ç³»ç»Ÿ...")
            
            # åˆå§‹åŒ–Spark
            self.spark = self._create_spark_session()
            
            # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
            self.rule_reader = TagRuleReader(self.spark, self.config.mysql)
            self.hive_reader = HiveDataReader(self.spark, self.config.s3)
            self.tag_engine = TagComputeEngine(self.spark)
            self.tag_merger = TagMerger(self.spark, self.config.mysql)
            self.mysql_writer = MySQLTagWriter(self.spark, self.config.mysql)
            
            logger.info("âœ… æ ‡ç­¾è®¡ç®—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _create_spark_session(self) -> SparkSession:
        """åˆ›å»ºSparkä¼šè¯"""
        builder = SparkSession.builder
        
        # åº”ç”¨Sparké…ç½®
        for key, value in self.config.spark.to_dict().items():
            builder = builder.config(key, value)
        
        # åº”ç”¨S3é…ç½®
        for key, value in self.config.s3.to_spark_config().items():
            builder = builder.config(key, value)
        
        spark = builder.getOrCreate()
        spark.sparkContext.setLogLevel("WARN")  # å‡å°‘æ—¥å¿—è¾“å‡º
        
        logger.info(f"Sparkä¼šè¯åˆ›å»ºæˆåŠŸ: {spark.sparkContext.applicationId}")
        return spark
    
    def run_full_tag_compute(self) -> bool:
        """è¿è¡Œå®Œæ•´çš„æ ‡ç­¾è®¡ç®—æµç¨‹"""
        try:
            start_time = time.time()
            logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œå®Œæ•´æ ‡ç­¾è®¡ç®—...")
            
            # 1. è¯»å–æ‰€æœ‰æ´»è·ƒçš„æ ‡ç­¾è§„åˆ™
            rules = self.rule_reader.read_active_rules()
            if not rules:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„æ ‡ç­¾è§„åˆ™")
                return False
            
            logger.info(f"å…±æ‰¾åˆ° {len(rules)} ä¸ªæ´»è·ƒæ ‡ç­¾è§„åˆ™")
            
            # 2. æŒ‰æ•°æ®è¡¨åˆ†ç»„è§„åˆ™
            table_groups = self.rule_reader.group_rules_by_table(rules)
            
            # 3. æŒ‰è¡¨æ‰¹é‡å¤„ç†æ ‡ç­¾
            all_tag_results = []
            
            for table_name, table_rules in table_groups.items():
                logger.info(f"å¼€å§‹å¤„ç†è¡¨: {table_name}, æ ‡ç­¾æ•°: {len(table_rules)}")
                
                try:
                    # è¯»å–ä¸šåŠ¡æ•°æ®
                    required_fields = self.rule_reader.get_all_required_fields(table_rules)
                    business_data = self.hive_reader.read_table_data(table_name, required_fields)
                    
                    # éªŒè¯æ•°æ®è´¨é‡
                    if not self.hive_reader.validate_data_quality(business_data, table_name):
                        logger.warning(f"è¡¨ {table_name} æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥ï¼Œè·³è¿‡")
                        continue
                    
                    # æ‰¹é‡è®¡ç®—æ ‡ç­¾
                    table_results = self.tag_engine.compute_batch_tags(business_data, table_rules)
                    all_tag_results.extend(table_results)
                    
                    logger.info(f"âœ… è¡¨ {table_name} å¤„ç†å®Œæˆï¼ŒæˆåŠŸè®¡ç®— {len(table_results)} ä¸ªæ ‡ç­¾")
                    
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†è¡¨ {table_name} å¤±è´¥: {str(e)}")
                    continue
            
            if not all_tag_results:
                logger.warning("æ²¡æœ‰æˆåŠŸè®¡ç®—å‡ºä»»ä½•æ ‡ç­¾")
                return False
            
            # 4. åˆå¹¶æ ‡ç­¾ç»“æœ
            logger.info("å¼€å§‹åˆå¹¶æ ‡ç­¾ç»“æœ...")
            merged_result = self.tag_merger.merge_user_tags(all_tag_results)
            
            if merged_result is None:
                logger.error("æ ‡ç­¾åˆå¹¶å¤±è´¥")
                return False
            
            # 5. éªŒè¯åˆå¹¶ç»“æœ
            if not self.tag_merger.validate_merge_result(merged_result):
                logger.error("åˆå¹¶ç»“æœéªŒè¯å¤±è´¥")
                return False
            
            # 6. å†™å…¥MySQL
            logger.info("å¼€å§‹å†™å…¥æ ‡ç­¾ç»“æœ...")
            write_success = self.mysql_writer.write_tag_results(merged_result)
            
            if not write_success:
                logger.error("æ ‡ç­¾ç»“æœå†™å…¥å¤±è´¥")
                return False
            
            # 7. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            end_time = time.time()
            execution_time = end_time - start_time
            
            stats = self.mysql_writer.get_write_statistics()
            
            logger.info(f"""
ğŸ‰ æ ‡ç­¾è®¡ç®—å®Œæˆï¼
â±ï¸  æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’
ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
   - æ€»ç”¨æˆ·æ•°: {stats.get('total_users', 'N/A')}
   - å¹³å‡æ ‡ç­¾æ•°/ç”¨æˆ·: {stats.get('average_tags_per_user', 'N/A')}
   - æœ€å¤§æ ‡ç­¾æ•°/ç”¨æˆ·: {stats.get('max_tags_per_user', 'N/A')}
   - å¤„ç†çš„æ ‡ç­¾è§„åˆ™æ•°: {len(rules)}
   - æˆåŠŸè®¡ç®—çš„æ ‡ç­¾æ•°: {len(all_tag_results)}
            """)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ ‡ç­¾è®¡ç®—æµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}")
            return False
    
    def run_incremental_compute(self, days_back: int = 1) -> bool:
        """è¿è¡Œå¢é‡æ ‡ç­¾è®¡ç®—"""
        try:
            logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œå¢é‡æ ‡ç­¾è®¡ç®—ï¼Œå›æº¯ {days_back} å¤©...")
            
            # è¯»å–è§„åˆ™
            rules = self.rule_reader.read_active_rules()
            if not rules:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„æ ‡ç­¾è§„åˆ™")
                return False
            
            table_groups = self.rule_reader.group_rules_by_table(rules)
            all_tag_results = []
            
            for table_name, table_rules in table_groups.items():
                try:
                    # è¯»å–å¢é‡æ•°æ®
                    required_fields = self.rule_reader.get_all_required_fields(table_rules)
                    
                    # å‡è®¾å„è¡¨éƒ½æœ‰updated_timeæˆ–created_timeå­—æ®µ
                    date_field = "updated_time"  # å¯ä»¥æ ¹æ®è¡¨é…ç½®
                    
                    incremental_data = self.hive_reader.read_incremental_data(
                        table_name, date_field, days_back, required_fields
                    )
                    
                    if incremental_data.count() == 0:
                        logger.info(f"è¡¨ {table_name} æ²¡æœ‰å¢é‡æ•°æ®")
                        continue
                    
                    # è®¡ç®—æ ‡ç­¾
                    table_results = self.tag_engine.compute_batch_tags(incremental_data, table_rules)
                    all_tag_results.extend(table_results)
                    
                except Exception as e:
                    logger.error(f"å¤„ç†å¢é‡è¡¨ {table_name} å¤±è´¥: {str(e)}")
                    continue
            
            if not all_tag_results:
                logger.info("æ²¡æœ‰å¢é‡æ ‡ç­¾éœ€è¦æ›´æ–°")
                return True
            
            # åˆå¹¶å’Œå†™å…¥
            merged_result = self.tag_merger.merge_user_tags(all_tag_results)
            if merged_result is None:
                return False
            
            # å¢é‡å†™å…¥
            return self.mysql_writer.write_incremental_tags(merged_result)
            
        except Exception as e:
            logger.error(f"å¢é‡è®¡ç®—å¤±è´¥: {str(e)}")
            return False
    
    def run_specific_tags(self, tag_ids: List[int]) -> bool:
        """è¿è¡ŒæŒ‡å®šæ ‡ç­¾çš„è®¡ç®—"""
        try:
            logger.info(f"ğŸ¯ å¼€å§‹è®¡ç®—æŒ‡å®šæ ‡ç­¾: {tag_ids}")
            
            # è¯»å–æŒ‡å®šæ ‡ç­¾çš„è§„åˆ™
            all_rules = self.rule_reader.read_active_rules()
            target_rules = [rule for rule in all_rules if rule['tag_id'] in tag_ids]
            
            if not target_rules:
                logger.warning(f"æ²¡æœ‰æ‰¾åˆ°æŒ‡å®šæ ‡ç­¾çš„è§„åˆ™: {tag_ids}")
                return False
            
            # æŒ‰è¡¨åˆ†ç»„å¹¶å¤„ç†
            table_groups = self.rule_reader.group_rules_by_table(target_rules)
            all_tag_results = []
            
            for table_name, table_rules in table_groups.items():
                try:
                    required_fields = self.rule_reader.get_all_required_fields(table_rules)
                    business_data = self.hive_reader.read_table_data(table_name, required_fields)
                    
                    table_results = self.tag_engine.compute_batch_tags(business_data, table_rules)
                    all_tag_results.extend(table_results)
                    
                except Exception as e:
                    logger.error(f"å¤„ç†æŒ‡å®šæ ‡ç­¾è¡¨ {table_name} å¤±è´¥: {str(e)}")
                    continue
            
            if not all_tag_results:
                logger.warning("æŒ‡å®šæ ‡ç­¾æ²¡æœ‰è®¡ç®—å‡ºç»“æœ")
                return False
            
            # åˆå¹¶å’Œå†™å…¥
            merged_result = self.tag_merger.merge_user_tags(all_tag_results)
            if merged_result is None:
                return False
            
            return self.mysql_writer.write_tag_results(merged_result)
            
        except Exception as e:
            logger.error(f"æŒ‡å®šæ ‡ç­¾è®¡ç®—å¤±è´¥: {str(e)}")
            return False
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.hive_reader:
                self.hive_reader.clear_cache()
            
            if self.spark:
                self.spark.stop()
                
            logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"èµ„æºæ¸…ç†å¤±è´¥: {str(e)}")
    
    def health_check(self) -> bool:
        """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
        try:
            logger.info("å¼€å§‹ç³»ç»Ÿå¥åº·æ£€æŸ¥...")
            
            # æ£€æŸ¥Sparkè¿æ¥
            if not self.spark or self.spark._sc._jsc is None:
                logger.error("Sparkè¿æ¥å¼‚å¸¸")
                return False
            
            # æ£€æŸ¥MySQLè¿æ¥
            test_df = self.spark.read.jdbc(
                url=self.config.mysql.jdbc_url,
                table="(SELECT 1 as test) as tmp",
                properties=self.config.mysql.connection_properties
            )
            
            if test_df.count() != 1:
                logger.error("MySQLè¿æ¥å¼‚å¸¸")
                return False
            
            # æ£€æŸ¥S3è¿æ¥
            try:
                test_schemas = self.hive_reader.get_table_schema("user_basic_info")
                if not test_schemas:
                    logger.warning("S3è¿æ¥æˆ–æ•°æ®è®¿é—®å¯èƒ½æœ‰é—®é¢˜")
            except:
                logger.warning("S3è¿æ¥æ£€æŸ¥å¤±è´¥")
            
            logger.info("âœ… ç³»ç»Ÿå¥åº·æ£€æŸ¥é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç³»ç»Ÿå¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            return False