import logging
import time
from typing import List, Dict, Any, Optional
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

from src.config.base import BaseConfig
from src.readers.hive_reader import HiveDataReader
from src.readers.rule_reader import RuleReader
from src.engine.parallel_tag_engine import ParallelTagEngine
from src.merger.tag_merger import UnifiedTagMerger, TagMergeStrategy
from src.writers.optimized_mysql_writer import OptimizedMySQLWriter

logger = logging.getLogger(__name__)


class TagScheduler:
    """æ ‡ç­¾è°ƒåº¦å™¨ - å®ç°6ä¸ªåŠŸèƒ½åœºæ™¯çš„å…·ä½“é€»è¾‘"""
    
    def __init__(self, config: BaseConfig, max_workers: int = 4):
        self.config = config
        self.max_workers = max_workers
        self.spark = None
        
        # ç»„ä»¶åˆå§‹åŒ–
        self.rule_reader = None
        self.hive_reader = None
        self.parallel_engine = None
        self.unified_merger = None
        self.mysql_writer = None
    
    def initialize(self):
        """åˆå§‹åŒ–ç»„ä»¶"""
        try:
            logger.info("ğŸš€ åˆå§‹åŒ–æ ‡ç­¾è°ƒåº¦å™¨...")
            
            # åˆå§‹åŒ–Spark
            self.spark = self._create_spark_session()
            
            # åˆå§‹åŒ–ç»„ä»¶
            self.rule_reader = RuleReader(self.spark, self.config.mysql)
            self.hive_reader = HiveDataReader(self.spark, self.config.s3)
            self.parallel_engine = ParallelTagEngine(self.spark, self.max_workers, self.config.mysql)
            self.unified_merger = UnifiedTagMerger(self.spark, self.config.mysql)
            self.mysql_writer = OptimizedMySQLWriter(self.spark, self.config.mysql)
            
            # åˆå§‹åŒ–æ—¶é¢„ç¼“å­˜MySQLç°æœ‰æ ‡ç­¾æ•°æ®
            self.cached_existing_tags = None
            
            # åˆå§‹åŒ–è§„åˆ™æ•°æ®
            self.rule_reader.initialize()
            
            # é¢„ç¼“å­˜ç°æœ‰æ ‡ç­¾æ•°æ®
            self._preload_existing_tags()
            
            logger.info("âœ… æ ‡ç­¾è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ ‡ç­¾è°ƒåº¦å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _create_spark_session(self) -> SparkSession:
        """åˆ›å»ºSparkä¼šè¯"""
        builder = SparkSession.builder
        
        for key, value in self.config.spark.to_dict().items():
            builder = builder.config(key, value)
        
        for key, value in self.config.s3.to_spark_config().items():
            builder = builder.config(key, value)
        
        spark = builder.getOrCreate()
        spark.sparkContext.setLogLevel("WARN")
        
        logger.info(f"Sparkä¼šè¯åˆ›å»ºæˆåŠŸ: {spark.sparkContext.applicationId}")
        return spark
    
    def _preload_existing_tags(self):
        """é¢„ç¼“å­˜MySQLä¸­çš„ç°æœ‰æ ‡ç­¾æ•°æ®"""
        try:
            logger.info("ğŸ”„ é¢„ç¼“å­˜MySQLç°æœ‰æ ‡ç­¾æ•°æ®...")
            
            existing_df = self.spark.read.jdbc(
                url=self.config.mysql.jdbc_url,
                table="user_tags",
                properties=self.config.mysql.connection_properties
            )
            
            if existing_df.count() == 0:
                logger.info("MySQLä¸­æ²¡æœ‰ç°æœ‰æ ‡ç­¾æ•°æ®")
                self.cached_existing_tags = None
                return
            
            # å°†JSONå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°ç»„ç±»å‹å¹¶ç¼“å­˜åˆ°å†…å­˜+ç£ç›˜
            from pyspark.sql.functions import from_json
            from pyspark.sql.types import ArrayType, IntegerType
            from pyspark import StorageLevel
            
            processed_df = existing_df.select(
                "user_id",
                from_json(col("tag_ids"), ArrayType(IntegerType())).alias("tag_ids"),
                "tag_details",
                "created_time",
                "updated_time"
            ).persist(StorageLevel.MEMORY_AND_DISK)
            
            # è§¦å‘ç¼“å­˜
            existing_count = processed_df.count()
            self.cached_existing_tags = processed_df
            
            logger.info(f"âœ… æˆåŠŸé¢„ç¼“å­˜ {existing_count} æ¡ç°æœ‰ç”¨æˆ·æ ‡ç­¾ï¼ˆå†…å­˜+ç£ç›˜æ¨¡å¼ï¼‰")
            
        except Exception as e:
            logger.warning(f"é¢„ç¼“å­˜ç°æœ‰æ ‡ç­¾å¤±è´¥: {str(e)}")
            self.cached_existing_tags = None
    
    # ==================== 6ä¸ªåŠŸèƒ½åœºæ™¯å®ç° ====================
    
    def scenario_1_full_users_full_tags(self) -> bool:
        """
        åœºæ™¯1: å…¨é‡ç”¨æˆ·æ‰“å…¨é‡æ ‡ç­¾
        - å¤šæ ‡ç­¾å¹¶è¡Œè®¡ç®—
        - å†…å­˜åˆå¹¶åŒç”¨æˆ·å¤šæ ‡ç­¾ç»“æœ
        - ä¸ä¸MySQLç°æœ‰æ ‡ç­¾åˆå¹¶
        - ç›´æ¥æ’å…¥MySQLï¼ˆåˆ©ç”¨å”¯ä¸€é”®çº¦æŸï¼‰
        """
        try:
            logger.info("ğŸ¯ åœºæ™¯1: å…¨é‡ç”¨æˆ·æ‰“å…¨é‡æ ‡ç­¾")
            start_time = time.time()
            
            # 1. è¯»å–å…¨é‡æ ‡ç­¾è§„åˆ™
            all_rules = self.rule_reader.read_active_rules()
            if not all_rules:
                logger.warning("æ²¡æœ‰æ´»è·ƒçš„æ ‡ç­¾è§„åˆ™")
                return False
            
            logger.info(f"åŠ è½½äº† {len(all_rules)} ä¸ªæ ‡ç­¾è§„åˆ™")
            
            # 2. è¯»å–å…¨é‡ç”¨æˆ·æ•°æ®
            user_data = self._get_full_user_data()
            user_count = user_data.count()
            logger.info(f"ğŸ“Š ç”Ÿæˆç”¨æˆ·æ•°æ®: {user_count} ä¸ªç”¨æˆ·")
            
            if user_count == 0:
                logger.warning("æ²¡æœ‰ç”¨æˆ·æ•°æ®")
                return False
                
            # æ˜¾ç¤ºæ•°æ®æ ·ä¾‹
            logger.info("ç”¨æˆ·æ•°æ®æ ·ä¾‹:")
            user_data.show(3, truncate=False)
            
            # 3. å¤šæ ‡ç­¾å¹¶è¡Œè®¡ç®— + å†…å­˜åˆå¹¶
            merged_result = self.parallel_engine.compute_tags_with_memory_merge(user_data, all_rules)
            if merged_result is None:
                logger.error("æ ‡ç­¾è®¡ç®—å’Œå†…å­˜åˆå¹¶å¤±è´¥")
                return False
            
            # æ˜¾ç¤ºè®¡ç®—ç»“æœ
            result_count = merged_result.count()
            logger.info(f"ğŸ“Š æ ‡ç­¾è®¡ç®—ç»“æœ: {result_count} ä¸ªç”¨æˆ·æœ‰æ ‡ç­¾")
            
            if result_count > 0:
                logger.info("æ ‡ç­¾è®¡ç®—ç»“æœæ ·ä¾‹:")
                merged_result.show(3, truncate=False)
            
            # 4. ç›´æ¥å†™å…¥MySQLï¼ˆä¸ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶ï¼‰
            success = self.mysql_writer.write_tag_results(merged_result)
            
            # 5. ç»Ÿè®¡è¾“å‡º
            end_time = time.time()
            stats = self.mysql_writer.get_write_statistics()
            
            logger.info(f"""
ğŸ‰ åœºæ™¯1å®Œæˆï¼
â±ï¸  æ‰§è¡Œæ—¶é—´: {end_time - start_time:.2f}ç§’
ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}
            """)
            
            return success
            
        except Exception as e:
            logger.error(f"åœºæ™¯1æ‰§è¡Œå¤±è´¥: {str(e)}")
            return False
    
    def scenario_2_full_users_specific_tags(self, tag_ids: List[int]) -> bool:
        """
        åœºæ™¯2: å…¨é‡ç”¨æˆ·æ‰“æŒ‡å®šæ ‡ç­¾
        - å¤šæ ‡ç­¾å¹¶è¡Œè®¡ç®—
        - å†…å­˜åˆå¹¶åŒç”¨æˆ·å¤šæ ‡ç­¾ç»“æœ
        - ä¸MySQLç°æœ‰æ ‡ç­¾åˆå¹¶
        - æ’å…¥MySQLï¼ˆåˆ©ç”¨å”¯ä¸€é”®çº¦æŸï¼‰
        """
        try:
            logger.info(f"ğŸ¯ åœºæ™¯2: å…¨é‡ç”¨æˆ·æ‰“æŒ‡å®šæ ‡ç­¾ {tag_ids}")
            start_time = time.time()
            
            # 1. è¯»å–æŒ‡å®šæ ‡ç­¾è§„åˆ™
            all_rules = self.rule_reader.read_active_rules()
            target_rules = [rule for rule in all_rules if rule['tag_id'] in tag_ids]
            
            if not target_rules:
                logger.warning(f"æ²¡æœ‰æ‰¾åˆ°æŒ‡å®šæ ‡ç­¾çš„è§„åˆ™: {tag_ids}")
                return False
            
            logger.info(f"åŠ è½½äº† {len(target_rules)} ä¸ªæŒ‡å®šæ ‡ç­¾è§„åˆ™")
            
            # 2. è¯»å–å…¨é‡ç”¨æˆ·æ•°æ®
            user_data = self._get_full_user_data()
            
            # 3. å¤šæ ‡ç­¾å¹¶è¡Œè®¡ç®— + å†…å­˜åˆå¹¶
            memory_merged = self.parallel_engine.compute_tags_with_memory_merge(user_data, target_rules)
            if memory_merged is None:
                logger.error("æ ‡ç­¾è®¡ç®—å’Œå†…å­˜åˆå¹¶å¤±è´¥")
                return False
            
            # 4. ä¸MySQLç°æœ‰æ ‡ç­¾åˆå¹¶ï¼ˆå…³é”®å·®å¼‚ï¼‰- ä½¿ç”¨é¢„ç¼“å­˜æ•°æ®
            final_merged = self.unified_merger.advanced_merger.merge_with_existing_tags(
                memory_merged, 
                self.cached_existing_tags
            )
            if final_merged is None:
                logger.error("ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶å¤±è´¥")
                return False
            
            # 5. å†™å…¥MySQL
            success = self.mysql_writer.write_tag_results(final_merged)
            
            # 6. ç»Ÿè®¡è¾“å‡º
            end_time = time.time()
            stats = self.mysql_writer.get_write_statistics()
            
            logger.info(f"""
ğŸ‰ åœºæ™¯2å®Œæˆï¼
â±ï¸  æ‰§è¡Œæ—¶é—´: {end_time - start_time:.2f}ç§’
ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}
            """)
            
            return success
            
        except Exception as e:
            logger.error(f"åœºæ™¯2æ‰§è¡Œå¤±è´¥: {str(e)}")
            return False
    
    def scenario_3_incremental_users_full_tags(self, days_back: int = 1) -> bool:
        """
        åœºæ™¯3: å¢é‡ç”¨æˆ·æ‰“å…¨é‡æ ‡ç­¾
        - è¯†åˆ«æ–°å¢ç”¨æˆ·
        - å¤šæ ‡ç­¾å¹¶è¡Œè®¡ç®—
        - å†…å­˜åˆå¹¶åŒç”¨æˆ·å¤šæ ‡ç­¾ç»“æœ
        - ä¸ä¸MySQLç°æœ‰æ ‡ç­¾åˆå¹¶ï¼ˆæ–°ç”¨æˆ·ï¼‰
        - ç›´æ¥æ’å…¥MySQL
        """
        try:
            logger.info(f"ğŸ¯ åœºæ™¯3: å¢é‡ç”¨æˆ·æ‰“å…¨é‡æ ‡ç­¾ï¼ˆå›æº¯{days_back}å¤©ï¼‰")
            start_time = time.time()
            
            # 1. è¯»å–å…¨é‡æ ‡ç­¾è§„åˆ™
            all_rules = self.rule_reader.read_active_rules()
            if not all_rules:
                logger.warning("æ²¡æœ‰æ´»è·ƒçš„æ ‡ç­¾è§„åˆ™")
                return False
            
            # 2. è¯†åˆ«çœŸæ­£çš„æ–°å¢ç”¨æˆ·
            new_users = self._identify_truly_new_users(days_back)
            new_user_count = new_users.count()
            
            if new_user_count == 0:
                logger.info("æ²¡æœ‰å‘ç°æ–°å¢ç”¨æˆ·")
                return True
            
            logger.info(f"å‘ç° {new_user_count} ä¸ªæ–°å¢ç”¨æˆ·")
            
            # 3. å¤šæ ‡ç­¾å¹¶è¡Œè®¡ç®— + å†…å­˜åˆå¹¶
            merged_result = self.parallel_engine.compute_tags_with_memory_merge(new_users, all_rules)
            if merged_result is None:
                logger.warning("æ–°å¢ç”¨æˆ·æ²¡æœ‰å‘½ä¸­ä»»ä½•æ ‡ç­¾")
                return True
            
            # 4. ç›´æ¥å†™å…¥MySQLï¼ˆæ–°ç”¨æˆ·ä¸éœ€è¦ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶ï¼‰
            success = self.mysql_writer.write_tag_results(merged_result)
            
            # 5. ç»Ÿè®¡è¾“å‡º
            end_time = time.time()
            stats = self.mysql_writer.get_write_statistics()
            
            logger.info(f"""
ğŸ‰ åœºæ™¯3å®Œæˆï¼
â±ï¸  æ‰§è¡Œæ—¶é—´: {end_time - start_time:.2f}ç§’
ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}
            """)
            
            return success
            
        except Exception as e:
            logger.error(f"åœºæ™¯3æ‰§è¡Œå¤±è´¥: {str(e)}")
            return False
    
    def scenario_4_incremental_users_specific_tags(self, days_back: int, tag_ids: List[int]) -> bool:
        """
        åœºæ™¯4: å¢é‡ç”¨æˆ·æ‰“æŒ‡å®šæ ‡ç­¾
        - è¯†åˆ«æ–°å¢ç”¨æˆ·
        - å¤šæ ‡ç­¾å¹¶è¡Œè®¡ç®—
        - å†…å­˜åˆå¹¶åŒç”¨æˆ·å¤šæ ‡ç­¾ç»“æœ
        - ä¸ä¸MySQLç°æœ‰æ ‡ç­¾åˆå¹¶ï¼ˆæ–°ç”¨æˆ·ï¼‰
        - ç›´æ¥æ’å…¥MySQL
        """
        try:
            logger.info(f"ğŸ¯ åœºæ™¯4: å¢é‡ç”¨æˆ·æ‰“æŒ‡å®šæ ‡ç­¾ï¼ˆå›æº¯{days_back}å¤©ï¼Œæ ‡ç­¾{tag_ids}ï¼‰")
            start_time = time.time()
            
            # 1. è¯»å–æŒ‡å®šæ ‡ç­¾è§„åˆ™
            all_rules = self.rule_reader.read_active_rules()
            target_rules = [rule for rule in all_rules if rule['tag_id'] in tag_ids]
            
            if not target_rules:
                logger.warning(f"æ²¡æœ‰æ‰¾åˆ°æŒ‡å®šæ ‡ç­¾çš„è§„åˆ™: {tag_ids}")
                return False
            
            # 2. è¯†åˆ«æ–°å¢ç”¨æˆ·
            new_users = self._identify_truly_new_users(days_back)
            new_user_count = new_users.count()
            
            if new_user_count == 0:
                logger.info("æ²¡æœ‰å‘ç°æ–°å¢ç”¨æˆ·")
                return True
            
            logger.info(f"å‘ç° {new_user_count} ä¸ªæ–°å¢ç”¨æˆ·")
            
            # 3. å¤šæ ‡ç­¾å¹¶è¡Œè®¡ç®— + å†…å­˜åˆå¹¶
            merged_result = self.parallel_engine.compute_tags_with_memory_merge(new_users, target_rules)
            if merged_result is None:
                logger.warning("æ–°å¢ç”¨æˆ·æ²¡æœ‰å‘½ä¸­æŒ‡å®šæ ‡ç­¾")
                return True
            
            # 4. ç›´æ¥å†™å…¥MySQLï¼ˆæ–°ç”¨æˆ·ä¸éœ€è¦ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶ï¼‰
            success = self.mysql_writer.write_tag_results(merged_result)
            
            # 5. ç»Ÿè®¡è¾“å‡º
            end_time = time.time()
            stats = self.mysql_writer.get_write_statistics()
            
            logger.info(f"""
ğŸ‰ åœºæ™¯4å®Œæˆï¼
â±ï¸  æ‰§è¡Œæ—¶é—´: {end_time - start_time:.2f}ç§’
ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}
            """)
            
            return success
            
        except Exception as e:
            logger.error(f"åœºæ™¯4æ‰§è¡Œå¤±è´¥: {str(e)}")
            return False
    
    def scenario_5_specific_users_full_tags(self, user_ids: List[str]) -> bool:
        """
        åœºæ™¯5: æŒ‡å®šç”¨æˆ·æ‰“å…¨é‡æ ‡ç­¾
        - è¿‡æ»¤æŒ‡å®šç”¨æˆ·
        - å¤šæ ‡ç­¾å¹¶è¡Œè®¡ç®—
        - å†…å­˜åˆå¹¶åŒç”¨æˆ·å¤šæ ‡ç­¾ç»“æœ
        - ä¸ä¸MySQLç°æœ‰æ ‡ç­¾åˆå¹¶
        - ç›´æ¥æ’å…¥MySQL
        """
        try:
            logger.info(f"ğŸ¯ åœºæ™¯5: æŒ‡å®šç”¨æˆ·æ‰“å…¨é‡æ ‡ç­¾ {user_ids}")
            start_time = time.time()
            
            # 1. è¯»å–å…¨é‡æ ‡ç­¾è§„åˆ™
            all_rules = self.rule_reader.read_active_rules()
            if not all_rules:
                logger.warning("æ²¡æœ‰æ´»è·ƒçš„æ ‡ç­¾è§„åˆ™")
                return False
            
            # 2. è·å–æŒ‡å®šç”¨æˆ·æ•°æ®
            user_data = self._get_specific_user_data(user_ids)
            filtered_count = user_data.count()
            
            if filtered_count == 0:
                logger.warning(f"æ²¡æœ‰æ‰¾åˆ°æŒ‡å®šç”¨æˆ·: {user_ids}")
                return False
            
            logger.info(f"æ‰¾åˆ° {filtered_count} ä¸ªæŒ‡å®šç”¨æˆ·")
            
            # 3. å¤šæ ‡ç­¾å¹¶è¡Œè®¡ç®— + å†…å­˜åˆå¹¶
            merged_result = self.parallel_engine.compute_tags_with_memory_merge(user_data, all_rules)
            if merged_result is None:
                logger.warning("æŒ‡å®šç”¨æˆ·æ²¡æœ‰å‘½ä¸­ä»»ä½•æ ‡ç­¾")
                return True
            
            # 4. ç›´æ¥å†™å…¥MySQLï¼ˆæŒ‡å®šç”¨æˆ·åœºæ™¯ä¸ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶ï¼‰
            success = self.mysql_writer.write_tag_results(merged_result)
            
            # 5. ç»Ÿè®¡è¾“å‡º
            end_time = time.time()
            stats = self.mysql_writer.get_write_statistics()
            
            logger.info(f"""
ğŸ‰ åœºæ™¯5å®Œæˆï¼
â±ï¸  æ‰§è¡Œæ—¶é—´: {end_time - start_time:.2f}ç§’
ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}
            """)
            
            return success
            
        except Exception as e:
            logger.error(f"åœºæ™¯5æ‰§è¡Œå¤±è´¥: {str(e)}")
            return False
    
    def scenario_6_specific_users_specific_tags(self, user_ids: List[str], tag_ids: List[int]) -> bool:
        """
        åœºæ™¯6: æŒ‡å®šç”¨æˆ·æ‰“æŒ‡å®šæ ‡ç­¾
        - è¿‡æ»¤æŒ‡å®šç”¨æˆ·
        - å¤šæ ‡ç­¾å¹¶è¡Œè®¡ç®—
        - å†…å­˜åˆå¹¶åŒç”¨æˆ·å¤šæ ‡ç­¾ç»“æœ
        - ä¸MySQLç°æœ‰æ ‡ç­¾åˆå¹¶
        - æ’å…¥MySQL
        """
        try:
            logger.info(f"ğŸ¯ åœºæ™¯6: æŒ‡å®šç”¨æˆ·æ‰“æŒ‡å®šæ ‡ç­¾ï¼ˆç”¨æˆ·{user_ids}ï¼Œæ ‡ç­¾{tag_ids}ï¼‰")
            start_time = time.time()
            
            # 1. è¯»å–æŒ‡å®šæ ‡ç­¾è§„åˆ™
            all_rules = self.rule_reader.read_active_rules()
            target_rules = [rule for rule in all_rules if rule['tag_id'] in tag_ids]
            
            if not target_rules:
                logger.warning(f"æ²¡æœ‰æ‰¾åˆ°æŒ‡å®šæ ‡ç­¾çš„è§„åˆ™: {tag_ids}")
                return False
            
            # 2. è·å–æŒ‡å®šç”¨æˆ·æ•°æ®
            user_data = self._get_specific_user_data(user_ids)
            filtered_count = user_data.count()
            
            if filtered_count == 0:
                logger.warning(f"æ²¡æœ‰æ‰¾åˆ°æŒ‡å®šç”¨æˆ·: {user_ids}")
                return False
            
            logger.info(f"æ‰¾åˆ° {filtered_count} ä¸ªæŒ‡å®šç”¨æˆ·")
            
            # 3. å¤šæ ‡ç­¾å¹¶è¡Œè®¡ç®— + å†…å­˜åˆå¹¶
            memory_merged = self.parallel_engine.compute_tags_with_memory_merge(user_data, target_rules)
            if memory_merged is None:
                logger.warning("æŒ‡å®šç”¨æˆ·æ²¡æœ‰å‘½ä¸­æŒ‡å®šæ ‡ç­¾")
                return True
            
            # 4. ä¸MySQLç°æœ‰æ ‡ç­¾åˆå¹¶ï¼ˆå…³é”®å·®å¼‚ï¼‰- ä½¿ç”¨é¢„ç¼“å­˜æ•°æ®
            final_merged = self.unified_merger.advanced_merger.merge_with_existing_tags(
                memory_merged, 
                self.cached_existing_tags
            )
            if final_merged is None:
                logger.error("ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶å¤±è´¥")
                return False
            
            # 5. å†™å…¥MySQL
            success = self.mysql_writer.write_tag_results(final_merged)
            
            # 6. ç»Ÿè®¡è¾“å‡º
            end_time = time.time()
            stats = self.mysql_writer.get_write_statistics()
            
            logger.info(f"""
ğŸ‰ åœºæ™¯6å®Œæˆï¼
â±ï¸  æ‰§è¡Œæ—¶é—´: {end_time - start_time:.2f}ç§’
ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}
            """)
            
            return success
            
        except Exception as e:
            logger.error(f"åœºæ™¯6æ‰§è¡Œå¤±è´¥: {str(e)}")
            return False
    
    # ==================== è¾…åŠ©æ–¹æ³• ====================
    
    def _get_full_user_data(self):
        """è·å–å…¨é‡ç”¨æˆ·æ•°æ®"""
        if self.config.environment == 'local':
            return self._generate_production_like_data()
        else:
            # ç”Ÿäº§ç¯å¢ƒä»Hiveè¯»å–
            return self.hive_reader.read_all_user_data()
    
    def _get_specific_user_data(self, user_ids: List[str]):
        """è·å–æŒ‡å®šç”¨æˆ·æ•°æ®"""
        full_data = self._get_full_user_data()
        return full_data.filter(col("user_id").isin(user_ids))
    
    def _identify_truly_new_users(self, days_back: int):
        """è¯†åˆ«çœŸæ­£çš„æ–°å¢ç”¨æˆ·"""
        try:
            logger.info(f"ğŸ” è¯†åˆ«æœ€è¿‘ {days_back} å¤©çš„æ–°å¢ç”¨æˆ·...")
            
            # 1. è·å–åŒ…å«æ–°å¢ç”¨æˆ·çš„å…¨é‡æ•°æ®
            hive_all_users = self._generate_hive_data_with_new_users(days_back)
            
            # 2. è¯»å–MySQLä¸­å·²æœ‰ç”¨æˆ·
            mysql_existing_users = self._read_existing_users_from_mysql()
            
            # 3. æ‰¾å‡ºæ–°å¢ç”¨æˆ·ï¼ˆleft_anti joinï¼‰
            new_users = hive_all_users.join(
                mysql_existing_users,
                "user_id",
                "left_anti"
            )
            
            new_user_count = new_users.count()
            logger.info(f"âœ… è¯†åˆ«å‡º {new_user_count} ä¸ªæ–°å¢ç”¨æˆ·")
            
            return new_users
            
        except Exception as e:
            logger.error(f"è¯†åˆ«æ–°å¢ç”¨æˆ·å¤±è´¥: {str(e)}")
            raise
    
    def _read_existing_users_from_mysql(self):
        """ä»MySQLè¯»å–å·²æœ‰ç”¨æˆ·åˆ—è¡¨"""
        try:
            existing_users_df = self.spark.read.jdbc(
                url=self.config.mysql.jdbc_url,
                table="user_tags",
                properties=self.config.mysql.connection_properties
            ).select("user_id").distinct()
            
            logger.info(f"MySQLä¸­å·²æœ‰ {existing_users_df.count()} ä¸ªç”¨æˆ·")
            return existing_users_df
            
        except Exception as e:
            logger.warning(f"è¯»å–å·²æœ‰ç”¨æˆ·å¤±è´¥ï¼ˆå¯èƒ½æ˜¯é¦–æ¬¡è¿è¡Œï¼‰: {str(e)}")
            # è¿”å›ç©ºDataFrame
            from pyspark.sql.types import StructType, StructField, StringType
            empty_schema = StructType([StructField("user_id", StringType(), True)])
            return self.spark.createDataFrame([], empty_schema)
    
    def _generate_production_like_data(self):
        """ç”Ÿæˆç”Ÿäº§çº§æ¨¡æ‹Ÿæ•°æ®ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰"""
        from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType
        from pyspark.sql import Row
        from datetime import date, timedelta
        import random
        
        schema = StructType([
            StructField("user_id", StringType(), True),
            StructField("age", IntegerType(), True),
            StructField("total_asset_value", DoubleType(), True),
            StructField("trade_count_30d", IntegerType(), True),
            StructField("risk_score", DoubleType(), True),
            StructField("registration_date", DateType(), True),
            StructField("user_level", StringType(), True),
            StructField("kyc_status", StringType(), True),
            StructField("cash_balance", DoubleType(), True),
            StructField("last_login_date", DateType(), True)
        ])
        
        test_users = []
        for i in range(100):
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„é«˜å‡€å€¼ç”¨æˆ·
            if i < 50:
                total_asset = random.uniform(150000, 500000)
                cash_balance = random.uniform(60000, 150000)
            else:
                total_asset = random.uniform(1000, 80000)
                cash_balance = random.uniform(1000, 40000)
            
            # ç¡®ä¿æœ‰VIPç”¨æˆ·
            if i < 20:
                user_level = random.choice(["VIP2", "VIP3"])
                kyc_status = "verified"
            else:
                user_level = random.choice(["BRONZE", "SILVER", "GOLD", "VIP1"])
                kyc_status = random.choice(["verified", "pending", "rejected"])
            
            # å¹´é¾„åˆ†å¸ƒ
            if i < 30:
                age = random.randint(18, 30)
            else:
                age = random.randint(31, 65)
            
            # äº¤æ˜“æ´»è·ƒåº¦
            if i < 80:
                trade_count = random.randint(15, 50)
            else:
                trade_count = random.randint(0, 8)
            
            # é£é™©è¯„åˆ†
            if i < 25:
                risk_score = random.uniform(10, 28)
            else:
                risk_score = random.uniform(35, 80)
            
            # æ³¨å†Œå’Œç™»å½•æ—¶é—´
            if i < 15:
                registration_date = date.today() - timedelta(days=random.randint(1, 25))
                last_login_date = date.today() - timedelta(days=random.randint(0, 5))
            else:
                registration_date = date.today() - timedelta(days=random.randint(40, 300))
                last_login_date = date.today() - timedelta(days=random.randint(10, 25))
                
            user_data = Row(
                user_id=f"user_{i+1:06d}",
                age=age,
                total_asset_value=total_asset,
                trade_count_30d=trade_count,
                risk_score=risk_score,
                registration_date=registration_date,
                user_level=user_level,
                kyc_status=kyc_status,
                cash_balance=cash_balance,
                last_login_date=last_login_date
            )
            test_users.append(user_data)
        
        test_df = self.spark.createDataFrame(test_users, schema)
        logger.info(f"ç”Ÿæˆäº† {test_df.count()} æ¡ç”Ÿäº§çº§æ¨¡æ‹Ÿæ•°æ®")
        return test_df
    
    def _generate_hive_data_with_new_users(self, days_back: int):
        """ç”ŸæˆåŒ…å«æ–°å¢ç”¨æˆ·çš„Hiveæ¨¡æ‹Ÿæ•°æ®"""
        from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType
        from pyspark.sql import Row
        from datetime import date, timedelta
        import random
        
        schema = StructType([
            StructField("user_id", StringType(), True),
            StructField("age", IntegerType(), True),
            StructField("total_asset_value", DoubleType(), True),
            StructField("trade_count_30d", IntegerType(), True),
            StructField("risk_score", DoubleType(), True),
            StructField("registration_date", DateType(), True),
            StructField("user_level", StringType(), True),
            StructField("kyc_status", StringType(), True),
            StructField("cash_balance", DoubleType(), True),
            StructField("last_login_date", DateType(), True)
        ])
        
        all_users = []
        
        # 1. æ·»åŠ ç°æœ‰ç”¨æˆ·ï¼ˆä¸MySQLé‡å¤ï¼‰
        for i in range(1, 6):
            user_data = Row(
                user_id=f"user_{i:06d}",
                age=random.randint(25, 50),
                total_asset_value=random.uniform(50000, 300000),
                trade_count_30d=random.randint(10, 30),
                risk_score=random.uniform(20, 70),
                registration_date=date.today() - timedelta(days=random.randint(30, 365)),
                user_level=random.choice(["SILVER", "GOLD", "VIP1"]),
                kyc_status="verified",
                cash_balance=random.uniform(10000, 100000),
                last_login_date=date.today() - timedelta(days=random.randint(0, 7))
            )
            all_users.append(user_data)
        
        # 2. æ·»åŠ æ–°å¢ç”¨æˆ·
        for i in range(10):
            reg_date = date.today() - timedelta(days=random.randint(1, days_back))
            
            user_data = Row(
                user_id=f"new_user_{i+1:04d}",
                age=random.randint(18, 45),
                total_asset_value=random.uniform(10000, 200000),
                trade_count_30d=random.randint(5, 25),
                risk_score=random.uniform(15, 60),
                registration_date=reg_date,
                user_level=random.choice(["BRONZE", "SILVER", "GOLD", "VIP1"]),
                kyc_status=random.choice(["verified", "pending"]),
                cash_balance=random.uniform(5000, 80000),
                last_login_date=date.today() - timedelta(days=random.randint(0, 3))
            )
            all_users.append(user_data)
        
        hive_df = self.spark.createDataFrame(all_users, schema)
        logger.info(f"ç”Ÿæˆäº† {hive_df.count()} æ¡Hiveæ¨¡æ‹Ÿæ•°æ®ï¼ˆåŒ…å«æ–°è€ç”¨æˆ·ï¼‰")
        return hive_df
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            logger.info("ğŸ§¹ æ¸…ç†åœºæ™¯è°ƒåº¦å™¨èµ„æº...")
            
            # æ¸…ç†é¢„ç¼“å­˜çš„æ ‡ç­¾æ•°æ®
            if self.cached_existing_tags is not None:
                self.cached_existing_tags.unpersist()
                logger.info("âœ… æ¸…ç†é¢„ç¼“å­˜æ ‡ç­¾æ•°æ®å®Œæˆ")
            
            if self.unified_merger:
                self.unified_merger.cleanup()
            
            if self.rule_reader:
                self.rule_reader.cleanup()
            
            if self.spark:
                self.spark.catalog.clearCache()
                self.spark.stop()
            
            logger.info("âœ… åœºæ™¯è°ƒåº¦å™¨èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"èµ„æºæ¸…ç†å¤±è´¥: {str(e)}")