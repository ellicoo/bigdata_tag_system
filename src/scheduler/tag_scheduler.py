import logging
import time
from typing import List, Dict, Any, Optional
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col

from src.config.base import BaseConfig
from src.readers.hive_reader import HiveDataReader
from src.readers.rule_reader import RuleReader
from src.engine.task_parallel_engine import TaskBasedParallelEngine
from src.merger.tag_merger import UnifiedTagMerger, TagMergeStrategy
from src.writers.optimized_mysql_writer import OptimizedMySQLWriter

logger = logging.getLogger(__name__)


class TagScheduler:
    """æ ‡ç­¾è°ƒåº¦å™¨ - åŸºäºä»»åŠ¡åŒ–æ¶æ„çš„æ ‡ç­¾è®¡ç®—è°ƒåº¦"""
    
    def __init__(self, config: BaseConfig, max_workers: int = 4):
        self.config = config
        self.max_workers = max_workers
        self.spark = None
        
        # ç»„ä»¶åˆå§‹åŒ–
        self.rule_reader = None
        self.hive_reader = None
        self.task_engine = None
        self.unified_merger = None
        self.mysql_writer = None
    
    def initialize(self):
        """åˆå§‹åŒ–ç»„ä»¶"""
        try:
            logger.info("ğŸš€ åˆå§‹åŒ–æ ‡ç­¾è°ƒåº¦å™¨...")
            
            # åˆå§‹åŒ–Spark
            self.spark = self._create_spark_session()
            
            # åˆå§‹åŒ–ç»„ä»¶
            self.rule_reader = RuleReader(self.spark, self.config.mysql)
            self.hive_reader = HiveDataReader(self.spark, self.config)
            
            # åˆå§‹åŒ–è§„åˆ™æ•°æ®
            self.rule_reader.initialize()
            
            # åˆå§‹åŒ–ä»»åŠ¡å¼•æ“ï¼Œä¼ å…¥å·²åˆå§‹åŒ–çš„rule_reader
            self.task_engine = TaskBasedParallelEngine(self.spark, self.config, self.max_workers, self.rule_reader)
            self.unified_merger = UnifiedTagMerger(self.spark, self.config.mysql)
            self.mysql_writer = OptimizedMySQLWriter(self.spark, self.config.mysql)
            
            # åˆå§‹åŒ–æ—¶é¢„ç¼“å­˜MySQLç°æœ‰æ ‡ç­¾æ•°æ®
            self.cached_existing_tags = None
            
            # é¢„ç¼“å­˜ç°æœ‰æ ‡ç­¾æ•°æ®
            self._preload_existing_tags()
            
            logger.info("âœ… æ ‡ç­¾è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ ‡ç­¾è°ƒåº¦å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _create_spark_session(self) -> SparkSession:
        """åˆ›å»ºSparkä¼šè¯"""
        builder = SparkSession.builder
        
        for key, value in self.config.spark.to_dict().items():
            builder = builder.config(key, value)
        
        # å¯ç”¨Hiveæ”¯æŒï¼ˆåŸºäºæ‚¨çš„æ–¹å¼ï¼‰
        builder = builder.enableHiveSupport()
        
        # ç§»é™¤S3é…ç½®ï¼Œç›´æ¥è¯»å–Hiveè¡¨
        
        spark = builder.getOrCreate()
        spark.sparkContext.setLogLevel("ERROR")  # æ›´ä¸¥æ ¼çš„æ—¥å¿—çº§åˆ«ï¼Œåªæ˜¾ç¤ºé”™è¯¯
        
        # å…³é—­Sparkçš„è¯¦ç»†æ—¥å¿—
        spark.sparkContext._jvm.org.apache.log4j.Logger.getLogger("org").setLevel(
            spark.sparkContext._jvm.org.apache.log4j.Level.ERROR
        )
        spark.sparkContext._jvm.org.apache.log4j.Logger.getLogger("akka").setLevel(
            spark.sparkContext._jvm.org.apache.log4j.Level.ERROR
        )
        
        logger.info(f"Sparkä¼šè¯åˆ›å»ºæˆåŠŸ: {spark.sparkContext.applicationId}")
        return spark
    
    def _preload_existing_tags(self):
        """é¢„ç¼“å­˜MySQLä¸­çš„ç°æœ‰æ ‡ç­¾æ•°æ®"""
        try:
            logger.info("ğŸ”„ é¢„ç¼“å­˜MySQLç°æœ‰ç”¨æˆ·æ ‡ç­¾æ•°æ®-persist(å†…å­˜&ç£ç›˜)...")
            
            existing_df = self.spark.read.jdbc(
                url=self.config.mysql.jdbc_url,
                table="user_tags",
                properties=self.config.mysql.connection_properties
            )
            
            if existing_df.count() == 0:
                logger.info("MySQLä¸­æš‚æ— ç°æœ‰ç”¨æˆ·æ ‡ç­¾æ•°æ®")
                self.cached_existing_tags = None
            else:
                # é¢„ç¼“å­˜ç°æœ‰æ ‡ç­¾æ•°æ®ï¼Œè¿›è¡ŒJSONè½¬æ¢
                from pyspark import StorageLevel
                from pyspark.sql.functions import from_json
                from pyspark.sql.types import ArrayType, IntegerType
                
                # JSONè½¬æ¢ï¼šå°†tag_idsä»å­—ç¬¦ä¸²è½¬ä¸ºæ•°ç»„
                processed_df = existing_df.select(
                    "user_id",
                    from_json(col("tag_ids"), ArrayType(IntegerType())).alias("tag_ids"),
                    "tag_details",
                    "created_time", 
                    "updated_time"
                )
                
                self.cached_existing_tags = processed_df.persist(StorageLevel.MEMORY_AND_DISK)
                # è§¦å‘ç¼“å­˜
                cached_count = self.cached_existing_tags.count()
                logger.info(f"âœ… é¢„ç¼“å­˜MySQLç°æœ‰ç”¨æˆ·æ ‡ç­¾æ•°æ®å®Œæˆï¼ˆå«JSONè½¬æ¢ï¼‰ï¼Œå…± {cached_count} æ¡è®°å½•")
            
        except Exception as e:
            logger.warning(f"âš ï¸ é¢„ç¼“å­˜MySQLç°æœ‰ç”¨æˆ·æ ‡ç­¾æ•°æ®å¤±è´¥: {str(e)}")
            self.cached_existing_tags = None
    
    def health_check(self) -> bool:
        """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
        try:
            logger.info("ğŸ¥ æ‰§è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥...")
            
            # 1. æ£€æŸ¥Sparkè¿æ¥
            if not self.spark:
                logger.error("âŒ Sparkä¼šè¯æœªåˆå§‹åŒ–")
                return False
            
            # 2. æ£€æŸ¥è§„åˆ™è¯»å–å™¨
            if not self.rule_reader:
                logger.error("âŒ è§„åˆ™è¯»å–å™¨æœªåˆå§‹åŒ–")
                return False
            
            # 3. æ£€æŸ¥ä»»åŠ¡å¼•æ“
            if not self.task_engine:
                logger.error("âŒ ä»»åŠ¡å¼•æ“æœªåˆå§‹åŒ–")
                return False
            
            # 4. æ£€æŸ¥æ•°æ®åº“è¿æ¥
            try:
                test_df = self.spark.read.jdbc(
                    url=self.config.mysql.jdbc_url,
                    table="(SELECT 1 as test_connection) as test",
                    properties=self.config.mysql.connection_properties
                )
                test_df.count()
                logger.info("âœ… MySQLè¿æ¥æ­£å¸¸")
            except Exception as e:
                logger.error(f"âŒ MySQLè¿æ¥å¤±è´¥: {str(e)}")
                return False
            
            # 5. æ£€æŸ¥ä»»åŠ¡æ³¨å†ŒçŠ¶æ€
            available_tasks = self.get_available_tasks()
            logger.info(f"âœ… å·²æ³¨å†Œä»»åŠ¡: {len(available_tasks)} ä¸ª")
            
            logger.info("ğŸ‰ ç³»ç»Ÿå¥åº·æ£€æŸ¥é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç³»ç»Ÿå¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            return False
    
    # ==================== ä»»åŠ¡åŒ–æ¶æ„æ–¹æ³• ====================
    
    def scenario_task_all_users_all_tags(self, user_filter: Optional[List[str]] = None) -> bool:
        """ä»»åŠ¡åŒ–åœºæ™¯ï¼šå…¨é‡ç”¨æˆ·å…¨é‡æ ‡ç­¾è®¡ç®—"""
        try:
            start_time = time.time()
            logger.info(f"ğŸ¯ å¼€å§‹ä»»åŠ¡åŒ–å…¨é‡ç”¨æˆ·å…¨é‡æ ‡ç­¾è®¡ç®—")
            logger.info(f"ğŸ‘¥ ç”¨æˆ·è¿‡æ»¤: {user_filter if user_filter else 'å…¨é‡ç”¨æˆ·'}")
            
            # 1. ä½¿ç”¨ä»»åŠ¡å¼•æ“æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            task_results = self.task_engine.execute_all_tasks(user_filter)
            
            if task_results is None or task_results.count() == 0:
                logger.info("ğŸ“Š å…¨é‡æ ‡ç­¾ä»»åŠ¡æ‰§è¡Œå®Œæˆ - æ— ç”¨æˆ·ç¬¦åˆä»»ä½•æ ‡ç­¾æ¡ä»¶ (è¿™æ˜¯æ­£å¸¸æƒ…å†µ)")
                return True
            
            # 2. ä¸MySQLå·²å­˜åœ¨æ ‡ç­¾åˆå¹¶ï¼ˆæ‰€æœ‰åœºæ™¯éƒ½éœ€è¦åˆå¹¶ï¼‰
            logger.info("ğŸ”„ å¼€å§‹ä¸MySQLå·²å­˜åœ¨æ ‡ç­¾åˆå¹¶...")
            final_merged = self.unified_merger.advanced_merger.merge_with_existing_tags(
                task_results, 
                self.cached_existing_tags
            )
            if final_merged is None:
                logger.error("âŒ ä¸MySQLå·²å­˜åœ¨æ ‡ç­¾åˆå¹¶å¤±è´¥")
                return False
            
            logger.info(f"âœ… ä¸MySQLå·²å­˜åœ¨æ ‡ç­¾åˆå¹¶å®Œæˆï¼Œæœ€ç»ˆå½±å“ç”¨æˆ·æ•°: {final_merged.count()}")
            
            # 3. å†™å…¥MySQL
            logger.info("ğŸ“ å†™å…¥åˆå¹¶åçš„æ ‡ç­¾ç»“æœåˆ°MySQL...")
            success = self.mysql_writer.write_tag_results(final_merged)
            
            # 4. ç»Ÿè®¡è¾“å‡º
            end_time = time.time()
            stats = self.mysql_writer.get_write_statistics()
            
            logger.info(f"""
ğŸ‰ ä»»åŠ¡åŒ–å…¨é‡ç”¨æˆ·å…¨é‡æ ‡ç­¾è®¡ç®—å®Œæˆï¼ˆå«MySQLæ ‡ç­¾åˆå¹¶ï¼‰ï¼
ğŸ·ï¸  æ‰§è¡Œäº†æ‰€æœ‰å·²æ³¨å†Œçš„ä»»åŠ¡ç±»
ğŸ‘¥ å½±å“ç”¨æˆ·æ•°: {final_merged.count()}
â±ï¸  æ‰§è¡Œæ—¶é—´: {end_time - start_time:.2f}ç§’
ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}
            """)
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡åŒ–å…¨é‡æ ‡ç­¾è®¡ç®—å¤±è´¥: {str(e)}")
            return False
        finally:
            # æ¸…ç†ä»»åŠ¡å¼•æ“ç¼“å­˜
            if self.task_engine:
                self.task_engine.cleanup_cache()
    
    def get_available_tasks(self) -> Dict[int, str]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„æ ‡ç­¾ä»»åŠ¡"""
        from src.tasks.task_factory import TagTaskFactory
        return TagTaskFactory.get_all_available_tasks()
    
    def get_task_summary(self) -> Dict[int, Dict[str, Any]]:
        """è·å–ä»»åŠ¡æ‘˜è¦ä¿¡æ¯"""
        from src.tasks.task_registry import get_task_summary
        return get_task_summary()
    
    def scenario_task_all_users_specific_tags(self, tag_ids: List[int]) -> Optional[DataFrame]:
        """
        ä»»åŠ¡åŒ–åœºæ™¯: å…¨é‡ç”¨æˆ·è·‘æŒ‡å®šæ ‡ç­¾ä»»åŠ¡
        - æ‰§è¡ŒæŒ‡å®šæ ‡ç­¾IDå¯¹åº”çš„ä»»åŠ¡ç±»
        - å¹¶è¡Œè®¡ç®—æŒ‡å®šæ ‡ç­¾
        - ä¸MySQLç°æœ‰æ ‡ç­¾åˆå¹¶
        - æ’å…¥MySQL
        """
        try:
            logger.info(f"ğŸ¯ ä»»åŠ¡åŒ–åœºæ™¯: å…¨é‡ç”¨æˆ·è·‘æŒ‡å®šæ ‡ç­¾ä»»åŠ¡ (æ ‡ç­¾ID: {tag_ids})")
            start_time = time.time()
            
            # ä½¿ç”¨ä»»åŠ¡å¼•æ“æ‰§è¡ŒæŒ‡å®šæ ‡ç­¾ä»»åŠ¡
            result = self.task_engine.execute_specific_tag_tasks(tag_ids)
            
            if result is None:
                logger.warning(f"æŒ‡å®šæ ‡ç­¾ä»»åŠ¡æ²¡æœ‰äº§ç”Ÿç»“æœ: {tag_ids}")
                return None
            
            # ä¸MySQLç°æœ‰æ ‡ç­¾åˆå¹¶
            merged_result = self.unified_merger.advanced_merger.merge_with_existing_tags(
                result, 
                self.cached_existing_tags
            )
            
            # å†™å…¥MySQL
            success = self.mysql_writer.write_tag_results(merged_result)
            
            # ç»Ÿè®¡è¾“å‡º
            end_time = time.time()
            stats = self.mysql_writer.get_write_statistics()
            
            logger.info(f"""
ğŸ‰ ä»»åŠ¡åŒ–åœºæ™¯å®Œæˆï¼
â±ï¸  æ‰§è¡Œæ—¶é—´: {end_time - start_time:.2f}ç§’
ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}
            """)
            
            return merged_result if success else None
            
        except Exception as e:
            logger.error(f"ä»»åŠ¡åŒ–åœºæ™¯æ‰§è¡Œå¤±è´¥: {str(e)}")
            return None
    
    def scenario_task_specific_users_specific_tags(self, user_ids: List[str], tag_ids: List[int]) -> Optional[DataFrame]:
        """
        ä»»åŠ¡åŒ–åœºæ™¯: æŒ‡å®šç”¨æˆ·è·‘æŒ‡å®šæ ‡ç­¾ä»»åŠ¡
        - æ‰§è¡ŒæŒ‡å®šæ ‡ç­¾IDå¯¹åº”çš„ä»»åŠ¡ç±»
        - è¿‡æ»¤æŒ‡å®šç”¨æˆ·
        - å¹¶è¡Œè®¡ç®—æŒ‡å®šæ ‡ç­¾
        - ä¸MySQLç°æœ‰æ ‡ç­¾åˆå¹¶
        - æ’å…¥MySQL
        """
        try:
            logger.info(f"ğŸ¯ ä»»åŠ¡åŒ–åœºæ™¯: æŒ‡å®šç”¨æˆ·è·‘æŒ‡å®šæ ‡ç­¾ä»»åŠ¡ (ç”¨æˆ·: {user_ids}, æ ‡ç­¾ID: {tag_ids})")
            start_time = time.time()
            
            # ä½¿ç”¨ä»»åŠ¡å¼•æ“æ‰§è¡ŒæŒ‡å®šæ ‡ç­¾ä»»åŠ¡ï¼Œä¼ å…¥ç”¨æˆ·è¿‡æ»¤
            result = self.task_engine.execute_specific_tag_tasks(tag_ids, user_ids)
            
            if result is None:
                logger.warning(f"æŒ‡å®šç”¨æˆ·æŒ‡å®šæ ‡ç­¾ä»»åŠ¡æ²¡æœ‰äº§ç”Ÿç»“æœ: ç”¨æˆ·{user_ids}, æ ‡ç­¾{tag_ids}")
                return None
            
            # ä¸MySQLç°æœ‰æ ‡ç­¾åˆå¹¶
            merged_result = self.unified_merger.advanced_merger.merge_with_existing_tags(
                result, 
                self.cached_existing_tags
            )
            
            # å†™å…¥MySQL
            success = self.mysql_writer.write_tag_results(merged_result)
            
            # ç»Ÿè®¡è¾“å‡º
            end_time = time.time()
            stats = self.mysql_writer.get_write_statistics()
            
            logger.info(f"""
ğŸ‰ ä»»åŠ¡åŒ–åœºæ™¯å®Œæˆï¼
â±ï¸  æ‰§è¡Œæ—¶é—´: {end_time - start_time:.2f}ç§’
ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}
            """)
            
            return merged_result if success else None
            
        except Exception as e:
            logger.error(f"ä»»åŠ¡åŒ–åœºæ™¯æ‰§è¡Œå¤±è´¥: {str(e)}")
            return None
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # æ¸…ç†ä»»åŠ¡å¼•æ“ç¼“å­˜
            if self.task_engine:
                self.task_engine.cleanup_cache()
            
            # æ¸…ç†é¢„ç¼“å­˜çš„æ ‡ç­¾æ•°æ®
            if self.cached_existing_tags:
                self.cached_existing_tags.unpersist()
            
            # åœæ­¢Sparkä¼šè¯
            if self.spark:
                self.spark.stop()
            
            logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ èµ„æºæ¸…ç†å¼‚å¸¸: {str(e)}")    
    def _generate_production_like_data(self, source_name: str = None) -> DataFrame:
        """ç”Ÿæˆç”Ÿäº§ç¯å¢ƒæ¨¡æ‹Ÿçš„æµ‹è¯•æ•°æ®"""
        try:
            logger.info("ğŸ“Š ç”Ÿæˆç”Ÿäº§ç¯å¢ƒæ¨¡æ‹Ÿæµ‹è¯•æ•°æ®...")
            
            import random
            from datetime import datetime, timedelta
            from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType
            from pyspark.sql.functions import lit
            import json
            
            # ç”ŸæˆåŸºç¡€ç”¨æˆ·æ•°æ®
            num_users = 300
            
            # ç”Ÿæˆç”¨æˆ·åŸºç¡€ä¿¡æ¯
            user_basic_data = []
            for i in range(num_users):
                user_id = f"user_{i:06d}"
                
                # ç”Ÿæˆä¸åŒç±»å‹çš„ç”¨æˆ·æ•°æ®
                if i < 50:  # é«˜å‡€å€¼ç”¨æˆ·
                    total_asset = random.uniform(150000, 500000)
                    cash_balance = random.uniform(60000, 150000)
                    user_level = random.choice(["VIP1", "VIP2", "VIP3"])
                    kyc_status = "verified"
                    risk_score = random.uniform(20, 50)
                    age = random.randint(30, 55)
                    trade_count = random.randint(15, 40)
                elif i < 70:  # VIPç”¨æˆ·
                    total_asset = random.uniform(80000, 200000)
                    cash_balance = random.uniform(30000, 80000)
                    user_level = random.choice(["VIP2", "VIP3"])
                    kyc_status = "verified"
                    risk_score = random.uniform(15, 35)
                    age = random.randint(25, 50)
                    trade_count = random.randint(10, 25)
                elif i < 100:  # å¹´è½»ç”¨æˆ·
                    total_asset = random.uniform(5000, 50000)
                    cash_balance = random.uniform(1000, 20000)
                    user_level = random.choice(["Regular", "VIP1"])
                    kyc_status = random.choice(["verified", "pending"])
                    risk_score = random.uniform(25, 60)
                    age = random.randint(18, 30)
                    trade_count = random.randint(5, 20)
                elif i < 180:  # æ´»è·ƒäº¤æ˜“è€…
                    total_asset = random.uniform(20000, 100000)
                    cash_balance = random.uniform(5000, 40000)
                    user_level = random.choice(["Regular", "VIP1", "VIP2"])
                    kyc_status = "verified"
                    risk_score = random.uniform(30, 70)
                    age = random.randint(25, 45)
                    trade_count = random.randint(16, 50)
                elif i < 205:  # ä½é£é™©ç”¨æˆ·
                    total_asset = random.uniform(30000, 120000)
                    cash_balance = random.uniform(10000, 50000)
                    user_level = random.choice(["Regular", "VIP1"])
                    kyc_status = "verified"
                    risk_score = random.uniform(10, 30)
                    age = random.randint(28, 50)
                    trade_count = random.randint(5, 15)
                elif i < 220:  # æ–°æ³¨å†Œç”¨æˆ·
                    total_asset = random.uniform(1000, 20000)
                    cash_balance = random.uniform(500, 10000)
                    user_level = "Regular"
                    kyc_status = random.choice(["pending", "verified"])
                    risk_score = random.uniform(40, 80)
                    age = random.randint(20, 40)
                    trade_count = random.randint(1, 8)
                elif i < 235:  # æœ€è¿‘æ´»è·ƒç”¨æˆ·
                    total_asset = random.uniform(15000, 80000)
                    cash_balance = random.uniform(5000, 30000)
                    user_level = random.choice(["Regular", "VIP1"])
                    kyc_status = "verified"
                    risk_score = random.uniform(25, 55)
                    age = random.randint(22, 45)
                    trade_count = random.randint(8, 25)
                else:  # æ™®é€šç”¨æˆ·
                    total_asset = random.uniform(2000, 40000)
                    cash_balance = random.uniform(500, 15000)
                    user_level = "Regular"
                    kyc_status = random.choice(["verified", "pending"])
                    risk_score = random.uniform(30, 90)
                    age = random.randint(20, 65)
                    trade_count = random.randint(1, 12)
                
                # è®¾ç½®æ—¶é—´æ•°æ®
                base_date = datetime.now().date()
                
                # æ–°æ³¨å†Œç”¨æˆ·çš„æ³¨å†Œæ—¶é—´åœ¨æœ€è¿‘30å¤©å†…
                if i < 220 and i >= 205:
                    registration_date = base_date - timedelta(days=random.randint(1, 30))
                else:
                    registration_date = base_date - timedelta(days=random.randint(30, 365))
                
                # æœ€è¿‘æ´»è·ƒç”¨æˆ·çš„æœ€åç™»å½•æ—¶é—´åœ¨æœ€è¿‘7å¤©å†…
                if i < 235 and i >= 220:
                    last_login_date = base_date - timedelta(days=random.randint(1, 7))
                else:
                    last_login_date = base_date - timedelta(days=random.randint(7, 90))
                
                user_basic_data.append({
                    "user_id": user_id,
                    "age": age,
                    "user_level": user_level,
                    "kyc_status": kyc_status,
                    "registration_date": registration_date,
                    "risk_score": risk_score,
                    "total_asset_value": total_asset,
                    "cash_balance": cash_balance,
                    "trade_count_30d": trade_count,
                    "last_login_date": last_login_date
                })
            
            # è½¬æ¢ä¸º DataFrame
            basic_schema = StructType([
                StructField("user_id", StringType(), True),
                StructField("age", IntegerType(), True),
                StructField("user_level", StringType(), True),
                StructField("kyc_status", StringType(), True),
                StructField("registration_date", DateType(), True),
                StructField("risk_score", DoubleType(), True),
                StructField("total_asset_value", DoubleType(), True),
                StructField("cash_balance", DoubleType(), True),
                StructField("trade_count_30d", IntegerType(), True),
                StructField("last_login_date", DateType(), True)
            ])
            
            all_data_df = self.spark.createDataFrame(user_basic_data, basic_schema)
            
            # åˆ†åˆ«åˆ›å»ºä¸‰ä¸ªæ•°æ®è¡¨
            user_basic_info = all_data_df.select(
                "user_id", "age", "user_level", "kyc_status", 
                "registration_date", "risk_score"
            )
            
            user_asset_summary = all_data_df.select(
                "user_id", "total_asset_value", "cash_balance"
            )
            
            user_activity_summary = all_data_df.select(
                "user_id", "trade_count_30d", "last_login_date"
            )
            
            logger.info(f"âœ… ç”Ÿæˆæµ‹è¯•æ•°æ®å®Œæˆ: {num_users} ä¸ªç”¨æˆ·")
            
            # æ ¹æ®source_nameè¿”å›å¯¹åº”çš„DataFrame
            if source_name == 'user_basic_info':
                return user_basic_info
            elif source_name == 'user_asset_summary':
                return user_asset_summary
            elif source_name == 'user_activity_summary':
                return user_activity_summary
            else:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šsource_nameï¼Œè¿”å›åˆå¹¶çš„å®Œæ•´æ•°æ®
                return all_data_df
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæµ‹è¯•æ•°æ®å¤±è´¥: {str(e)}")
            return None
