"""
æ ‡ç­¾ä»»åŠ¡æŠ½è±¡åŸºç±» - é‡æ„ä¸ºè‡ªåŒ…å«çš„ä»»åŠ¡æ¶æ„
"""

from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
from pyspark.sql import DataFrame, SparkSession
import logging

logger = logging.getLogger(__name__)


class BaseTagTask(ABC):
    """
    æ ‡ç­¾ä»»åŠ¡åŸºç±» - æ¯ä¸ªæ ‡ç­¾ä»»åŠ¡è‡ªåŒ…å«ï¼Œè‡ªå·±è´Ÿè´£æ•°æ®åŠ è½½å’Œæ‰§è¡Œ
    
    æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼š
    1. æ¯ä¸ªä»»åŠ¡åªå…³å¿ƒè‡ªå·±éœ€è¦çš„æ•°æ®æºå’Œå­—æ®µ
    2. ä»»åŠ¡è‡ªå·±è´Ÿè´£æŒ‰éœ€åŠ è½½æ•°æ®
    3. ä»»åŠ¡è‡ªå·±è·å–è‡ªå·±çš„è§„åˆ™
    4. ä»»åŠ¡ç‹¬ç«‹æ‰§è¡Œï¼Œä¸ä¾èµ–å…¶ä»–ä»»åŠ¡
    """
    
    def __init__(self, task_config: Dict[str, Any], spark: SparkSession, system_config):
        self.task_config = task_config
        self.tag_id = task_config['tag_id']
        self.tag_name = task_config['tag_name']
        self.tag_category = task_config['tag_category']
        self.rule_conditions = task_config.get('rule_conditions', {})
        
        # ç³»ç»Ÿä¾èµ–ï¼ˆåªä¼ å…¥å¿…è¦çš„ä¾èµ–ï¼‰
        self.spark = spark
        self.system_config = system_config
        
        # ä»»åŠ¡ä¸“ç”¨çš„ç»„ä»¶ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._rule_processor = None
        self._data_cache = {}
    
    @abstractmethod
    def get_required_fields(self) -> List[str]:
        """
        è¿”å›è¯¥æ ‡ç­¾éœ€è¦çš„æ•°æ®å­—æ®µ
        
        Returns:
            List[str]: å¿…éœ€çš„å­—æ®µåˆ—è¡¨
        """
        pass
    
    @abstractmethod
    def get_data_sources(self) -> Dict[str, str]:
        """
        è¿”å›è¯¥æ ‡ç­¾éœ€è¦çš„æ•°æ®æºé…ç½®
        
        Returns:
            Dict[str, str]: æ•°æ®æºæ˜ å°„ {source_name: source_path}
        """
        pass
    
    
    def preprocess_data(self, raw_data: DataFrame) -> DataFrame:
        """
        æ•°æ®é¢„å¤„ç† - æ¯ä¸ªæ ‡ç­¾å¯ä»¥æœ‰è‡ªå·±çš„æ•°æ®å¤„ç†é€»è¾‘
        é»˜è®¤å®ç°ï¼šç›´æ¥è¿”å›åŸæ•°æ®
        
        Args:
            raw_data: åŸå§‹æ•°æ®DataFrame
            
        Returns:
            DataFrame: é¢„å¤„ç†åçš„æ•°æ®
        """
        return raw_data
    
    def post_process_result(self, tagged_users: DataFrame) -> DataFrame:
        """
        ç»“æœåå¤„ç† - å¯é€‰çš„ä¸šåŠ¡é€»è¾‘
        é»˜è®¤å®ç°ï¼šç›´æ¥è¿”å›ç»“æœ
        
        Args:
            tagged_users: æ ‡ç­¾è®¡ç®—ç»“æœ
            
        Returns:
            DataFrame: åå¤„ç†çš„ç»“æœ
        """
        return tagged_users
    
    def validate_data(self, data: DataFrame) -> bool:
        """
        éªŒè¯æ•°æ®æ˜¯å¦æ»¡è¶³ä»»åŠ¡éœ€æ±‚
        
        Args:
            data: è¾“å…¥æ•°æ®
            
        Returns:
            bool: æ•°æ®æ˜¯å¦æœ‰æ•ˆ
        """
        required_fields = self.get_required_fields()
        missing_fields = set(required_fields) - set(data.columns)
        
        if missing_fields:
            logger.warning(f"æ ‡ç­¾ä»»åŠ¡ {self.tag_name} ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")
            return False
        
        return True
    
    def load_my_data(self, user_filter: Optional[List[str]] = None) -> DataFrame:
        """
        ä»»åŠ¡è‡ªå·±åŠ è½½éœ€è¦çš„æ•°æ® - æŒ‰éœ€åŠ è½½ï¼Œåªè¯»å–è‡ªå·±éœ€è¦çš„æ•°æ®æºå’Œå­—æ®µ
        
        Args:
            user_filter: å¯é€‰çš„ç”¨æˆ·IDè¿‡æ»¤åˆ—è¡¨
            
        Returns:
            DataFrame: å½“å‰ä»»åŠ¡éœ€è¦çš„æ•°æ®
        """
        try:
            data_sources = self.get_data_sources()
            required_fields = self.get_required_fields()
            
            logger.info(f"ğŸ” ä»»åŠ¡ {self.tag_name} å¼€å§‹åŠ è½½æ•°æ®...")
            logger.info(f"   ğŸ“Š æ•°æ®æº: {data_sources}")
            logger.info(f"   ğŸ“‹ å­—æ®µ: {required_fields}")
            
            # åŠ è½½ä¸»è¦æ•°æ®æº
            primary_source = data_sources.get('primary')
            if not primary_source:
                raise ValueError(f"ä»»åŠ¡ {self.tag_name} æ²¡æœ‰å®šä¹‰ä¸»è¦æ•°æ®æº")
            
            # ä»ç¼“å­˜æ£€æŸ¥
            cache_key = f"{primary_source}_{','.join(sorted(required_fields))}"
            if cache_key in self._data_cache:
                logger.info(f"   âœ… ä½¿ç”¨ç¼“å­˜æ•°æ®: {cache_key}")
                data = self._data_cache[cache_key]
            else:
                # æŒ‰éœ€åŠ è½½æ•°æ®
                data = self._load_data_from_source(primary_source, required_fields)
                self._data_cache[cache_key] = data
                logger.info(f"   âœ… æ•°æ®åŠ è½½å®Œæˆ: {data.count()} æ¡è®°å½•")
            
            # ç”¨æˆ·è¿‡æ»¤
            if user_filter:
                data = data.filter(data.user_id.isin(user_filter))
                logger.info(f"   ğŸ¯ ç”¨æˆ·è¿‡æ»¤å: {data.count()} æ¡è®°å½•")
            
            # æ•°æ®é¢„å¤„ç†
            processed_data = self.preprocess_data(data)
            
            # éªŒè¯æ•°æ®
            if not self.validate_data(processed_data):
                raise ValueError(f"ä»»åŠ¡ {self.tag_name} æ•°æ®éªŒè¯å¤±è´¥")
            
            return processed_data
            
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡ {self.tag_name} æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            raise
    
    def _load_data_from_source(self, source_name: str, fields: List[str]) -> DataFrame:
        """ä»æŒ‡å®šæ•°æ®æºåŠ è½½æŒ‡å®šå­—æ®µçš„æ•°æ®"""
        try:
            # æ ¹æ®ç¯å¢ƒé€‰æ‹©æ•°æ®åŠ è½½æ–¹å¼
            if self.system_config.environment == 'local':
                # æœ¬åœ°ç¯å¢ƒï¼šç”Ÿæˆæµ‹è¯•æ•°æ®æˆ–ä»MinIOè¯»å–
                return self._load_local_data(source_name, fields)
            else:
                # Glueç¯å¢ƒï¼šä»S3è¯»å–
                return self._load_s3_data(source_name, fields)
                
        except Exception as e:
            logger.error(f"âŒ ä»æ•°æ®æº {source_name} åŠ è½½æ•°æ®å¤±è´¥: {str(e)}")
            raise
    
    def _load_local_data(self, source_name: str, fields: List[str]) -> DataFrame:
        """æœ¬åœ°ç¯å¢ƒæ•°æ®åŠ è½½ - ä¼˜å…ˆä½¿ç”¨çœŸå®çš„S3/Hiveæ•°æ®"""
        logger.info(f"ğŸ“Š æœ¬åœ°ç¯å¢ƒåŠ è½½æ•°æ®æº: {source_name}")
        
        # ä¼˜å…ˆå°è¯•ä»çœŸå®çš„S3/Hiveè¯»å–
        try:
            from src.batch.core.data_loader import BatchDataLoader
            data_loader = BatchDataLoader(self.spark, self.system_config)
            
            logger.info(f"ğŸ”„ å°è¯•ä»S3/Hiveè¯»å–çœŸå®æ•°æ®: {source_name}")
            df = data_loader.hive_loader.read_hive_table(source_name, fields)
            
            if df is not None and df.count() > 0:
                logger.info(f"âœ… æˆåŠŸä»S3/Hiveè¯»å–æ•°æ®: {source_name}, è®°å½•æ•°: {df.count()}")
                return df
            else:
                logger.info(f"â„¹ï¸ S3/Hiveæ•°æ®ä¸å¯ç”¨ï¼Œä½¿ç”¨ç”Ÿäº§çº§æ¨¡æ‹Ÿæ•°æ®: {source_name}")
                return self._generate_production_like_data(source_name, fields)
                
        except Exception as e:
            logger.info(f"â„¹ï¸ S3/Hiveè¯»å–å¤±è´¥ï¼Œä½¿ç”¨ç”Ÿäº§çº§æ¨¡æ‹Ÿæ•°æ®: {source_name} (åŸå› : JARä¾èµ–é—®é¢˜)")
            # é™çº§åˆ°ç”Ÿäº§çº§æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ
            return self._generate_production_like_data(source_name, fields)
    
    def _generate_simple_test_data(self, source_name: str, fields: List[str]) -> DataFrame:
        """ç”Ÿæˆç®€å•çš„æµ‹è¯•æ•°æ®"""
        from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType
        from datetime import datetime, timedelta
        
        # ç”ŸæˆåŸºæœ¬æµ‹è¯•æ•°æ®
        data = []
        for i in range(100):
            user_id = f"user_{i:06d}"
            base_date = datetime.now().date()
            
            data.append({
                "user_id": user_id,
                "total_asset_value": 100000.0 + i * 1000,
                "cash_balance": 50000.0 + i * 500,
                "age": 25 + (i % 40),
                "user_level": "VIP1" if i % 5 == 0 else "Regular",
                "kyc_status": "verified",
                "trade_count_30d": 10 + (i % 20),
                "risk_score": 30.0 + (i % 50),
                "last_login_date": base_date - timedelta(days=i % 30),  # æ·»åŠ ç™»å½•æ—¥æœŸ
                "registration_date": base_date - timedelta(days=30 + (i % 300))  # æ·»åŠ æ³¨å†Œæ—¥æœŸ
            })
        
        # åˆ›å»ºschema
        schema = StructType([
            StructField("user_id", StringType(), True),
            StructField("total_asset_value", DoubleType(), True),
            StructField("cash_balance", DoubleType(), True),
            StructField("age", IntegerType(), True),
            StructField("user_level", StringType(), True),
            StructField("kyc_status", StringType(), True),
            StructField("trade_count_30d", IntegerType(), True),
            StructField("risk_score", DoubleType(), True),
            StructField("last_login_date", DateType(), True),
            StructField("registration_date", DateType(), True)
        ])
        
        df = self.spark.createDataFrame(data, schema)
        
        # åªè¿”å›éœ€è¦çš„å­—æ®µ
        if fields:
            available_fields = [f for f in fields if f in df.columns]
            if available_fields:
                return df.select(*available_fields)
        
        return df
    
    def _generate_production_like_data(self, source_name: str, fields: List[str]) -> DataFrame:
        """ç”Ÿæˆç”Ÿäº§çº§æ¨¡æ‹Ÿæ•°æ®ï¼Œæ¥è¿‘çœŸå®Hiveè¡¨æ•°æ®è´¨é‡"""
        logger.info(f"ğŸ­ ç”Ÿæˆç”Ÿäº§çº§æ¨¡æ‹Ÿæ•°æ®: {source_name}")
        
        try:
            # ä½¿ç”¨ç¼–æ’å™¨çš„ç”Ÿäº§çº§æ•°æ®ç”Ÿæˆå™¨
            from src.batch.orchestrator.batch_orchestrator import BatchOrchestrator
            orchestrator = BatchOrchestrator(self.system_config)
            orchestrator.spark = self.spark  # ä½¿ç”¨å½“å‰çš„Sparkä¼šè¯
            
            # ç”Ÿæˆå¯¹åº”æ•°æ®æºçš„æ•°æ®
            df = orchestrator._generate_production_like_data(source_name)
            
            if df is None:
                # å¦‚æœç”Ÿäº§çº§æ•°æ®ç”Ÿæˆå¤±è´¥ï¼Œé™çº§åˆ°ç®€å•æ•°æ®
                logger.warning(f"âš ï¸ ç”Ÿäº§çº§æ•°æ®ç”Ÿæˆå¤±è´¥ï¼Œé™çº§åˆ°ç®€å•æµ‹è¯•æ•°æ®: {source_name}")
                return self._generate_simple_test_data(source_name, fields)
            
            # é€‰æ‹©éœ€è¦çš„å­—æ®µ
            if fields:
                available_fields = [f for f in fields if f in df.columns]
                if available_fields:
                    df = df.select(*available_fields)
            
            logger.info(f"âœ… ç”Ÿäº§çº§æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå®Œæˆ: {source_name}, è®°å½•æ•°: {df.count()}")
            return df
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç”Ÿäº§çº§æ•°æ®ç”Ÿæˆå¼‚å¸¸ï¼Œé™çº§åˆ°ç®€å•æµ‹è¯•æ•°æ®: {str(e)}")
            return self._generate_simple_test_data(source_name, fields)
    
    def _load_s3_data(self, source_name: str, fields: List[str]) -> DataFrame:
        """S3ç¯å¢ƒæ•°æ®åŠ è½½"""
        try:
            s3_config = self.system_config.s3
            table_path = f"{s3_config.warehouse_path}{source_name}/"
            
            logger.info(f"ğŸ“– ä»S3è¯»å–: {table_path}")
            df = self.spark.read.parquet(table_path)
            
            if fields:
                available_fields = [f for f in fields if f in df.columns]
                if available_fields:
                    df = df.select(*available_fields)
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ S3æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            raise
    
    def get_my_rule(self) -> Dict[str, Any]:
        """
        è·å–å½“å‰ä»»åŠ¡çš„è§„åˆ™æ¡ä»¶
        
        Returns:
            Dict[str, Any]: è§„åˆ™æ¡ä»¶
        """
        return self.rule_conditions
    
    def execute(self, user_filter: Optional[List[str]] = None) -> Optional[DataFrame]:
        """
        å®Œæ•´çš„ä»»åŠ¡æ‰§è¡Œæµç¨‹ - è¿™æ˜¯ä»»åŠ¡çš„ä¸»å…¥å£
        
        Args:
            user_filter: å¯é€‰çš„ç”¨æˆ·IDè¿‡æ»¤åˆ—è¡¨
            
        Returns:
            DataFrame: æ ‡ç­¾è®¡ç®—ç»“æœ (user_id, tag_id, tag_detail)
        """
        try:
            logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œä»»åŠ¡: {self.tag_name} (ID: {self.tag_id})")
            
            # 1. åŠ è½½å½“å‰ä»»åŠ¡éœ€è¦çš„æ•°æ®
            task_data = self.load_my_data(user_filter)
            
            if task_data.count() == 0:
                logger.warning(f"âš ï¸ ä»»åŠ¡ {self.tag_name} æ²¡æœ‰å¯ç”¨æ•°æ®")
                return None
            
            # 2. è·å–å½“å‰ä»»åŠ¡çš„è§„åˆ™
            rule_conditions = self.get_my_rule()
            
            # 3. åˆå§‹åŒ–è§„åˆ™å¤„ç†å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
            if not self._rule_processor:
                from src.batch.core.rule_processor import RuleProcessor
                self._rule_processor = RuleProcessor()
            
            # 4. åº”ç”¨è§„åˆ™è¿‡æ»¤
            filtered_users = self._rule_processor.apply_rules(task_data, rule_conditions)
            
            if filtered_users.count() == 0:
                logger.info(f"ğŸ“Š ä»»åŠ¡ {self.tag_name} æ²¡æœ‰ç”¨æˆ·å‘½ä¸­æ¡ä»¶")
                return None
            
            # 5. æ„å»ºæ ‡ç­¾ç»“æœï¼ˆç¡®ä¿ç”¨æˆ·å”¯ä¸€æ€§ï¼‰
            from pyspark.sql.functions import lit
            result = filtered_users.select(
                "user_id",
                lit(self.tag_id).alias("tag_id"),
                lit(f"{self.tag_name} - {self.tag_category}").alias("tag_detail")
            ).distinct()  # ç¡®ä¿ç”¨æˆ·å”¯ä¸€æ€§
            
            # 6. åå¤„ç†
            final_result = self.post_process_result(result)
            
            result_count = final_result.count()
            logger.info(f"âœ… ä»»åŠ¡ {self.tag_name} æ‰§è¡Œå®Œæˆ: {result_count} ä¸ªç”¨æˆ·å‘½ä¸­")
            
            return final_result
            
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡ {self.tag_name} æ‰§è¡Œå¤±è´¥: {str(e)}")
            raise
    
    def cleanup(self):
        """æ¸…ç†ä»»åŠ¡èµ„æº"""
        try:
            # æ¸…ç†æ•°æ®ç¼“å­˜
            for df in self._data_cache.values():
                if hasattr(df, 'unpersist'):
                    df.unpersist()
            self._data_cache.clear()
            
            logger.debug(f"ğŸ§¹ ä»»åŠ¡ {self.tag_name} èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ä»»åŠ¡ {self.tag_name} èµ„æºæ¸…ç†å¼‚å¸¸: {str(e)}")
    
    def get_task_metadata(self) -> Dict[str, Any]:
        """
        è·å–ä»»åŠ¡å…ƒæ•°æ®
        
        Returns:
            Dict[str, Any]: ä»»åŠ¡å…ƒæ•°æ®
        """
        return {
            'tag_id': self.tag_id,
            'tag_name': self.tag_name,
            'tag_category': self.tag_category,
            'required_fields': self.get_required_fields(),
            'data_sources': self.get_data_sources(),
            'task_class': self.__class__.__name__
        }
    
    def __str__(self) -> str:
        return f"TagTask({self.tag_id}: {self.tag_name})"
    
    def __repr__(self) -> str:
        return self.__str__()