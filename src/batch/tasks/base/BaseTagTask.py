"""
æ ‡ç­¾ä»»åŠ¡æŠ½è±¡åŸºç±» - å‚è€ƒTFECUserPortraitè®¾è®¡
æ¨¡ä»¿AbstractBaseModelï¼Œæä¾›S3æ•°æ®è¯»å–çš„æŠ½è±¡èƒ½åŠ›
"""

from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
from pyspark.sql import DataFrame, SparkSession
import logging

logger = logging.getLogger(__name__)


class BaseTagTask(ABC):
    """
    æ ‡ç­¾ä»»åŠ¡åŸºç±» - æ¨¡ä»¿TFECUserPortraitçš„AbstractBaseModelè®¾è®¡
    
    æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼š
    1. çˆ¶ç±»æŠ½è±¡S3è¯»å–åŠŸèƒ½ï¼Œå­ç±»æŒ‡å®šå…·ä½“è¡¨åœ°å€
    2. æ¯ä¸ªä»»åŠ¡è‡ªåŒ…å«ï¼Œè‡ªå·±è´Ÿè´£æ•°æ®åŠ è½½å’Œæ‰§è¡Œ
    3. ä»»åŠ¡ç‹¬ç«‹æ‰§è¡Œï¼Œæ”¯æŒå¹¶è¡Œ
    4. ä¸€ä¸ªç±»ä¸€ä¸ªæ–‡ä»¶ï¼Œé©¼å³°å‘½å
    """
    
    def __init__(self, taskConfig: Dict[str, Any], spark: SparkSession, systemConfig):
        """
        åˆå§‹åŒ–æ ‡ç­¾ä»»åŠ¡
        
        Args:
            taskConfig: ä»»åŠ¡é…ç½®
            spark: Sparkä¼šè¯
            systemConfig: ç³»ç»Ÿé…ç½®
        """
        self.taskConfig = taskConfig
        self.tagId = taskConfig['tag_id']
        self.tagName = taskConfig['tag_name']
        self.tagCategory = taskConfig['tag_category']
        self.ruleConditions = taskConfig.get('rule_conditions', {})
        
        # ç³»ç»Ÿä¾èµ–
        self.spark = spark
        self.systemConfig = systemConfig
        
        # ä»»åŠ¡ä¸“ç”¨ç»„ä»¶ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._ruleProcessor = None
        self._dataCache = {}
    
    @abstractmethod
    def getRequiredFields(self) -> List[str]:
        """
        è¿”å›è¯¥æ ‡ç­¾éœ€è¦çš„æ•°æ®å­—æ®µ
        å­ç±»å¿…é¡»å®ç°æ­¤æ–¹æ³•ï¼ŒæŒ‡å®šéœ€è¦å“ªäº›å­—æ®µ
        
        Returns:
            List[str]: å¿…éœ€çš„å­—æ®µåˆ—è¡¨
        """
        pass
    
    @abstractmethod
    def getHiveTableConfig(self) -> Dict[str, str]:
        """
        è¿”å›è¯¥æ ‡ç­¾éœ€è¦çš„Hiveè¡¨é…ç½® - æ ¸å¿ƒæŠ½è±¡æ–¹æ³•
        å­ç±»æŒ‡å®šå…·ä½“çš„å®Œæ•´S3è·¯å¾„
        
        Returns:
            Dict[str, str]: Hiveè¡¨é…ç½® {table_name: å®Œæ•´S3è·¯å¾„}
            ä¾‹å¦‚: {
                'user_basic_info': 's3a://my-bucket/warehouse/user_basic_info/',
                'user_asset_summary': 's3a://my-bucket/warehouse/user_asset_summary/'
            }
        """
        pass
    
    def loadHiveData(self, tableName: str, fields: List[str] = None) -> DataFrame:
        """
        ä»S3åŠ è½½Hiveè¡¨æ•°æ® - çˆ¶ç±»æŠ½è±¡å®ç°
        å­ç±»é€šè¿‡getHiveTableConfig()æŒ‡å®šè¡¨åœ°å€ï¼Œçˆ¶ç±»è´Ÿè´£å…·ä½“è¯»å–
        
        Args:
            tableName: è¡¨å
            fields: éœ€è¦çš„å­—æ®µåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰å­—æ®µ
            
        Returns:
            DataFrame: åŠ è½½çš„æ•°æ®
        """
        try:
            hiveConfig = self.getHiveTableConfig()
            if tableName not in hiveConfig:
                raise ValueError(f"ä»»åŠ¡ {self.tagName} æœªé…ç½®è¡¨ {tableName}")
            
            tablePath = hiveConfig[tableName]
            logger.info(f"ğŸ” ä»»åŠ¡ {self.tagName} å¼€å§‹åŠ è½½Hiveè¡¨: {tableName}")
            logger.info(f"   ğŸ“ è¡¨è·¯å¾„: {tablePath}")
            
            # æ ¹æ®ç¯å¢ƒé€‰æ‹©æ•°æ®åŠ è½½æ–¹å¼
            if self.systemConfig.environment == 'local':
                return self._loadLocalHiveData(tablePath, fields)
            else:
                return self._loadS3HiveData(tablePath, fields)
                
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡ {self.tagName} åŠ è½½Hiveè¡¨å¤±è´¥: {tableName}, é”™è¯¯: {str(e)}")
            raise
    
    def _loadS3HiveData(self, tablePath: str, fields: List[str] = None) -> DataFrame:
        """
        ä»S3åŠ è½½Hiveè¡¨æ•°æ®
        
        Args:
            tablePath: å®Œæ•´çš„S3è¡¨è·¯å¾„ï¼ˆä»»åŠ¡è‡ªä¸»æŒ‡å®šï¼‰
            fields: éœ€è¦çš„å­—æ®µåˆ—è¡¨
            
        Returns:
            DataFrame: åŠ è½½çš„æ•°æ®
        """
        try:
            # ğŸ¯ å…³é”®æ”¹åŠ¨ï¼šç›´æ¥ä½¿ç”¨ä»»åŠ¡æä¾›çš„å®Œæ•´S3è·¯å¾„ï¼Œä¸å†æ‹¼æ¥
            fullPath = tablePath.rstrip('/')  # å»é™¤æœ«å°¾æ–œæ ç»Ÿä¸€æ ¼å¼
            
            logger.info(f"ğŸ“– ä»S3è¯»å–Hiveè¡¨: {fullPath}")
            df = self.spark.read.parquet(fullPath)
            
            # å­—æ®µé€‰æ‹©ä¼˜åŒ–
            if fields:
                availableFields = [f for f in fields if f in df.columns]
                if availableFields:
                    df = df.select(*availableFields)
                    logger.info(f"   ğŸ“‹ å­—æ®µç­›é€‰: {availableFields}")
            
            logger.info(f"   âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œè®°å½•æ•°: {df.count()}")
            return df
            
        except Exception as e:
            logger.error(f"âŒ S3æ•°æ®åŠ è½½å¤±è´¥: {tablePath}, é”™è¯¯: {str(e)}")
            raise
    
    def _loadLocalHiveData(self, tablePath: str, fields: List[str] = None) -> DataFrame:
        """
        æœ¬åœ°ç¯å¢ƒHiveæ•°æ®åŠ è½½ - ä¼˜å…ˆä½¿ç”¨çœŸå®æ•°æ®ï¼Œé™çº§åˆ°æ¨¡æ‹Ÿæ•°æ®
        
        Args:
            tablePath: è¡¨è·¯å¾„
            fields: éœ€è¦çš„å­—æ®µåˆ—è¡¨
            
        Returns:
            DataFrame: åŠ è½½çš„æ•°æ®
        """
        logger.info(f"ğŸ“Š æœ¬åœ°ç¯å¢ƒåŠ è½½Hiveè¡¨: {tablePath}")
        
        # ä¼˜å…ˆå°è¯•ä»çœŸå®çš„S3/Hiveè¯»å–
        try:
            from src.batch.utils.HiveDataReader import HiveDataReader
            hiveReader = HiveDataReader(self.spark, self.systemConfig)
            
            logger.info(f"ğŸ”„ å°è¯•ä»S3/Hiveè¯»å–çœŸå®æ•°æ®: {tablePath}")
            df = hiveReader.readHiveTable(tablePath, fields)
            
            if df is not None and df.count() > 0:
                logger.info(f"âœ… æˆåŠŸä»S3/Hiveè¯»å–æ•°æ®: {tablePath}, è®°å½•æ•°: {df.count()}")
                return df
            else:
                logger.info(f"â„¹ï¸ S3/Hiveæ•°æ®ä¸å¯ç”¨ï¼Œä½¿ç”¨ç”Ÿäº§çº§æ¨¡æ‹Ÿæ•°æ®: {tablePath}")
                return self._generateProductionLikeData(tablePath, fields)
                
        except Exception as e:
            logger.info(f"â„¹ï¸ S3/Hiveè¯»å–å¤±è´¥ï¼Œä½¿ç”¨ç”Ÿäº§çº§æ¨¡æ‹Ÿæ•°æ®: {tablePath} (åŸå› : JARä¾èµ–é—®é¢˜)")
            return self._generateProductionLikeData(tablePath, fields)
    
    def _generateProductionLikeData(self, tableName: str, fields: List[str] = None) -> DataFrame:
        """
        ç”Ÿæˆç”Ÿäº§çº§æ¨¡æ‹Ÿæ•°æ®
        
        Args:
            tableName: è¡¨å
            fields: éœ€è¦çš„å­—æ®µåˆ—è¡¨
            
        Returns:
            DataFrame: æ¨¡æ‹Ÿæ•°æ®
        """
        logger.info(f"ğŸ­ ç”Ÿæˆç”Ÿäº§çº§æ¨¡æ‹Ÿæ•°æ®: {tableName}")
        
        try:
            # ä½¿ç”¨ç¼–æ’å™¨çš„ç”Ÿäº§çº§æ•°æ®ç”Ÿæˆå™¨
            from src.batch.engine.BatchOrchestrator import BatchOrchestrator
            orchestrator = BatchOrchestrator(self.systemConfig)
            orchestrator.spark = self.spark  # ä½¿ç”¨å½“å‰çš„Sparkä¼šè¯
            
            # ç”Ÿæˆå¯¹åº”æ•°æ®æºçš„æ•°æ®
            df = orchestrator._generate_production_like_data(tableName)
            
            if df is None:
                # å¦‚æœç”Ÿäº§çº§æ•°æ®ç”Ÿæˆå¤±è´¥ï¼Œé™çº§åˆ°ç®€å•æ•°æ®
                logger.warning(f"âš ï¸ ç”Ÿäº§çº§æ•°æ®ç”Ÿæˆå¤±è´¥ï¼Œé™çº§åˆ°ç®€å•æµ‹è¯•æ•°æ®: {tableName}")
                return self._generateSimpleTestData(tableName, fields)
            
            # é€‰æ‹©éœ€è¦çš„å­—æ®µ
            if fields:
                availableFields = [f for f in fields if f in df.columns]
                if availableFields:
                    df = df.select(*availableFields)
            
            logger.info(f"âœ… ç”Ÿäº§çº§æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå®Œæˆ: {tableName}, è®°å½•æ•°: {df.count()}")
            return df
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç”Ÿäº§çº§æ•°æ®ç”Ÿæˆå¼‚å¸¸ï¼Œé™çº§åˆ°ç®€å•æµ‹è¯•æ•°æ®: {str(e)}")
            return self._generateSimpleTestData(tableName, fields)
    
    def _generateSimpleTestData(self, tableName: str, fields: List[str] = None) -> DataFrame:
        """
        ç”Ÿæˆç®€å•çš„æµ‹è¯•æ•°æ® - æœ€åçš„é™çº§æ–¹æ¡ˆ
        
        Args:
            tableName: è¡¨å
            fields: éœ€è¦çš„å­—æ®µåˆ—è¡¨
            
        Returns:
            DataFrame: ç®€å•æµ‹è¯•æ•°æ®
        """
        from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType
        from datetime import datetime, timedelta
        
        # ç”ŸæˆåŸºæœ¬æµ‹è¯•æ•°æ®
        data = []
        for i in range(100):
            userId = f"user_{i:06d}"
            baseDate = datetime.now().date()
            
            data.append({
                "user_id": userId,
                "total_asset_value": 100000.0 + i * 1000,
                "cash_balance": 50000.0 + i * 500,
                "age": 25 + (i % 40),
                "user_level": "VIP1" if i % 5 == 0 else "Regular",
                "kyc_status": "verified",
                "trade_count_30d": 10 + (i % 20),
                "risk_score": 30.0 + (i % 50),
                "last_login_date": baseDate - timedelta(days=i % 30),
                "registration_date": baseDate - timedelta(days=30 + (i % 300))
            })
        
        # åˆ›å»ºschema
        schema = StructType([
            StructField("user_id", StringType(), True),
            StructField("total_asset_value", DoubleType(), True),
            StructField("cash_balance", DoubleType(), True),
            StructField("age", IntegerType(), True),
            StructField("user_level", StringType(), True),
            StructField("kyc_status", StringType(), True),
            StructField("trade_count_30d", IntegerType(), True),
            StructField("risk_score", DoubleType(), True),
            StructField("last_login_date", DateType(), True),
            StructField("registration_date", DateType(), True)
        ])
        
        df = self.spark.createDataFrame(data, schema)
        
        # åªè¿”å›éœ€è¦çš„å­—æ®µ
        if fields:
            availableFields = [f for f in fields if f in df.columns]
            if availableFields:
                return df.select(*availableFields)
        
        return df
    
    def preprocessData(self, rawData: DataFrame) -> DataFrame:
        """
        æ•°æ®é¢„å¤„ç† - æ¯ä¸ªæ ‡ç­¾å¯ä»¥æœ‰è‡ªå·±çš„æ•°æ®å¤„ç†é€»è¾‘
        é»˜è®¤å®ç°ï¼šç›´æ¥è¿”å›åŸæ•°æ®
        
        Args:
            rawData: åŸå§‹æ•°æ®DataFrame
            
        Returns:
            DataFrame: é¢„å¤„ç†åçš„æ•°æ®
        """
        return rawData
    
    def postProcessResult(self, taggedUsers: DataFrame) -> DataFrame:
        """
        ç»“æœåå¤„ç† - å¯é€‰çš„ä¸šåŠ¡é€»è¾‘
        é»˜è®¤å®ç°ï¼šç›´æ¥è¿”å›ç»“æœ
        
        Args:
            taggedUsers: æ ‡ç­¾è®¡ç®—ç»“æœ
            
        Returns:
            DataFrame: åå¤„ç†çš„ç»“æœ
        """
        return taggedUsers
    
    def validateData(self, data: DataFrame) -> bool:
        """
        éªŒè¯æ•°æ®æ˜¯å¦æ»¡è¶³ä»»åŠ¡éœ€æ±‚
        
        Args:
            data: è¾“å…¥æ•°æ®
            
        Returns:
            bool: æ•°æ®æ˜¯å¦æœ‰æ•ˆ
        """
        requiredFields = self.getRequiredFields()
        missingFields = set(requiredFields) - set(data.columns)
        
        if missingFields:
            logger.warning(f"æ ‡ç­¾ä»»åŠ¡ {self.tagName} ç¼ºå°‘å¿…éœ€å­—æ®µ: {missingFields}")
            return False
        
        return True
    
    def loadMyData(self, userFilter: Optional[List[str]] = None) -> DataFrame:
        """
        ä»»åŠ¡è‡ªå·±åŠ è½½éœ€è¦çš„æ•°æ® - ä½¿ç”¨æŠ½è±¡çš„Hiveè¡¨è¯»å–èƒ½åŠ›
        
        Args:
            userFilter: å¯é€‰çš„ç”¨æˆ·IDè¿‡æ»¤åˆ—è¡¨
            
        Returns:
            DataFrame: å½“å‰ä»»åŠ¡éœ€è¦çš„æ•°æ®
        """
        try:
            hiveConfig = self.getHiveTableConfig()
            requiredFields = self.getRequiredFields()
            
            logger.info(f"ğŸ” ä»»åŠ¡ {self.tagName} å¼€å§‹åŠ è½½æ•°æ®...")
            logger.info(f"   ğŸ“Š Hiveè¡¨: {list(hiveConfig.keys())}")
            logger.info(f"   ğŸ“‹ å­—æ®µ: {requiredFields}")
            
            # åŠ è½½ä¸»è¡¨æ•°æ®ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªé…ç½®çš„è¡¨ï¼‰
            primaryTable = list(hiveConfig.keys())[0]
            
            # æ£€æŸ¥ç¼“å­˜
            cacheKey = f"{primaryTable}_{','.join(sorted(requiredFields))}"
            if cacheKey in self._dataCache:
                logger.info(f"   âœ… ä½¿ç”¨ç¼“å­˜æ•°æ®: {cacheKey}")
                data = self._dataCache[cacheKey]
            else:
                # æŒ‰éœ€åŠ è½½æ•°æ®
                data = self.loadHiveData(primaryTable, requiredFields)
                self._dataCache[cacheKey] = data
                logger.info(f"   âœ… æ•°æ®åŠ è½½å®Œæˆ: {data.count()} æ¡è®°å½•")
            
            # ç”¨æˆ·è¿‡æ»¤
            if userFilter:
                data = data.filter(data.user_id.isin(userFilter))
                logger.info(f"   ğŸ¯ ç”¨æˆ·è¿‡æ»¤å: {data.count()} æ¡è®°å½•")
            
            # æ•°æ®é¢„å¤„ç†
            processedData = self.preprocessData(data)
            
            # éªŒè¯æ•°æ®
            if not self.validateData(processedData):
                raise ValueError(f"ä»»åŠ¡ {self.tagName} æ•°æ®éªŒè¯å¤±è´¥")
            
            return processedData
            
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡ {self.tagName} æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            raise
    
    def getMyRule(self) -> Dict[str, Any]:
        """
        è·å–å½“å‰ä»»åŠ¡çš„è§„åˆ™æ¡ä»¶
        
        Returns:
            Dict[str, Any]: è§„åˆ™æ¡ä»¶
        """
        return self.ruleConditions
    
    def execute(self, userFilter: Optional[List[str]] = None) -> Optional[DataFrame]:
        """
        å®Œæ•´çš„ä»»åŠ¡æ‰§è¡Œæµç¨‹ - è¿™æ˜¯ä»»åŠ¡çš„ä¸»å…¥å£
        
        Args:
            userFilter: å¯é€‰çš„ç”¨æˆ·IDè¿‡æ»¤åˆ—è¡¨
            
        Returns:
            DataFrame: æ ‡ç­¾è®¡ç®—ç»“æœ (user_id, tag_id, tag_detail)
        """
        try:
            logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œä»»åŠ¡: {self.tagName} (ID: {self.tagId})")
            
            # 1. åŠ è½½å½“å‰ä»»åŠ¡éœ€è¦çš„æ•°æ®
            taskData = self.loadMyData(userFilter)
            
            if taskData.count() == 0:
                logger.warning(f"âš ï¸ ä»»åŠ¡ {self.tagName} æ²¡æœ‰å¯ç”¨æ•°æ®")
                return None
            
            # 2. è·å–å½“å‰ä»»åŠ¡çš„è§„åˆ™
            ruleConditions = self.getMyRule()
            
            # 3. åˆå§‹åŒ–è§„åˆ™å¤„ç†å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
            if not self._ruleProcessor:
                from src.batch.utils.RuleProcessor import RuleProcessor
                self._ruleProcessor = RuleProcessor()
            
            # 4. åº”ç”¨è§„åˆ™è¿‡æ»¤
            filteredUsers = self._ruleProcessor.applyRules(taskData, ruleConditions)
            
            if filteredUsers.count() == 0:
                logger.info(f"ğŸ“Š ä»»åŠ¡ {self.tagName} æ²¡æœ‰ç”¨æˆ·å‘½ä¸­æ¡ä»¶")
                return None
            
            # 5. æ„å»ºæ ‡ç­¾ç»“æœï¼ˆç¡®ä¿ç”¨æˆ·å”¯ä¸€æ€§ï¼‰
            from pyspark.sql.functions import lit
            result = filteredUsers.select(
                "user_id",
                lit(self.tagId).alias("tag_id"),
                lit(f"{self.tagName} - {self.tagCategory}").alias("tag_detail")
            ).distinct()  # ç¡®ä¿ç”¨æˆ·å”¯ä¸€æ€§
            
            # 6. åå¤„ç†
            finalResult = self.postProcessResult(result)
            
            resultCount = finalResult.count()
            logger.info(f"âœ… ä»»åŠ¡ {self.tagName} æ‰§è¡Œå®Œæˆ: {resultCount} ä¸ªç”¨æˆ·å‘½ä¸­")
            
            return finalResult
            
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡ {self.tagName} æ‰§è¡Œå¤±è´¥: {str(e)}")
            raise
    
    def cleanup(self):
        """æ¸…ç†ä»»åŠ¡èµ„æº"""
        try:
            # æ¸…ç†æ•°æ®ç¼“å­˜
            for df in self._dataCache.values():
                if hasattr(df, 'unpersist'):
                    df.unpersist()
            self._dataCache.clear()
            
            logger.debug(f"ğŸ§¹ ä»»åŠ¡ {self.tagName} èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ä»»åŠ¡ {self.tagName} èµ„æºæ¸…ç†å¼‚å¸¸: {str(e)}")
    
    def getTaskMetadata(self) -> Dict[str, Any]:
        """
        è·å–ä»»åŠ¡å…ƒæ•°æ®
        
        Returns:
            Dict[str, Any]: ä»»åŠ¡å…ƒæ•°æ®
        """
        return {
            'tagId': self.tagId,
            'tagName': self.tagName,
            'tagCategory': self.tagCategory,
            'requiredFields': self.getRequiredFields(),
            'hiveTableConfig': self.getHiveTableConfig(),
            'taskClass': self.__class__.__name__
        }
    
    def __str__(self) -> str:
        return f"TagTask({self.tagId}: {self.tagName})"
    
    def __repr__(self) -> str:
        return self.__str__()