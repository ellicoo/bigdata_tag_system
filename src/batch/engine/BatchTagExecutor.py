"""
æ‰¹å¤„ç†æ ‡ç­¾æ‰§è¡Œå™¨ - é‡æ„ä¸ºé©¼å³°å‘½åé£æ ¼
æ•´åˆåŸæœ‰çš„TaskBasedParallelEngineåŠŸèƒ½ï¼Œæ”¯æŒå…¨é‡å’ŒæŒ‡å®šæ ‡ç­¾çš„ä»»åŠ¡åŒ–è®¡ç®—
å®ç°å¹¶è¡Œä»»åŠ¡æ‰§è¡Œå’Œç»“æœåˆå¹¶
"""

import logging
from typing import List, Dict, Any, Optional
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col, lit

from src.batch.config.BaseConfig import BaseConfig
from src.batch.bean.TaskExecutionContext import TaskExecutionContext
from src.batch.bean.TagResult import TagResult
from src.batch.bean.UserTagsSummary import UserTagsSummary
from src.batch.bean.HiveTableMeta import HiveTableMeta
from src.batch.bean.BatchExecutionResult import BatchExecutionResult

logger = logging.getLogger(__name__)


class BatchTagExecutor:
    """æ‰¹å¤„ç†æ ‡ç­¾æ‰§è¡Œå™¨ï¼ˆåŸTaskBasedParallelEngineåŠŸèƒ½ï¼‰"""
    
    def __init__(self, systemConfig: BaseConfig, spark: SparkSession):
        self.systemConfig = systemConfig
        self.spark = spark
        
        # å»¶è¿Ÿåˆå§‹åŒ–çš„ç»„ä»¶
        self._taskFactory = None
        self._ruleReader = None
        self._hiveReader = None
        
        self.logger = logging.getLogger(__name__)
    
    def executeTagTasksInParallel(self, tagIds: List[int], 
                                 userFilter: Optional[List[str]] = None) -> 'BatchExecutionResult':
        """
        å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæ ‡ç­¾ä»»åŠ¡çš„å®Œæ•´æµç¨‹
        1. å¯åŠ¨åŒä¸€ä¸ªSparkä¼šè¯
        2. å¹¶è¡Œè¯»å–S3ä¸šåŠ¡æ•°æ®
        3. ä»ç¼“å­˜ä¸­å–å‡ºMySQLæ ‡ç­¾è§„åˆ™
        4. ä½¿ç”¨æ ‡ç­¾è§„åˆ™ç”ŸæˆSQLå¹¶æ‰§è¡Œæ ‡ç­¾è®¡ç®—
        5. å†…å­˜åˆå¹¶å„ä»»åŠ¡ç»“æœ
        6. è¿”å›åŒ…å«æ‰§è¡Œç»Ÿè®¡çš„ç»“æœBean
        
        Args:
            tagIds: è¦æ‰§è¡Œçš„æ ‡ç­¾IDåˆ—è¡¨
            userFilter: å¯é€‰çš„ç”¨æˆ·IDè¿‡æ»¤åˆ—è¡¨
            
        Returns:
            BatchExecutionResult: åŒ…å«ç»“æœDataFrameå’Œæ‰§è¡Œç»Ÿè®¡çš„Beanå¯¹è±¡
        """
        # åˆ›å»ºæ‰§è¡Œç»“æœBean
        executionResult = BatchExecutionResult.createEmpty()
        executionResult.markStart()
        
        try:
            self.logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œæ‰§è¡Œ {len(tagIds)} ä¸ªæ ‡ç­¾ä»»åŠ¡: {tagIds}")
            
            # 1. åˆå§‹åŒ–ç»„ä»¶
            self._initializeComponents()
            
            # 2. é¢„ç¼“å­˜MySQLæ ‡ç­¾è§„åˆ™ï¼ˆæ‰€æœ‰ä»»åŠ¡å…±äº«ï¼‰
            rulesCache = self._precacheTagRules(tagIds)
            
            # 3. å¹¶è¡Œæ‰§è¡Œå„ä¸ªæ ‡ç­¾ä»»åŠ¡
            taskResults = self._executeTagTasksInParallel(tagIds, userFilter, rulesCache)
            
            if not taskResults:
                self.logger.warning("âš ï¸ æ²¡æœ‰ä»»åŠ¡äº§ç”Ÿç»“æœ")
                executionResult.markEnd(success=False)
                return executionResult
            
            # 4. å†…å­˜åˆå¹¶å„ä»»åŠ¡ç»“æœ
            mergedResult = self._mergeTaskResultsInMemory(taskResults)
            
            # 5. æ›´æ–°æ‰§è¡Œç»“æœ
            executionResult.resultDataFrame = mergedResult
            executionResult.totalUsers = mergedResult.count()
            executionResult.markEnd(success=True)
            
            # 6. è®°å½•æ‰§è¡Œç»Ÿè®¡
            self._logParallelExecutionStatistics(tagIds, taskResults, mergedResult)
            
            return executionResult
            
        except Exception as e:
            executionResult.addError(str(e))
            executionResult.markEnd(success=False)
            self.logger.error(f"âŒ å¹¶è¡Œæ ‡ç­¾ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")
            return executionResult
    
    def _initializeComponents(self):
        """åˆå§‹åŒ–ç»„ä»¶"""
        try:
            if self._taskFactory is None:
                from src.batch.tasks.base.TagTaskFactory import TagTaskFactory
                self._taskFactory = TagTaskFactory()
            
            if self._ruleReader is None:
                from src.batch.utils.RuleReader import RuleReader
                self._ruleReader = RuleReader(self.spark, self.systemConfig.mysql)
            
            if self._hiveReader is None:
                from src.batch.utils.HiveDataReader import HiveDataReader
                self._hiveReader = HiveDataReader(self.spark, self.systemConfig.s3)
                
        except Exception as e:
            self.logger.error(f"âŒ ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _precacheTagRules(self, tagIds: List[int]) -> DataFrame:
        """
        é¢„ç¼“å­˜MySQLæ ‡ç­¾è§„åˆ™ï¼ˆæ‰€æœ‰ä»»åŠ¡å…±äº«ï¼‰
        
        Args:
            tagIds: æ ‡ç­¾IDåˆ—è¡¨
            
        Returns:
            DataFrame: ç¼“å­˜çš„è§„åˆ™æ•°æ®
        """
        try:
            self.logger.info(f"ğŸ“š é¢„ç¼“å­˜ {len(tagIds)} ä¸ªæ ‡ç­¾è§„åˆ™...")
            
            # è·å–æ‰€æœ‰æ´»è·ƒè§„åˆ™ï¼ˆå¦‚æœå·²ç¼“å­˜åˆ™ç›´æ¥ä½¿ç”¨ç¼“å­˜ï¼‰
            allRulesDataFrame = self._ruleReader.getActiveRulesDataFrame()
            
            # è¿‡æ»¤å‡ºéœ€è¦çš„æ ‡ç­¾è§„åˆ™
            filteredRules = allRulesDataFrame.filter(col("tag_id").isin(tagIds))
            
            # å¼ºåˆ¶ç¼“å­˜åˆ°å†…å­˜å’Œç£ç›˜
            from pyspark import StorageLevel
            cachedRules = filteredRules.persist(StorageLevel.MEMORY_AND_DISK)
            
            # è§¦å‘ç¼“å­˜
            ruleCount = cachedRules.count()
            self.logger.info(f"âœ… æˆåŠŸç¼“å­˜ {ruleCount} æ¡æ ‡ç­¾è§„åˆ™")
            
            # éªŒè¯è§„åˆ™å®Œæ•´æ€§
            for tagId in tagIds:
                rule = cachedRules.filter(col("tag_id") == tagId).first()
                if rule is None:
                    self.logger.warning(f"âš ï¸ æ ‡ç­¾ {tagId} æ²¡æœ‰å¯¹åº”çš„è§„åˆ™ï¼Œå°†è·³è¿‡")
            
            return cachedRules
            
        except Exception as e:
            self.logger.error(f"âŒ é¢„ç¼“å­˜æ ‡ç­¾è§„åˆ™å¤±è´¥: {str(e)}")
            raise
    
    def _executeTagTasksInParallel(self, tagIds: List[int], 
                                  userFilter: Optional[List[str]], 
                                  rulesCache: DataFrame) -> List[DataFrame]:
        """
        å¹¶è¡Œæ‰§è¡Œå„ä¸ªæ ‡ç­¾ä»»åŠ¡
        
        Args:
            tagIds: æ ‡ç­¾IDåˆ—è¡¨
            userFilter: ç”¨æˆ·è¿‡æ»¤åˆ—è¡¨
            rulesCache: ç¼“å­˜çš„è§„åˆ™æ•°æ®
            
        Returns:
            List[DataFrame]: å„ä»»åŠ¡çš„æ‰§è¡Œç»“æœ
        """
        try:
            self.logger.info(f"âš¡ å¹¶è¡Œæ‰§è¡Œ {len(tagIds)} ä¸ªæ ‡ç­¾ä»»åŠ¡...")
            
            # åˆ›å»ºä»»åŠ¡ç»“æœæ”¶é›†å™¨
            from src.batch.utils.TaskResultCollector import TaskResultCollector
            resultCollector = TaskResultCollector()
            
            # åˆ›å»ºå…±äº«çš„ä¸šåŠ¡æ•°æ®è¯»å–å™¨
            from src.batch.utils.HiveDataReader import HiveDataReader
            hiveReader = HiveDataReader(self.spark, self.systemConfig.s3)
            
            # ä¸ºæ¯ä¸ªæ ‡ç­¾åˆ›å»ºå¹¶æ‰§è¡Œä»»åŠ¡
            for i, tagId in enumerate(tagIds, 1):
                # åˆ›å»ºä»»åŠ¡æ‰§è¡Œä¸Šä¸‹æ–‡
                taskId = f"tag_{tagId}_{i}"
                executionContext = None
                
                try:
                    self.logger.info(f"ğŸ¯ æ‰§è¡Œä»»åŠ¡ {i}/{len(tagIds)}: æ ‡ç­¾ {tagId}")
                    
                    # ä»ç¼“å­˜ä¸­è·å–è§„åˆ™
                    ruleRow = rulesCache.filter(col("tag_id") == tagId).first()
                    if ruleRow is None:
                        self.logger.warning(f"âš ï¸ æ ‡ç­¾ {tagId} è§„åˆ™ç¼ºå¤±ï¼Œè·³è¿‡")
                        continue
                    
                    # åˆ›å»ºä»»åŠ¡æ‰§è¡Œä¸Šä¸‹æ–‡
                    executionContext = TaskExecutionContext.createForTask(
                        taskId=taskId,
                        tagId=tagId,
                        tagName=ruleRow['tag_name'],
                        userFilter=userFilter
                    )
                    executionContext.startExecution()
                    
                    # æ„å»ºä»»åŠ¡é…ç½®
                    taskConfig = {
                        'tag_id': tagId,
                        'tag_name': ruleRow['tag_name'],
                        'tag_category': ruleRow['tag_category'],
                        'rule_conditions': ruleRow['rule_conditions']
                    }
                    
                    # åˆ›å»ºä»»åŠ¡å®ä¾‹
                    task = self._taskFactory.createTask(tagId, taskConfig, self.spark, self.systemConfig)
                    if task is None:
                        if executionContext:
                            executionContext.failExecution("ä»»åŠ¡åˆ›å»ºå¤±è´¥")
                        self.logger.warning(f"âš ï¸ æ ‡ç­¾ {tagId} ä»»åŠ¡åˆ›å»ºå¤±è´¥ï¼Œè·³è¿‡")
                        continue
                    
                    # æ‰§è¡Œä»»åŠ¡ï¼ˆè¿™é‡Œä½¿ç”¨å…±äº«çš„Sparkä¼šè¯å’Œæ•°æ®è¯»å–å™¨ï¼‰
                    taskResult = task.execute(userFilter)
                    
                    # ä½¿ç”¨ç»“æœæ”¶é›†å™¨æ”¶é›†ç»“æœ
                    if taskResult is not None and taskResult.count() > 0:
                        taggedUserCount = taskResult.count()
                        resultCollector.addTaskResult(tagId, ruleRow['tag_name'], taskResult)
                        
                        # æ›´æ–°æ‰§è¡Œä¸Šä¸‹æ–‡
                        if executionContext:
                            executionContext.completeExecution(taggedUserCount)
                        
                        self.logger.info(f"âœ… æ ‡ç­¾ {tagId} ä»»åŠ¡å®Œæˆ: {taggedUserCount} ä¸ªç”¨æˆ·")
                    else:
                        if executionContext:
                            executionContext.completeExecution(0)
                        self.logger.info(f"ğŸ“Š æ ‡ç­¾ {tagId} ä»»åŠ¡æ— åŒ¹é…ç”¨æˆ·")
                        
                except Exception as e:
                    if executionContext:
                        executionContext.failExecution(str(e))
                    self.logger.error(f"âŒ æ ‡ç­¾ {tagId} ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")
                    continue
            
            # éªŒè¯å’Œæ”¶é›†ç»“æœ
            isValid, errors = resultCollector.validateAllResults()
            if not isValid:
                self.logger.warning(f"âš ï¸ ç»“æœéªŒè¯å‘ç°é—®é¢˜: {errors}")
            
            # è®°å½•è¯¦ç»†ç»Ÿè®¡
            stats = resultCollector.getDetailedStatistics()
            self.logger.info(f"ğŸ“Š ä»»åŠ¡æ‰§è¡Œç»Ÿè®¡: {stats}")
            
            # è·å–æ‰€æœ‰ç»“æœ
            allResults = resultCollector.collectAllResults()
            
            self.logger.info(f"âœ… å¹¶è¡Œä»»åŠ¡æ‰§è¡Œå®Œæˆï¼ŒæˆåŠŸ {len(allResults)} / {len(tagIds)} ä¸ªä»»åŠ¡")
            return allResults
            
        except Exception as e:
            self.logger.error(f"âŒ å¹¶è¡Œä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")
            return []
    
    def _mergeTaskResultsInMemory(self, taskResults: List[DataFrame]) -> DataFrame:
        """
        å†…å­˜åˆå¹¶å„ä»»åŠ¡ç»“æœ
        
        Args:
            taskResults: å„ä»»åŠ¡çš„DataFrameç»“æœåˆ—è¡¨
            
        Returns:
            DataFrame: åˆå¹¶åçš„ç»“æœ (user_id, tag_ids)
        """
        try:
            self.logger.info(f"ğŸ§© å†…å­˜åˆå¹¶ {len(taskResults)} ä¸ªä»»åŠ¡ç»“æœ...")
            
            if not taskResults:
                return self._createEmptyResult()
            
            # ä½¿ç”¨æ‰¹å¤„ç†ç»“æœåˆå¹¶å™¨
            from src.batch.utils.BatchResultMerger import BatchResultMerger
            resultMerger = BatchResultMerger(self.systemConfig.mysql)
            
            # åˆå¹¶å¤šä¸ªæ ‡ç­¾ç»“æœ
            mergedResult = resultMerger.mergeMultipleTagResults(taskResults)
            
            mergedUserCount = mergedResult.count()
            self.logger.info(f"âœ… å†…å­˜åˆå¹¶å®Œæˆ: {mergedUserCount} ä¸ªç”¨æˆ·")
            
            return mergedResult
            
        except Exception as e:
            self.logger.error(f"âŒ å†…å­˜åˆå¹¶å¤±è´¥: {str(e)}")
            raise
    
    def _createEmptyResult(self) -> DataFrame:
        """
        åˆ›å»ºç©ºç»“æœDataFrameï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰
        
        Returns:
            DataFrame: ç©ºçš„æ ‡ç­¾ç»“æœDataFrame
        """
        try:
            from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType
            
            schema = StructType([
                StructField("user_id", StringType(), False),
                StructField("tag_ids", ArrayType(IntegerType()), False)
            ])
            
            emptyResult = self.spark.createDataFrame([], schema)
            self.logger.info("ğŸ“ åˆ›å»ºç©ºç»“æœDataFrame")
            
            return emptyResult
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»ºç©ºç»“æœå¤±è´¥: {str(e)}")
            raise
    
    def _logParallelExecutionStatistics(self, tagIds: List[int], 
                                        taskResults: List[DataFrame], 
                                        mergedResult: DataFrame):
        """
        è®°å½•å¹¶è¡Œæ‰§è¡Œç»Ÿè®¡ä¿¡æ¯
        
        Args:
            tagIds: è®¡åˆ’æ‰§è¡Œçš„æ ‡ç­¾IDåˆ—è¡¨
            taskResults: å„ä»»åŠ¡çš„æ‰§è¡Œç»“æœ
            mergedResult: åˆå¹¶åçš„æœ€ç»ˆç»“æœ
        """
        try:
            totalPlannedTags = len(tagIds)
            successfulTasks = len(taskResults)
            finalUserCount = mergedResult.count()
            
            # è®¡ç®—å„ä»»åŠ¡çš„ç”¨æˆ·æ•°
            taskUserCounts = []
            totalTaskUsers = 0
            
            for i, result in enumerate(taskResults):
                try:
                    userCount = result.count()
                    taskUserCounts.append(userCount)
                    totalTaskUsers += userCount
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ç»Ÿè®¡ç¬¬ {i+1} ä¸ªä»»åŠ¡ç»“æœå¤±è´¥: {str(e)}")
                    taskUserCounts.append(0)
            
            # è®°å½•è¯¦ç»†ç»Ÿè®¡
            self.logger.info("ğŸ“Š å¹¶è¡Œæ‰§è¡Œç»Ÿè®¡:")
            self.logger.info(f"   ğŸ¯ è®¡åˆ’æ‰§è¡Œæ ‡ç­¾: {totalPlannedTags} ä¸ª")
            self.logger.info(f"   âœ… æˆåŠŸæ‰§è¡Œä»»åŠ¡: {successfulTasks} ä¸ª")
            self.logger.info(f"   ğŸ‘¥ ä»»åŠ¡ç”¨æˆ·æ€»æ•°: {totalTaskUsers} ä¸ª")
            self.logger.info(f"   ğŸ¯ æœ€ç»ˆåˆå¹¶ç”¨æˆ·: {finalUserCount} ä¸ª")
            self.logger.info(f"   ğŸ“ˆ æ‰§è¡ŒæˆåŠŸç‡: {round(successfulTasks/totalPlannedTags*100, 1)}%")
            
            # å„ä»»åŠ¡è¯¦ç»†ç»Ÿè®¡
            for i, (tagId, userCount) in enumerate(zip(tagIds[:len(taskUserCounts)], taskUserCounts)):
                self.logger.info(f"   ğŸ“‹ æ ‡ç­¾ {tagId}: {userCount} ç”¨æˆ·")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ è®°å½•å¹¶è¡Œæ‰§è¡Œç»Ÿè®¡å¤±è´¥: {str(e)}")
    
    def _createTaskInstances(self, tagIds: List[int]) -> List[Any]:
        """
        åˆ›å»ºä»»åŠ¡å®ä¾‹
        
        Args:
            tagIds: æ ‡ç­¾IDåˆ—è¡¨
            
        Returns:
            List[BaseTagTask]: ä»»åŠ¡å®ä¾‹åˆ—è¡¨
        """
        try:
            self.logger.info(f"ğŸ”§ åˆ›å»º {len(tagIds)} ä¸ªä»»åŠ¡å®ä¾‹...")
            
            # è·å–æ ‡ç­¾è§„åˆ™æ•°æ®
            rulesDataFrame = self._ruleReader.getActiveRulesDataFrame()
            
            tasks = []
            for tagId in tagIds:
                try:
                    # è·å–æ ‡ç­¾é…ç½®
                    ruleRow = rulesDataFrame.filter(col("tag_id") == tagId).first()
                    if ruleRow is None:
                        self.logger.warning(f"âš ï¸ æ ‡ç­¾ {tagId} æ²¡æœ‰æ‰¾åˆ°å¯¹åº”è§„åˆ™ï¼Œè·³è¿‡")
                        continue
                    
                    # æ„å»ºä»»åŠ¡é…ç½®
                    taskConfig = {
                        'tag_id': tagId,
                        'tag_name': ruleRow['tag_name'],
                        'tag_category': ruleRow['tag_category'],
                        'rule_conditions': ruleRow['rule_conditions']
                    }
                    
                    # åˆ›å»ºä»»åŠ¡å®ä¾‹
                    task = self._taskFactory.createTask(tagId, taskConfig, self.spark, self.systemConfig)
                    if task:
                        tasks.append(task)
                        self.logger.info(f"âœ… ä»»åŠ¡åˆ›å»ºæˆåŠŸ: {task.tagName}")
                    else:
                        self.logger.warning(f"âš ï¸ æ ‡ç­¾ {tagId} ä»»åŠ¡åˆ›å»ºå¤±è´¥ï¼Œè·³è¿‡")
                        
                except Exception as e:
                    self.logger.error(f"âŒ åˆ›å»ºæ ‡ç­¾ {tagId} ä»»åŠ¡å¤±è´¥: {str(e)}")
                    continue
            
            self.logger.info(f"âœ… æˆåŠŸåˆ›å»º {len(tasks)} ä¸ªä»»åŠ¡å®ä¾‹")
            return tasks
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»ºä»»åŠ¡å®ä¾‹å¤±è´¥: {str(e)}")
            return []
    
    def _executeTasksInParallel(self, tasks: List[Any], 
                               userFilter: Optional[List[str]] = None) -> List[DataFrame]:
        """
        å¹¶è¡Œæ‰§è¡Œä»»åŠ¡
        
        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨
            userFilter: ç”¨æˆ·è¿‡æ»¤åˆ—è¡¨
            
        Returns:
            List[DataFrame]: æ‰§è¡Œç»“æœåˆ—è¡¨
        """
        try:
            self.logger.info(f"âš¡ å¼€å§‹å¹¶è¡Œæ‰§è¡Œ {len(tasks)} ä¸ªä»»åŠ¡...")
            
            results = []
            
            # ç”±äºPySparkçš„é™åˆ¶ï¼Œè¿™é‡Œé‡‡ç”¨é¡ºåºæ‰§è¡Œä½†å†…éƒ¨å¹¶è¡Œçš„æ–¹å¼
            # æ¯ä¸ªä»»åŠ¡å†…éƒ¨ä½¿ç”¨Sparkçš„åˆ†å¸ƒå¼å¹¶è¡Œèƒ½åŠ›
            for i, task in enumerate(tasks, 1):
                try:
                    self.logger.info(f"ğŸ”„ æ‰§è¡Œä»»åŠ¡ {i}/{len(tasks)}: {task.tagName}")
                    
                    # æ‰§è¡Œå•ä¸ªä»»åŠ¡
                    taskResult = task.execute(userFilter)
                    
                    if taskResult is not None and taskResult.count() > 0:
                        results.append(taskResult)
                        self.logger.info(f"âœ… ä»»åŠ¡ {task.tagName} å®Œæˆ: {taskResult.count()} ä¸ªç”¨æˆ·")
                    else:
                        self.logger.info(f"ğŸ“Š ä»»åŠ¡ {task.tagName} æ²¡æœ‰åŒ¹é…ç”¨æˆ·")
                        
                except Exception as e:
                    self.logger.error(f"âŒ ä»»åŠ¡ {task.tagName} æ‰§è¡Œå¤±è´¥: {str(e)}")
                    continue
            
            self.logger.info(f"âœ… å¹¶è¡Œä»»åŠ¡æ‰§è¡Œå®Œæˆï¼ŒæˆåŠŸ {len(results)} / {len(tasks)} ä¸ªä»»åŠ¡")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ å¹¶è¡Œä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")
            return []
    
    def _logExecutionStatistics(self, tagIds: List[int], results: List[DataFrame]):
        """
        è®°å½•æ‰§è¡Œç»Ÿè®¡ä¿¡æ¯
        
        Args:
            tagIds: æ ‡ç­¾IDåˆ—è¡¨
            results: ç»“æœåˆ—è¡¨
        """
        try:
            totalTaggedUsers = 0
            taskStats = []
            
            for result in results:
                try:
                    userCount = result.count()
                    totalTaggedUsers += userCount
                    
                    # è·å–æ ‡ç­¾IDï¼ˆå‡è®¾ç»“æœä¸­åŒ…å«tag_idåˆ—ï¼‰
                    if 'tag_id' in result.columns:
                        tagId = result.select("tag_id").first()['tag_id']
                        taskStats.append({
                            'tag_id': tagId,
                            'user_count': userCount
                        })
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ç»Ÿè®¡å•ä¸ªç»“æœå¤±è´¥: {str(e)}")
                    continue
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            self.logger.info("ğŸ“Š æ ‡ç­¾æ‰§è¡Œç»Ÿè®¡:")
            self.logger.info(f"   ğŸ¯ è®¡åˆ’æ‰§è¡Œæ ‡ç­¾: {len(tagIds)} ä¸ª")
            self.logger.info(f"   âœ… æˆåŠŸæ‰§è¡Œæ ‡ç­¾: {len(results)} ä¸ª")
            self.logger.info(f"   ğŸ‘¥ æ€»è®¡æ ‡ç­¾ç”¨æˆ·: {totalTaggedUsers} ä¸ª")
            
            # è¯¦ç»†ç»Ÿè®¡
            for stat in taskStats:
                self.logger.info(f"   ğŸ“‹ æ ‡ç­¾ {stat['tag_id']}: {stat['user_count']} ç”¨æˆ·")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ è®°å½•æ‰§è¡Œç»Ÿè®¡å¤±è´¥: {str(e)}")
    
    def executeWithMySQLMerge(self, tagIds: List[int], 
                             userFilter: Optional[List[str]] = None) -> BatchExecutionResult:
        """
        æ‰§è¡Œæ ‡ç­¾ä»»åŠ¡å¹¶ä¸MySQLç°æœ‰æ ‡ç­¾åˆå¹¶
        
        Args:
            tagIds: è¦æ‰§è¡Œçš„æ ‡ç­¾IDåˆ—è¡¨
            userFilter: å¯é€‰çš„ç”¨æˆ·IDè¿‡æ»¤åˆ—è¡¨
            
        Returns:
            BatchExecutionResult: åŒ…å«MySQLåˆå¹¶åç»“æœçš„æ‰§è¡Œç»“æœBean
        """
        try:
            self.logger.info(f"ğŸ”„ æ‰§è¡Œæ ‡ç­¾ä»»åŠ¡å¹¶åˆå¹¶MySQLç°æœ‰æ ‡ç­¾: {tagIds}")
            
            # æ‰§è¡Œå¹¶è¡Œä»»åŠ¡
            executionResult = self.executeTagTasksInParallel(tagIds, userFilter)
            
            if not executionResult.success or executionResult.resultDataFrame.count() == 0:
                self.logger.warning("âš ï¸ æ²¡æœ‰æ–°æ ‡ç­¾ç»“æœéœ€è¦åˆå¹¶")
                executionResult.metadata['mysql_merge'] = False
                return executionResult
            
            # ä¸MySQLç°æœ‰æ ‡ç­¾åˆå¹¶
            from src.batch.utils.BatchResultMerger import BatchResultMerger
            resultMerger = BatchResultMerger(self.systemConfig.mysql)
            
            finalResult = resultMerger.mergeWithExistingTags(executionResult.resultDataFrame)
            
            # æ›´æ–°æ‰§è¡Œç»“æœ
            executionResult.resultDataFrame = finalResult
            executionResult.totalUsers = finalResult.count()
            executionResult.metadata['mysql_merge'] = True
            
            self.logger.info(f"âœ… MySQLæ ‡ç­¾åˆå¹¶å®Œæˆ: {executionResult.totalUsers} ä¸ªç”¨æˆ·")
            
            return executionResult
            
        except Exception as e:
            errorResult = BatchExecutionResult.createEmpty()
            errorResult.addError(str(e))
            errorResult.markEnd(success=False)
            self.logger.error(f"âŒ æ ‡ç­¾ä»»åŠ¡ä¸MySQLåˆå¹¶å¤±è´¥: {str(e)}")
            return errorResult
    
    def executeSingleTag(self, tagId: int, 
                        userFilter: Optional[List[str]] = None) -> Optional[DataFrame]:
        """
        æ‰§è¡Œå•ä¸ªæ ‡ç­¾ä»»åŠ¡ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼Œè¿”å›DataFrameï¼‰
        
        Args:
            tagId: æ ‡ç­¾ID
            userFilter: å¯é€‰çš„ç”¨æˆ·IDè¿‡æ»¤åˆ—è¡¨
            
        Returns:
            DataFrame: æ ‡ç­¾è®¡ç®—ç»“æœï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            self.logger.info(f"ğŸ¯ æ‰§è¡Œå•ä¸ªæ ‡ç­¾ä»»åŠ¡: {tagId}")
            
            # ä½¿ç”¨Beanæ–¹æ³•æ‰§è¡Œ
            executionResult = self.executeTagTasksInParallel([tagId], userFilter)
            
            if executionResult.success and executionResult.resultDataFrame.count() > 0:
                return executionResult.resultDataFrame
            else:
                self.logger.warning(f"âš ï¸ æ ‡ç­¾ {tagId} æ²¡æœ‰è®¡ç®—ç»“æœ")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ å•ä¸ªæ ‡ç­¾ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")
            return None
    
    def validateTaskExecution(self, tagIds: List[int]) -> Dict[str, Any]:
        """
        éªŒè¯ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›
        
        Args:
            tagIds: è¦éªŒè¯çš„æ ‡ç­¾IDåˆ—è¡¨
            
        Returns:
            Dict[str, Any]: éªŒè¯ç»“æœ
        """
        try:
            self.logger.info(f"ğŸ” éªŒè¯ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›: {tagIds}")
            
            validationResult = {
                'overall_status': 'success',
                'tag_validation': {},
                'statistics': {}
            }
            
            # åˆå§‹åŒ–ç»„ä»¶
            self._initializeComponents()
            
            # éªŒè¯æ¯ä¸ªæ ‡ç­¾
            validTasks = 0
            for tagId in tagIds:
                try:
                    # å°è¯•åˆ›å»ºä»»åŠ¡
                    tasks = self._createTaskInstances([tagId])
                    if tasks:
                        validationResult['tag_validation'][tagId] = 'valid'
                        validTasks += 1
                    else:
                        validationResult['tag_validation'][tagId] = 'invalid'
                        
                except Exception as e:
                    validationResult['tag_validation'][tagId] = f'error: {str(e)}'
            
            # ç»Ÿè®¡ç»“æœ
            validationResult['statistics'] = {
                'total_tags': len(tagIds),
                'valid_tags': validTasks,
                'invalid_tags': len(tagIds) - validTasks
            }
            
            if validTasks == 0:
                validationResult['overall_status'] = 'failed'
            elif validTasks < len(tagIds):
                validationResult['overall_status'] = 'partial'
            
            self.logger.info(f"ğŸ“Š éªŒè¯å®Œæˆ: {validationResult['overall_status']}")
            return validationResult
            
        except Exception as e:
            self.logger.error(f"âŒ éªŒè¯ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›å¤±è´¥: {str(e)}")
            return {
                'overall_status': 'error',
                'error': str(e)
            }
    
    def getExecutorStatistics(self) -> Dict[str, Any]:
        """
        è·å–æ‰§è¡Œå™¨ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            stats = {
                'spark_version': self.spark.version,
                'spark_app_name': self.spark.sparkContext.appName,
                'spark_master': self.spark.sparkContext.master,
                'environment': self.systemConfig.environment
            }
            
            # è·å–å¯ç”¨æ ‡ç­¾æ•°
            if self._ruleReader:
                ruleStats = self._ruleReader.getStatistics()
                stats.update(ruleStats)
            
            return stats
            
        except Exception as e:
            self.logger.error(f"âŒ è·å–æ‰§è¡Œå™¨ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {}
    
    def cleanup(self):
        """æ¸…ç†æ‰§è¡Œå™¨èµ„æº"""
        try:
            self.logger.info("ğŸ§¹ æ¸…ç†æ ‡ç­¾æ‰§è¡Œå™¨èµ„æº...")
            
            # æ¸…ç†è§„åˆ™è¯»å–å™¨ç¼“å­˜
            if self._ruleReader:
                self._ruleReader.clearCache()
            
            # æ¸…ç†ä»»åŠ¡å·¥å‚
            if self._taskFactory:
                self._taskFactory.clearRegistry()
            
            # é‡ç½®ç»„ä»¶å¼•ç”¨
            self._ruleReader = None
            self._taskFactory = None
            self._hiveReader = None
            
            self.logger.info("âœ… æ ‡ç­¾æ‰§è¡Œå™¨èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ æ¸…ç†æ‰§è¡Œå™¨èµ„æºå¼‚å¸¸: {str(e)}")
    
    def __str__(self) -> str:
        return f"BatchTagExecutor(env={self.systemConfig.environment})"
    
    def __repr__(self) -> str:
        return self.__str__()