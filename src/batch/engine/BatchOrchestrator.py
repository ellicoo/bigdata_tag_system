"""
æ‰¹å¤„ç†ç¼–æ’å™¨ - é‡æ„ä¸ºé©¼å³°å‘½åé£æ ¼
ç³»ç»Ÿçš„æ ¸å¿ƒç¼–æ’å™¨ï¼Œæ•´åˆåŸæœ‰TagSchedulerçš„åŠŸèƒ½ï¼Œåè°ƒæ‰€æœ‰ç»„ä»¶
"""

import logging
from typing import List, Dict, Any, Optional
from pyspark.sql import SparkSession, DataFrame
from pyspark import StorageLevel

from src.batch.config.ConfigManager import ConfigManager
from src.batch.config.BaseConfig import BaseConfig

logger = logging.getLogger(__name__)


class BatchOrchestrator:
    """æ‰¹å¤„ç†ç¼–æ’å™¨ï¼ˆåŸTagScheduleråŠŸèƒ½ï¼‰"""
    
    def __init__(self, systemConfig: BaseConfig):
        self.systemConfig = systemConfig
        self.spark = None
        
        # æ ¸å¿ƒç»„ä»¶ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._dataLoader = None
        self._tagExecutor = None
        self._resultMerger = None
        self._dataWriter = None
        
        # ç¼“å­˜
        self._existingTagsCache = None
        
        self.logger = logging.getLogger(__name__)
    
    def initializeSystem(self) -> bool:
        """
        åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶
        
        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        try:
            self.logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–æ‰¹å¤„ç†æ ‡ç­¾ç³»ç»Ÿ...")
            
            # 1. åˆå§‹åŒ–Sparkä¼šè¯
            self._initializeSpark()
            
            # 2. åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
            self._initializeCoreComponents()
            
            # 3. éªŒè¯ç³»ç»Ÿå¥åº·çŠ¶æ€
            if not self._validateSystemHealth():
                raise Exception("ç³»ç»Ÿå¥åº·æ£€æŸ¥å¤±è´¥")
            
            self.logger.info("âœ… æ‰¹å¤„ç†æ ‡ç­¾ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    def _initializeSpark(self):
        """åˆå§‹åŒ–Sparkä¼šè¯"""
        try:
            self.logger.info("âš¡ åˆå§‹åŒ–Sparkä¼šè¯...")
            
            sparkConfig = self.systemConfig.spark
            
            builder = SparkSession.builder \
                .appName(f"BatchTagSystem-{self.systemConfig.environment}") \
                .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
                .config("spark.sql.adaptive.enabled", "true") \
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
            
            # æ·»åŠ JARé…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if hasattr(sparkConfig, 'jars') and sparkConfig.jars:
                builder = builder.config("spark.jars", sparkConfig.jars)
                self.logger.info(f"ğŸ“¦ åŠ è½½JARæ–‡ä»¶: {sparkConfig.jars}")
            
            # ç¯å¢ƒç‰¹å®šé…ç½®
            if self.systemConfig.environment == 'local':
                builder = builder.master("local[*]") \
                    .config("spark.sql.shuffle.partitions", "4") \
                    .config("spark.driver.memory", "2g") \
                    .config("spark.executor.memory", "2g")
            else:
                # Glueç¯å¢ƒé…ç½®
                builder = builder \
                    .config("spark.sql.shuffle.partitions", "200") \
                    .config("spark.sql.adaptive.advisory.partitionSizeInBytes", "128MB")
            
            # S3é…ç½®ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if hasattr(self.systemConfig, 's3') and self.systemConfig.s3:
                s3Config = self.systemConfig.s3
                builder = builder \
                    .config("spark.hadoop.fs.s3a.access.key", s3Config.accessKey) \
                    .config("spark.hadoop.fs.s3a.secret.key", s3Config.secretKey) \
                    .config("spark.hadoop.fs.s3a.endpoint", s3Config.endpoint) \
                    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
                    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
            
            self.spark = builder.getOrCreate()
            self.spark.sparkContext.setLogLevel("WARN")
            
            self.logger.info(f"âœ… Sparkä¼šè¯åˆå§‹åŒ–å®Œæˆ: {self.spark.version}")
            
        except Exception as e:
            self.logger.error(f"âŒ Sparkåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _initializeCoreComponents(self):
        """åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶"""
        try:
            self.logger.info("ğŸ”§ åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶...")
            
            # æ•°æ®åŠ è½½å™¨
            from src.batch.utils.HiveDataReader import HiveDataReader
            from src.batch.utils.RuleReader import RuleReader
            self._hiveReader = HiveDataReader(self.spark, self.systemConfig.s3)
            self._ruleReader = RuleReader(self.spark, self.systemConfig.mysql)
            
            # æ ‡ç­¾æ‰§è¡Œå™¨
            from src.batch.engine.BatchTagExecutor import BatchTagExecutor
            self._tagExecutor = BatchTagExecutor(self.systemConfig, self.spark)
            
            # ç»“æœåˆå¹¶å™¨
            from src.batch.utils.BatchResultMerger import BatchResultMerger
            self._resultMerger = BatchResultMerger(self.systemConfig.mysql)
            
            # æ•°æ®å†™å…¥å™¨
            from src.batch.utils.BatchDataWriter import BatchDataWriter
            self._dataWriter = BatchDataWriter(self.systemConfig.mysql)
            
            self.logger.info("âœ… æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _validateSystemHealth(self) -> bool:
        """éªŒè¯ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        try:
            self.logger.info("ğŸ” æ‰§è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥...")
            
            # 1. éªŒè¯Sparkä¼šè¯
            if self.spark is None:
                self.logger.error("âŒ Sparkä¼šè¯æœªåˆå§‹åŒ–")
                return False
            
            # 2. éªŒè¯MySQLè¿æ¥ï¼ˆåªæµ‹è¯•ä¸€æ¬¡ï¼Œé¿å…é‡å¤æ—¥å¿—ï¼‰
            if not self._dataWriter.testConnection():
                self.logger.error("âŒ MySQLè¿æ¥å¤±è´¥")
                return False
            
            # 3. è·³è¿‡RuleReaderçš„è¿æ¥éªŒè¯ï¼Œå¤ç”¨ä¸Šé¢çš„MySQLè¿æ¥æµ‹è¯•ç»“æœ
            # MySQLè¿æ¥å·²ç»éªŒè¯æˆåŠŸï¼ŒRuleReaderä½¿ç”¨ç›¸åŒçš„è¿æ¥é…ç½®
            
            # 4. åŸºæœ¬åŠŸèƒ½æµ‹è¯•
            testDataFrame = self.spark.range(1).toDF("test")
            if testDataFrame.count() != 1:
                self.logger.error("âŒ SparkåŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥")
                return False
            
            self.logger.info("âœ… ç³»ç»Ÿå¥åº·æ£€æŸ¥é€šè¿‡")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ç³»ç»Ÿå¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            return False
    
    def executeFullWorkflow(self, userFilter: Optional[List[str]] = None) -> bool:
        """
        æ‰§è¡Œå…¨é‡æ ‡ç­¾è®¡ç®—å·¥ä½œæµ
        
        Args:
            userFilter: å¯é€‰çš„ç”¨æˆ·IDè¿‡æ»¤åˆ—è¡¨
            
        Returns:
            bool: æ‰§è¡Œæ˜¯å¦æˆåŠŸ
        """
        try:
            self.logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œå…¨é‡æ ‡ç­¾è®¡ç®—å·¥ä½œæµ...")
            
            # 1. é¢„ç¼“å­˜ç°æœ‰æ ‡ç­¾
            self._precacheExistingTags()
            
            # 2. è·å–æ‰€æœ‰æ´»è·ƒæ ‡ç­¾
            allTagIds = self._getAllActiveTagIds()
            if not allTagIds:
                self.logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒæ ‡ç­¾")
                return True
            
            # 3. æ‰§è¡Œæ ‡ç­¾è®¡ç®—
            executionResult = self._tagExecutor.executeTagTasksInParallel(allTagIds, userFilter)
            tagResults = [executionResult.resultDataFrame] if executionResult.success else []
            
            if not tagResults:
                self.logger.warning("âš ï¸ æ²¡æœ‰æ ‡ç­¾è®¡ç®—ç»“æœ")
                return True
            
            # 4. åˆå¹¶ç»“æœ
            mergedResults = self._resultMerger.mergeMultipleTagResults(tagResults)
            
            # 5. ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶
            finalResults = self._resultMerger.mergeWithExistingTags(
                mergedResults, self._existingTagsCache
            )
            
            # 6. å†™å…¥æ•°æ®åº“
            writeSuccess = self._dataWriter.writeTaggedUsers(finalResults)
            
            if writeSuccess:
                self.logger.info("âœ… å…¨é‡æ ‡ç­¾è®¡ç®—å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
                return True
            else:
                self.logger.error("âŒ æ•°æ®å†™å…¥å¤±è´¥")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ å…¨é‡æ ‡ç­¾è®¡ç®—å·¥ä½œæµå¤±è´¥: {str(e)}")
            return False
    
    def executeSpecificTagsWorkflow(self, tagIds: List[int], 
                                   userFilter: Optional[List[str]] = None) -> bool:
        """
        æ‰§è¡ŒæŒ‡å®šæ ‡ç­¾è®¡ç®—å·¥ä½œæµ
        
        Args:
            tagIds: è¦è®¡ç®—çš„æ ‡ç­¾IDåˆ—è¡¨
            userFilter: å¯é€‰çš„ç”¨æˆ·IDè¿‡æ»¤åˆ—è¡¨
            
        Returns:
            bool: æ‰§è¡Œæ˜¯å¦æˆåŠŸ
        """
        try:
            self.logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡ŒæŒ‡å®šæ ‡ç­¾è®¡ç®—: {tagIds}")
            
            # 1. é¢„ç¼“å­˜ç°æœ‰æ ‡ç­¾
            self._precacheExistingTags()
            
            # 2. éªŒè¯æ ‡ç­¾IDæœ‰æ•ˆæ€§
            validTagIds = self._validateTagIds(tagIds)
            if not validTagIds:
                self.logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æ ‡ç­¾ID")
                return True
            
            # 3. æ‰§è¡Œæ ‡ç­¾è®¡ç®—
            executionResult = self._tagExecutor.executeTagTasksInParallel(validTagIds, userFilter)
            tagResults = [executionResult.resultDataFrame] if executionResult.success else []
            
            if not tagResults:
                self.logger.warning("âš ï¸ æ²¡æœ‰æ ‡ç­¾è®¡ç®—ç»“æœ")
                return True
            
            # 4. åˆå¹¶ç»“æœ
            mergedResults = self._resultMerger.mergeMultipleTagResults(tagResults)
            
            # 5. ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶
            finalResults = self._resultMerger.mergeWithExistingTags(
                mergedResults, self._existingTagsCache
            )
            
            # 6. å†™å…¥æ•°æ®åº“
            writeSuccess = self._dataWriter.writeTaggedUsers(finalResults)
            
            if writeSuccess:
                self.logger.info("âœ… æŒ‡å®šæ ‡ç­¾è®¡ç®—å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
                return True
            else:
                self.logger.error("âŒ æ•°æ®å†™å…¥å¤±è´¥")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ æŒ‡å®šæ ‡ç­¾è®¡ç®—å·¥ä½œæµå¤±è´¥: {str(e)}")
            return False
    
    def executeSpecificUsersWorkflow(self, userIds: List[str], 
                                    tagIds: Optional[List[int]] = None) -> bool:
        """
        æ‰§è¡ŒæŒ‡å®šç”¨æˆ·æ ‡ç­¾è®¡ç®—å·¥ä½œæµ
        
        Args:
            userIds: ç”¨æˆ·IDåˆ—è¡¨
            tagIds: å¯é€‰çš„æ ‡ç­¾IDåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰æ ‡ç­¾
            
        Returns:
            bool: æ‰§è¡Œæ˜¯å¦æˆåŠŸ
        """
        try:
            self.logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡ŒæŒ‡å®šç”¨æˆ·æ ‡ç­¾è®¡ç®—: {len(userIds)} ä¸ªç”¨æˆ·")
            
            # 1. é¢„ç¼“å­˜ç°æœ‰æ ‡ç­¾
            self._precacheExistingTags()
            
            # 2. ç¡®å®šè¦è®¡ç®—çš„æ ‡ç­¾
            targetTagIds = tagIds if tagIds else self._getAllActiveTagIds()
            if not targetTagIds:
                self.logger.warning("âš ï¸ æ²¡æœ‰è¦è®¡ç®—çš„æ ‡ç­¾")
                return True
            
            # 3. æ‰§è¡Œæ ‡ç­¾è®¡ç®—
            executionResult = self._tagExecutor.executeTagTasksInParallel(targetTagIds, userIds)
            tagResults = [executionResult.resultDataFrame] if executionResult.success else []
            
            if not tagResults:
                self.logger.warning("âš ï¸ æ²¡æœ‰æ ‡ç­¾è®¡ç®—ç»“æœ")
                return True
            
            # 4. åˆå¹¶ç»“æœ
            mergedResults = self._resultMerger.mergeMultipleTagResults(tagResults)
            
            # 5. ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶
            finalResults = self._resultMerger.mergeWithExistingTags(
                mergedResults, self._existingTagsCache
            )
            
            # 6. å†™å…¥æ•°æ®åº“
            writeSuccess = self._dataWriter.writeTaggedUsers(finalResults)
            
            if writeSuccess:
                self.logger.info("âœ… æŒ‡å®šç”¨æˆ·æ ‡ç­¾è®¡ç®—å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
                return True
            else:
                self.logger.error("âŒ æ•°æ®å†™å…¥å¤±è´¥")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ æŒ‡å®šç”¨æˆ·æ ‡ç­¾è®¡ç®—å·¥ä½œæµå¤±è´¥: {str(e)}")
            return False
    
    def _precacheExistingTags(self):
        """é¢„ç¼“å­˜ç°æœ‰æ ‡ç­¾"""
        try:
            if self._existingTagsCache is None:
                self.logger.info("ğŸ“š é¢„ç¼“å­˜ç°æœ‰æ ‡ç­¾...")
                self._existingTagsCache = self._ruleReader.loadExistingUserTags()
                
                if self._existingTagsCache is not None:
                    # ç¼“å­˜åˆ°å†…å­˜å’Œç£ç›˜
                    self._existingTagsCache = self._existingTagsCache.persist(StorageLevel.MEMORY_AND_DISK)
                    cacheCount = self._existingTagsCache.count()
                    self.logger.info(f"âœ… ç°æœ‰æ ‡ç­¾é¢„ç¼“å­˜å®Œæˆ: {cacheCount} ä¸ªç”¨æˆ·")
                else:
                    self.logger.info("â„¹ï¸ æ²¡æœ‰ç°æœ‰æ ‡ç­¾æ•°æ®ï¼ˆå¯èƒ½æ˜¯é¦–æ¬¡è¿è¡Œï¼‰")
                    
        except Exception as e:
            self.logger.warning(f"âš ï¸ é¢„ç¼“å­˜ç°æœ‰æ ‡ç­¾å¤±è´¥: {str(e)}")
            self._existingTagsCache = None
    
    def _getAllActiveTagIds(self) -> List[int]:
        """è·å–æ‰€æœ‰æ´»è·ƒæ ‡ç­¾ID"""
        try:
            rulesDataFrame = self._ruleReader.getActiveRulesDataFrame()
            tagIds = [row['tag_id'] for row in rulesDataFrame.select("tag_id").distinct().collect()]
            
            self.logger.info(f"ğŸ“‹ è·å–åˆ° {len(tagIds)} ä¸ªæ´»è·ƒæ ‡ç­¾: {tagIds}")
            return tagIds
            
        except Exception as e:
            self.logger.error(f"âŒ è·å–æ´»è·ƒæ ‡ç­¾IDå¤±è´¥: {str(e)}")
            return []
    
    def _validateTagIds(self, tagIds: List[int]) -> List[int]:
        """éªŒè¯æ ‡ç­¾IDæœ‰æ•ˆæ€§"""
        try:
            allActiveTagIds = self._getAllActiveTagIds()
            validTagIds = [tagId for tagId in tagIds if tagId in allActiveTagIds]
            
            invalidTagIds = set(tagIds) - set(validTagIds)
            if invalidTagIds:
                self.logger.warning(f"âš ï¸ æ— æ•ˆçš„æ ‡ç­¾ID: {list(invalidTagIds)}")
            
            self.logger.info(f"âœ… æœ‰æ•ˆæ ‡ç­¾ID: {validTagIds}")
            return validTagIds
            
        except Exception as e:
            self.logger.error(f"âŒ éªŒè¯æ ‡ç­¾IDå¤±è´¥: {str(e)}")
            return []
    
    def performHealthCheck(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥
        
        Returns:
            Dict[str, Any]: å¥åº·æ£€æŸ¥ç»“æœ
        """
        try:
            self.logger.info("ğŸ” æ‰§è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥...")
            
            healthStatus = {
                'overall_status': 'healthy',
                'components': {},
                'statistics': {}
            }
            
            # æ£€æŸ¥å„ä¸ªç»„ä»¶
            healthStatus['components']['spark'] = self.spark is not None
            
            # åªæ£€æŸ¥ä¸€æ¬¡MySQLè¿æ¥ï¼Œé¿å…é‡å¤æ—¥å¿—
            mysql_connection_ok = self._dataWriter.testConnection()
            healthStatus['components']['mysql_connection'] = mysql_connection_ok
            
            # å¦‚æœMySQLè¿æ¥å¤±è´¥ï¼Œè·³è¿‡è§„åˆ™è¯»å–å™¨éªŒè¯
            if mysql_connection_ok:
                healthStatus['components']['rule_reader'] = True  # å¤ç”¨MySQLè¿æ¥çŠ¶æ€
            else:
                healthStatus['components']['rule_reader'] = False
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            healthStatus['statistics'] = self._ruleReader.getStatistics()
            
            # åˆ¤æ–­æ•´ä½“çŠ¶æ€
            if not all(healthStatus['components'].values()):
                healthStatus['overall_status'] = 'unhealthy'
            
            self.logger.info(f"ğŸ“Š å¥åº·æ£€æŸ¥å®Œæˆ: {healthStatus['overall_status']}")
            return healthStatus
            
        except Exception as e:
            self.logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            return {
                'overall_status': 'error',
                'error': str(e)
            }
    
    def get_available_tasks(self) -> Dict[int, str]:
        """
        è·å–æ‰€æœ‰å¯ç”¨çš„æ ‡ç­¾ä»»åŠ¡
        
        Returns:
            Dict[int, str]: {tag_id: task_class_name}
        """
        try:
            self.logger.info("ğŸ“‹ è·å–å¯ç”¨æ ‡ç­¾ä»»åŠ¡...")
            
            # ç¡¬ç¼–ç å·²æœ‰çš„ä»»åŠ¡æ˜ å°„å…³ç³»ï¼ˆä¿®æ­£åçš„æ˜ å°„ï¼‰
            task_mapping = {
                1: "HighNetWorthUserTask",     # é«˜å‡€å€¼ç”¨æˆ·
                2: "ActiveTraderTask",         # æ´»è·ƒäº¤æ˜“è€…
                3: "LowRiskUserTask",          # ä½é£é™©ç”¨æˆ·
                4: "NewUserTask",              # æ–°æ³¨å†Œç”¨æˆ·
                5: "VIPUserTask",              # VIPå®¢æˆ·
                6: "CashRichUserTask",         # ç°é‡‘å……è¶³ç”¨æˆ·
                7: "YoungUserTask",            # å¹´è½»ç”¨æˆ·
                8: "RecentActiveTask"          # æœ€è¿‘æ´»è·ƒç”¨æˆ·
            }
            
            # å°è¯•ä»æ•°æ®åº“è·å–æ´»è·ƒæ ‡ç­¾ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›æ‰€æœ‰ä»»åŠ¡
            try:
                if not hasattr(self, '_ruleReader') or self._ruleReader is None:
                    from src.batch.utils.RuleReader import RuleReader
                    self._ruleReader = RuleReader(self.spark, self.systemConfig.mysql)
                
                # è·å–æ‰€æœ‰æ´»è·ƒæ ‡ç­¾
                rulesDataFrame = self._ruleReader.getActiveRulesDataFrame()
                available_tasks = {}
                
                if rulesDataFrame:
                    active_tag_ids = [row['tag_id'] for row in rulesDataFrame.select("tag_id").distinct().orderBy("tag_id").collect()]
                    
                    # åªè¿”å›åœ¨æ•°æ®åº“ä¸­å­˜åœ¨ä¸”æœ‰å¯¹åº”ä»»åŠ¡ç±»çš„æ ‡ç­¾
                    for tag_id in active_tag_ids:
                        if tag_id in task_mapping:
                            available_tasks[tag_id] = task_mapping[tag_id]
                else:
                    # æ•°æ®åº“ä¸­æ²¡æœ‰æ•°æ®ï¼Œè¿”å›æ‰€æœ‰ä»»åŠ¡
                    available_tasks = task_mapping
                    
            except Exception as db_error:
                self.logger.warning(f"âš ï¸ æ— æ³•ä»æ•°æ®åº“è·å–æ ‡ç­¾ä¿¡æ¯ï¼Œè¿”å›æ‰€æœ‰å¯ç”¨ä»»åŠ¡: {str(db_error)}")
                available_tasks = task_mapping
            
            # æ˜¾ç¤ºä»»åŠ¡æ˜ å°„å…³ç³»ï¼šID:TaskClassæ ¼å¼
            task_mappings = [f"{task_id}:{task_class}" for task_id, task_class in available_tasks.items()]
            self.logger.info(f"âœ… æ‰¾åˆ° {len(available_tasks)} ä¸ªå¯ç”¨ä»»åŠ¡: {task_mappings}")
            return available_tasks
            
        except Exception as e:
            self.logger.error(f"âŒ è·å–å¯ç”¨ä»»åŠ¡å¤±è´¥: {str(e)}")
            return {}
    
    def get_task_summary(self, available_tasks: Optional[Dict[int, str]] = None) -> Dict[int, Dict[str, Any]]:
        """
        è·å–ä»»åŠ¡è¯¦ç»†æ‘˜è¦ä¿¡æ¯ï¼Œä½¿ç”¨getRuleByTagIdæ–¹æ³•ä»MySQLè¯»å–çœŸå®æ ‡ç­¾è§„åˆ™
        
        Args:
            available_tasks: å¯ç”¨ä»»åŠ¡å­—å…¸ï¼Œå¦‚æœä¸æä¾›åˆ™å†…éƒ¨è·å–ï¼ˆé¿å…é‡å¤è°ƒç”¨ï¼‰
            
        Returns:
            Dict[int, Dict[str, Any]]: {tag_id: {tag_name, tag_category, rule_conditions, status, ...}}
        """
        try:
            self.logger.info("ğŸ“Š è·å–ä»»åŠ¡æ‘˜è¦ä¿¡æ¯...")
            
            # å¦‚æœæ²¡æœ‰æä¾›available_tasksï¼Œåˆ™è·å–ï¼ˆä½†ä¸è®°å½•æ—¥å¿—é¿å…é‡å¤ï¼‰
            if available_tasks is None:
                available_tasks = self._get_available_tasks_silent()
            
            task_summaries = {}
            
            try:
                # ç¡®ä¿RuleReaderå·²åˆå§‹åŒ–
                if not hasattr(self, '_ruleReader') or self._ruleReader is None:
                    from src.batch.utils.RuleReader import RuleReader
                    self._ruleReader = RuleReader(self.spark, self.systemConfig.mysql)
                
                self.logger.info("ğŸ” ä½¿ç”¨getRuleByTagIdä»MySQLè¯»å–å„æ ‡ç­¾è§„åˆ™...")
                
                # ä¸ºæ¯ä¸ªå¯ç”¨ä»»åŠ¡è·å–å¯¹åº”çš„æ ‡ç­¾è§„åˆ™
                for tag_id, task_class in available_tasks.items():
                    rule_dict = self._ruleReader.getRuleByTagId(tag_id)
                    
                    if rule_dict:
                        # ä½¿ç”¨getRuleByTagIdè¿”å›çš„å®Œæ•´è§„åˆ™ä¿¡æ¯
                        summary = {
                            'tag_name': rule_dict.get('tag_name', f'æ ‡ç­¾{tag_id}'),
                            'tag_category': rule_dict.get('tag_category', 'Unknown'),
                            'description': rule_dict.get('tag_description', f'æ ‡ç­¾{tag_id}æè¿°'),
                            'rule_conditions': rule_dict.get('rule_conditions', '{}'),
                            'status': 'active' if rule_dict.get('is_active', 1) == 1 else 'inactive',
                            'data_source': 'mysql',
                            'task_class': task_class,
                            # ä»è§„åˆ™æ¡ä»¶ä¸­è§£ææ‰€éœ€å­—æ®µ
                            'required_fields': self._extractRequiredFields(rule_dict.get('rule_conditions', '{}')),
                            'data_sources': self._inferDataSources(rule_dict.get('rule_conditions', '{}'))
                        }
                        task_summaries[tag_id] = summary
                    else:
                        self.logger.warning(f"âš ï¸ æ ‡ç­¾ {tag_id} åœ¨MySQLä¸­æ²¡æœ‰æ‰¾åˆ°å¯¹åº”è§„åˆ™")
                
                if task_summaries:
                    self.logger.info(f"âœ… æˆåŠŸä»MySQLè¯»å– {len(task_summaries)} ä¸ªæ ‡ç­¾çš„è§„åˆ™")
                else:
                    self.logger.warning("âš ï¸ MySQLä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ ‡ç­¾è§„åˆ™æ•°æ®")
                    
            except Exception as db_error:
                self.logger.error(f"âŒ æ— æ³•ä»MySQLè¯»å–æ ‡ç­¾è§„åˆ™: {str(db_error)}")
                # å®Œå…¨å¤±è´¥æ—¶è¿”å›ç©ºå­—å…¸ï¼Œä¸æä¾›é™çº§æ–¹æ¡ˆ
                return {}
            
            return task_summaries
            
        except Exception as e:
            self.logger.error(f"âŒ è·å–ä»»åŠ¡æ‘˜è¦å¤±è´¥: {str(e)}")
            return {}
    
    def _get_available_tasks_silent(self) -> Dict[int, str]:
        """
        é™é»˜è·å–å¯ç”¨ä»»åŠ¡ï¼Œä¸è®°å½•æ—¥å¿—ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼Œé¿å…é‡å¤æ—¥å¿—ï¼‰
        """
        try:
            # ç¡¬ç¼–ç å·²æœ‰çš„ä»»åŠ¡æ˜ å°„å…³ç³»ï¼ˆä¿®æ­£åçš„æ˜ å°„ï¼‰
            task_mapping = {
                1: "HighNetWorthUserTask",     # é«˜å‡€å€¼ç”¨æˆ·
                2: "ActiveTraderTask",         # æ´»è·ƒäº¤æ˜“è€…
                3: "LowRiskUserTask",          # ä½é£é™©ç”¨æˆ·
                4: "NewUserTask",              # æ–°æ³¨å†Œç”¨æˆ·
                5: "VIPUserTask",              # VIPå®¢æˆ·
                6: "CashRichUserTask",         # ç°é‡‘å……è¶³ç”¨æˆ·
                7: "YoungUserTask",            # å¹´è½»ç”¨æˆ·
                8: "RecentActiveTask"          # æœ€è¿‘æ´»è·ƒç”¨æˆ·
            }
            
            # å°è¯•ä»æ•°æ®åº“è·å–æ´»è·ƒæ ‡ç­¾ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›æ‰€æœ‰ä»»åŠ¡
            try:
                if not hasattr(self, '_ruleReader') or self._ruleReader is None:
                    from src.batch.utils.RuleReader import RuleReader
                    self._ruleReader = RuleReader(self.spark, self.systemConfig.mysql)
                
                # è·å–æ‰€æœ‰æ´»è·ƒæ ‡ç­¾
                rulesDataFrame = self._ruleReader.getActiveRulesDataFrame()
                available_tasks = {}
                
                if rulesDataFrame:
                    active_tag_ids = [row['tag_id'] for row in rulesDataFrame.select("tag_id").distinct().orderBy("tag_id").collect()]
                    
                    # åªè¿”å›åœ¨æ•°æ®åº“ä¸­å­˜åœ¨ä¸”æœ‰å¯¹åº”ä»»åŠ¡ç±»çš„æ ‡ç­¾
                    for tag_id in active_tag_ids:
                        if tag_id in task_mapping:
                            available_tasks[tag_id] = task_mapping[tag_id]
                else:
                    # æ•°æ®åº“ä¸­æ²¡æœ‰æ•°æ®ï¼Œè¿”å›æ‰€æœ‰ä»»åŠ¡
                    available_tasks = task_mapping
                    
            except Exception:
                available_tasks = task_mapping
            
            return available_tasks
            
        except Exception:
            return {}
    
    def _extractRequiredFields(self, rule_conditions: str) -> List[str]:
        """ä»è§„åˆ™æ¡ä»¶JSONä¸­æå–æ‰€éœ€å­—æ®µ"""
        try:
            import json
            conditions = json.loads(rule_conditions)
            fields = []
            
            # é€’å½’æå–æ¡ä»¶ä¸­çš„å­—æ®µ
            def extract_fields(obj):
                if isinstance(obj, dict):
                    if 'field' in obj:
                        fields.append(obj['field'])
                    if 'conditions' in obj:
                        for condition in obj['conditions']:
                            extract_fields(condition)
                elif isinstance(obj, list):
                    for item in obj:
                        extract_fields(item)
            
            extract_fields(conditions)
            # å§‹ç»ˆåŒ…å«user_id
            if 'user_id' not in fields:
                fields.insert(0, 'user_id')
            
            return fields
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ è§£æè§„åˆ™æ¡ä»¶å¤±è´¥: {str(e)}")
            return ['user_id']
    
    def _inferDataSources(self, rule_conditions: str) -> List[str]:
        """æ ¹æ®è§„åˆ™æ¡ä»¶æ¨æ–­æ•°æ®æº"""
        # åŸºäºå­—æ®µåç§°çš„ç®€å•æ¨æ–­è§„åˆ™
        field_to_source_mapping = {
            'total_asset_value': 'user_asset_summary',
            'cash_balance': 'user_asset_summary', 
            'trade_count_30d': 'user_activity_summary',
            'last_login_date': 'user_activity_summary',
            'age': 'user_basic_info',
            'registration_date': 'user_basic_info',
            'risk_score': 'user_basic_info',
            'user_level': 'user_basic_info',
            'kyc_status': 'user_basic_info'
        }
        
        # ä»è§„åˆ™æ¡ä»¶ä¸­è·å–å­—æ®µï¼Œç„¶åæ˜ å°„åˆ°æ•°æ®æº
        try:
            fields = self._extractRequiredFields(rule_conditions)
            sources = []
            for field in fields:
                if field in field_to_source_mapping:
                    source = field_to_source_mapping[field]
                    if source not in sources:
                        sources.append(source)
            return sources if sources else ['user_basic_info']
        except Exception:
            pass
        
        return ['user_basic_info']
    
    def cleanup(self):
        """æ¸…ç†ç³»ç»Ÿèµ„æº"""
        try:
            self.logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†ç³»ç»Ÿèµ„æº...")
            
            # æ¸…ç†ç¼“å­˜
            if self._existingTagsCache is not None:
                self._existingTagsCache.unpersist()
                self._existingTagsCache = None
            
            if hasattr(self, '_ruleReader') and self._ruleReader:
                self._ruleReader.clearCache()
            
            # åœæ­¢Sparkä¼šè¯
            if self.spark:
                self.spark.stop()
                self.spark = None
            
            self.logger.info("âœ… ç³»ç»Ÿèµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ç³»ç»Ÿèµ„æºæ¸…ç†å¼‚å¸¸: {str(e)}")
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.initializeSystem()
        return self
    
    def __exit__(self, excType, excVal, excTb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        self.cleanup()
        
        if excType is not None:
            self.logger.error(f"âŒ ç³»ç»Ÿæ‰§è¡Œå¼‚å¸¸: {excType.__name__}: {excVal}")
        
        return False  # ä¸æŠ‘åˆ¶å¼‚å¸¸