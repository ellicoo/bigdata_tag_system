"""
è§„åˆ™æ•°æ®è¯»å–å™¨ - é‡æ„ä¸ºé©¼å³°å‘½åé£æ ¼
ä»MySQLè¯»å–æ ‡ç­¾è§„åˆ™å’Œå®šä¹‰ï¼Œæ”¯æŒDataFrameçº§åˆ«çš„ç¼“å­˜
"""

import logging
from typing import List, Dict, Any, Optional
from pyspark.sql import SparkSession, DataFrame
from pyspark import StorageLevel
from pyspark.sql.functions import col, from_json
from pyspark.sql.types import ArrayType, IntegerType

from src.batch.config.BaseConfig import MySQLConfig

logger = logging.getLogger(__name__)


class RuleReader:
    """è§„åˆ™æ•°æ®è¯»å–å™¨ï¼ˆåŸRuleDataLoaderåŠŸèƒ½ï¼‰"""
    
    def __init__(self, spark: SparkSession, mysqlConfig: MySQLConfig):
        self.spark = spark
        self.mysqlConfig = mysqlConfig
        self._rulesCache = None
        self._existingTagsCache = None
    
    def getActiveRulesDataFrame(self) -> DataFrame:
        """
        è·å–æ´»è·ƒè§„åˆ™çš„DataFrame
        
        Returns:
            DataFrame: åŒ…å«æ‰€æœ‰æ´»è·ƒè§„åˆ™çš„DataFrame
        """
        if self._rulesCache is not None:
            return self._rulesCache
        
        try:
            logger.info("ğŸ“– ä»MySQLè¯»å–æ ‡ç­¾è§„åˆ™...")
            
            # æ„å»ºMySQLè¿æ¥å±æ€§
            mysqlProps = {
                "user": self.mysqlConfig.username,
                "password": self.mysqlConfig.password,
                "driver": "com.mysql.cj.jdbc.Driver",
                "useSSL": "false",
                "allowPublicKeyRetrieval": "true",
                "connectionCollation": "utf8mb4_unicode_ci"
            }
            
            # ä½¿ç”¨JOINæŸ¥è¯¢è·å–è§„åˆ™å’Œå®šä¹‰
            query = """
                (SELECT 
                    tr.rule_id,
                    tr.tag_id,
                    tr.rule_conditions,
                    tr.is_active,
                    td.tag_name,
                    td.description as tag_description,
                    td.tag_category
                FROM tag_rules tr
                JOIN tag_definition td ON tr.tag_id = td.tag_id
                WHERE tr.is_active = 1 AND td.is_active = 1
                ORDER BY tr.tag_id
                ) as rules_query
            """
            
            df = self.spark.read.jdbc(
                url=self.mysqlConfig.jdbcUrl,
                table=query,
                properties=mysqlProps
            )
            
            # ä¸éœ€è¦è§£æJSONï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²
            
            # ç¼“å­˜åˆ°å†…å­˜å’Œç£ç›˜
            df = df.persist(StorageLevel.MEMORY_AND_DISK)
            
            recordCount = df.count()
            logger.info(f"âœ… æˆåŠŸè¯»å– {recordCount} æ¡æ´»è·ƒè§„åˆ™å¹¶ç¼“å­˜åˆ°å†…å­˜")
            
            self._rulesCache = df
            return df
            
        except Exception as e:
            logger.error(f"âŒ è¯»å–è§„åˆ™æ•°æ®å¤±è´¥: {str(e)}")
            raise
    
    def getRuleByTagId(self, tagId: int) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®æ ‡ç­¾IDè·å–è§„åˆ™
        
        Args:
            tagId: æ ‡ç­¾ID
            
        Returns:
            Dict[str, Any]: è§„åˆ™å­—å…¸ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            rulesDataFrame = self.getActiveRulesDataFrame()
            ruleRow = rulesDataFrame.filter(col("tag_id") == tagId).first()
            
            if ruleRow is None:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ ‡ç­¾ID {tagId} çš„è§„åˆ™")
                return None
            
            # è½¬æ¢ä¸ºå­—å…¸
            ruleDict = ruleRow.asDict()
            return ruleDict
            
        except Exception as e:
            logger.error(f"âŒ è·å–æ ‡ç­¾ {tagId} è§„åˆ™å¤±è´¥: {str(e)}")
            return None
    
    def loadExistingUserTags(self) -> Optional[DataFrame]:
        """
        åŠ è½½ç°æœ‰ç”¨æˆ·æ ‡ç­¾æ•°æ®
        
        Returns:
            DataFrame: ç°æœ‰æ ‡ç­¾æ•°æ®ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        if self._existingTagsCache is not None:
            logger.info("âœ… ä½¿ç”¨ç¼“å­˜çš„ç°æœ‰æ ‡ç­¾æ•°æ®")
            return self._existingTagsCache
        
        try:
            logger.info("ğŸ“– ä»MySQLè¯»å–ç°æœ‰ç”¨æˆ·æ ‡ç­¾...")
            
            mysqlProps = {
                "user": self.mysqlConfig.username,
                "password": self.mysqlConfig.password,
                "driver": "com.mysql.cj.jdbc.Driver",
                "useSSL": "false",
                "allowPublicKeyRetrieval": "true",
                "connectionCollation": "utf8mb4_unicode_ci"
            }
            
            # è¯»å–user_tagsè¡¨
            query = """
                (SELECT user_id, tag_ids, tag_details, updated_time
                FROM user_tags
                WHERE tag_ids IS NOT NULL AND tag_ids != '[]'
                ) as existing_tags_query
            """
            
            df = self.spark.read.jdbc(
                url=self.mysqlConfig.jdbcUrl,
                table=query,
                properties=mysqlProps
            )
            
            # JSONè½¬æ¢ï¼štag_idsä»å­—ç¬¦ä¸²è½¬ä¸ºIntegerArray
            arraySchema = ArrayType(IntegerType(), True)
            df = df.withColumn("tag_ids_array", from_json(col("tag_ids"), arraySchema))
            
            # ç¼“å­˜æ•°æ®
            df = df.persist(StorageLevel.MEMORY_AND_DISK)
            
            recordCount = df.count()
            logger.info(f"âœ… æˆåŠŸè¯»å– {recordCount} æ¡ç°æœ‰æ ‡ç­¾è®°å½•")
            
            self._existingTagsCache = df
            return df
            
        except Exception as e:
            logger.warning(f"âš ï¸ è¯»å–ç°æœ‰æ ‡ç­¾æ•°æ®å¤±è´¥ï¼ˆå¯èƒ½æ˜¯é¦–æ¬¡è¿è¡Œï¼‰: {str(e)}")
            return None
    
    def getTagDefinitions(self) -> DataFrame:
        """
        è·å–æ‰€æœ‰æ ‡ç­¾å®šä¹‰
        
        Returns:
            DataFrame: æ ‡ç­¾å®šä¹‰æ•°æ®
        """
        try:
            logger.info("ğŸ“– ä»MySQLè¯»å–æ ‡ç­¾å®šä¹‰...")
            
            mysqlProps = {
                "user": self.mysqlConfig.username,
                "password": self.mysqlConfig.password,
                "driver": "com.mysql.cj.jdbc.Driver",
                "useSSL": "false",
                "allowPublicKeyRetrieval": "true",
                "connectionCollation": "utf8mb4_unicode_ci"
            }
            
            df = self.spark.read.jdbc(
                url=self.mysqlConfig.jdbcUrl,
                table="tag_definition",
                properties=mysqlProps
            )
            
            activeDefinitions = df.filter(col("is_active") == 1)
            recordCount = activeDefinitions.count()
            logger.info(f"âœ… æˆåŠŸè¯»å– {recordCount} æ¡æ´»è·ƒæ ‡ç­¾å®šä¹‰")
            
            return activeDefinitions
            
        except Exception as e:
            logger.error(f"âŒ è¯»å–æ ‡ç­¾å®šä¹‰å¤±è´¥: {str(e)}")
            raise
    
    def getTagCategories(self) -> DataFrame:
        """
        è·å–æ‰€æœ‰æ ‡ç­¾åˆ†ç±»
        
        Returns:
            DataFrame: æ ‡ç­¾åˆ†ç±»æ•°æ®
        """
        try:
            logger.info("ğŸ“– ä»MySQLè¯»å–æ ‡ç­¾åˆ†ç±»...")
            
            mysqlProps = {
                "user": self.mysqlConfig.username,
                "password": self.mysqlConfig.password,
                "driver": "com.mysql.cj.jdbc.Driver",
                "useSSL": "false",
                "allowPublicKeyRetrieval": "true",
                "connectionCollation": "utf8mb4_unicode_ci"
            }
            
            df = self.spark.read.jdbc(
                url=self.mysqlConfig.jdbcUrl,
                table="tag_category",
                properties=mysqlProps
            )
            
            activeCategories = df.filter(col("is_active") == 1)
            recordCount = activeCategories.count()
            logger.info(f"âœ… æˆåŠŸè¯»å– {recordCount} æ¡æ´»è·ƒæ ‡ç­¾åˆ†ç±»")
            
            return activeCategories
            
        except Exception as e:
            logger.error(f"âŒ è¯»å–æ ‡ç­¾åˆ†ç±»å¤±è´¥: {str(e)}")
            raise
    
    def validateConnection(self) -> bool:
        """
        éªŒè¯MySQLè¿æ¥æ˜¯å¦æ­£å¸¸
        
        Returns:
            bool: è¿æ¥æ˜¯å¦æ­£å¸¸
        """
        try:
            logger.info("ğŸ” éªŒè¯MySQLè¿æ¥...")
            
            mysqlProps = {
                "user": self.mysqlConfig.username,
                "password": self.mysqlConfig.password,
                "driver": "com.mysql.cj.jdbc.Driver"
            }
            
            # æ‰§è¡Œç®€å•æŸ¥è¯¢æµ‹è¯•è¿æ¥
            testQuery = "(SELECT 1 as test_connection) as connection_test"
            testDataFrame = self.spark.read.jdbc(
                url=self.mysqlConfig.jdbcUrl,
                table=testQuery,
                properties=mysqlProps
            )
            
            testDataFrame.count()  # è§¦å‘å®é™…æ‰§è¡Œ
            logger.info("âœ… MySQLè¿æ¥éªŒè¯æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ MySQLè¿æ¥éªŒè¯å¤±è´¥: {str(e)}")
            return False
    
    def clearCache(self):
        """æ¸…ç†ç¼“å­˜æ•°æ®"""
        try:
            if self._rulesCache is not None:
                self._rulesCache.unpersist()
                self._rulesCache = None
            
            if self._existingTagsCache is not None:
                self._existingTagsCache.unpersist()
                self._existingTagsCache = None
            
            logger.info("ğŸ§¹ è§„åˆ™è¯»å–å™¨ç¼“å­˜æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç¼“å­˜æ¸…ç†å¼‚å¸¸: {str(e)}")
    
    def getStatistics(self) -> Dict[str, int]:
        """
        è·å–è§„åˆ™å’Œæ ‡ç­¾çš„ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict[str, int]: ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            rulesDataFrame = self.getActiveRulesDataFrame()
            definitionsDataFrame = self.getTagDefinitions()
            categoriesDataFrame = self.getTagCategories()
            existingTagsDataFrame = self.loadExistingUserTags()
            
            stats = {
                'active_rules': rulesDataFrame.count(),
                'tag_definitions': definitionsDataFrame.count(),
                'tag_categories': categoriesDataFrame.count(),
                'users_with_tags': existingTagsDataFrame.count() if existingTagsDataFrame else 0
            }
            
            logger.info(f"ğŸ“Š è§„åˆ™ç»Ÿè®¡: {stats}")
            return stats
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}