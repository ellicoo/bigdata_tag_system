"""
Hiveæ•°æ®è¯»å–å™¨ - é‡æ„ä¸ºé©¼å³°å‘½åé£æ ¼
ä»S3è¯»å–Hiveè¡¨æ•°æ®ï¼Œæ”¯æŒåˆ—é€‰æ‹©ä¼˜åŒ–å’Œæ‰¹é‡è¯»å–
"""

import logging
from typing import List, Dict, Optional
from pyspark.sql import SparkSession, DataFrame

from src.batch.config.BaseConfig import S3Config
from src.batch.bean.HiveTableMeta import HiveTableMeta

logger = logging.getLogger(__name__)


class HiveDataReader:
    """Hiveæ•°æ®è¯»å–å™¨ï¼ˆåŸHiveDataLoaderåŠŸèƒ½ï¼‰"""
    
    def __init__(self, spark: SparkSession, s3Config: S3Config):
        self.spark = spark
        self.s3Config = s3Config
        
    def readHiveTable(self, tablePath: str, selectedColumns: List[str] = None) -> Optional[DataFrame]:
        """
        ä»S3è¯»å–Hiveè¡¨æ•°æ®
        
        Args:
            tablePath: è¡¨è·¯å¾„ï¼Œä¾‹å¦‚ 'user_basic_info/'
            selectedColumns: éœ€è¦é€‰æ‹©çš„åˆ—ï¼Œä¸ºNoneæ—¶é€‰æ‹©æ‰€æœ‰åˆ—
            
        Returns:
            DataFrameæˆ–None
        """
        try:
            logger.info(f"ğŸ“– ä»S3è¯»å–Hiveè¡¨: {tablePath}")
            
            # æ„å»ºå®Œæ•´çš„S3è·¯å¾„
            fullPath = f"s3a://{self.s3Config.bucket}/warehouse/{tablePath.rstrip('/')}"
            logger.info(f"ğŸ“ S3è·¯å¾„: {fullPath}")
            
            # è¯»å–Parquetæ ¼å¼æ•°æ®
            df = self.spark.read.parquet(fullPath)
            
            # åˆ—é€‰æ‹©ä¼˜åŒ–
            if selectedColumns:
                # éªŒè¯åˆ—æ˜¯å¦å­˜åœ¨
                availableColumns = df.columns
                validColumns = [col for col in selectedColumns if col in availableColumns]
                
                if len(validColumns) != len(selectedColumns):
                    missingColumns = set(selectedColumns) - set(validColumns)
                    logger.warning(f"âš ï¸ ç¼ºå¤±åˆ—: {missingColumns}")
                
                if validColumns:
                    df = df.select(*validColumns)
                    logger.info(f"ğŸ“Š å·²é€‰æ‹©åˆ—: {validColumns}")
                else:
                    logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„åˆ—å¯é€‰æ‹©")
                    return None
            
            recordCount = df.count()
            logger.info(f"âœ… æˆåŠŸè¯»å– {tablePath}ï¼Œè®°å½•æ•°: {recordCount}")
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ è¯»å–Hiveè¡¨å¤±è´¥ {tablePath}: {str(e)}")
            return None
    
    def readMultipleTables(self, tableConfigs: Dict[str, List[str]]) -> Dict[str, DataFrame]:
        """
        æ‰¹é‡è¯»å–å¤šä¸ªè¡¨
        
        Args:
            tableConfigs: {è¡¨å: [åˆ—ååˆ—è¡¨]} çš„å­—å…¸
            
        Returns:
            {è¡¨å: DataFrame} çš„å­—å…¸
        """
        result = {}
        
        for tableName, columns in tableConfigs.items():
            logger.info(f"ğŸ“Š æ‰¹é‡è¯»å–è¡¨: {tableName}")
            df = self.readHiveTable(tableName, columns)
            if df is not None:
                result[tableName] = df
            else:
                logger.warning(f"âš ï¸ è¡¨ {tableName} è¯»å–å¤±è´¥ï¼Œè·³è¿‡")
        
        logger.info(f"âœ… æ‰¹é‡è¯»å–å®Œæˆï¼ŒæˆåŠŸè¯»å– {len(result)}/{len(tableConfigs)} ä¸ªè¡¨")
        return result
    
    def validateTableExists(self, tablePath: str) -> bool:
        """
        éªŒè¯è¡¨æ˜¯å¦å­˜åœ¨
        
        Args:
            tablePath: è¡¨è·¯å¾„
            
        Returns:
            bool: è¡¨æ˜¯å¦å­˜åœ¨
        """
        try:
            fullPath = f"s3a://{self.s3Config.bucket}/warehouse/{tablePath.rstrip('/')}"
            df = self.spark.read.parquet(fullPath)
            df.count()  # è§¦å‘è¯»å–æ“ä½œ
            return True
        except Exception as e:
            logger.debug(f"è¡¨ {tablePath} ä¸å­˜åœ¨æˆ–ä¸å¯è¯»: {str(e)}")
            return False
    
    def getTableSchema(self, tablePath: str) -> Optional[List[str]]:
        """
        è·å–è¡¨çš„åˆ—ååˆ—è¡¨
        
        Args:
            tablePath: è¡¨è·¯å¾„
            
        Returns:
            List[str]: åˆ—ååˆ—è¡¨ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            fullPath = f"s3a://{self.s3Config.bucket}/warehouse/{tablePath.rstrip('/')}"
            df = self.spark.read.parquet(fullPath)
            return df.columns
        except Exception as e:
            logger.error(f"âŒ è·å–è¡¨ç»“æ„å¤±è´¥ {tablePath}: {str(e)}")
            return None
    
    def getTableStats(self, tablePath: str) -> Optional[Dict[str, int]]:
        """
        è·å–è¡¨çš„ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            tablePath: è¡¨è·¯å¾„
            
        Returns:
            Dict[str, int]: ç»Ÿè®¡ä¿¡æ¯ï¼ˆè®°å½•æ•°ã€åˆ—æ•°ï¼‰ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            df = self.readHiveTable(tablePath)
            if df is None:
                return None
            
            recordCount = df.count()
            columnCount = len(df.columns)
            
            stats = {
                'record_count': recordCount,
                'column_count': columnCount
            }
            
            logger.info(f"ğŸ“Š è¡¨ {tablePath} ç»Ÿè®¡: {stats}")
            return stats
            
        except Exception as e:
            logger.error(f"âŒ è·å–è¡¨ç»Ÿè®¡å¤±è´¥ {tablePath}: {str(e)}")
            return None
    
    def getTableMeta(self, tablePath: str) -> Optional[HiveTableMeta]:
        """
        è·å–è¡¨å…ƒæ•°æ®Beanå¯¹è±¡
        
        Args:
            tablePath: è¡¨è·¯å¾„
            
        Returns:
            HiveTableMeta: è¡¨å…ƒæ•°æ®Beanï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            logger.info(f"ğŸ“Š è·å–è¡¨å…ƒæ•°æ®: {tablePath}")
            
            # è·å–è¡¨çš„åŸºæœ¬ä¿¡æ¯
            columns = self.getTableSchema(tablePath)
            if columns is None:
                return None
            
            # åˆ›å»ºå…ƒæ•°æ®Bean
            tableMeta = HiveTableMeta.fromBasicInfo(
                tableName=tablePath.split('/')[-1],
                tablePath=f"s3a://{self.s3Config.bucket}/warehouse/{tablePath.rstrip('/')}",
                columns=columns
            )
            
            # è·å–è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
            try:
                df = self.readHiveTable(tablePath)
                if df is not None:
                    tableMeta.recordCount = df.count()
                    
                    # æ·»åŠ ä¸€äº›æ ·æœ¬æ•°æ®
                    sampleRows = df.limit(3).collect()
                    for row in sampleRows:
                        tableMeta.addSampleRow(row.asDict())
                        
            except Exception as e:
                logger.warning(f"âš ï¸ è·å–è¡¨ {tablePath} ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            
            logger.info(f"âœ… è¡¨å…ƒæ•°æ®è·å–æˆåŠŸ: {tableMeta}")
            return tableMeta
            
        except Exception as e:
            logger.error(f"âŒ è·å–è¡¨å…ƒæ•°æ®å¤±è´¥ {tablePath}: {str(e)}")
            return None
    
    def readHiveTableWithMeta(self, tablePath: str, selectedColumns: List[str] = None) -> tuple[Optional[DataFrame], Optional[HiveTableMeta]]:
        """
        ä» S3 è¯»å– Hive è¡¨æ•°æ®å¹¶è¿”å›å…ƒæ•°æ®
        
        Args:
            tablePath: è¡¨è·¯å¾„ï¼Œä¾‹å¦‚ 'user_basic_info/'
            selectedColumns: éœ€è¦é€‰æ‹©çš„åˆ—ï¼Œä¸ºNoneæ—¶é€‰æ‹©æ‰€æœ‰åˆ—
            
        Returns:
            tuple[DataFrame, HiveTableMeta]: DataFrameå’Œå…ƒæ•°æ®Bean
        """
        try:
            logger.info(f"ğŸ“š è¯»å–Hiveè¡¨å¹¶è·å–å…ƒæ•°æ®: {tablePath}")
            
            # è¯»å–DataFrame
            df = self.readHiveTable(tablePath, selectedColumns)
            
            # è·å–å…ƒæ•°æ®
            tableMeta = self.getTableMeta(tablePath)
            
            if df is not None and tableMeta is not None:
                # æ›´æ–°å…ƒæ•°æ®ä¸­çš„ç»Ÿè®¡ä¿¡æ¯
                if selectedColumns:
                    tableMeta.statistics['selected_columns'] = selectedColumns
                    tableMeta.statistics['selected_column_count'] = len(selectedColumns)
                
                logger.info(f"âœ… æˆåŠŸè¯»å–è¡¨å’Œå…ƒæ•°æ®: {tablePath}")
            
            return df, tableMeta
            
        except Exception as e:
            logger.error(f"âŒ è¯»å–è¡¨å’Œå…ƒæ•°æ®å¤±è´¥ {tablePath}: {str(e)}")
            return None, None