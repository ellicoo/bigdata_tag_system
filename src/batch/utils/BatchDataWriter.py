"""
æ‰¹å¤„ç†æ•°æ®å†™å…¥å™¨ - é‡æ„ä¸ºé©¼å³°å‘½åé£æ ¼
æ”¯æŒUPSERTæ“ä½œå’Œæ—¶é—´æˆ³æ§åˆ¶ï¼Œæä¾›æ•°æ®é¢„å¤„ç†ã€åˆ†åŒºä¼˜åŒ–å’Œå†™å…¥éªŒè¯
"""

import logging
import json
from typing import List, Dict, Any, Optional
from pyspark.sql import DataFrame
from pyspark.sql.functions import col, collect_list, array_distinct, size, when, isnan, isnull

from src.batch.config.BaseConfig import MySQLConfig

logger = logging.getLogger(__name__)


class BatchDataWriter:
    """æ‰¹å¤„ç†æ•°æ®å†™å…¥å™¨ï¼ˆåŸOptimizedMySQLWriteråŠŸèƒ½ï¼‰"""
    
    def __init__(self, mysqlConfig: MySQLConfig):
        self.mysqlConfig = mysqlConfig
        self.logger = logging.getLogger(__name__)
    
    def writeTaggedUsers(self, taggedUsersDataFrame: DataFrame, enableValidation: bool = True) -> bool:
        """
        å†™å…¥æ ‡ç­¾ç”¨æˆ·æ•°æ®åˆ°MySQL
        
        Args:
            taggedUsersDataFrame: æ ‡ç­¾ç”¨æˆ·DataFrameï¼Œå¿…é¡»åŒ…å«user_id, tag_idåˆ—
            enableValidation: æ˜¯å¦å¯ç”¨å†™å…¥éªŒè¯
            
        Returns:
            bool: å†™å…¥æ˜¯å¦æˆåŠŸ
        """
        try:
            self.logger.info("ğŸš€ å¼€å§‹å†™å…¥æ ‡ç­¾ç”¨æˆ·æ•°æ®...")
            
            # 1. æ•°æ®é¢„å¤„ç†å’ŒéªŒè¯
            processedDataFrame = self._preprocessTaggedData(taggedUsersDataFrame)
            if processedDataFrame is None or processedDataFrame.count() == 0:
                self.logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ•°æ®éœ€è¦å†™å…¥")
                return True
            
            # 2. åŠ¨æ€åˆ†åŒºä¼˜åŒ–
            optimizedDataFrame = self._optimizePartitions(processedDataFrame)
            
            # 3. æ‰§è¡ŒUPSERTå†™å…¥
            self._executeUpsertWrite(optimizedDataFrame)
            
            # 4. å†™å…¥éªŒè¯ï¼ˆå¯é€‰ï¼‰
            if enableValidation:
                validationResult = self._validateWriteResult(optimizedDataFrame)
                if not validationResult:
                    self.logger.error("âŒ å†™å…¥éªŒè¯å¤±è´¥")
                    return False
            
            self.logger.info("âœ… æ ‡ç­¾ç”¨æˆ·æ•°æ®å†™å…¥å®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ å†™å…¥æ ‡ç­¾ç”¨æˆ·æ•°æ®å¤±è´¥: {str(e)}")
            return False
    
    def _preprocessTaggedData(self, rawDataFrame: DataFrame) -> Optional[DataFrame]:
        """
        é¢„å¤„ç†æ ‡ç­¾æ•°æ®
        
        Args:
            rawDataFrame: åŸå§‹æ ‡ç­¾æ•°æ®
            
        Returns:
            DataFrame: é¢„å¤„ç†åçš„æ•°æ®
        """
        try:
            self.logger.info("ğŸ”„ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
            
            # éªŒè¯å¿…éœ€å­—æ®µ
            requiredFields = ['user_id', 'tag_id']
            missingFields = set(requiredFields) - set(rawDataFrame.columns)
            if missingFields:
                self.logger.error(f"âŒ ç¼ºå°‘å¿…éœ€å­—æ®µ: {missingFields}")
                return None
            
            # è¿‡æ»¤ç©ºå€¼å’Œé‡å¤æ•°æ®
            cleanedDataFrame = rawDataFrame.filter(
                col("user_id").isNotNull() & 
                col("tag_id").isNotNull() & 
                ~col("user_id").isin("", "null", "NULL")
            ).dropDuplicates(["user_id", "tag_id"])
            
            # æŒ‰ç”¨æˆ·èšåˆæ ‡ç­¾
            aggregatedDataFrame = cleanedDataFrame.groupBy("user_id").agg(
                array_distinct(collect_list("tag_id")).alias("tag_ids"),
                collect_list("tag_detail").alias("tag_details_list")
            )
            
            # æ„å»ºæœ€ç»ˆæ ¼å¼
            finalDataFrame = aggregatedDataFrame.select(
                col("user_id"),
                col("tag_ids"),
                when(size(col("tag_details_list")) > 0, 
                     col("tag_details_list")).otherwise(None).alias("tag_details")
            )
            
            originalCount = rawDataFrame.count()
            processedCount = finalDataFrame.count()
            
            self.logger.info(f"ğŸ“Š æ•°æ®é¢„å¤„ç†å®Œæˆ: {originalCount} -> {processedCount} ç”¨æˆ·")
            return finalDataFrame
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {str(e)}")
            return None
    
    def _optimizePartitions(self, dataFrame: DataFrame) -> DataFrame:
        """
        åŠ¨æ€åˆ†åŒºä¼˜åŒ–
        
        Args:
            dataFrame: è¾“å…¥æ•°æ®
            
        Returns:
            DataFrame: ä¼˜åŒ–åˆ†åŒºåçš„æ•°æ®
        """
        try:
            recordCount = dataFrame.count()
            
            # æ ¹æ®æ•°æ®é‡åŠ¨æ€è°ƒæ•´åˆ†åŒºæ•°
            if recordCount < 1000:
                partitionCount = 1
            elif recordCount < 10000:
                partitionCount = 2
            elif recordCount < 100000:
                partitionCount = 4
            else:
                partitionCount = 8
            
            optimizedDataFrame = dataFrame.repartition(partitionCount)
            self.logger.info(f"ğŸ”§ åˆ†åŒºä¼˜åŒ–: {recordCount} è®°å½•ä½¿ç”¨ {partitionCount} ä¸ªåˆ†åŒº")
            
            return optimizedDataFrame
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ åˆ†åŒºä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸæ•°æ®: {str(e)}")
            return dataFrame
    
    def _executeUpsertWrite(self, dataFrame: DataFrame):
        """
        æ‰§è¡ŒUPSERTå†™å…¥æ“ä½œ
        
        Args:
            dataFrame: è¦å†™å…¥çš„æ•°æ®
        """
        def upsertPartition(iterator):
            """åˆ†åŒºçº§åˆ«çš„UPSERTæ“ä½œ"""
            import pymysql
            
            connection = None
            try:
                # å»ºç«‹MySQLè¿æ¥
                connection = pymysql.connect(
                    host=self.mysqlConfig.host,
                    port=self.mysqlConfig.port,
                    user=self.mysqlConfig.username,
                    password=self.mysqlConfig.password,
                    database=self.mysqlConfig.database,
                    charset='utf8mb4',
                    autocommit=False
                )
                
                cursor = connection.cursor()
                batchSize = 100
                batch = []
                
                for row in iterator:
                    userId = row['user_id']
                    tagIds = json.dumps(row['tag_ids']) if row['tag_ids'] else '[]'
                    tagDetails = json.dumps(row['tag_details']) if row['tag_details'] else '{}'
                    
                    batch.append((userId, tagIds, tagDetails))
                    
                    if len(batch) >= batchSize:
                        self._executeBatch(cursor, batch)
                        batch = []
                
                # å¤„ç†å‰©ä½™æ•°æ®
                if batch:
                    self._executeBatch(cursor, batch)
                
                connection.commit()
                
            except Exception as e:
                if connection:
                    connection.rollback()
                logger.error(f"âŒ åˆ†åŒºUPSERTå¤±è´¥: {str(e)}")
                raise
                
            finally:
                if connection:
                    connection.close()
            
            return iter([])
        
        # æ‰§è¡Œåˆ†åŒºçº§åˆ«çš„UPSERT
        self.logger.info("ğŸ’¾ æ‰§è¡ŒUPSERTå†™å…¥æ“ä½œ...")
        dataFrame.foreachPartition(upsertPartition)
    
    def _executeBatch(self, cursor, batch: List[tuple]):
        """
        æ‰§è¡Œæ‰¹é‡UPSERT
        
        Args:
            cursor: æ•°æ®åº“æ¸¸æ ‡
            batch: æ‰¹é‡æ•°æ®
        """
        if not batch:
            return
        
        # ä¼˜åŒ–çš„UPSERT SQL - æ™ºèƒ½æ—¶é—´æˆ³æ§åˆ¶
        upsertSql = """
            INSERT INTO user_tags (user_id, tag_ids, tag_details, created_time, updated_time)
            VALUES (%s, %s, %s, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
            ON DUPLICATE KEY UPDATE
                updated_time = CASE 
                    WHEN JSON_EXTRACT(tag_ids, '$') <> JSON_EXTRACT(VALUES(tag_ids), '$')
                    THEN CURRENT_TIMESTAMP 
                    ELSE updated_time 
                END,
                tag_ids = VALUES(tag_ids),
                tag_details = VALUES(tag_details)
        """
        
        cursor.executemany(upsertSql, batch)
    
    def _validateWriteResult(self, writtenDataFrame: DataFrame) -> bool:
        """
        éªŒè¯å†™å…¥ç»“æœ
        
        Args:
            writtenDataFrame: å·²å†™å…¥çš„æ•°æ®
            
        Returns:
            bool: éªŒè¯æ˜¯å¦æˆåŠŸ
        """
        try:
            self.logger.info("ğŸ” å¼€å§‹å†™å…¥éªŒè¯...")
            
            # è·å–å·²å†™å…¥çš„ç”¨æˆ·IDé›†åˆ
            writtenUserIds = set(
                row['user_id'] for row in writtenDataFrame.select("user_id").distinct().collect()
            )
            
            if not writtenUserIds:
                self.logger.warning("âš ï¸ æ²¡æœ‰ç”¨æˆ·IDéœ€è¦éªŒè¯")
                return True
            
            # ä»æ•°æ®åº“è¯»å–éªŒè¯
            from src.batch.utils.RuleReader import RuleReader
            from pyspark.sql import SparkSession
            
            spark = SparkSession.getActiveSession()
            ruleReader = RuleReader(spark, self.mysqlConfig)
            
            existingTagsDataFrame = ruleReader.loadExistingUserTags()
            if existingTagsDataFrame is None:
                self.logger.warning("âš ï¸ æ— æ³•è¯»å–ç°æœ‰æ ‡ç­¾æ•°æ®è¿›è¡ŒéªŒè¯")
                return True
            
            # éªŒè¯å†™å…¥çš„ç”¨æˆ·æ˜¯å¦åœ¨æ•°æ®åº“ä¸­
            dbUserIds = set(
                row['user_id'] for row in existingTagsDataFrame.select("user_id").distinct().collect()
            )
            
            missingUsers = writtenUserIds - dbUserIds
            if missingUsers:
                self.logger.warning(f"âš ï¸ éƒ¨åˆ†ç”¨æˆ·æœªåœ¨æ•°æ®åº“ä¸­æ‰¾åˆ°: {len(missingUsers)} ä¸ªç”¨æˆ·")
                # æŠ½æ ·æ˜¾ç¤ºç¼ºå¤±ç”¨æˆ·
                sampleMissing = list(missingUsers)[:5]
                self.logger.warning(f"   ç¤ºä¾‹ç¼ºå¤±ç”¨æˆ·: {sampleMissing}")
                return False
            
            self.logger.info(f"âœ… å†™å…¥éªŒè¯æˆåŠŸ: {len(writtenUserIds)} ä¸ªç”¨æˆ·å…¨éƒ¨éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ å†™å…¥éªŒè¯å¤±è´¥: {str(e)}")
            return False
    
    def getWriteStatistics(self, dataFrame: DataFrame) -> Dict[str, Any]:
        """
        è·å–å†™å…¥ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            dataFrame: æ•°æ®DataFrame
            
        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            totalUsers = dataFrame.count()
            totalTags = dataFrame.agg({"tag_ids": "sum"}).collect()[0][0] or 0
            
            stats = {
                'total_users': totalUsers,
                'total_tags': totalTags,
                'avg_tags_per_user': round(totalTags / totalUsers, 2) if totalUsers > 0 else 0
            }
            
            self.logger.info(f"ğŸ“Š å†™å…¥ç»Ÿè®¡: {stats}")
            return stats
            
        except Exception as e:
            self.logger.error(f"âŒ è·å–å†™å…¥ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {}
    
    def testConnection(self) -> bool:
        """
        æµ‹è¯•MySQLè¿æ¥
        
        Returns:
            bool: è¿æ¥æ˜¯å¦æˆåŠŸ
        """
        try:
            import pymysql
            
            connection = pymysql.connect(
                host=self.mysqlConfig.host,
                port=self.mysqlConfig.port,
                user=self.mysqlConfig.username,
                password=self.mysqlConfig.password,
                database=self.mysqlConfig.database,
                charset='utf8mb4'
            )
            
            cursor = connection.cursor()
            cursor.execute("SELECT 1")
            result = cursor.fetchone()
            
            connection.close()
            
            if result:
                self.logger.info("âœ… MySQLè¿æ¥æµ‹è¯•æˆåŠŸ")
                return True
            else:
                self.logger.error("âŒ MySQLè¿æ¥æµ‹è¯•å¤±è´¥")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ MySQLè¿æ¥æµ‹è¯•å¼‚å¸¸: {str(e)}")
            return False