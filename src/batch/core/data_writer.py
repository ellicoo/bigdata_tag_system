"""
æ•°æ®å†™å…¥å™¨ - æ•´åˆåŸæœ‰çš„OptimizedMySQLWriteråŠŸèƒ½
"""

import logging
from typing import Dict, Any, Optional
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col, to_json, current_date

from src.common.config.base import MySQLConfig

logger = logging.getLogger(__name__)


class BatchDataWriter:
    """æ‰¹å¤„ç†æ•°æ®å†™å…¥å™¨ï¼ˆåŸOptimizedMySQLWriteråŠŸèƒ½ï¼‰"""
    
    def __init__(self, spark: SparkSession, mysql_config: MySQLConfig):
        self.spark = spark
        self.mysql_config = mysql_config
        self.write_statistics = {
            "total_records": 0,
            "affected_users": 0,
            "write_time_seconds": 0,
            "last_write_timestamp": None
        }
    
    def write_tag_results(self, results_df: DataFrame) -> bool:
        """
        å†™å…¥æ ‡ç­¾ç»“æœåˆ°MySQL
        
        Args:
            results_df: æ ‡ç­¾ç»“æœDataFrame (user_id, tag_ids, tag_details)
            
        Returns:
            bool: å†™å…¥æ˜¯å¦æˆåŠŸ
        """
        try:
            import time
            start_time = time.time()
            
            if results_df is None:
                logger.warning("âš ï¸ ç»“æœDataFrameä¸ºç©ºï¼Œè·³è¿‡å†™å…¥")
                return True
            
            record_count = results_df.count()
            if record_count == 0:
                logger.info("ğŸ“Š æ²¡æœ‰æ ‡ç­¾ç»“æœéœ€è¦å†™å…¥MySQL")
                return True
            
            logger.info(f"ğŸ“ å¼€å§‹å†™å…¥æ ‡ç­¾ç»“æœåˆ°MySQLï¼Œå…± {record_count} æ¡è®°å½•")
            
            # 1. æ•°æ®é¢„å¤„ç†å’ŒéªŒè¯
            processed_df = self._preprocess_for_write(results_df)
            if processed_df is None:
                logger.error("âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥")
                return False
            
            # 2. åŠ¨æ€åˆ†åŒºä¼˜åŒ–
            optimized_df = self._optimize_partitions(processed_df)
            
            # 3. æ‰§è¡ŒUPSERTå†™å…¥
            success = self._execute_upsert_write(optimized_df)
            
            # 4. éªŒè¯å†™å…¥ç»“æœ
            if success:
                verification_success = self._verify_write_results(optimized_df)
                if verification_success:
                    # 5. æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                    end_time = time.time()
                    self.write_statistics.update({
                        "total_records": record_count,
                        "affected_users": optimized_df.select("user_id").distinct().count(),
                        "write_time_seconds": round(end_time - start_time, 2),
                        "last_write_timestamp": end_time
                    })
                    
                    logger.info(f"âœ… æ ‡ç­¾ç»“æœå†™å…¥MySQLæˆåŠŸ")
                    # ç»Ÿè®¡ä¿¡æ¯ä¸åœ¨è¿™é‡Œæ‰“å°ï¼Œç”±è°ƒç”¨æ–¹ç»Ÿä¸€å¤„ç†
                    return True
                else:
                    logger.error("âŒ å†™å…¥éªŒè¯å¤±è´¥")
                    return False
            else:
                logger.error("âŒ æ ‡ç­¾ç»“æœå†™å…¥MySQLå¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"âŒ å†™å…¥æ ‡ç­¾ç»“æœæ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            return False
    
    def _preprocess_for_write(self, df: DataFrame) -> Optional[DataFrame]:
        """é¢„å¤„ç†æ•°æ®ï¼Œå‡†å¤‡å†™å…¥"""
        try:
            logger.info("ğŸ”„ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
            
            # 1. ç¡®ä¿å¿…éœ€å­—æ®µå­˜åœ¨
            required_columns = ["user_id", "tag_ids", "tag_details"]
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                logger.error(f"âŒ ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_columns}")
                return None
            
            # 2. æ•°æ®æ ¼å¼è½¬æ¢
            processed_df = df.select(
                col("user_id").cast("string"),
                to_json(col("tag_ids")).alias("tag_ids"),  # è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                col("tag_details").cast("string")
            )
            
            # 3. æ•°æ®å»é‡ï¼ˆåŸºäºuser_idï¼‰
            deduplicated_df = processed_df.dropDuplicates(["user_id"])
            
            original_count = df.count()
            processed_count = deduplicated_df.count()
            
            if original_count != processed_count:
                logger.info(f"ğŸ”„ ç”¨æˆ·çº§åˆ«å»é‡: {original_count} â†’ {processed_count} æ¡è®°å½•")
            
            # 4. æ•°æ®éªŒè¯
            if not self._validate_data_quality(deduplicated_df):
                logger.error("âŒ æ•°æ®è´¨é‡éªŒè¯å¤±è´¥")
                return None
            
            logger.info(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œå‡†å¤‡å†™å…¥ {processed_count} æ¡è®°å½•")
            return deduplicated_df
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {str(e)}")
            return None
    
    def _validate_data_quality(self, df: DataFrame) -> bool:
        """éªŒè¯æ•°æ®è´¨é‡"""
        try:
            # 1. æ£€æŸ¥ç©ºå€¼
            null_user_ids = df.filter(col("user_id").isNull()).count()
            if null_user_ids > 0:
                logger.error(f"âŒ å‘ç° {null_user_ids} æ¡user_idä¸ºç©ºçš„è®°å½•")
                return False
            
            # 2. æ£€æŸ¥ç©ºæ ‡ç­¾
            empty_tag_ids = df.filter(
                (col("tag_ids").isNull()) | 
                (col("tag_ids") == "[]") |
                (col("tag_ids") == "null")
            ).count()
            
            if empty_tag_ids > 0:
                logger.warning(f"âš ï¸ å‘ç° {empty_tag_ids} æ¡æ ‡ç­¾ä¸ºç©ºçš„è®°å½•ï¼Œä½†è¿™å¯èƒ½æ˜¯æ­£å¸¸çš„")
            
            # 3. æ£€æŸ¥user_idæ ¼å¼
            invalid_user_ids = df.filter(
                col("user_id").rlike("^\\s*$") |  # ç©ºç™½å­—ç¬¦
                col("user_id").contains("null")    # åŒ…å«nullå­—ç¬¦ä¸²
            ).count()
            
            if invalid_user_ids > 0:
                logger.error(f"âŒ å‘ç° {invalid_user_ids} æ¡æ— æ•ˆçš„user_id")
                return False
            
            logger.info("âœ… æ•°æ®è´¨é‡éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®è´¨é‡éªŒè¯å¤±è´¥: {str(e)}")
            return False
    
    def _optimize_partitions(self, df: DataFrame) -> DataFrame:
        """åŠ¨æ€åˆ†åŒºä¼˜åŒ–"""
        try:
            record_count = df.count()
            
            # æ ¹æ®æ•°æ®é‡åŠ¨æ€è°ƒæ•´åˆ†åŒºæ•°
            if record_count < 1000:
                target_partitions = 1
            elif record_count < 10000:
                target_partitions = 2
            elif record_count < 100000:
                target_partitions = 4
            else:
                target_partitions = 8
            
            current_partitions = df.rdd.getNumPartitions()
            
            if current_partitions != target_partitions:
                logger.info(f"ğŸ”„ åˆ†åŒºä¼˜åŒ–: {current_partitions} â†’ {target_partitions} ä¸ªåˆ†åŒº")
                df = df.coalesce(target_partitions)
            else:
                logger.info(f"âœ… åˆ†åŒºå·²ä¼˜åŒ–: {current_partitions} ä¸ªåˆ†åŒº")
            
            return df
            
        except Exception as e:
            logger.warning(f"âš ï¸ åˆ†åŒºä¼˜åŒ–å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨åŸå§‹åˆ†åŒº")
            return df
    
    def _execute_upsert_write(self, df: DataFrame) -> bool:
        """æ‰§è¡ŒUPSERTå†™å…¥ - ä½¿ç”¨å·²éªŒè¯çš„foreachPartition + pymysqlæ–¹æ¡ˆ"""
        import pymysql
        
        # é…ç½®å‚æ•°
        host = self.mysql_config.host
        port = self.mysql_config.port
        username = self.mysql_config.username
        password = self.mysql_config.password
        database = self.mysql_config.database
        
        def upsert_partition(partition_data):
            """åˆ†åŒºUPSERTé€»è¾‘ - å¤åˆ¶è‡ªOptimizedMySQLWriter"""
            import pymysql
            
            rows = list(partition_data)
            if not rows:
                return
            
            connection = None
            try:
                connection = pymysql.connect(
                    host=host,
                    port=port,
                    user=username,
                    password=password,
                    database=database,
                    charset='utf8mb4',
                    autocommit=False,
                    connect_timeout=30,
                    read_timeout=60,
                    write_timeout=60
                )
                
                cursor = connection.cursor()
                
                # ä½¿ç”¨å·²éªŒè¯çš„UPSERT SQL
                upsert_sql = """
                INSERT INTO user_tags (user_id, tag_ids, tag_details) 
                VALUES (%s, %s, %s)
                ON DUPLICATE KEY UPDATE
                    updated_time = CASE 
                        WHEN JSON_EXTRACT(tag_ids, '$') <> JSON_EXTRACT(VALUES(tag_ids), '$')
                        THEN CURRENT_TIMESTAMP 
                        ELSE updated_time 
                    END,
                    tag_ids = VALUES(tag_ids),
                    tag_details = VALUES(tag_details)
                """
                
                # æ‰¹é‡å¤„ç†
                batch_size = 1000
                for i in range(0, len(rows), batch_size):
                    batch = rows[i:i + batch_size]
                    batch_data = []
                    
                    for row in batch:
                        batch_data.append((
                            str(row.user_id),
                            str(row.tag_ids) if row.tag_ids else '[]',
                            str(row.tag_details) if row.tag_details else '{}'
                        ))
                    
                    cursor.executemany(upsert_sql, batch_data)
                
                connection.commit()
                
            except Exception as e:
                if connection:
                    connection.rollback()
                raise
            finally:
                if connection:
                    connection.close()
        
        try:
            logger.info("ğŸ“ æ‰§è¡ŒUPSERTå†™å…¥åˆ°MySQL (ä½¿ç”¨å·²éªŒè¯çš„foreachPartitionæ–¹æ¡ˆ)...")
            
            # ä¼˜åŒ–åˆ†åŒºæ•°é‡
            total_count = df.count()
            optimal_partitions = min(8, max(1, total_count // 5000))
            
            logger.info(f"ğŸ“Š UPSERTå†™å…¥ï¼š{total_count} æ¡è®°å½•ï¼Œ{optimal_partitions} ä¸ªåˆ†åŒº")
            
            # é‡åˆ†åŒºå¹¶æ‰§è¡Œ
            repartitioned_df = df.repartition(optimal_partitions, "user_id")
            
            # æ‰§è¡Œåˆ†åŒºUPSERT
            def upsert_partition_with_logging(partition_data):
                try:
                    rows = list(partition_data)
                    if not rows:
                        return
                    
                    print(f"ğŸ”„ åˆ†åŒºå¤„ç† {len(rows)} æ¡è®°å½•...")
                    upsert_partition(rows)
                    print(f"âœ… åˆ†åŒºUPSERTå®Œæˆï¼Œå¤„ç†äº† {len(rows)} æ¡è®°å½•")
                except Exception as e:
                    print(f"âŒ åˆ†åŒºUPSERTå¤±è´¥: {str(e)}")
                    import traceback
                    traceback.print_exc()
                    raise
            
            repartitioned_df.foreachPartition(upsert_partition_with_logging)
            
            logger.info("âœ… UPSERTå†™å…¥æ‰§è¡Œå®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ UPSERTå†™å…¥å¤±è´¥: {str(e)}")
            return False
    
    def _verify_write_results(self, written_df: DataFrame) -> bool:
        """éªŒè¯å†™å…¥ç»“æœ"""
        try:
            logger.info("ğŸ” å¼€å§‹éªŒè¯å†™å…¥ç»“æœ...")
            
            # 1. è·å–å†™å…¥çš„ç”¨æˆ·IDé›†åˆ
            written_user_ids = set([row.user_id for row in written_df.select("user_id").collect()])
            written_count = len(written_user_ids)
            
            if written_count == 0:
                logger.warning("âš ï¸ æ²¡æœ‰ç”¨æˆ·éœ€è¦éªŒè¯")
                return True
            
            # 2. ä»MySQLæŸ¥è¯¢è¿™äº›ç”¨æˆ·çš„æ ‡ç­¾
            user_ids_str = "', '".join(written_user_ids)
            verification_query = f"""
            (SELECT user_id, tag_ids 
             FROM user_tags 
             WHERE user_id IN ('{user_ids_str}')) as verification_data
            """
            
            mysql_df = self.spark.read.jdbc(
                url=self.mysql_config.jdbc_url,
                table=verification_query,
                properties=self.mysql_config.connection_properties
            )
            
            mysql_user_ids = set([row.user_id for row in mysql_df.select("user_id").collect()])
            mysql_count = len(mysql_user_ids)
            
            # 3. éªŒè¯å†™å…¥å®Œæ•´æ€§
            if mysql_count == written_count:
                logger.info(f"âœ… å†™å…¥éªŒè¯æˆåŠŸ: {mysql_count}/{written_count} ç”¨æˆ·æ ‡ç­¾å·²æ­£ç¡®å†™å…¥MySQL")
                
                # 4. æŠ½æ ·éªŒè¯æ•°æ®ä¸€è‡´æ€§
                self._sample_verify_data_consistency(written_df, mysql_df)
                return True
            else:
                missing_users = written_user_ids - mysql_user_ids
                logger.error(f"âŒ å†™å…¥éªŒè¯å¤±è´¥: {mysql_count}/{written_count} ç”¨æˆ·å†™å…¥æˆåŠŸ")
                logger.error(f"âŒ ç¼ºå¤±ç”¨æˆ·: {list(missing_users)[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ª
                return False
                
        except Exception as e:
            logger.error(f"âŒ å†™å…¥éªŒè¯å¤±è´¥: {str(e)}")
            return False
    
    def _sample_verify_data_consistency(self, written_df: DataFrame, mysql_df: DataFrame):
        """æŠ½æ ·éªŒè¯æ•°æ®ä¸€è‡´æ€§"""
        try:
            # æŠ½å–3ä¸ªç”¨æˆ·è¿›è¡Œè¯¦ç»†éªŒè¯
            sample_users = written_df.limit(3).collect()
            
            logger.info("ğŸ” æŠ½æ ·éªŒè¯æ•°æ®ä¸€è‡´æ€§ï¼ˆå‰3ä¸ªç”¨æˆ·ï¼‰:")
            
            for user_row in sample_users:
                user_id = user_row.user_id
                written_tag_ids = user_row.tag_ids
                
                # ä»MySQLæŸ¥è¯¢å¯¹åº”ç”¨æˆ·çš„æ ‡ç­¾
                mysql_user_data = mysql_df.filter(col("user_id") == user_id).collect()
                if mysql_user_data:
                    mysql_tag_ids = mysql_user_data[0].tag_ids
                    
                    # å°†JSONå­—ç¬¦ä¸²è§£æä¸ºPythonå¯¹è±¡è¿›è¡Œæ¯”è¾ƒï¼Œé¿å…æ ¼å¼å·®å¼‚
                    import json
                    try:
                        written_data = json.loads(written_tag_ids) if written_tag_ids else []
                        mysql_data = json.loads(mysql_tag_ids) if mysql_tag_ids else []
                        
                        # æ’åºåæ¯”è¾ƒï¼Œç¡®ä¿é¡ºåºæ— å…³çš„ä¸€è‡´æ€§
                        written_sorted = sorted(written_data) if isinstance(written_data, list) else written_data
                        mysql_sorted = sorted(mysql_data) if isinstance(mysql_data, list) else mysql_data
                        
                        if written_sorted == mysql_sorted:
                            logger.info(f"   âœ… ç”¨æˆ· {user_id}: æ•°æ®ä¸€è‡´")
                            logger.info(f"      æ ‡ç­¾ID: {written_sorted}")
                        else:
                            logger.warning(f"   âš ï¸ ç”¨æˆ· {user_id}: æ•°æ®ä¸ä¸€è‡´")
                            logger.warning(f"      å†™å…¥: {written_sorted}")
                            logger.warning(f"      MySQL: {mysql_sorted}")
                    except json.JSONDecodeError as e:
                        logger.warning(f"   âš ï¸ ç”¨æˆ· {user_id}: JSONè§£æå¤±è´¥")
                        logger.warning(f"      å†™å…¥åŸå§‹: {written_tag_ids}")
                        logger.warning(f"      MySQLåŸå§‹: {mysql_tag_ids}")
                        logger.warning(f"      é”™è¯¯: {str(e)}")
                else:
                    logger.warning(f"   âš ï¸ ç”¨æˆ· {user_id}: MySQLä¸­æœªæ‰¾åˆ°æ•°æ®")
            
        except Exception as e:
            logger.warning(f"âš ï¸ æŠ½æ ·éªŒè¯å¤±è´¥: {str(e)}")
    
    def get_write_statistics(self) -> Dict[str, Any]:
        """è·å–å†™å…¥ç»Ÿè®¡ä¿¡æ¯"""
        return self.write_statistics.copy()
    
    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.write_statistics = {
            "total_records": 0,
            "affected_users": 0,
            "write_time_seconds": 0,
            "last_write_timestamp": None
        }
        logger.info("ğŸ“Š å†™å…¥ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ¸…ç†é€»è¾‘ï¼Œå¦‚å…³é—­è¿æ¥ç­‰
            logger.info("âœ… æ•°æ®å†™å…¥å™¨æ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.warning(f"âš ï¸ æ•°æ®å†™å…¥å™¨æ¸…ç†å¼‚å¸¸: {str(e)}")