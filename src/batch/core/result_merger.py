"""
ç»“æœåˆå¹¶å™¨ - æ•´åˆåŸæœ‰çš„AdvancedTagMergerå’ŒUnifiedTagMergeråŠŸèƒ½
"""

import logging
from typing import Optional
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import col, from_json, lit, udf, array_distinct
from pyspark.sql.types import ArrayType, IntegerType, StringType

from src.common.config.base import MySQLConfig

logger = logging.getLogger(__name__)


class BatchResultMerger:
    """æ‰¹å¤„ç†ç»“æœåˆå¹¶å™¨ï¼ˆåŸAdvancedTagMergerå’ŒUnifiedTagMergeråŠŸèƒ½ï¼‰"""
    
    def __init__(self, spark: SparkSession, mysql_config: MySQLConfig):
        self.spark = spark
        self.mysql_config = mysql_config
    
    def merge_with_existing_tags(self, new_tags_df: DataFrame, cached_existing_tags: DataFrame = None) -> Optional[DataFrame]:
        """
        ä¸MySQLä¸­ç°æœ‰æ ‡ç­¾åˆå¹¶
        
        Args:
            new_tags_df: æ–°è®¡ç®—çš„æ ‡ç­¾DataFrame (user_id, tag_ids, tag_details)
            cached_existing_tags: é¢„ç¼“å­˜çš„ç°æœ‰æ ‡ç­¾
            
        Returns:
            åˆå¹¶åçš„DataFrame
        """
        try:
            logger.info("å¼€å§‹ä¸MySQLä¸­ç°æœ‰æ ‡ç­¾åˆå¹¶...")
            
            # 1. ä½¿ç”¨é¢„ç¼“å­˜çš„ç°æœ‰æ ‡ç­¾æ•°æ®
            if cached_existing_tags is not None:
                existing_tags = cached_existing_tags
                logger.info("ä½¿ç”¨é¢„ç¼“å­˜çš„ç°æœ‰æ ‡ç­¾æ•°æ®")
            else:
                # å…œåº•ï¼šè¯»å–ç°æœ‰æ ‡ç­¾å¹¶ä½¿ç”¨å†…å­˜+ç£ç›˜æŒä¹…åŒ–
                existing_tags = self._read_existing_user_tags()
                if existing_tags is not None:
                    from pyspark import StorageLevel
                    existing_tags = existing_tags.persist(StorageLevel.MEMORY_AND_DISK)
            
            if existing_tags is None or existing_tags.count() == 0:
                logger.info("æ•°æ®åº“ä¸­æ²¡æœ‰ç°æœ‰æ ‡ç­¾ï¼Œç›´æ¥è¿”å›æ–°æ ‡ç­¾")
                return new_tags_df
            
            existing_count = existing_tags.count()
            logger.info(f"ç°æœ‰æ ‡ç­¾æ•°æ®: {existing_count} æ¡ç”¨æˆ·æ ‡ç­¾")
            
            # 3. å·¦è¿æ¥åˆå¹¶ - ä¿®å¤åˆ—åå†²çªé—®é¢˜
            merged_df = new_tags_df.alias("new").join(
                existing_tags.select("user_id", "tag_ids").alias("existing"),
                "user_id",
                "left"
            )
            
            # 4. åˆå¹¶æ ‡ç­¾æ•°ç»„
            final_merged = merged_df.select(
                col("user_id"),
                self._merge_tag_arrays(
                    col("existing.tag_ids"), 
                    col("new.tag_ids")
                ).alias("tag_ids"),
                col("new.tag_details")
            )
            
            # 5. è¯¦ç»†çš„æ ‡ç­¾åˆå¹¶è¿‡ç¨‹è¿½è¸ªæ—¥å¿—
            logger.info("ğŸ“Š æ ‡ç­¾åˆå¹¶å®Œæ•´è¿‡ç¨‹è¿½è¸ªï¼ˆå±•ç¤ºæœ‰æ ‡ç­¾çš„ç”¨æˆ·å‰3ä¸ªï¼‰:")
            
            # è·å–æœ‰åˆå¹¶ç»“æœçš„ç”¨æˆ·ï¼ˆè€Œä¸æ˜¯éšæœºå‰3ä¸ªç”¨æˆ·ï¼‰
            sample_users = final_merged.limit(3).collect()
            sample_user_ids = [row.user_id for row in sample_users]
            
            # æ³¨æ„ï¼šè¿™é‡Œçš„new_tags_dfå®é™…ä¸Šæ˜¯ç»è¿‡å†…å­˜åˆå¹¶åçš„ç»“æœï¼Œä¸æ˜¯å•ä¸ªä»»åŠ¡çš„åŸå§‹ç»“æœ
            # éœ€è¦å±•ç¤ºå®Œæ•´çš„åˆå¹¶é“¾è·¯
            for user_id in sample_user_ids:
                logger.info(f"   ğŸ‘¤ ç”¨æˆ· {user_id} æ ‡ç­¾åˆå¹¶å…¨è¿‡ç¨‹:")
                
                # 1. å¤šä»»åŠ¡å†…å­˜åˆå¹¶åçš„æ ‡ç­¾ï¼ˆè¿™æ˜¯ä¼ å…¥çš„new_tags_dfï¼‰
                memory_merged_tags = new_tags_df.filter(col("user_id") == user_id).collect()
                memory_merged_tag_ids = memory_merged_tags[0].tag_ids if memory_merged_tags else []
                
                # 2. MySQLç°æœ‰æ ‡ç­¾
                mysql_existing_tags = existing_tags.filter(col("user_id") == user_id).collect()
                mysql_existing_tag_ids = mysql_existing_tags[0].tag_ids if mysql_existing_tags else []
                
                # 3. æœ€ç»ˆåˆå¹¶åæ ‡ç­¾
                final_merged_tags = final_merged.filter(col("user_id") == user_id).collect()
                final_merged_tag_ids = final_merged_tags[0].tag_ids if final_merged_tags else []
                
                logger.info(f"      ğŸ“ å¤šä»»åŠ¡å†…å­˜åˆå¹¶åæ ‡ç­¾: {memory_merged_tag_ids}")
                logger.info(f"      ğŸ—„ï¸  MySQLç°æœ‰æ ‡ç­¾: {mysql_existing_tag_ids}")
                logger.info(f"      âœ… æœ€ç»ˆåˆå¹¶åæ ‡ç­¾: {final_merged_tag_ids}")
                
                # 4. åˆ†æåˆå¹¶å˜åŒ–
                if mysql_existing_tag_ids:
                    # æœ‰ç°æœ‰æ ‡ç­¾çš„æƒ…å†µ
                    added_from_memory = [tag for tag in final_merged_tag_ids if tag not in mysql_existing_tag_ids]
                    if added_from_memory:
                        logger.info(f"      â• ä»å†…å­˜åˆå¹¶æ–°å¢: {added_from_memory}")
                    else:
                        logger.info(f"      â• ä»å†…å­˜åˆå¹¶æ–°å¢: æ—  (æ ‡ç­¾é‡å¤æˆ–æ— å˜åŒ–)")
                else:
                    # é¦–æ¬¡æ ‡ç­¾çš„æƒ…å†µ
                    logger.info(f"      â• é¦–æ¬¡æ ‡ç­¾åˆå¹¶: {final_merged_tag_ids}")
                
                # 5. åˆå¹¶é€»è¾‘éªŒè¯
                expected_merged = sorted(list(set(memory_merged_tag_ids + mysql_existing_tag_ids)))
                actual_merged = sorted(final_merged_tag_ids)
                if expected_merged == actual_merged:
                    logger.info(f"      âœ… åˆå¹¶é€»è¾‘æ­£ç¡®")
                else:
                    logger.info(f"      âŒ åˆå¹¶é€»è¾‘å¼‚å¸¸ - æœŸæœ›: {expected_merged}, å®é™…: {actual_merged}")
                
                logger.info(f"      â”€" * 60)
            
            # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œæ¸…ç†é¢„ç¼“å­˜æ•°æ®ï¼Œç”±åœºæ™¯è°ƒåº¦å™¨ç»Ÿä¸€ç®¡ç†
            if cached_existing_tags is None and existing_tags is not None:
                # åªæœ‰éé¢„ç¼“å­˜æ•°æ®æ‰éœ€è¦åœ¨è¿™é‡Œæ¸…ç†
                existing_tags.unpersist()
            
            merge_count = final_merged.count()
            logger.info(f"âœ… ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶å®Œæˆï¼Œå½±å“ {merge_count} ä¸ªç”¨æˆ·")
            
            return final_merged
            
        except Exception as e:
            logger.error(f"ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶å¤±è´¥: {str(e)}")
            # å¤±è´¥æ—¶è¿”å›åŸå§‹æ•°æ®
            return new_tags_df
    
    def _read_existing_user_tags(self) -> Optional[DataFrame]:
        """ä»MySQLè¯»å–ç°æœ‰ç”¨æˆ·æ ‡ç­¾å¹¶ç¼“å­˜åˆ°å†…å­˜/ç£ç›˜"""
        try:
            logger.info("ğŸ“– ä»MySQLè¯»å–ç°æœ‰ç”¨æˆ·æ ‡ç­¾...")
            
            existing_df = self.spark.read.jdbc(
                url=self.mysql_config.jdbc_url,
                table="user_tags",
                properties=self.mysql_config.connection_properties
            )
            
            if existing_df.count() == 0:
                logger.info("MySQLä¸­æ²¡æœ‰ç°æœ‰æ ‡ç­¾æ•°æ®")
                return None
            
            # å°†JSONå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°ç»„ç±»å‹ï¼Œä¿ç•™æ—¶é—´å­—æ®µç”¨äºè°ƒè¯•
            processed_df = existing_df.select(
                "user_id",
                from_json(col("tag_ids"), ArrayType(IntegerType())).alias("tag_ids"),
                "tag_details",
                "created_time",
                "updated_time"
            )
            
            # æŒä¹…åŒ–åˆ°å†…å­˜å’Œç£ç›˜
            processed_df = processed_df.persist()
            
            logger.info(f"æˆåŠŸè¯»å–å¹¶ç¼“å­˜ç°æœ‰æ ‡ç­¾æ•°æ®")
            return processed_df
            
        except Exception as e:
            logger.info(f"è¯»å–ç°æœ‰æ ‡ç­¾å¤±è´¥ï¼ˆå¯èƒ½æ˜¯é¦–æ¬¡è¿è¡Œï¼‰: {str(e)}")
            return None
    
    def _merge_tag_arrays(self, existing_tags_col, new_tags_col):
        """åˆå¹¶ä¸¤ä¸ªæ ‡ç­¾æ•°ç»„å¹¶å»é‡"""
        @udf(returnType=ArrayType(IntegerType()))
        def merge_arrays(existing_tags, new_tags):
            # ç¡®ä¿è¾“å…¥éƒ½æ˜¯åˆ—è¡¨ç±»å‹
            if existing_tags is None:
                existing_tags = []
            if new_tags is None:
                new_tags = []
                
            # å¦‚æœè¾“å…¥ä¸æ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
            if not isinstance(existing_tags, list):
                existing_tags = []
            if not isinstance(new_tags, list):
                new_tags = []
            
            # åˆå¹¶å¹¶å»é‡ï¼Œä¿æŒæ’åº
            merged = list(set(existing_tags + new_tags))
            return sorted(merged)
        
        return merge_arrays(existing_tags_col, new_tags_col)
    
    def cleanup_cache(self):
        """æ¸…ç†ç¼“å­˜èµ„æº"""
        try:
            self.spark.catalog.clearCache()
            logger.info("âœ… æ¸…ç†æ ‡ç­¾åˆå¹¶ç¼“å­˜å®Œæˆ")
        except Exception as e:
            logger.warning(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {str(e)}")


class TagMergeStrategy:
    """æ ‡ç­¾åˆå¹¶ç­–ç•¥æšä¸¾"""
    
    # ä¸ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶ï¼Œç›´æ¥å†…å­˜åˆå¹¶ç»“æœ
    MEMORY_ONLY = "memory_only"
    
    # ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶ï¼Œå†…å­˜åˆå¹¶åå†ä¸MySQLæ ‡ç­¾åˆå¹¶
    MEMORY_THEN_DATABASE = "memory_then_database"


class UnifiedTagMerger:
    """ç»Ÿä¸€æ ‡ç­¾åˆå¹¶å™¨ - æ ¹æ®åœºæ™¯é€‰æ‹©åˆå¹¶ç­–ç•¥ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰"""
    
    def __init__(self, spark: SparkSession, mysql_config: MySQLConfig):
        self.spark = spark
        self.mysql_config = mysql_config
        self.advanced_merger = BatchResultMerger(spark, mysql_config)
    
    def merge_tags(self, tag_results: list, strategy: str) -> Optional[DataFrame]:
        """
        æ ¹æ®ç­–ç•¥åˆå¹¶æ ‡ç­¾
        
        Args:
            tag_results: æ ‡ç­¾è®¡ç®—ç»“æœåˆ—è¡¨
            strategy: åˆå¹¶ç­–ç•¥ (MEMORY_ONLY | MEMORY_THEN_DATABASE)
            
        Returns:
            åˆå¹¶åçš„DataFrame
        """
        try:
            if not tag_results:
                logger.warning("æ²¡æœ‰æ ‡ç­¾ç»“æœéœ€è¦åˆå¹¶")
                return None
            
            logger.info(f"ä½¿ç”¨ç­–ç•¥ {strategy} åˆå¹¶æ ‡ç­¾")
            
            # ç¬¬ä¸€æ­¥ï¼šå†…å­˜åˆå¹¶ï¼ˆæ‰€æœ‰ç­–ç•¥éƒ½éœ€è¦ï¼‰
            memory_merged = self._memory_merge(tag_results)
            if memory_merged is None:
                return None
            
            # ç¬¬äºŒæ­¥ï¼šæ ¹æ®ç­–ç•¥å†³å®šæ˜¯å¦ä¸æ•°æ®åº“åˆå¹¶
            if strategy == TagMergeStrategy.MEMORY_ONLY:
                logger.info("ä»…å†…å­˜åˆå¹¶ï¼Œä¸ä¸æ•°æ®åº“ç°æœ‰æ ‡ç­¾åˆå¹¶")
                return memory_merged
            
            elif strategy == TagMergeStrategy.MEMORY_THEN_DATABASE:
                logger.info("å†…å­˜åˆå¹¶åï¼Œå†ä¸æ•°æ®åº“ç°æœ‰æ ‡ç­¾åˆå¹¶")
                return self.advanced_merger.merge_with_existing_tags(memory_merged)
            
            else:
                logger.error(f"æœªçŸ¥çš„åˆå¹¶ç­–ç•¥: {strategy}")
                return memory_merged
                
        except Exception as e:
            logger.error(f"æ ‡ç­¾åˆå¹¶å¤±è´¥: {str(e)}")
            return None
    
    def _memory_merge(self, tag_results: list) -> Optional[DataFrame]:
        """å†…å­˜åˆå¹¶ï¼šå°†åŒä¸€ç”¨æˆ·çš„å¤šä¸ªæ ‡ç­¾åˆå¹¶"""
        try:
            from functools import reduce
            from pyspark.sql.functions import collect_list, struct
            
            # åˆå¹¶æ‰€æœ‰æ ‡ç­¾ç»“æœ
            all_tags = reduce(lambda df1, df2: df1.union(df2), tag_results)
            
            if all_tags.count() == 0:
                return None
            
            # å»é‡
            deduplicated = all_tags.dropDuplicates(["user_id", "tag_id"])
            
            # ä¸°å¯Œæ ‡ç­¾ä¿¡æ¯
            enriched = self._enrich_with_tag_info(deduplicated)
            
            # æŒ‰ç”¨æˆ·èšåˆ
            aggregated = enriched.groupBy("user_id").agg(
                collect_list("tag_id").alias("tag_ids_raw"),
                collect_list(struct("tag_id", "tag_name", "tag_category")).alias("tag_info_list")
            )
            
            # å»é‡å¹¶æ ¼å¼åŒ–
            final_result = aggregated.select(
                "user_id",
                array_distinct("tag_ids_raw").alias("tag_ids"),
                "tag_info_list"
            )
            
            return self._format_output(final_result)
            
        except Exception as e:
            logger.error(f"å†…å­˜åˆå¹¶å¤±è´¥: {str(e)}")
            return None
    
    def _enrich_with_tag_info(self, tags_df: DataFrame) -> DataFrame:
        """ä¸°å¯Œæ ‡ç­¾ä¿¡æ¯"""
        try:
            tag_definitions = self.spark.read.jdbc(
                url=self.mysql_config.jdbc_url,
                table="tag_definition",
                properties=self.mysql_config.connection_properties
            ).select("tag_id", "tag_name", "tag_category")
            
            return tags_df.join(
                tag_definitions, "tag_id", "left"
            ).select(
                "user_id", "tag_id", 
                col("tag_name"), col("tag_category"), "tag_detail"
            )
            
        except Exception as e:
            logger.error(f"ä¸°å¯Œæ ‡ç­¾ä¿¡æ¯å¤±è´¥: {str(e)}")
            return tags_df.select(
                "user_id", "tag_id",
                lit("unknown").alias("tag_name"),
                lit("unknown").alias("tag_category"),
                "tag_detail"
            )
    
    def _format_output(self, user_tags_df: DataFrame) -> DataFrame:
        """æ ¼å¼åŒ–è¾“å‡º"""
        import json
        
        @udf(returnType=StringType())
        def build_tag_details(tag_info_list):
            if not tag_info_list:
                return "{}"
            
            tag_details = {}
            for tag_info in tag_info_list:
                tag_id = str(tag_info['tag_id'])
                tag_details[tag_id] = {
                    'tag_name': tag_info['tag_name'],
                    'tag_category': tag_info['tag_category']
                }
            return json.dumps(tag_details, ensure_ascii=False)
        
        return user_tags_df.select(
            col("user_id"),
            col("tag_ids"),
            build_tag_details(col("tag_info_list")).alias("tag_details")
        )
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.advanced_merger.cleanup_cache()