"""
æ‰¹å¤„ç†ç»Ÿä¸€æ•°æ®åŠ è½½å™¨
æ•´åˆåŸæœ‰çš„HiveDataReaderå’ŒRuleReaderåŠŸèƒ½
"""

import logging
from typing import List, Dict, Any, Optional
from pyspark.sql import SparkSession, DataFrame
from pyspark import StorageLevel
from pyspark.sql.functions import col, from_json
from pyspark.sql.types import ArrayType, IntegerType

from src.common.config.base import BaseConfig, S3Config, MySQLConfig

logger = logging.getLogger(__name__)


class BatchDataLoader:
    """æ‰¹å¤„ç†ç»Ÿä¸€æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, spark: SparkSession, config: BaseConfig):
        self.spark = spark
        self.config = config
        self.hive_loader = HiveDataLoader(spark, config.s3)
        self.rule_loader = RuleDataLoader(spark, config.mysql)
        
    def load_user_data(self, source_name: str, fields: List[str] = None) -> DataFrame:
        """åŠ è½½ç”¨æˆ·æ•°æ®"""
        return self.hive_loader.read_hive_table(source_name, fields)
        
    def load_rules(self) -> DataFrame:
        """åŠ è½½æ ‡ç­¾è§„åˆ™"""
        return self.rule_loader.get_active_rules_df()
        
    def preload_existing_tags(self) -> DataFrame:
        """é¢„åŠ è½½ç°æœ‰æ ‡ç­¾"""
        return self.rule_loader.load_existing_user_tags()


class HiveDataLoader:
    """Hiveæ•°æ®åŠ è½½å™¨ï¼ˆåŸHiveDataReaderåŠŸèƒ½ï¼‰"""
    
    def __init__(self, spark: SparkSession, s3_config: S3Config):
        self.spark = spark
        self.s3_config = s3_config
        
    def read_hive_table(self, table_path: str, selected_columns: List[str] = None) -> Optional[DataFrame]:
        """
        ä»S3è¯»å–Hiveè¡¨æ•°æ®
        
        Args:
            table_path: è¡¨è·¯å¾„ï¼Œä¾‹å¦‚ 'user_basic_info/'
            selected_columns: éœ€è¦é€‰æ‹©çš„åˆ—ï¼Œä¸ºNoneæ—¶é€‰æ‹©æ‰€æœ‰åˆ—
            
        Returns:
            DataFrameæˆ–None
        """
        try:
            logger.info(f"ğŸ“– ä»S3è¯»å–Hiveè¡¨: {table_path}")
            
            # æ„å»ºå®Œæ•´çš„S3è·¯å¾„
            full_path = f"s3a://{self.s3_config.bucket}/warehouse/{table_path.rstrip('/')}"
            logger.info(f"ğŸ“ S3è·¯å¾„: {full_path}")
            
            # è¯»å–Parquetæ ¼å¼æ•°æ®
            df = self.spark.read.parquet(full_path)
            
            # åˆ—é€‰æ‹©ä¼˜åŒ–
            if selected_columns:
                # éªŒè¯åˆ—æ˜¯å¦å­˜åœ¨
                available_columns = df.columns
                valid_columns = [col for col in selected_columns if col in available_columns]
                
                if len(valid_columns) != len(selected_columns):
                    missing_columns = set(selected_columns) - set(valid_columns)
                    logger.warning(f"âš ï¸ ç¼ºå¤±åˆ—: {missing_columns}")
                
                if valid_columns:
                    df = df.select(*valid_columns)
                    logger.info(f"ğŸ“Š å·²é€‰æ‹©åˆ—: {valid_columns}")
                else:
                    logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„åˆ—å¯é€‰æ‹©")
                    return None
            
            record_count = df.count()
            logger.info(f"âœ… æˆåŠŸè¯»å– {table_path}ï¼Œè®°å½•æ•°: {record_count}")
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ è¯»å–Hiveè¡¨å¤±è´¥ {table_path}: {str(e)}")
            return None
    
    def read_multiple_tables(self, table_configs: Dict[str, List[str]]) -> Dict[str, DataFrame]:
        """
        æ‰¹é‡è¯»å–å¤šä¸ªè¡¨
        
        Args:
            table_configs: {è¡¨å: [åˆ—ååˆ—è¡¨]} çš„å­—å…¸
            
        Returns:
            {è¡¨å: DataFrame} çš„å­—å…¸
        """
        results = {}
        
        for table_name, columns in table_configs.items():
            df = self.read_hive_table(table_name, columns)
            if df is not None:
                results[table_name] = df
            else:
                logger.warning(f"âš ï¸ è·³è¿‡è¡¨ {table_name}ï¼ˆè¯»å–å¤±è´¥ï¼‰")
        
        logger.info(f"ğŸ“Š æ‰¹é‡è¯»å–å®Œæˆï¼ŒæˆåŠŸåŠ è½½ {len(results)}/{len(table_configs)} ä¸ªè¡¨")
        return results


class RuleDataLoader:
    """è§„åˆ™æ•°æ®åŠ è½½å™¨ï¼ˆåŸRuleReaderåŠŸèƒ½ï¼‰"""
    
    def __init__(self, spark: SparkSession, mysql_config: MySQLConfig):
        self.spark = spark
        self.mysql_config = mysql_config
        self._rules_df = None
        self._initialized = False
    
    def initialize(self):
        """åˆå§‹åŒ–è§„åˆ™æ•°æ®"""
        if self._initialized:
            logger.info("è§„åˆ™è¯»å–å™¨å·²åˆå§‹åŒ–ï¼Œä½¿ç”¨ç¼“å­˜æ•°æ®")
            return
        
        logger.info("ğŸ”„ å¼€å§‹ä¸€æ¬¡æ€§åŠ è½½å®Œæ•´è§„åˆ™æ•°æ®ï¼ˆJOINåŒ…å«æ ‡ç­¾å®šä¹‰ï¼‰...")
        
        try:
            self._load_rules_df()
            self._initialized = True
            logger.info("âœ… è§„åˆ™è¯»å–å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ è§„åˆ™è¯»å–å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _load_rules_df(self):
        """åŠ è½½æ ‡ç­¾è§„åˆ™DataFrameå¹¶persist"""
        logger.info("ğŸ“– åŠ è½½æ ‡ç­¾è§„åˆ™...")
        
        query = """
        (SELECT 
            tr.rule_id,
            tr.tag_id,
            tr.rule_conditions,
            tr.is_active as rule_active,
            td.tag_name,
            td.tag_category,
            td.description as tag_description,
            td.is_active as tag_active
         FROM tag_rules tr 
         JOIN tag_definition td ON tr.tag_id = td.tag_id 
         WHERE tr.is_active = 1 AND td.is_active = 1) as active_rules
        """
        
        self._rules_df = self.spark.read.jdbc(
            url=self.mysql_config.jdbc_url,
            table=query,
            properties=self.mysql_config.connection_properties
        ).persist(StorageLevel.MEMORY_AND_DISK)
        
        # è§¦å‘æŒä¹…åŒ–å¹¶è·å–ç»Ÿè®¡
        rule_count = self._rules_df.count()
        logger.info(f"âœ… å®Œæ•´è§„åˆ™DataFrameå·²persist(å†…å­˜&ç£ç›˜)ï¼Œå…± {rule_count} æ¡ï¼ˆåŒ…å«æ ‡ç­¾å®šä¹‰ï¼‰")
    
    def get_active_rules_df(self) -> DataFrame:
        """è·å–æ´»è·ƒæ ‡ç­¾è§„åˆ™DataFrame"""
        if not self._initialized:
            self.initialize()
        return self._rules_df
    
    def get_tag_definitions_df(self) -> DataFrame:
        """è·å–æ ‡ç­¾å®šä¹‰DataFrame"""
        if not self._initialized:
            self.initialize()
        return self._rules_df.select("tag_id", "tag_name", "tag_category", "tag_description").distinct()
    
    def load_existing_user_tags(self) -> Optional[DataFrame]:
        """é¢„ç¼“å­˜MySQLä¸­çš„ç°æœ‰æ ‡ç­¾æ•°æ®"""
        try:
            logger.info("ğŸ”„ é¢„ç¼“å­˜MySQLç°æœ‰ç”¨æˆ·æ ‡ç­¾æ•°æ®-persist(å†…å­˜&ç£ç›˜)...")
            
            existing_df = self.spark.read.jdbc(
                url=self.mysql_config.jdbc_url,
                table="user_tags",
                properties=self.mysql_config.connection_properties
            )
            
            if existing_df.count() == 0:
                logger.info("MySQLä¸­æš‚æ— ç°æœ‰ç”¨æˆ·æ ‡ç­¾æ•°æ®")
                return None
            else:
                # JSONè½¬æ¢ï¼šå°†tag_idsä»å­—ç¬¦ä¸²è½¬ä¸ºæ•°ç»„
                processed_df = existing_df.select(
                    "user_id",
                    from_json(col("tag_ids"), ArrayType(IntegerType())).alias("tag_ids"),
                    "tag_details",
                    "created_time", 
                    "updated_time"
                )
                
                cached_df = processed_df.persist(StorageLevel.MEMORY_AND_DISK)
                # è§¦å‘ç¼“å­˜
                cached_count = cached_df.count()
                logger.info(f"âœ… é¢„ç¼“å­˜MySQLç°æœ‰ç”¨æˆ·æ ‡ç­¾æ•°æ®å®Œæˆï¼ˆå«JSONè½¬æ¢ï¼‰ï¼Œå…± {cached_count} æ¡è®°å½•")
                return cached_df
            
        except Exception as e:
            logger.warning(f"âš ï¸ é¢„ç¼“å­˜MySQLç°æœ‰ç”¨æˆ·æ ‡ç­¾æ•°æ®å¤±è´¥: {str(e)}")
            return None
    
    def cleanup(self):
        """æ¸…ç†ç¼“å­˜ï¼Œé‡Šæ”¾èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†è§„åˆ™è¯»å–å™¨ç¼“å­˜...")
        
        try:
            if self._rules_df is not None:
                logger.info("ğŸ§¹ é‡Šæ”¾è§„åˆ™DataFrame persistç¼“å­˜")
                self._rules_df.unpersist()
                
            # æ¸…ç©ºå¼•ç”¨
            self._rules_df = None
            self._initialized = False
            
            logger.info("âœ… è§„åˆ™è¯»å–å™¨ç¼“å­˜æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ è§„åˆ™è¯»å–å™¨ç¼“å­˜æ¸…ç†å¼‚å¸¸: {str(e)}")