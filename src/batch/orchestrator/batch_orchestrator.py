"""
æ‰¹å¤„ç†ç¼–æ’å™¨ - æ•´åˆåŸæœ‰çš„TagScheduleråŠŸèƒ½
è¿™æ˜¯é‡æ„åçš„ä¸»è°ƒåº¦å™¨ï¼Œè´Ÿè´£åè°ƒæ‰€æœ‰æ‰¹å¤„ç†ç»„ä»¶
"""

import logging
import os
import time
from typing import List, Dict, Any, Optional
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col

from src.common.config.base import BaseConfig
from src.batch.core.data_loader import BatchDataLoader
from src.batch.core.tag_executor import BatchTagExecutor
from src.batch.core.result_merger import BatchResultMerger
from src.batch.core.data_writer import BatchDataWriter

logger = logging.getLogger(__name__)


class BatchOrchestrator:
    """æ‰¹å¤„ç†ç¼–æ’å™¨ï¼ˆåŸTagScheduleré‡æ„ï¼‰"""
    
    def __init__(self, config: BaseConfig, max_workers: int = 4):
        self.config = config
        self.max_workers = max_workers
        self.spark = None
        
        # æ ¸å¿ƒç»„ä»¶ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.data_loader = None
        self.tag_executor = None
        self.result_merger = None
        self.data_writer = None
        
        # ç¼“å­˜ç®¡ç†
        self.cached_existing_tags = None
    
    def initialize(self):
        """åˆå§‹åŒ–ç»„ä»¶"""
        try:
            logger.info("ğŸš€ åˆå§‹åŒ–æ‰¹å¤„ç†ç¼–æ’å™¨...")
            
            # åˆå§‹åŒ–Spark
            self.spark = self._create_spark_session()
            
            # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
            self.data_loader = BatchDataLoader(self.spark, self.config)
            self.data_loader.rule_loader.initialize()  # åˆå§‹åŒ–è§„åˆ™æ•°æ®
            
            self.tag_executor = BatchTagExecutor(self.spark, self.config, self.data_loader, self.max_workers)
            self.result_merger = BatchResultMerger(self.spark, self.config.mysql)
            self.data_writer = BatchDataWriter(self.spark, self.config.mysql)
            
            # é¢„ç¼“å­˜ç°æœ‰æ ‡ç­¾æ•°æ®
            self._preload_existing_tags()
            
            logger.info("âœ… æ‰¹å¤„ç†ç¼–æ’å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹å¤„ç†ç¼–æ’å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _create_spark_session(self) -> SparkSession:
        """åˆ›å»ºSparkä¼šè¯"""
        builder = SparkSession.builder
        
        # æ·»åŠ åŸºç¡€é…ç½®
        for key, value in self.config.spark.to_dict().items():
            builder = builder.config(key, value)
        
        # æ·»åŠ S3é…ç½®
        for key, value in self.config.s3.to_spark_config().items():
            builder = builder.config(key, value)
        
        # å¼ºåˆ¶æ·»åŠ S3ç›¸å…³é…ç½®ä»¥ç¡®ä¿è¿æ¥æ­£å¸¸
        builder = builder.config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        builder = builder.config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000")
        builder = builder.config("spark.hadoop.fs.s3a.access.key", "minioadmin")
        builder = builder.config("spark.hadoop.fs.s3a.secret.key", "minioadmin")
        builder = builder.config("spark.hadoop.fs.s3a.path.style.access", "true")
        builder = builder.config("spark.hadoop.fs.s3a.aws.credentials.provider", 
                                "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider")
        
        # åŠ è½½JARæ–‡ä»¶ - åŒ…å«S3ç›¸å…³JARä»¥æ”¯æŒçœŸå®çš„S3/Hiveè¯»å–
        if self.config.spark.jars:
            jar_files = self.config.spark.jars.split(',')
            existing_jars = [jar for jar in jar_files if jar and os.path.exists(jar)]
            
            if existing_jars:
                jar_paths = ",".join(existing_jars)
                logger.info(f"ğŸ“¦ åŠ è½½JARæ–‡ä»¶: {[os.path.basename(jar) for jar in existing_jars]}")
                builder = builder.config("spark.jars", jar_paths)
                builder = builder.config("spark.driver.extraClassPath", jar_paths)
                builder = builder.config("spark.executor.extraClassPath", jar_paths)
            else:
                logger.info("ğŸ“¦ JARæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡JARåŠ è½½")
        
        spark = builder.getOrCreate()
        spark.sparkContext.setLogLevel("ERROR")  # æ›´ä¸¥æ ¼çš„æ—¥å¿—çº§åˆ«ï¼Œåªæ˜¾ç¤ºé”™è¯¯
        
        # å…³é—­Sparkçš„è¯¦ç»†æ—¥å¿—
        try:
            spark.sparkContext._jvm.org.apache.log4j.Logger.getLogger("org").setLevel(
                spark.sparkContext._jvm.org.apache.log4j.Level.ERROR
            )
            spark.sparkContext._jvm.org.apache.log4j.Logger.getLogger("akka").setLevel(
                spark.sparkContext._jvm.org.apache.log4j.Level.ERROR
            )
        except:
            pass  # å¿½ç•¥æ—¥å¿—é…ç½®é”™è¯¯
        
        logger.info(f"Sparkä¼šè¯åˆ›å»ºæˆåŠŸ: {spark.sparkContext.applicationId}")
        return spark
    
    def _preload_existing_tags(self):
        """é¢„ç¼“å­˜MySQLä¸­çš„ç°æœ‰æ ‡ç­¾æ•°æ®"""
        try:
            logger.info("ğŸ”„ é¢„ç¼“å­˜MySQLç°æœ‰ç”¨æˆ·æ ‡ç­¾æ•°æ®...")
            self.cached_existing_tags = self.data_loader.preload_existing_tags()
            
            if self.cached_existing_tags is not None:
                cached_count = self.cached_existing_tags.count()
                logger.info(f"âœ… é¢„ç¼“å­˜å®Œæˆï¼Œå…± {cached_count} æ¡è®°å½•")
            else:
                logger.info("MySQLä¸­æš‚æ— ç°æœ‰ç”¨æˆ·æ ‡ç­¾æ•°æ®")
            
        except Exception as e:
            logger.warning(f"âš ï¸ é¢„ç¼“å­˜MySQLç°æœ‰ç”¨æˆ·æ ‡ç­¾æ•°æ®å¤±è´¥: {str(e)}")
            self.cached_existing_tags = None
    
    def health_check(self) -> bool:
        """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
        try:
            logger.info("ğŸ¥ æ‰§è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥...")
            
            # 1. æ£€æŸ¥Sparkè¿æ¥
            if not self.spark:
                logger.error("âŒ Sparkä¼šè¯æœªåˆå§‹åŒ–")
                return False
            
            # 2. æ£€æŸ¥æ•°æ®åŠ è½½å™¨
            if not self.data_loader:
                logger.error("âŒ æ•°æ®åŠ è½½å™¨æœªåˆå§‹åŒ–")
                return False
            
            # 3. æ£€æŸ¥æ ‡ç­¾æ‰§è¡Œå™¨
            if not self.tag_executor:
                logger.error("âŒ æ ‡ç­¾æ‰§è¡Œå™¨æœªåˆå§‹åŒ–")
                return False
            
            # 4. æ£€æŸ¥æ•°æ®åº“è¿æ¥
            try:
                test_df = self.spark.read.jdbc(
                    url=self.config.mysql.jdbc_url,
                    table="(SELECT 1 as test_connection) as test",
                    properties=self.config.mysql.connection_properties
                )
                test_df.count()
                logger.info("âœ… MySQLè¿æ¥æ­£å¸¸")
            except Exception as e:
                logger.error(f"âŒ MySQLè¿æ¥å¤±è´¥: {str(e)}")
                return False
            
            # 5. æ£€æŸ¥ä»»åŠ¡æ³¨å†ŒçŠ¶æ€
            available_tasks = self.get_available_tasks()
            logger.info(f"âœ… å·²æ³¨å†Œä»»åŠ¡: {len(available_tasks)} ä¸ª")
            
            logger.info("ğŸ‰ ç³»ç»Ÿå¥åº·æ£€æŸ¥é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç³»ç»Ÿå¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            return False
    
    def execute_full_workflow(self, user_filter: Optional[List[str]] = None) -> bool:
        """
        æ‰§è¡Œå®Œæ•´æ‰¹å¤„ç†å·¥ä½œæµï¼ˆåŸscenario_task_all_users_all_tagsï¼‰
        
        Args:
            user_filter: å¯é€‰çš„ç”¨æˆ·IDè¿‡æ»¤åˆ—è¡¨
            
        Returns:
            bool: æ‰§è¡Œæ˜¯å¦æˆåŠŸ
        """
        try:
            start_time = time.time()
            logger.info(f"ğŸ¯ å¼€å§‹æ‰§è¡Œå®Œæ•´æ‰¹å¤„ç†å·¥ä½œæµ")
            logger.info(f"ğŸ‘¥ ç”¨æˆ·è¿‡æ»¤: {user_filter if user_filter else 'å…¨é‡ç”¨æˆ·'}")
            
            # 1. ä½¿ç”¨æ ‡ç­¾æ‰§è¡Œå™¨æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            task_results = self.tag_executor.execute_all_tasks(user_filter)
            
            if task_results is None or task_results.count() == 0:
                logger.info("ğŸ“Š å…¨é‡æ ‡ç­¾ä»»åŠ¡æ‰§è¡Œå®Œæˆ - æ— ç”¨æˆ·ç¬¦åˆä»»ä½•æ ‡ç­¾æ¡ä»¶ (è¿™æ˜¯æ­£å¸¸æƒ…å†µ)")
                return True
            
            # 2. ä¸MySQLå·²å­˜åœ¨æ ‡ç­¾åˆå¹¶
            logger.info("ğŸ”„ å¼€å§‹ä¸MySQLå·²å­˜åœ¨æ ‡ç­¾åˆå¹¶...")
            final_merged = self.result_merger.merge_with_existing_tags(
                task_results, 
                self.cached_existing_tags
            )
            if final_merged is None:
                logger.error("âŒ ä¸MySQLå·²å­˜åœ¨æ ‡ç­¾åˆå¹¶å¤±è´¥")
                return False
            
            logger.info(f"âœ… ä¸MySQLå·²å­˜åœ¨æ ‡ç­¾åˆå¹¶å®Œæˆï¼Œæœ€ç»ˆå½±å“ç”¨æˆ·æ•°: {final_merged.count()}")
            
            # 3. å†™å…¥MySQL
            logger.info("ğŸ“ å†™å…¥åˆå¹¶åçš„æ ‡ç­¾ç»“æœåˆ°MySQL...")
            success = self.data_writer.write_tag_results(final_merged)
            
            # 4. ç»Ÿè®¡è¾“å‡º
            end_time = time.time()
            stats = self.data_writer.get_write_statistics()
            
            logger.info(f"""
ğŸ‰ å®Œæ•´æ‰¹å¤„ç†å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼
ğŸ·ï¸  æ‰§è¡Œäº†æ‰€æœ‰å·²æ³¨å†Œçš„ä»»åŠ¡ç±»
ğŸ‘¥ å½±å“ç”¨æˆ·æ•°: {final_merged.count()}
â±ï¸  æ‰§è¡Œæ—¶é—´: {end_time - start_time:.2f}ç§’
ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}
            """)
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ å®Œæ•´æ‰¹å¤„ç†å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}")
            return False
    
    def execute_specific_tags_workflow(self, tag_ids: List[int]) -> bool:
        """
        æ‰§è¡ŒæŒ‡å®šæ ‡ç­¾å·¥ä½œæµï¼ˆåŸscenario_task_all_users_specific_tagsï¼‰
        
        Args:
            tag_ids: æ ‡ç­¾IDåˆ—è¡¨
            
        Returns:
            bool: æ‰§è¡Œæ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"ğŸ¯ å¼€å§‹æ‰§è¡ŒæŒ‡å®šæ ‡ç­¾å·¥ä½œæµ (æ ‡ç­¾ID: {tag_ids})")
            start_time = time.time()
            
            # ä½¿ç”¨æ ‡ç­¾æ‰§è¡Œå™¨æ‰§è¡ŒæŒ‡å®šæ ‡ç­¾ä»»åŠ¡
            result = self.tag_executor.execute_specific_tasks(tag_ids)
            
            if result is None:
                logger.warning(f"æŒ‡å®šæ ‡ç­¾ä»»åŠ¡æ²¡æœ‰äº§ç”Ÿç»“æœ: {tag_ids}")
                return True  # æ²¡æœ‰ç»“æœä¹Ÿç®—æˆåŠŸ
            
            # ä¸MySQLç°æœ‰æ ‡ç­¾åˆå¹¶
            merged_result = self.result_merger.merge_with_existing_tags(
                result, 
                self.cached_existing_tags
            )
            
            # å†™å…¥MySQL
            success = self.data_writer.write_tag_results(merged_result)
            
            # ç»Ÿè®¡è¾“å‡º
            end_time = time.time()
            stats = self.data_writer.get_write_statistics()
            
            logger.info(f"""
ğŸ‰ æŒ‡å®šæ ‡ç­¾å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼
â±ï¸  æ‰§è¡Œæ—¶é—´: {end_time - start_time:.2f}ç§’
ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}
            """)
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ æŒ‡å®šæ ‡ç­¾å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}")
            return False
    
    def execute_specific_users_workflow(self, user_ids: List[str], tag_ids: List[int]) -> bool:
        """
        æ‰§è¡ŒæŒ‡å®šç”¨æˆ·å·¥ä½œæµï¼ˆåŸscenario_task_specific_users_specific_tagsï¼‰
        
        Args:
            user_ids: ç”¨æˆ·IDåˆ—è¡¨
            tag_ids: æ ‡ç­¾IDåˆ—è¡¨
            
        Returns:
            bool: æ‰§è¡Œæ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"ğŸ¯ å¼€å§‹æ‰§è¡ŒæŒ‡å®šç”¨æˆ·å·¥ä½œæµ (ç”¨æˆ·: {user_ids}, æ ‡ç­¾ID: {tag_ids})")
            start_time = time.time()
            
            # ä½¿ç”¨æ ‡ç­¾æ‰§è¡Œå™¨æ‰§è¡ŒæŒ‡å®šæ ‡ç­¾ä»»åŠ¡ï¼Œä¼ å…¥ç”¨æˆ·è¿‡æ»¤
            result = self.tag_executor.execute_specific_tasks(tag_ids, user_ids)
            
            if result is None:
                logger.warning(f"æŒ‡å®šç”¨æˆ·æŒ‡å®šæ ‡ç­¾ä»»åŠ¡æ²¡æœ‰äº§ç”Ÿç»“æœ: ç”¨æˆ·{user_ids}, æ ‡ç­¾{tag_ids}")
                return True  # æ²¡æœ‰ç»“æœä¹Ÿç®—æˆåŠŸ
            
            # ä¸MySQLç°æœ‰æ ‡ç­¾åˆå¹¶
            merged_result = self.result_merger.merge_with_existing_tags(
                result, 
                self.cached_existing_tags
            )
            
            # å†™å…¥MySQL
            success = self.data_writer.write_tag_results(merged_result)
            
            # ç»Ÿè®¡è¾“å‡º
            end_time = time.time()
            stats = self.data_writer.get_write_statistics()
            
            logger.info(f"""
ğŸ‰ æŒ‡å®šç”¨æˆ·å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼
â±ï¸  æ‰§è¡Œæ—¶é—´: {end_time - start_time:.2f}ç§’
ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}
            """)
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ æŒ‡å®šç”¨æˆ·å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}")
            return False
    
    def get_available_tasks(self) -> Dict[int, str]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„æ ‡ç­¾ä»»åŠ¡"""
        from src.batch.tasks.task_factory import TagTaskFactory
        return TagTaskFactory.get_all_available_tasks()
    
    def get_task_summary(self) -> Dict[int, Dict[str, Any]]:
        """è·å–ä»»åŠ¡æ‘˜è¦ä¿¡æ¯"""
        from src.batch.tasks.task_registry import get_task_summary
        return get_task_summary()
    
    def _cleanup_caches(self):
        """æ¸…ç†ç¼“å­˜"""
        try:
            # æ¸…ç†æ ‡ç­¾æ‰§è¡Œå™¨ç¼“å­˜
            if self.tag_executor:
                self.tag_executor.cleanup_cache()
            
            # æ¸…ç†ç»“æœåˆå¹¶å™¨ç¼“å­˜
            if self.result_merger:
                self.result_merger.cleanup_cache()
            
            # æ¸…ç†æ•°æ®åŠ è½½å™¨ç¼“å­˜
            if self.data_loader and self.data_loader.rule_loader:
                self.data_loader.rule_loader.cleanup()
            
            logger.info("ğŸ§¹ æ‰¹å¤„ç†ç¼–æ’å™¨ç¼“å­˜æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç¼“å­˜æ¸…ç†å¼‚å¸¸: {str(e)}")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # æ¸…ç†æ‰€æœ‰ç»„ä»¶ç¼“å­˜
            self._cleanup_caches()
            
            # æ¸…ç†é¢„ç¼“å­˜çš„æ ‡ç­¾æ•°æ®
            if self.cached_existing_tags:
                self.cached_existing_tags.unpersist()
            
            # åœæ­¢Sparkä¼šè¯
            if self.spark:
                self.spark.stop()
            
            logger.info("âœ… æ‰¹å¤„ç†ç¼–æ’å™¨èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ èµ„æºæ¸…ç†å¼‚å¸¸: {str(e)}")
    
    def _generate_production_like_data(self, source_name: str = None) -> DataFrame:
        """ç”Ÿæˆç”Ÿäº§ç¯å¢ƒæ¨¡æ‹Ÿçš„æµ‹è¯•æ•°æ®ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰"""
        try:
            logger.info("ğŸ“Š ç”Ÿæˆç”Ÿäº§ç¯å¢ƒæ¨¡æ‹Ÿæµ‹è¯•æ•°æ®...")
            
            import random
            from datetime import datetime, timedelta
            from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType
            from pyspark.sql.functions import lit
            
            # ç”ŸæˆåŸºç¡€ç”¨æˆ·æ•°æ®
            num_users = 300
            
            # ç”Ÿæˆç”¨æˆ·åŸºç¡€ä¿¡æ¯
            user_basic_data = []
            for i in range(num_users):
                user_id = f"user_{i:06d}"
                
                # ç”Ÿæˆä¸åŒç±»å‹çš„ç”¨æˆ·æ•°æ®
                if i < 50:  # é«˜å‡€å€¼ç”¨æˆ·
                    total_asset = random.uniform(150000, 500000)
                    cash_balance = random.uniform(60000, 150000)
                    user_level = random.choice(["VIP1", "VIP2", "VIP3"])
                    kyc_status = "verified"
                    risk_score = random.uniform(20, 50)
                    age = random.randint(30, 55)
                    trade_count = random.randint(15, 40)
                elif i < 70:  # VIPç”¨æˆ·
                    total_asset = random.uniform(80000, 200000)
                    cash_balance = random.uniform(30000, 80000)
                    user_level = random.choice(["VIP2", "VIP3"])
                    kyc_status = "verified"
                    risk_score = random.uniform(15, 35)
                    age = random.randint(25, 50)
                    trade_count = random.randint(10, 25)
                elif i < 100:  # å¹´è½»ç”¨æˆ·
                    total_asset = random.uniform(5000, 50000)
                    cash_balance = random.uniform(1000, 20000)
                    user_level = random.choice(["Regular", "VIP1"])
                    kyc_status = random.choice(["verified", "pending"])
                    risk_score = random.uniform(25, 60)
                    age = random.randint(18, 30)
                    trade_count = random.randint(5, 20)
                elif i < 180:  # æ´»è·ƒäº¤æ˜“è€…
                    total_asset = random.uniform(20000, 100000)
                    cash_balance = random.uniform(5000, 40000)
                    user_level = random.choice(["Regular", "VIP1", "VIP2"])
                    kyc_status = "verified"
                    risk_score = random.uniform(30, 70)
                    age = random.randint(25, 45)
                    trade_count = random.randint(16, 50)
                elif i < 205:  # ä½é£é™©ç”¨æˆ·
                    total_asset = random.uniform(30000, 120000)
                    cash_balance = random.uniform(10000, 50000)
                    user_level = random.choice(["Regular", "VIP1"])
                    kyc_status = "verified"
                    risk_score = random.uniform(10, 30)
                    age = random.randint(28, 50)
                    trade_count = random.randint(5, 15)
                elif i < 220:  # æ–°æ³¨å†Œç”¨æˆ·
                    total_asset = random.uniform(1000, 20000)
                    cash_balance = random.uniform(500, 10000)
                    user_level = "Regular"
                    kyc_status = random.choice(["pending", "verified"])
                    risk_score = random.uniform(40, 80)
                    age = random.randint(20, 40)
                    trade_count = random.randint(1, 8)
                elif i < 235:  # æœ€è¿‘æ´»è·ƒç”¨æˆ·
                    total_asset = random.uniform(15000, 80000)
                    cash_balance = random.uniform(5000, 30000)
                    user_level = random.choice(["Regular", "VIP1"])
                    kyc_status = "verified"
                    risk_score = random.uniform(25, 55)
                    age = random.randint(22, 45)
                    trade_count = random.randint(8, 25)
                else:  # æ™®é€šç”¨æˆ·
                    total_asset = random.uniform(2000, 40000)
                    cash_balance = random.uniform(500, 15000)
                    user_level = "Regular"
                    kyc_status = random.choice(["verified", "pending"])
                    risk_score = random.uniform(30, 90)
                    age = random.randint(20, 65)
                    trade_count = random.randint(1, 12)
                
                # è®¾ç½®æ—¶é—´æ•°æ®
                base_date = datetime.now().date()
                
                # æ–°æ³¨å†Œç”¨æˆ·çš„æ³¨å†Œæ—¶é—´åœ¨æœ€è¿‘30å¤©å†…
                if i < 220 and i >= 205:
                    registration_date = base_date - timedelta(days=random.randint(1, 30))
                else:
                    registration_date = base_date - timedelta(days=random.randint(30, 365))
                
                # æœ€è¿‘æ´»è·ƒç”¨æˆ·çš„æœ€åç™»å½•æ—¶é—´åœ¨æœ€è¿‘7å¤©å†…
                if i < 235 and i >= 220:
                    last_login_date = base_date - timedelta(days=random.randint(1, 7))
                else:
                    last_login_date = base_date - timedelta(days=random.randint(7, 90))
                
                user_basic_data.append({
                    "user_id": user_id,
                    "age": age,
                    "user_level": user_level,
                    "kyc_status": kyc_status,
                    "registration_date": registration_date,
                    "risk_score": risk_score,
                    "total_asset_value": total_asset,
                    "cash_balance": cash_balance,
                    "trade_count_30d": trade_count,
                    "last_login_date": last_login_date
                })
            
            # è½¬æ¢ä¸º DataFrame
            basic_schema = StructType([
                StructField("user_id", StringType(), True),
                StructField("age", IntegerType(), True),
                StructField("user_level", StringType(), True),
                StructField("kyc_status", StringType(), True),
                StructField("registration_date", DateType(), True),
                StructField("risk_score", DoubleType(), True),
                StructField("total_asset_value", DoubleType(), True),
                StructField("cash_balance", DoubleType(), True),
                StructField("trade_count_30d", IntegerType(), True),
                StructField("last_login_date", DateType(), True)
            ])
            
            all_data_df = self.spark.createDataFrame(user_basic_data, basic_schema)
            
            # åˆ†åˆ«åˆ›å»ºä¸‰ä¸ªæ•°æ®è¡¨
            user_basic_info = all_data_df.select(
                "user_id", "age", "user_level", "kyc_status", 
                "registration_date", "risk_score"
            )
            
            user_asset_summary = all_data_df.select(
                "user_id", "total_asset_value", "cash_balance"
            )
            
            user_activity_summary = all_data_df.select(
                "user_id", "trade_count_30d", "last_login_date"
            )
            
            logger.info(f"âœ… ç”Ÿæˆæµ‹è¯•æ•°æ®å®Œæˆ: {num_users} ä¸ªç”¨æˆ·")
            
            # æ ¹æ®source_nameè¿”å›å¯¹åº”çš„DataFrame
            if source_name == 'user_basic_info':
                return user_basic_info
            elif source_name == 'user_asset_summary':
                return user_asset_summary
            elif source_name == 'user_activity_summary':
                return user_activity_summary
            else:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šsource_nameï¼Œè¿”å›åˆå¹¶çš„å®Œæ•´æ•°æ®
                return all_data_df
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæµ‹è¯•æ•°æ®å¤±è´¥: {str(e)}")
            return None