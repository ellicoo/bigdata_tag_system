#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ‡ç­¾è®¡ç®—å¼•æ“
ä¸»è¦è´Ÿè´£æ ‡ç­¾è®¡ç®—æµç¨‹çš„ç¼–æ’å’Œæ‰§è¡Œ
"""
from typing import List, Optional, Dict
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import *

from ..meta.HiveMeta import HiveMeta
from ..meta.MysqlMeta import MysqlMeta
from ..parser.TagRuleParser import TagRuleParser
from .TagGroup import TagGroup
from ..utils.SparkUdfs import merge_with_existing_tags


class TagEngine:
    """æ ‡ç­¾è®¡ç®—å¼•æ“
    
    èŒè´£ï¼š
    1. ç¼–æ’æ•´ä¸ªæ ‡ç­¾è®¡ç®—æµç¨‹
    2. ç®¡ç†Hiveå’ŒMySQLæ•°æ®æº
    3. åè°ƒæ ‡ç­¾åˆ†ç»„å’Œå¹¶è¡Œè®¡ç®—
    4. æ‰§è¡Œæ ‡ç­¾åˆå¹¶å’Œç»“æœå†™å…¥
    """
    
    def __init__(self, spark: SparkSession, hiveConfig: Dict = None, mysqlConfig: Dict = None):
        """åˆå§‹åŒ–æ ‡ç­¾å¼•æ“
        
        Args:
            spark: Sparkä¼šè¯
            hiveConfig: Hiveé…ç½®ï¼ˆå¯é€‰ï¼‰
            mysqlConfig: MySQLé…ç½®
        """
        self.spark = spark
        self.hiveConfig = hiveConfig or {}
        self.mysqlConfig = mysqlConfig
        
        # åˆå§‹åŒ–æ•°æ®æºç®¡ç†å™¨
        self.hiveMeta = HiveMeta(spark)
        self.mysqlMeta = MysqlMeta(spark, mysqlConfig)
        self.ruleParser = TagRuleParser()
        
        print("ğŸš€ TagEngineåˆå§‹åŒ–å®Œæˆ")
    
    def computeTags(self, mode: str = "full", tagIds: Optional[List[int]] = None) -> bool:
        """æ‰§è¡Œæ ‡ç­¾è®¡ç®— - æµæ°´çº¿æ¶æ„
        
        Args:
            mode: è®¡ç®—æ¨¡å¼ï¼ˆfull/specificï¼‰
            tagIds: æŒ‡å®šæ ‡ç­¾IDåˆ—è¡¨ï¼ˆä»…åœ¨specificæ¨¡å¼ä¸‹æœ‰æ•ˆï¼‰
            
        Returns:
            bool: è®¡ç®—æ˜¯å¦æˆåŠŸ
        """
        print(f"ğŸš€ å¼€å§‹æ ‡ç­¾è®¡ç®—ï¼Œæ¨¡å¼: {mode}")
        
        try:
            # 1. åŠ è½½æ ‡ç­¾è§„åˆ™
            rulesDF = self._loadTagRules(tagIds)
            if rulesDF.count() == 0:
                print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„æ ‡ç­¾è§„åˆ™")
                return True
            
            # 2. åˆ†æä¾èµ–å…³ç³»å¹¶æ™ºèƒ½åˆ†ç»„
            tagGroups = self._analyzeAndGroupTags(rulesDF)
            if not tagGroups:
                print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°å¯è®¡ç®—çš„æ ‡ç­¾ç»„")
                return True
            
            # ğŸš€ å…³é”®æ”¹è¿›ï¼šç»„é—´æµæ°´çº¿å¤„ç†ï¼Œæ¯ç»„è®¡ç®—å®Œç«‹å³å†™å…¥MySQLå¹¶æ¸…ç†èµ„æº
            success = self._computeAllTagGroupsPipeline(tagGroups, rulesDF)
            
            if success:
                print("âœ… æ ‡ç­¾è®¡ç®—å®Œæˆï¼ˆæµæ°´çº¿æ¨¡å¼ï¼‰")
                self._printStatistics()
            else:
                print("âŒ æ ‡ç­¾è®¡ç®—å¤±è´¥")
            
            return success
            
        except Exception as e:
            print(f"âŒ æ ‡ç­¾è®¡ç®—å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def healthCheck(self) -> bool:
        """å¥åº·æ£€æŸ¥
        
        Returns:
            bool: ç³»ç»Ÿæ˜¯å¦å¥åº·
        """
        print("ğŸ” æ‰§è¡Œæ ‡ç­¾ç³»ç»Ÿå¥åº·æ£€æŸ¥...")
        
        try:
            # 1. æµ‹è¯•MySQLè¿æ¥
            mysqlOk = self.mysqlMeta.testConnection()
            
            # 2. æµ‹è¯•Hiveè¡¨è®¿é—®ï¼ˆä½¿ç”¨ç®€å•è¡¨æµ‹è¯•ï¼‰
            hiveOk = self._testHiveAccess()
            
            # 3. æµ‹è¯•UDFåŠŸèƒ½
            udfOk = self._testUdfFunctions()
            
            # 4. æ£€æŸ¥æ ‡ç­¾è§„åˆ™
            rulesOk = self._checkTagRules()
            
            allOk = mysqlOk and hiveOk and udfOk and rulesOk
            
            if allOk:
                print("âœ… ç³»ç»Ÿå¥åº·æ£€æŸ¥é€šè¿‡")
            else:
                print("âŒ ç³»ç»Ÿå¥åº·æ£€æŸ¥å¤±è´¥")
                print(f"   MySQL: {'âœ…' if mysqlOk else 'âŒ'}")
                print(f"   Hive: {'âœ…' if hiveOk else 'âŒ'}")
                print(f"   UDF: {'âœ…' if udfOk else 'âŒ'}")
                print(f"   Rules: {'âœ…' if rulesOk else 'âŒ'}")
            
            return allOk
            
        except Exception as e:
            print(f"âŒ å¥åº·æ£€æŸ¥å¼‚å¸¸: {e}")
            return False
    
    # ========== ç§æœ‰æ–¹æ³• ==========
    
    def _loadTagRules(self, tagIds: Optional[List[int]] = None) -> DataFrame:
        """åŠ è½½æ ‡ç­¾è§„åˆ™"""
        print("ğŸ“‹ åŠ è½½æ ‡ç­¾è§„åˆ™...")
        return self.mysqlMeta.loadTagRules(tagIds)
    
    def _analyzeAndGroupTags(self, rulesDF: DataFrame) -> List[TagGroup]:
        """åˆ†æä¾èµ–å…³ç³»å¹¶è¿›è¡Œæ™ºèƒ½åˆ†ç»„"""
        print("ğŸ¯ åˆ†ææ ‡ç­¾ä¾èµ–å…³ç³»...")
        
        # åˆ†ææ‰€æœ‰æ ‡ç­¾çš„è¡¨ä¾èµ–
        dependencies = self.ruleParser.analyzeDependencies(rulesDF)
        
        # æ™ºèƒ½åˆ†ç»„
        tagGroups = self.ruleParser.groupTagsByTables(dependencies)
        
        return tagGroups
    
    def _computeAllTagGroupsPipeline(self, tagGroups: List[TagGroup], rulesDF: DataFrame) -> bool:
        """æµæ°´çº¿å¤„ç†æ‰€æœ‰æ ‡ç­¾ç»„ï¼šæ¯ç»„è®¡ç®—å®Œç«‹å³å†™å…¥MySQLå¹¶æ¸…ç†èµ„æº"""
        print(f"ğŸš€ æµæ°´çº¿å¤„ç† {len(tagGroups)} ä¸ªæ ‡ç­¾ç»„...")
        
        successCount = 0
        totalGroups = len(tagGroups)
        
        for i, group in enumerate(tagGroups):
            print(f"\nğŸ“¦ å¤„ç†æ ‡ç­¾ç»„ {i+1}/{totalGroups}: {group.name}")
            
            try:
                # ç¬¬1æ­¥ï¼šè¿‡æ»¤è¯¥ç»„ç›¸å…³çš„æ ‡ç­¾è§„åˆ™ï¼ˆæ¯ç»„åªåŠ è½½è‡ªå·±çš„è§„åˆ™ï¼‰
                groupRulesDF = rulesDF.filter(col("tag_id").isin(group.tagIds))
                print(f"   ğŸ“‹ è¯¥ç»„æ ‡ç­¾è§„åˆ™æ•°: {groupRulesDF.count()}")
                
                # ç¬¬2æ­¥ï¼šè®¡ç®—è¯¥ç»„æ ‡ç­¾
                print(f"   âš¡ è®¡ç®—é˜¶æ®µï¼šå¹¶è¡Œæ‰§è¡Œæ ‡ç­¾ {group.tagIds}")
                groupResult = group.computeTags(self.hiveMeta, groupRulesDF)
                
                if groupResult.count() == 0:
                    print(f"   âš ï¸  æ ‡ç­¾ç»„ {group.name} æ— åŒ¹é…ç”¨æˆ·ï¼Œè·³è¿‡å†™å…¥")
                    successCount += 1
                    continue
                
                # ç¬¬3æ­¥ï¼šç«‹å³ä¸MySQLç°æœ‰æ ‡ç­¾åˆå¹¶å¹¶å†™å…¥
                print(f"   ğŸ’¾ å†™å…¥é˜¶æ®µï¼šç«‹å³å†™å…¥MySQL")
                writeSuccess = self._mergeWithExistingAndSaveGroup(groupResult, group.name)
                
                if writeSuccess:
                    print(f"   âœ… æ ‡ç­¾ç»„ {group.name} å¤„ç†å®Œæˆ")
                    successCount += 1
                else:
                    print(f"   âŒ æ ‡ç­¾ç»„ {group.name} å†™å…¥å¤±è´¥")
                
                # ç¬¬4æ­¥ï¼šæ¸…ç†è¯¥ç»„ç›¸å…³ç¼“å­˜ï¼ˆé‡Šæ”¾å†…å­˜ï¼‰
                print(f"   ğŸ§¹ æ¸…ç†é˜¶æ®µï¼šé‡Šæ”¾{group.name}ç›¸å…³ç¼“å­˜")
                self._clearGroupCache(group.requiredTables)
                
            except Exception as e:
                print(f"   âŒ æ ‡ç­¾ç»„ {group.name} å¤„ç†å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        print(f"\nâœ… æµæ°´çº¿å¤„ç†å®Œæˆ: {successCount}/{totalGroups} ä¸ªç»„æˆåŠŸ")
        return successCount == totalGroups
    
    def _mergeWithExistingAndSaveGroup(self, groupResult: DataFrame, groupName: str) -> bool:
        """ä¸ºå•ä¸ªæ ‡ç­¾ç»„ä¸MySQLç°æœ‰æ ‡ç­¾åˆå¹¶å¹¶ä¿å­˜"""
        try:
            # åŠ è½½ç°æœ‰æ ‡ç­¾
            existingTagsDF = self.mysqlMeta.loadExistingTags()
            
            # LEFT JOIN åˆå¹¶
            joinedDF = groupResult.alias("new").join(
                existingTagsDF.alias("existing"),
                col("new.user_id") == col("existing.user_id"),
                "left"
            )
            
            # ä½¿ç”¨SparkUdfsæ¨¡å—åˆå¹¶æ ‡ç­¾
            finalDF = joinedDF.withColumn(
                "final_tag_ids",
                merge_with_existing_tags(
                    col("new.tag_ids_array"),
                    col("existing.existing_tag_ids")
                )
            ).withColumn(
                "final_tag_ids_json",
                to_json(col("final_tag_ids"))
            ).select(
                col("new.user_id").alias("user_id"),
                col("final_tag_ids_json")
            )
            
            # å†™å…¥MySQL
            success = self.mysqlMeta.writeTagResults(finalDF)
            
            if success:
                userCount = finalDF.count()
                print(f"   âœ… {groupName} æ ‡ç­¾ç»“æœä¿å­˜æˆåŠŸ: {userCount} ä¸ªç”¨æˆ·")
            
            return success
            
        except Exception as e:
            print(f"   âŒ {groupName} æ ‡ç­¾åˆå¹¶ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def _clearGroupCache(self, groupTables: List[str]):
        """æ¸…ç†ç‰¹å®šæ ‡ç­¾ç»„çš„ç¼“å­˜"""
        try:
            self.hiveMeta.clearGroupCache(groupTables)
            print(f"   ğŸ§¹ å·²æ¸…ç†è¡¨ç¼“å­˜: {groupTables}")
        except Exception as e:
            print(f"   âš ï¸  ç¼“å­˜æ¸…ç†å¼‚å¸¸: {e}")
    
    def _testHiveAccess(self) -> bool:
        """æµ‹è¯•Hiveè¡¨è®¿é—®"""
        try:
            # å°è¯•åˆ—å‡ºè¡¨
            tables = self.spark.sql("SHOW TABLES").collect()
            print(f"   âœ… Hiveè®¿é—®æ­£å¸¸ï¼Œå‘ç° {len(tables)} ä¸ªè¡¨")
            return True
        except Exception as e:
            print(f"   âŒ Hiveè®¿é—®å¤±è´¥: {e}")
            return False
    
    def _testUdfFunctions(self) -> bool:
        """æµ‹è¯•UDFå‡½æ•°"""
        try:
            # åˆ›å»ºæµ‹è¯•DataFrame
            testData = [("user1", [1, 2, 3]), ("user2", [2, 3, 4])]
            testDF = self.spark.createDataFrame(testData, ["user_id", "tags"])
            
            # æµ‹è¯•SparkUdfsæ¨¡å—å‡½æ•°
            from ..utils.SparkUdfs import merge_user_tags
            resultDF = testDF.withColumn(
                "merged_tags",
                merge_user_tags(col("tags"))
            )
            
            resultCount = resultDF.count()
            print(f"   âœ… UDFå‡½æ•°æµ‹è¯•é€šè¿‡ï¼Œå¤„ç† {resultCount} æ¡æ•°æ®")
            return True
            
        except Exception as e:
            print(f"   âŒ UDFå‡½æ•°æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def _checkTagRules(self) -> bool:
        """æ£€æŸ¥æ ‡ç­¾è§„åˆ™"""
        try:
            rulesDF = self.mysqlMeta.loadTagRules()
            ruleCount = rulesDF.count()
            
            if ruleCount > 0:
                print(f"   âœ… æ ‡ç­¾è§„åˆ™æ£€æŸ¥é€šè¿‡ï¼Œå‘ç° {ruleCount} ä¸ªæ´»è·ƒæ ‡ç­¾")
                return True
            else:
                print("   âš ï¸  æ²¡æœ‰å‘ç°æ´»è·ƒçš„æ ‡ç­¾è§„åˆ™")
                return False
                
        except Exception as e:
            print(f"   âŒ æ ‡ç­¾è§„åˆ™æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def _printStatistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = self.mysqlMeta.getTagStatistics()
            if stats:
                print("\nğŸ“Š æ ‡ç­¾ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯:")
                print(f"   æ´»è·ƒæ ‡ç­¾æ•°: {stats.get('activeTagCount', 0)}")
                print(f"   æœ‰æ ‡ç­¾ç”¨æˆ·æ•°: {stats.get('taggedUserCount', 0)}")
                print(f"   æ€»æ ‡ç­¾æ•°: {stats.get('totalTagCount', 0)}")
                print(f"   å¹³å‡æ¯ç”¨æˆ·æ ‡ç­¾æ•°: {stats.get('avgTagsPerUser', 0)}")
        except:
            print("   âš ï¸  æ— æ³•è·å–ç»Ÿè®¡ä¿¡æ¯")
    
    def _createEmptyGroupResult(self) -> DataFrame:
        """åˆ›å»ºç©ºçš„æ ‡ç­¾ç»„ç»“æœ"""
        from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType
        
        schema = StructType([
            StructField("user_id", StringType(), False),
            StructField("tag_ids_array", ArrayType(IntegerType()), True)
        ])
        
        return self.spark.createDataFrame([], schema)
    
    def _createEmptyUserTagsResult(self) -> DataFrame:
        """åˆ›å»ºç©ºçš„ç”¨æˆ·æ ‡ç­¾ç»“æœ"""
        from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType
        
        schema = StructType([
            StructField("user_id", StringType(), False),
            StructField("merged_tag_ids", ArrayType(IntegerType()), True)
        ])
        
        return self.spark.createDataFrame([], schema)
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            self.hiveMeta.clearCache()
            print("ğŸ§¹ TagEngineèµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸  èµ„æºæ¸…ç†å¼‚å¸¸: {e}")
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        self.cleanup()