#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ‡ç­¾è®¡ç®—å¼•æ“
ä¸»è¦è´Ÿè´£æ ‡ç­¾è®¡ç®—æµç¨‹çš„ç¼–æ’å’Œæ‰§è¡Œ
"""
from typing import List, Optional, Dict
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import *

from ..meta.HiveMeta import HiveMeta
from ..meta.MysqlMeta import MysqlMeta
from ..parser.TagRuleParser import TagRuleParser
from .TagGroup import TagGroup
from ..utils.SparkUdfs import merge_with_existing_tags


class TagEngine:
    """æ ‡ç­¾è®¡ç®—å¼•æ“
    
    èŒè´£ï¼š
    1. ç¼–æ’æ•´ä¸ªæ ‡ç­¾è®¡ç®—æµç¨‹
    2. ç®¡ç†Hiveå’ŒMySQLæ•°æ®æº
    3. åè°ƒæ ‡ç­¾åˆ†ç»„å’Œå¹¶è¡Œè®¡ç®—
    4. æ‰§è¡Œæ ‡ç­¾åˆå¹¶å’Œç»“æœå†™å…¥
    """
    
    def __init__(self, spark: SparkSession, hiveConfig: Dict = None, mysqlConfig: Dict = None):
        """åˆå§‹åŒ–æ ‡ç­¾å¼•æ“
        
        Args:
            spark: Sparkä¼šè¯
            hiveConfig: Hiveé…ç½®ï¼ˆå¯é€‰ï¼‰
            mysqlConfig: MySQLé…ç½®
        """
        self.spark = spark
        self.hiveConfig = hiveConfig or {}
        self.mysqlConfig = mysqlConfig
        
        # åˆå§‹åŒ–æ•°æ®æºç®¡ç†å™¨
        self.hiveMeta = HiveMeta(spark)
        self.mysqlMeta = MysqlMeta(spark, mysqlConfig)
        self.ruleParser = TagRuleParser()
        
        print("ğŸš€ TagEngineåˆå§‹åŒ–å®Œæˆ")
    
    def computeTags(self, mode: str = "full", tagIds: Optional[List[int]] = None) -> bool:
        """æ‰§è¡Œæ ‡ç­¾è®¡ç®— - ç®€åŒ–çš„ä¸»æµç¨‹ç¼–æ’
        
        Args:
            mode: è®¡ç®—æ¨¡å¼ï¼ˆfull/specificï¼‰
            tagIds: æŒ‡å®šæ ‡ç­¾IDåˆ—è¡¨ï¼ˆä»…åœ¨specificæ¨¡å¼ä¸‹æœ‰æ•ˆï¼‰
            
        Returns:
            bool: è®¡ç®—æ˜¯å¦æˆåŠŸ
        """
        print(f"ğŸš€ å¼€å§‹æ ‡ç­¾è®¡ç®—ï¼Œæ¨¡å¼: {mode}")
        
        try:
            # 1. åŠ è½½æ ‡ç­¾è§„åˆ™
            rulesDF = self._loadTagRules(tagIds)
            if rulesDF.count() == 0:
                print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„æ ‡ç­¾è§„åˆ™")
                return True
            
            # 2. æ™ºèƒ½åˆ†ç»„ï¼ˆåŸºäºè¡¨ä¾èµ–ï¼‰
            tagGroups = self._analyzeAndGroupTags(rulesDF)
            if not tagGroups:
                print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°å¯è®¡ç®—çš„æ ‡ç­¾ç»„")
                return True
            
            # 3. æµæ°´çº¿å¤„ç†æ‰€æœ‰æ ‡ç­¾ç»„
            success = self._processTagGroupsPipeline(tagGroups, rulesDF)
            
            if success:
                print("âœ… æ ‡ç­¾è®¡ç®—å®Œæˆ")
                self._printStatistics()
            
            return success
            
        except Exception as e:
            print(f"âŒ æ ‡ç­¾è®¡ç®—å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def healthCheck(self) -> bool:
        """ç³»ç»Ÿå¥åº·æ£€æŸ¥ - è®¡ç®—å‰çš„å¿…è¦éªŒè¯
        
        Returns:
            bool: ç³»ç»Ÿæ˜¯å¦å¥åº·
        """
        print("ğŸ” æ‰§è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥...")
        
        try:
            checks = {
                "MySQLè¿æ¥": self.mysqlMeta.testConnection(),
                "Hiveè®¿é—®": self._testHiveAccess(),
                "UDFåŠŸèƒ½": self._testUdfFunctions(),
                "æ ‡ç­¾è§„åˆ™": self._checkTagRules()
            }
            
            allOk = all(checks.values())
            
            print("ğŸ“‹ å¥åº·æ£€æŸ¥ç»“æœ:")
            for check, result in checks.items():
                print(f"   {check}: {'âœ…' if result else 'âŒ'}")
            
            return allOk
            
        except Exception as e:
            print(f"âŒ å¥åº·æ£€æŸ¥å¼‚å¸¸: {e}")
            return False
    
    # ========== ç§æœ‰æ–¹æ³• ==========
    
    def _loadTagRules(self, tagIds: Optional[List[int]] = None) -> DataFrame:
        """åŠ è½½æ ‡ç­¾è§„åˆ™"""
        print("ğŸ“‹ åŠ è½½æ ‡ç­¾è§„åˆ™...")
        return self.mysqlMeta.loadTagRules(tagIds)
    
    def _analyzeAndGroupTags(self, rulesDF: DataFrame) -> List[TagGroup]:
        """åˆ†æä¾èµ–å…³ç³»å¹¶è¿›è¡Œæ™ºèƒ½åˆ†ç»„"""
        print("ğŸ¯ åˆ†ææ ‡ç­¾ä¾èµ–å…³ç³»...")
        
        # åˆ†ææ‰€æœ‰æ ‡ç­¾çš„è¡¨ä¾èµ–
        dependencies = self.ruleParser.analyzeDependencies(rulesDF)
        
        # æ™ºèƒ½åˆ†ç»„
        tagGroups = self.ruleParser.groupTagsByTables(dependencies)
        
        return tagGroups
    
    def _processTagGroupsPipeline(self, tagGroups: List[TagGroup], rulesDF: DataFrame) -> bool:
        """ç®€åŒ–çš„æµæ°´çº¿å¤„ç†ï¼šè®¡ç®—æ ‡ç­¾ç»„å¹¶å†™å…¥MySQL"""
        print(f"ğŸš€ æµæ°´çº¿å¤„ç† {len(tagGroups)} ä¸ªæ ‡ç­¾ç»„...")
        
        successCount = 0
        
        for i, group in enumerate(tagGroups):
            print(f"\nğŸ“¦ å¤„ç†æ ‡ç­¾ç»„ {i+1}/{len(tagGroups)}: {group.name}")
            
            try:
                # è¿‡æ»¤è¯¥ç»„ç›¸å…³çš„æ ‡ç­¾è§„åˆ™
                groupRulesDF = rulesDF.filter(col("tag_id").isin(group.tagIds))
                
                # è®¡ç®—è¯¥ç»„æ ‡ç­¾
                groupResult = group.computeTags(self.hiveMeta, groupRulesDF)
                
                if groupResult.count() == 0:
                    print(f"   âš ï¸  æ ‡ç­¾ç»„ {group.name} æ— åŒ¹é…ç”¨æˆ·ï¼Œè·³è¿‡")
                    successCount += 1
                    continue
                
                # åˆå¹¶å¹¶å†™å…¥MySQL
                if self._mergeAndSaveGroup(groupResult, group.name):
                    print(f"   âœ… æ ‡ç­¾ç»„ {group.name} å¤„ç†å®Œæˆ")
                    successCount += 1
                
                # æ¸…ç†ç¼“å­˜
                self.hiveMeta.clearGroupCache(group.requiredTables)
                
            except Exception as e:
                print(f"   âŒ æ ‡ç­¾ç»„ {group.name} å¤„ç†å¤±è´¥: {e}")
        
        return successCount == len(tagGroups)
    
    def _mergeAndSaveGroup(self, groupResult: DataFrame, groupName: str) -> bool:
        """åˆå¹¶æ ‡ç­¾å¹¶ä¿å­˜åˆ°MySQL"""
        try:
            # åŠ è½½ç°æœ‰æ ‡ç­¾
            existingTagsDF = self.mysqlMeta.loadExistingTags()
            
            # LEFT JOIN åˆå¹¶
            joinedDF = groupResult.alias("new").join(
                existingTagsDF.alias("existing"),
                col("new.user_id") == col("existing.user_id"),
                "left"
            )
            
            # ä½¿ç”¨SparkUdfsæ¨¡å—åˆå¹¶æ ‡ç­¾
            from ..utils.SparkUdfs import array_to_json
            finalDF = joinedDF.withColumn(
                "final_tag_ids",
                merge_with_existing_tags(
                    col("new.tag_ids_array"),
                    col("existing.existing_tag_ids")
                )
            ).withColumn(
                "final_tag_ids_json",
                array_to_json(col("final_tag_ids"))
            ).select(
                col("new.user_id").alias("user_id"),
                col("final_tag_ids_json")
            )
            
            # å†™å…¥MySQL
            success = self.mysqlMeta.writeTagResults(finalDF)
            
            if success:
                userCount = finalDF.count()
                print(f"   âœ… {groupName}: {userCount} ä¸ªç”¨æˆ·")
            
            return success
            
        except Exception as e:
            print(f"   âŒ {groupName} ä¿å­˜å¤±è´¥: {e}")
            return False
    
    
    def _testHiveAccess(self) -> bool:
        """æµ‹è¯•Hiveè¡¨è®¿é—®"""
        try:
            # å°è¯•åˆ—å‡ºè¡¨
            tables = self.spark.sql("SHOW TABLES").collect()
            print(f"   âœ… Hiveè®¿é—®æ­£å¸¸ï¼Œå‘ç° {len(tables)} ä¸ªè¡¨")
            return True
        except Exception as e:
            print(f"   âŒ Hiveè®¿é—®å¤±è´¥: {e}")
            return False
    
    def _testUdfFunctions(self) -> bool:
        """æµ‹è¯•UDFå‡½æ•°"""
        try:
            # åˆ›å»ºæµ‹è¯•DataFrame
            testData = [("user1", [1, 2, 3]), ("user2", [2, 3, 4])]
            testDF = self.spark.createDataFrame(testData, ["user_id", "tags"])
            
            # æµ‹è¯•SparkUdfsæ¨¡å—å‡½æ•°
            from ..utils.SparkUdfs import merge_user_tags
            resultDF = testDF.withColumn(
                "merged_tags",
                merge_user_tags(col("tags"))
            )
            
            resultCount = resultDF.count()
            print(f"   âœ… UDFå‡½æ•°æµ‹è¯•é€šè¿‡ï¼Œå¤„ç† {resultCount} æ¡æ•°æ®")
            return True
            
        except Exception as e:
            print(f"   âŒ UDFå‡½æ•°æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def _checkTagRules(self) -> bool:
        """æ£€æŸ¥æ ‡ç­¾è§„åˆ™"""
        try:
            rulesDF = self.mysqlMeta.loadTagRules()
            ruleCount = rulesDF.count()
            
            if ruleCount > 0:
                print(f"   âœ… æ ‡ç­¾è§„åˆ™æ£€æŸ¥é€šè¿‡ï¼Œå‘ç° {ruleCount} ä¸ªæ´»è·ƒæ ‡ç­¾")
                return True
            else:
                print("   âš ï¸  æ²¡æœ‰å‘ç°æ´»è·ƒçš„æ ‡ç­¾è§„åˆ™")
                return False
                
        except Exception as e:
            print(f"   âŒ æ ‡ç­¾è§„åˆ™æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def _printStatistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = self.mysqlMeta.getTagStatistics()
            if stats:
                print("\nğŸ“Š æ ‡ç­¾ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯:")
                print(f"   æ´»è·ƒæ ‡ç­¾æ•°: {stats.get('activeTagCount', 0)}")
                print(f"   æœ‰æ ‡ç­¾ç”¨æˆ·æ•°: {stats.get('taggedUserCount', 0)}")
                print(f"   æ€»æ ‡ç­¾æ•°: {stats.get('totalTagCount', 0)}")
                print(f"   å¹³å‡æ¯ç”¨æˆ·æ ‡ç­¾æ•°: {stats.get('avgTagsPerUser', 0)}")
        except:
            print("   âš ï¸  æ— æ³•è·å–ç»Ÿè®¡ä¿¡æ¯")
    
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            self.hiveMeta.clearCache()
            print("ğŸ§¹ TagEngineèµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸  èµ„æºæ¸…ç†å¼‚å¸¸: {e}")