#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ‡ç­¾è®¡ç®—å¼•æ“
ä¸»è¦è´Ÿè´£æ ‡ç­¾è®¡ç®—æµç¨‹çš„ç¼–æ’å’Œæ‰§è¡Œ
"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

from ..meta.HiveMeta import HiveMeta
from ..meta.MysqlMeta import MysqlMeta
from ..parser.TagRuleParser import TagRuleParser
from .TagGroup import TagGroup
from ..utils.TagUdfs import tagUdfs


class TagEngine:
    """æ ‡ç­¾è®¡ç®—å¼•æ“
    
    èŒè´£ï¼š
    1. ç¼–æ’æ•´ä¸ªæ ‡ç­¾è®¡ç®—æµç¨‹
    2. ç®¡ç†Hiveå’ŒMySQLæ•°æ®æº
    3. åè°ƒæ ‡ç­¾åˆ†ç»„å’Œå¹¶è¡Œè®¡ç®—
    4. æ‰§è¡Œæ ‡ç­¾åˆå¹¶å’Œç»“æœå†™å…¥
    """
    
    def __init__(self, spark: SparkSession, hiveConfig: Dict = None, mysqlConfig: Dict = None):
        """åˆå§‹åŒ–æ ‡ç­¾å¼•æ“
        
        Args:
            spark: Sparkä¼šè¯
            hiveConfig: Hiveé…ç½®ï¼ˆå¯é€‰ï¼‰
            mysqlConfig: MySQLé…ç½®
        """
        self.spark = spark
        self.hiveConfig = hiveConfig or {}
        self.mysqlConfig = mysqlConfig
        
        # åˆå§‹åŒ–æ•°æ®æºç®¡ç†å™¨
        self.hiveMeta = HiveMeta(spark)
        self.mysqlMeta = MysqlMeta(spark, mysqlConfig)
        self.ruleParser = TagRuleParser()
        
        print("ğŸš€ TagEngineåˆå§‹åŒ–å®Œæˆ")
    
    def computeTags(self, mode: str = "full", tagIds: Optional[List[int]] = None) -> bool:
        """æ‰§è¡Œæ ‡ç­¾è®¡ç®—
        
        Args:
            mode: è®¡ç®—æ¨¡å¼ï¼ˆfull/specificï¼‰
            tagIds: æŒ‡å®šæ ‡ç­¾IDåˆ—è¡¨ï¼ˆä»…åœ¨specificæ¨¡å¼ä¸‹æœ‰æ•ˆï¼‰
            
        Returns:
            bool: è®¡ç®—æ˜¯å¦æˆåŠŸ
        """
        print(f"ğŸš€ å¼€å§‹æ ‡ç­¾è®¡ç®—ï¼Œæ¨¡å¼: {mode}")
        
        try:
            # 1. åŠ è½½æ ‡ç­¾è§„åˆ™
            rulesDF = self._loadTagRules(tagIds)
            if rulesDF.count() == 0:
                print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„æ ‡ç­¾è§„åˆ™")
                return True
            
            # 2. åˆ†æä¾èµ–å…³ç³»å¹¶æ™ºèƒ½åˆ†ç»„
            tagGroups = self._analyzeAndGroupTags(rulesDF)
            if not tagGroups:
                print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°å¯è®¡ç®—çš„æ ‡ç­¾ç»„")
                return True
            
            # 3. å¹¶è¡Œè®¡ç®—æ‰€æœ‰æ ‡ç­¾ç»„
            allResults = self._computeAllTagGroups(tagGroups, rulesDF)
            
            # 4. åˆå¹¶æ ‡ç­¾ç»“æœ
            finalResults = self._mergeAllTagResults(allResults)
            
            # 5. ä¸MySQLç°æœ‰æ ‡ç­¾åˆå¹¶å¹¶å†™å…¥
            success = self._mergeWithExistingAndSave(finalResults)
            
            if success:
                print("âœ… æ ‡ç­¾è®¡ç®—å®Œæˆ")
                self._printStatistics()
            else:
                print("âŒ æ ‡ç­¾è®¡ç®—å¤±è´¥")
            
            return success
            
        except Exception as e:
            print(f"âŒ æ ‡ç­¾è®¡ç®—å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def healthCheck(self) -> bool:
        """å¥åº·æ£€æŸ¥
        
        Returns:
            bool: ç³»ç»Ÿæ˜¯å¦å¥åº·
        """
        print("ğŸ” æ‰§è¡Œæ ‡ç­¾ç³»ç»Ÿå¥åº·æ£€æŸ¥...")
        
        try:
            # 1. æµ‹è¯•MySQLè¿æ¥
            mysqlOk = self.mysqlMeta.testConnection()
            
            # 2. æµ‹è¯•Hiveè¡¨è®¿é—®ï¼ˆä½¿ç”¨ç®€å•è¡¨æµ‹è¯•ï¼‰
            hiveOk = self._testHiveAccess()
            
            # 3. æµ‹è¯•UDFåŠŸèƒ½
            udfOk = self._testUdfFunctions()
            
            # 4. æ£€æŸ¥æ ‡ç­¾è§„åˆ™
            rulesOk = self._checkTagRules()
            
            allOk = mysqlOk and hiveOk and udfOk and rulesOk
            
            if allOk:
                print("âœ… ç³»ç»Ÿå¥åº·æ£€æŸ¥é€šè¿‡")
            else:
                print("âŒ ç³»ç»Ÿå¥åº·æ£€æŸ¥å¤±è´¥")
                print(f"   MySQL: {'âœ…' if mysqlOk else 'âŒ'}")
                print(f"   Hive: {'âœ…' if hiveOk else 'âŒ'}")
                print(f"   UDF: {'âœ…' if udfOk else 'âŒ'}")
                print(f"   Rules: {'âœ…' if rulesOk else 'âŒ'}")
            
            return allOk
            
        except Exception as e:
            print(f"âŒ å¥åº·æ£€æŸ¥å¼‚å¸¸: {e}")
            return False
    
    # ========== ç§æœ‰æ–¹æ³• ==========
    
    def _loadTagRules(self, tagIds: Optional[List[int]] = None) -> DataFrame:
        """åŠ è½½æ ‡ç­¾è§„åˆ™"""
        print("ğŸ“‹ åŠ è½½æ ‡ç­¾è§„åˆ™...")
        return self.mysqlMeta.loadTagRules(tagIds)
    
    def _analyzeAndGroupTags(self, rulesDF: DataFrame) -> List[TagGroup]:
        """åˆ†æä¾èµ–å…³ç³»å¹¶è¿›è¡Œæ™ºèƒ½åˆ†ç»„"""
        print("ğŸ¯ åˆ†ææ ‡ç­¾ä¾èµ–å…³ç³»...")
        
        # åˆ†ææ‰€æœ‰æ ‡ç­¾çš„è¡¨ä¾èµ–
        dependencies = self.ruleParser.analyzeDependencies(rulesDF)
        
        # æ™ºèƒ½åˆ†ç»„
        tagGroups = self.ruleParser.groupTagsByTables(dependencies)
        
        return tagGroups
    
    def _computeAllTagGroups(self, tagGroups: List[TagGroup], rulesDF: DataFrame) -> List[DataFrame]:
        """å¹¶è¡Œè®¡ç®—æ‰€æœ‰æ ‡ç­¾ç»„"""
        print(f"ğŸš€ å¹¶è¡Œè®¡ç®— {len(tagGroups)} ä¸ªæ ‡ç­¾ç»„...")
        
        allResults = []
        
        for i, group in enumerate(tagGroups):
            print(f"   ğŸ“¦ è®¡ç®—æ ‡ç­¾ç»„ {i+1}/{len(tagGroups)}: {group.name}")
            
            try:
                groupResult = group.computeTags(self.hiveMeta, self.mysqlMeta, rulesDF)
                allResults.append(groupResult)
                
            except Exception as e:
                print(f"   âŒ æ ‡ç­¾ç»„ {group.name} è®¡ç®—å¤±è´¥: {e}")
                # æ·»åŠ ç©ºç»“æœï¼Œé¿å…å½±å“å…¶ä»–ç»„
                emptyResult = self._createEmptyGroupResult()
                allResults.append(emptyResult)
        
        print(f"âœ… æ‰€æœ‰æ ‡ç­¾ç»„è®¡ç®—å®Œæˆï¼ŒæˆåŠŸ: {len(allResults)} ä¸ª")
        return allResults
    
    def _mergeAllTagResults(self, allResults: List[DataFrame]) -> DataFrame:
        """åˆå¹¶æ‰€æœ‰æ ‡ç­¾ç»„çš„ç»“æœ"""
        print("ğŸ”€ åˆå¹¶æ‰€æœ‰æ ‡ç­¾ç»„ç»“æœ...")
        
        if not allResults:
            return self._createEmptyUserTagsResult()
        
        # å°è¯•åˆå¹¶ï¼Œå¦‚æœåˆå¹¶åä¸ºç©ºå†å¤„ç†
        try:
            mergedDF = allResults[0]
            for resultDF in allResults[1:]:
                mergedDF = mergedDF.union(resultDF)
            
            # åªåœ¨æœ€åæ£€æŸ¥ä¸€æ¬¡
            finalCount = mergedDF.count()
            if finalCount == 0:
                print("   âš ï¸  æ‰€æœ‰æ ‡ç­¾ç»„ç»“æœéƒ½ä¸ºç©º")
                return self._createEmptyUserTagsResult()
                
            # æŒ‰ç”¨æˆ·é‡æ–°èšåˆæ‰€æœ‰æ ‡ç­¾
            finalDF = mergedDF.groupBy("user_id").agg(
                tagUdfs.mergeUserTags(
                    flatten(collect_list("tag_ids_array"))
                ).alias("merged_tag_ids")
            )
            
            print(f"   âœ… æ ‡ç­¾ç»“æœåˆå¹¶å®Œæˆ: {finalDF.count()} ä¸ªç”¨æˆ·")
            return finalDF
            
        except Exception as e:
            print(f"   âŒ åˆå¹¶è¿‡ç¨‹å¼‚å¸¸: {e}")
            return self._createEmptyUserTagsResult()
    
    def _mergeWithExistingAndSave(self, newTagsDF: DataFrame) -> bool:
        """ä¸MySQLç°æœ‰æ ‡ç­¾åˆå¹¶å¹¶ä¿å­˜"""
        print("ğŸ’¾ ä¸ç°æœ‰æ ‡ç­¾åˆå¹¶å¹¶ä¿å­˜...")
        
        try:
            # åŠ è½½ç°æœ‰æ ‡ç­¾
            existingTagsDF = self.mysqlMeta.loadExistingTags()
            
            # LEFT JOIN åˆå¹¶
            joinedDF = newTagsDF.alias("new").join(
                existingTagsDF.alias("existing"),
                col("new.user_id") == col("existing.user_id"),
                "left"
            )
            
            # ä½¿ç”¨UDFåˆå¹¶æ ‡ç­¾
            finalDF = joinedDF.withColumn(
                "final_tag_ids",
                tagUdfs.mergeWithExistingTags(
                    col("new.merged_tag_ids"),
                    col("existing.existing_tag_ids")
                )
            ).withColumn(
                "final_tag_ids_json",
                tagUdfs.arrayToJson(col("final_tag_ids"))
            ).select(
                col("new.user_id").alias("user_id"),
                col("final_tag_ids_json")
            )
            
            # å†™å…¥MySQL
            success = self.mysqlMeta.writeTagResults(finalDF)
            
            if success:
                print(f"   âœ… æ ‡ç­¾ç»“æœä¿å­˜æˆåŠŸ: {finalDF.count()} ä¸ªç”¨æˆ·")
            
            return success
            
        except Exception as e:
            print(f"   âŒ æ ‡ç­¾åˆå¹¶ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def _testHiveAccess(self) -> bool:
        """æµ‹è¯•Hiveè¡¨è®¿é—®"""
        try:
            # å°è¯•åˆ—å‡ºè¡¨
            tables = self.spark.sql("SHOW TABLES").collect()
            print(f"   âœ… Hiveè®¿é—®æ­£å¸¸ï¼Œå‘ç° {len(tables)} ä¸ªè¡¨")
            return True
        except Exception as e:
            print(f"   âŒ Hiveè®¿é—®å¤±è´¥: {e}")
            return False
    
    def _testUdfFunctions(self) -> bool:
        """æµ‹è¯•UDFå‡½æ•°"""
        try:
            # åˆ›å»ºæµ‹è¯•DataFrame
            testData = [("user1", [1, 2, 3]), ("user2", [2, 3, 4])]
            testDF = self.spark.createDataFrame(testData, ["user_id", "tags"])
            
            # æµ‹è¯•UDF
            resultDF = testDF.withColumn(
                "merged_tags",
                tagUdfs.mergeUserTags(col("tags"))
            )
            
            resultCount = resultDF.count()
            print(f"   âœ… UDFå‡½æ•°æµ‹è¯•é€šè¿‡ï¼Œå¤„ç† {resultCount} æ¡æ•°æ®")
            return True
            
        except Exception as e:
            print(f"   âŒ UDFå‡½æ•°æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def _checkTagRules(self) -> bool:
        """æ£€æŸ¥æ ‡ç­¾è§„åˆ™"""
        try:
            rulesDF = self.mysqlMeta.loadTagRules()
            ruleCount = rulesDF.count()
            
            if ruleCount > 0:
                print(f"   âœ… æ ‡ç­¾è§„åˆ™æ£€æŸ¥é€šè¿‡ï¼Œå‘ç° {ruleCount} ä¸ªæ´»è·ƒæ ‡ç­¾")
                return True
            else:
                print("   âš ï¸  æ²¡æœ‰å‘ç°æ´»è·ƒçš„æ ‡ç­¾è§„åˆ™")
                return False
                
        except Exception as e:
            print(f"   âŒ æ ‡ç­¾è§„åˆ™æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def _printStatistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = self.mysqlMeta.getTagStatistics()
            if stats:
                print("\nğŸ“Š æ ‡ç­¾ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯:")
                print(f"   æ´»è·ƒæ ‡ç­¾æ•°: {stats.get('activeTagCount', 0)}")
                print(f"   æœ‰æ ‡ç­¾ç”¨æˆ·æ•°: {stats.get('taggedUserCount', 0)}")
                print(f"   æ€»æ ‡ç­¾æ•°: {stats.get('totalTagCount', 0)}")
                print(f"   å¹³å‡æ¯ç”¨æˆ·æ ‡ç­¾æ•°: {stats.get('avgTagsPerUser', 0)}")
        except:
            print("   âš ï¸  æ— æ³•è·å–ç»Ÿè®¡ä¿¡æ¯")
    
    def _createEmptyGroupResult(self) -> DataFrame:
        """åˆ›å»ºç©ºçš„æ ‡ç­¾ç»„ç»“æœ"""
        from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType
        
        schema = StructType([
            StructField("user_id", StringType(), False),
            StructField("tag_ids_array", ArrayType(IntegerType()), True)
        ])
        
        return self.spark.createDataFrame([], schema)
    
    def _createEmptyUserTagsResult(self) -> DataFrame:
        """åˆ›å»ºç©ºçš„ç”¨æˆ·æ ‡ç­¾ç»“æœ"""
        from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType
        
        schema = StructType([
            StructField("user_id", StringType(), False),
            StructField("merged_tag_ids", ArrayType(IntegerType()), True)
        ])
        
        return self.spark.createDataFrame([], schema)
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            self.hiveMeta.clearCache()
            print("ğŸ§¹ TagEngineèµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸  èµ„æºæ¸…ç†å¼‚å¸¸: {e}")
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        self.cleanup()