#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ‡ç­¾è®¡ç®—ç»„
å°†ç›¸åŒè¡¨ä¾èµ–çš„æ ‡ç­¾å½’ä¸ºä¸€ç»„ï¼Œå®žçŽ°å¹¶è¡Œé«˜æ•ˆè®¡ç®—
"""
from typing import List, Dict, Tuple
from pyspark.sql import DataFrame
from pyspark.sql.functions import *

from ..parser.TagRuleParser import TagRuleParser


class TagGroup:
    """æ ‡ç­¾è®¡ç®—ç»„
    
    èŒè´£ï¼š
    1. ç®¡ç†ä¸€ç»„å…·æœ‰ç›¸åŒè¡¨ä¾èµ–çš„æ ‡ç­¾
    2. æ‰§è¡Œè¯¥ç»„æ ‡ç­¾çš„å¹¶è¡Œè®¡ç®—
    3. ä¼˜åŒ–è¡¨è¯»å–å’ŒJOINæ“ä½œ
    4. ä½¿ç”¨SparkåŽŸç”Ÿå‡½æ•°è¿›è¡Œæ ‡ç­¾åˆå¹¶
    """
    
    def __init__(self, tagIds: List[int], requiredTables: List[str]):
        """åˆå§‹åŒ–æ ‡ç­¾è®¡ç®—ç»„
        
        Args:
            tagIds: æ ‡ç­¾IDåˆ—è¡¨
            requiredTables: æ‰€éœ€çš„è¡¨ååˆ—è¡¨
        """
        self.tagIds = tagIds
        self.requiredTables = requiredTables
        self.name = f"Group_{len(tagIds)}tags_{len(requiredTables)}tables"
        
        print(f"ðŸ“¦ åˆ›å»ºæ ‡ç­¾ç»„: {self.name}")
        print(f"   ðŸ·ï¸  æ ‡ç­¾: {tagIds}")
        print(f"   ðŸ“Š ä¾èµ–è¡¨: {requiredTables}")
    
    def computeTags(self, hiveMeta, rulesDF: DataFrame) -> Tuple[DataFrame, List[int]]:
        """è®¡ç®—è¯¥ç»„æ‰€æœ‰æ ‡ç­¾ - å…±äº«ç»„å†…è¡¨å†…å­˜ï¼Œå¹¶è¡Œè®¡ç®—åŽç›´æŽ¥èšåˆï¼Œè·Ÿè¸ªå¤±è´¥æ ‡ç­¾
        
        Args:
            hiveMeta: Hiveæ•°æ®æºç®¡ç†å™¨
            rulesDF: æ ‡ç­¾è§„åˆ™DataFrame
            
        Returns:
            Tuple[DataFrame, List[int]]: (æ ‡ç­¾è®¡ç®—ç»“æžœDataFrame, å¤±è´¥çš„æ ‡ç­¾IDåˆ—è¡¨)
        """
        print(f"ðŸš€ å¼€å§‹è®¡ç®—æ ‡ç­¾ç»„: {self.name}")
        
        try:
            # 1. è¿‡æ»¤è¯¥ç»„ç›¸å…³çš„æ ‡ç­¾è§„åˆ™
            groupRulesDF = rulesDF.filter(col("tag_id").isin(self.tagIds))
            print(f"   ðŸ“‹ è¯¥ç»„æ ‡ç­¾è§„åˆ™æ•°: {groupRulesDF.count()}")
            
            # 2. åˆ†æžå­—æ®µä¾èµ–
            fieldDependencies = self._analyzeFieldDependencies(groupRulesDF)
            
            # 3. æž„å»ºæ ‡ç­¾IDåˆ°ä¾èµ–è¡¨çš„æ˜ å°„
            tagTableMapping = self._buildTagTableMapping(groupRulesDF)
            
            # 4. ðŸš€ å…³é”®æ”¹è¿›ï¼šä½¿ç”¨å®¹é”™åŠ è½½æ–¹æ³•
            joinedDF, failed_tag_ids = hiveMeta.loadAndJoinTablesWithFailureTracking(
                self.requiredTables, tagTableMapping, fieldDependencies)
            
            # èŽ·å–æˆåŠŸåŠ è½½çš„è¡¨åˆ—è¡¨
            successful_tables = hiveMeta.getSuccessfulTables()

            if failed_tag_ids:
                print(f"   âš ï¸  è¡¨åŠ è½½å¤±è´¥å¯¼è‡´ {len(failed_tag_ids)} ä¸ªæ ‡ç­¾å¤±è´¥: {failed_tag_ids}")
                print(f"   ðŸ“‹ æˆåŠŸåŠ è½½çš„è¡¨: {successful_tables}")
                print(f"   âŒ {hiveMeta.getFailureSummary()}")
                
                # æ›´æ–°å¯è®¡ç®—çš„æ ‡ç­¾åˆ—è¡¨
                available_tag_ids = [tag_id for tag_id in self.tagIds if tag_id not in failed_tag_ids]
                if not available_tag_ids:
                    print(f"   âŒ æ‰€æœ‰æ ‡ç­¾éƒ½å› è¡¨åŠ è½½å¤±è´¥è€Œæ— æ³•è®¡ç®—")
                    return self._createEmptyResult(hiveMeta.spark), failed_tag_ids
                print(f"   âœ… å¯è®¡ç®—çš„æ ‡ç­¾: {available_tag_ids}")
            else:
                available_tag_ids = self.tagIds
                successful_tables = self.requiredTables
                print(f"   âœ… æ‰€æœ‰è¡¨åŠ è½½æˆåŠŸ")
            
            print(f"   ðŸ”— ç»„å†…å…±äº«è¡¨JOINå®Œæˆï¼Œç”¨æˆ·æ•°: {joinedDF.count()}")
            
            # 5. ðŸš€ å…³é”®ä¼˜åŒ–ï¼šä¸ºå¯ç”¨æ ‡ç­¾å¹¶è¡Œè®¡ç®—ï¼Œä½¿ç”¨æˆåŠŸçš„è¡¨åˆ—è¡¨ç”ŸæˆSQL
            # åªè®¡ç®—æ²¡æœ‰å¤±è´¥çš„æ ‡ç­¾ï¼Œå¹¶ä¸”åªä½¿ç”¨æˆåŠŸåŠ è½½çš„è¡¨ç”ŸæˆSQLæ¡ä»¶
            available_rules_df = groupRulesDF.filter(col("tag_id").isin(available_tag_ids))
            userTagsDF = self._computeAllTagsParallelAndAggregate(joinedDF, available_rules_df, successful_tables)
            
            failed_display = failed_tag_ids if failed_tag_ids else "[None]"
            successful_tag_ids = [tag_id for tag_id in self.tagIds if tag_id not in failed_tag_ids]
            successful_display = successful_tag_ids if successful_tag_ids else "[None]"
            print(f"âœ… æ ‡ç­¾ç»„è®¡ç®—å®Œæˆ: {userTagsDF.count()} ä¸ªç”¨æˆ·ï¼ŒæˆåŠŸæ ‡ç­¾: {successful_display}ï¼Œå¤±è´¥æ ‡ç­¾: {failed_display}")
            return userTagsDF, failed_tag_ids
            
        except Exception as e:
            print(f"âŒ æ ‡ç­¾ç»„è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # å¦‚æžœæ•´ä¸ªç»„è®¡ç®—å¤±è´¥ï¼Œæ‰€æœ‰æ ‡ç­¾éƒ½æ ‡è®°ä¸ºå¤±è´¥
            return self._createEmptyResult(hiveMeta.spark), self.tagIds
    
    def _analyzeFieldDependencies(self, rulesDF: DataFrame) -> Dict[str, List[str]]:
        """åˆ†æžè¯¥ç»„æ ‡ç­¾çš„å­—æ®µä¾èµ–å…³ç³»"""
        parser = TagRuleParser()
        fieldDependencies = parser.analyzeFieldDependencies(rulesDF)
        
        return fieldDependencies
    
    def _buildTagTableMapping(self, rulesDF: DataFrame) -> Dict[int, List[str]]:
        """æž„å»ºæ ‡ç­¾IDåˆ°ä¾èµ–è¡¨çš„æ˜ å°„å…³ç³»
        
        Args:
            rulesDF: æ ‡ç­¾è§„åˆ™DataFrame
            
        Returns:
            Dict[int, List[str]]: {tag_id: [dependent_tables]}
        """
        parser = TagRuleParser()
        tagTableMapping = {}
        
        # æ”¶é›†è§„åˆ™åˆ°Driveråˆ†æž
        rules = rulesDF.select("tag_id", "rule_conditions").collect()
        
        for row in rules:
            tag_id = row['tag_id']
            rule_conditions = row['rule_conditions']
            
            # åˆ†æžè¯¥æ ‡ç­¾ä¾èµ–çš„è¡¨
            dependencies = parser._extractTablesFromRule(rule_conditions)
            dependent_tables = list(dependencies)
            
            tagTableMapping[tag_id] = dependent_tables
            print(f"   ðŸ·ï¸  æ ‡ç­¾ {tag_id} ä¾èµ–è¡¨: {dependent_tables}")
        
        return tagTableMapping
    
    def _computeAllTagsParallelAndAggregate(self, joinedDF: DataFrame, groupRulesDF: DataFrame, available_tables: List[str] = None) -> DataFrame:
        """å¹¶è¡Œè®¡ç®—è¯¥ç»„æ‰€æœ‰æ ‡ç­¾å¹¶ç›´æŽ¥èšåˆ - ä¸€æ­¥åˆ°ä½çš„ä¼˜åŒ–æ–¹æ¡ˆï¼Œåªä½¿ç”¨æˆåŠŸåŠ è½½çš„è¡¨"""
        print(f"   ðŸŽ¯ å¹¶è¡Œè®¡ç®—å¹¶èšåˆ {len(self.tagIds)} ä¸ªæ ‡ç­¾...")
        
        # ä½¿ç”¨æˆåŠŸåŠ è½½çš„è¡¨åˆ—è¡¨ï¼Œå¦‚æžœæ²¡æœ‰æä¾›åˆ™ä½¿ç”¨åŽŸæ¥çš„æ‰€æœ‰è¡¨
        tables_to_use = available_tables if available_tables is not None else self.requiredTables
        print(f"   ðŸ“‹ ä½¿ç”¨æˆåŠŸåŠ è½½çš„è¡¨è¿›è¡ŒSQLç”Ÿæˆ: {tables_to_use}")
        
        # æ”¶é›†è§„åˆ™åˆ°Driverè¿›è¡ŒSQLæ¡ä»¶è§£æž
        rules = groupRulesDF.select("tag_id", "rule_conditions").collect()
        
        # è§£æžæ‰€æœ‰è§„åˆ™ä¸ºSQLæ¡ä»¶
        parser = TagRuleParser()
        
        # æž„å»ºæ‰€æœ‰æ ‡ç­¾çš„å¹¶è¡Œè®¡ç®—è¡¨è¾¾å¼
        tag_conditions = []
        
        for row in rules:
            tagId = row['tag_id']
            ruleConditions = row['rule_conditions']
            
            print(f"      ðŸ·ï¸  è§£æžæ ‡ç­¾ {tagId} è§„åˆ™...")
            
            # ðŸš€ å…³é”®ä¿®å¤ï¼šåªä½¿ç”¨æˆåŠŸåŠ è½½çš„è¡¨ç”ŸæˆSQLæ¡ä»¶
            sqlCondition = parser.parseRuleToSql(ruleConditions, tables_to_use)
            print(f"         ðŸ” æ ‡ç­¾ {tagId} SQLæ¡ä»¶: {sqlCondition}")
            
            # ä¸ºæ¯ä¸ªæ ‡ç­¾æž„å»ºæ¡ä»¶è¡¨è¾¾å¼
            if sqlCondition and sqlCondition.strip() and sqlCondition != "1=0":
                # æœ‰æ•ˆè§„åˆ™ï¼šæž„å»ºwhenè¡¨è¾¾å¼
                tag_conditions.append({
                    'tag_id': tagId,
                    'condition': sqlCondition
                })
            else:
                print(f"         âš ï¸  æ ‡ç­¾ {tagId} æ— æœ‰æ•ˆè§„åˆ™æ¡ä»¶ï¼Œè·³è¿‡")
        
        if not tag_conditions:
            print("         âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„æ ‡ç­¾è§„åˆ™")
            return self._createEmptyResult(joinedDF.sparkSession)
        
        # ðŸš€ å…³é”®æ”¹è¿›ï¼šä½¿ç”¨ç‹¬ç«‹å·¥å…·æ¨¡å—æž„å»ºå¹¶è¡Œè¡¨è¾¾å¼
        print(f"   âš¡ ä½¿ç”¨å¹¶è¡Œè¡¨è¾¾å¼å·¥å…·æž„å»º {len(tag_conditions)} ä¸ªæ ‡ç­¾æ¡ä»¶...")
        
        # ä½¿ç”¨ç‹¬ç«‹å·¥å…·æ¨¡å—æž„å»ºå¹¶è¡Œæ ‡ç­¾è¡¨è¾¾å¼
        from ..utils.tagExpressionUtils import buildParallelTagExpression
        combined_tags_expr = buildParallelTagExpression(tag_conditions)
        
        # ä¸€æ¬¡æ€§ä¸ºæ‰€æœ‰ç”¨æˆ·è®¡ç®—å…¶åŒ¹é…çš„æ ‡ç­¾æ•°ç»„ï¼Œå¹¶è¿‡æ»¤æŽ‰ç©ºæ•°ç»„ç”¨æˆ·
        # ðŸ”§ å…³é”®ä¿®å¤ï¼šå…ˆè®¡ç®—æ ‡ç­¾ï¼Œå†é€‰æ‹©éœ€è¦çš„å­—æ®µï¼Œé¿å…è¿‡æ—©ä¸¢å¼ƒä¸šåŠ¡å­—æ®µ
        userTagsDF = joinedDF.withColumn("tag_ids_array", combined_tags_expr) \
                           .select("user_id", "tag_ids_array") \
                           .filter(size(col("tag_ids_array")) > 0)
        
        # ç»Ÿè®¡ç»“æžœ
        try:
            userCount = userTagsDF.count()
            print(f"   âœ… å¹¶è¡Œè®¡ç®—å¹¶èšåˆå®Œæˆ: {userCount} ä¸ªæœ‰æ ‡ç­¾ç”¨æˆ·")
        except:
            print(f"   âœ… å¹¶è¡Œè®¡ç®—å¹¶èšåˆå®Œæˆ")
        
        return userTagsDF
    
    def _createEmptyResult(self, spark) -> DataFrame:
        """åˆ›å»ºç©ºçš„è®¡ç®—ç»“æžœ"""
        from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType
        
        schema = StructType([
            StructField("user_id", StringType(), False),
            StructField("tag_ids_array", ArrayType(IntegerType()), True)
        ])
        
        return spark.createDataFrame([], schema)
    
    
    def __str__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return f"TagGroup(tags={self.tagIds}, tables={self.requiredTables})"
    
    def __repr__(self) -> str:
        """å¯¹è±¡è¡¨ç¤º"""
        return self.__str__()