#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ‡ç­¾è®¡ç®—ç»„
å°†ç›¸åŒè¡¨ä¾èµ–çš„æ ‡ç­¾å½’ä¸ºä¸€ç»„ï¼Œå®žçŽ°å¹¶è¡Œé«˜æ•ˆè®¡ç®—
"""
from typing import List, Dict
from pyspark.sql import DataFrame
from pyspark.sql.functions import *


class TagGroup:
    """æ ‡ç­¾è®¡ç®—ç»„
    
    èŒè´£ï¼š
    1. ç®¡ç†ä¸€ç»„å…·æœ‰ç›¸åŒè¡¨ä¾èµ–çš„æ ‡ç­¾
    2. æ‰§è¡Œè¯¥ç»„æ ‡ç­¾çš„å¹¶è¡Œè®¡ç®—
    3. ä¼˜åŒ–è¡¨è¯»å–å’ŒJOINæ“ä½œ
    4. ä½¿ç”¨SparkåŽŸç”Ÿå‡½æ•°è¿›è¡Œæ ‡ç­¾åˆå¹¶
    """
    
    def __init__(self, tagIds: List[int], requiredTables: List[str]):
        """åˆå§‹åŒ–æ ‡ç­¾è®¡ç®—ç»„
        
        Args:
            tagIds: æ ‡ç­¾IDåˆ—è¡¨
            requiredTables: æ‰€éœ€çš„è¡¨ååˆ—è¡¨
        """
        self.tagIds = tagIds
        self.requiredTables = requiredTables
        self.name = f"Group_{len(tagIds)}tags_{len(requiredTables)}tables"
        
        print(f"ðŸ“¦ åˆ›å»ºæ ‡ç­¾ç»„: {self.name}")
        print(f"   ðŸ·ï¸  æ ‡ç­¾: {tagIds}")
        print(f"   ðŸ“Š ä¾èµ–è¡¨: {requiredTables}")
    
    def computeTags(self, hiveMeta, rulesDF: DataFrame) -> DataFrame:
        """è®¡ç®—è¯¥ç»„æ‰€æœ‰æ ‡ç­¾ - å…±äº«ç»„å†…è¡¨å†…å­˜ï¼Œå¹¶è¡Œè®¡ç®—åŽç›´æŽ¥èšåˆ
        
        Args:
            hiveMeta: Hiveæ•°æ®æºç®¡ç†å™¨
            rulesDF: æ ‡ç­¾è§„åˆ™DataFrame
            
        Returns:
            DataFrame: æ ‡ç­¾è®¡ç®—ç»“æžœï¼ŒåŒ…å« user_id, tag_ids_array å­—æ®µ
        """
        print(f"ðŸš€ å¼€å§‹è®¡ç®—æ ‡ç­¾ç»„: {self.name}")
        
        try:
            # 1. è¿‡æ»¤è¯¥ç»„ç›¸å…³çš„æ ‡ç­¾è§„åˆ™
            groupRulesDF = rulesDF.filter(col("tag_id").isin(self.tagIds))
            print(f"   ðŸ“‹ è¯¥ç»„æ ‡ç­¾è§„åˆ™æ•°: {groupRulesDF.count()}")
            
            # 2. åˆ†æžå­—æ®µä¾èµ–
            fieldDependencies = self._analyzeFieldDependencies(groupRulesDF)
            
            # 3. ðŸš€ å…³é”®ä¼˜åŒ–ï¼šä¸€æ¬¡æ€§åŠ è½½å¹¶JOINæ‰€éœ€çš„Hiveè¡¨ï¼ˆç»„å†…å…±äº«ï¼‰
            joinedDF = hiveMeta.loadAndJoinTables(self.requiredTables, fieldDependencies)
            print(f"   ðŸ”— ç»„å†…å…±äº«è¡¨JOINå®Œæˆï¼Œç”¨æˆ·æ•°: {joinedDF.count()}")
            
            # 4. ðŸš€ å…³é”®ä¼˜åŒ–ï¼šä¸ºè¯¥ç»„å¹¶è¡Œè®¡ç®—æ‰€æœ‰æ ‡ç­¾ï¼Œç›´æŽ¥è¿”å›žèšåˆç»“æžœ
            userTagsDF = self._computeAllTagsParallelAndAggregate(joinedDF, groupRulesDF)
            
            print(f"âœ… æ ‡ç­¾ç»„è®¡ç®—å®Œæˆ: {userTagsDF.count()} ä¸ªç”¨æˆ·")
            return userTagsDF
            
        except Exception as e:
            print(f"âŒ æ ‡ç­¾ç»„è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return self._createEmptyResult(hiveMeta.spark)
    
    def _analyzeFieldDependencies(self, rulesDF: DataFrame) -> Dict[str, List[str]]:
        """åˆ†æžè¯¥ç»„æ ‡ç­¾çš„å­—æ®µä¾èµ–å…³ç³»"""
        from ..parser.TagRuleParser import TagRuleParser
        
        parser = TagRuleParser()
        fieldDependencies = parser.analyzeFieldDependencies(rulesDF)
        
        return fieldDependencies
    
    def _computeAllTagsParallelAndAggregate(self, joinedDF: DataFrame, groupRulesDF: DataFrame) -> DataFrame:
        """å¹¶è¡Œè®¡ç®—è¯¥ç»„æ‰€æœ‰æ ‡ç­¾å¹¶ç›´æŽ¥èšåˆ - ä¸€æ­¥åˆ°ä½çš„ä¼˜åŒ–æ–¹æ¡ˆ"""
        print(f"   ðŸŽ¯ å¹¶è¡Œè®¡ç®—å¹¶èšåˆ {len(self.tagIds)} ä¸ªæ ‡ç­¾...")
        
        # æ”¶é›†è§„åˆ™åˆ°Driverè¿›è¡ŒSQLæ¡ä»¶è§£æž
        rules = groupRulesDF.select("tag_id", "rule_conditions").collect()
        
        # è§£æžæ‰€æœ‰è§„åˆ™ä¸ºSQLæ¡ä»¶
        from ..parser.TagRuleParser import TagRuleParser
        parser = TagRuleParser()
        
        # æž„å»ºæ‰€æœ‰æ ‡ç­¾çš„å¹¶è¡Œè®¡ç®—è¡¨è¾¾å¼
        tag_conditions = []
        
        for row in rules:
            tagId = row['tag_id']
            ruleConditions = row['rule_conditions']
            
            print(f"      ðŸ·ï¸  è§£æžæ ‡ç­¾ {tagId} è§„åˆ™...")
            
            sqlCondition = parser.parseRuleToSql(ruleConditions, self.requiredTables)
            print(f"         ðŸ” æ ‡ç­¾ {tagId} SQLæ¡ä»¶: {sqlCondition}")
            
            # ä¸ºæ¯ä¸ªæ ‡ç­¾æž„å»ºæ¡ä»¶è¡¨è¾¾å¼
            if sqlCondition and sqlCondition.strip() and sqlCondition != "1=0":
                # æœ‰æ•ˆè§„åˆ™ï¼šæž„å»ºwhenè¡¨è¾¾å¼
                tag_conditions.append({
                    'tag_id': tagId,
                    'condition': sqlCondition
                })
            else:
                print(f"         âš ï¸  æ ‡ç­¾ {tagId} æ— æœ‰æ•ˆè§„åˆ™æ¡ä»¶ï¼Œè·³è¿‡")
        
        if not tag_conditions:
            print("         âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„æ ‡ç­¾è§„åˆ™")
            return self._createEmptyResult(joinedDF.sql_ctx.sparkSession)
        
        # ðŸš€ å…³é”®æ”¹è¿›ï¼šä½¿ç”¨Spark DataFrameåŽŸç”Ÿå¹¶è¡Œå¤„ç† + ç›´æŽ¥èšåˆ
        print(f"   âš¡ ä½¿ç”¨SparkåŽŸç”Ÿå¹¶è¡Œå¤„ç†å¹¶èšåˆ {len(tag_conditions)} ä¸ªæ ‡ç­¾æ¡ä»¶...")
        
        # æž„å»ºtag_idæ•°ç»„è¡¨è¾¾å¼ï¼šæ ¹æ®æ¡ä»¶åˆ¤æ–­ç”¨æˆ·æ˜¯å¦åŒ¹é…æ¯ä¸ªæ ‡ç­¾
        tag_array_expressions = []
        
        for tag_info in tag_conditions:
            tag_id = tag_info['tag_id']
            condition = tag_info['condition']
            
            # å¯¹æ¯ä¸ªæ ‡ç­¾ï¼šå¦‚æžœæ»¡è¶³æ¡ä»¶åˆ™åŒ…å«tag_idï¼Œå¦åˆ™åŒ…å«null
            tag_expr = when(expr(condition), lit(tag_id)).otherwise(lit(None))
            tag_array_expressions.append(tag_expr)
        
        # ðŸš€ å…³é”®ä¼˜åŒ–ï¼šç›´æŽ¥æž„å»ºæœ€ç»ˆçš„tag_ids_arrayï¼Œæ— éœ€ä¸­é—´æ­¥éª¤
        # ä½¿ç”¨array()å‡½æ•°å°†æ‰€æœ‰æ ‡ç­¾æ¡ä»¶ç»„åˆæˆä¸€ä¸ªæ•°ç»„ï¼Œç„¶åŽè¿‡æ»¤æŽ‰nullå€¼å¹¶æŽ’åº
        combined_tags_expr = array_distinct(array_sort(array_remove(array(*tag_array_expressions), None)))
        
        # ä¸€æ¬¡æ€§ä¸ºæ‰€æœ‰ç”¨æˆ·è®¡ç®—å…¶åŒ¹é…çš„æ ‡ç­¾æ•°ç»„ï¼Œå¹¶è¿‡æ»¤æŽ‰ç©ºæ•°ç»„ç”¨æˆ·
        userTagsDF = joinedDF.select("user_id") \
                           .withColumn("tag_ids_array", combined_tags_expr) \
                           .filter(size(col("tag_ids_array")) > 0)
        
        # ç»Ÿè®¡ç»“æžœ
        try:
            userCount = userTagsDF.count()
            print(f"   âœ… å¹¶è¡Œè®¡ç®—å¹¶èšåˆå®Œæˆ: {userCount} ä¸ªæœ‰æ ‡ç­¾ç”¨æˆ·")
        except:
            print(f"   âœ… å¹¶è¡Œè®¡ç®—å¹¶èšåˆå®Œæˆ")
        
        return userTagsDF
    
    def _createEmptyResult(self, spark) -> DataFrame:
        """åˆ›å»ºç©ºçš„è®¡ç®—ç»“æžœ"""
        from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType
        
        schema = StructType([
            StructField("user_id", StringType(), False),
            StructField("tag_ids_array", ArrayType(IntegerType()), True)
        ])
        
        return spark.createDataFrame([], schema)
    
    def _createEmptyTagResult(self, baseDF: DataFrame) -> DataFrame:
        """åˆ›å»ºç©ºçš„æ ‡ç­¾ç»“æžœDataFrame"""
        return baseDF.select("user_id") \
                    .withColumn("tag_id", lit(0)) \
                    .limit(0)
    
    def getGroupInfo(self) -> Dict[str, any]:
        """èŽ·å–æ ‡ç­¾ç»„ä¿¡æ¯
        
        Returns:
            Dict: æ ‡ç­¾ç»„ä¿¡æ¯
        """
        return {
            "name": self.name,
            "tagIds": self.tagIds,
            "tagCount": len(self.tagIds),
            "requiredTables": self.requiredTables,
            "tableCount": len(self.requiredTables)
        }
    
    def __str__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return f"TagGroup(tags={self.tagIds}, tables={self.requiredTables})"
    
    def __repr__(self) -> str:
        """å¯¹è±¡è¡¨ç¤º"""
        return self.__str__()