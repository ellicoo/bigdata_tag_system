#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ‡ç­¾è®¡ç®—ç»„
å°†ç›¸åŒè¡¨ä¾èµ–çš„æ ‡ç­¾å½’ä¸ºä¸€ç»„ï¼Œå®žçŽ°å¹¶è¡Œé«˜æ•ˆè®¡ç®—
"""
from typing import List, Dict, Set
from pyspark.sql import DataFrame
from pyspark.sql.functions import *


class TagGroup:
    """æ ‡ç­¾è®¡ç®—ç»„
    
    èŒè´£ï¼š
    1. ç®¡ç†ä¸€ç»„å…·æœ‰ç›¸åŒè¡¨ä¾èµ–çš„æ ‡ç­¾
    2. æ‰§è¡Œè¯¥ç»„æ ‡ç­¾çš„å¹¶è¡Œè®¡ç®—
    3. ä¼˜åŒ–è¡¨è¯»å–å’ŒJOINæ“ä½œ
    4. ä½¿ç”¨UDFè¿›è¡Œæ ‡ç­¾åˆå¹¶
    """
    
    def __init__(self, tagIds: List[int], requiredTables: List[str]):
        """åˆå§‹åŒ–æ ‡ç­¾è®¡ç®—ç»„
        
        Args:
            tagIds: æ ‡ç­¾IDåˆ—è¡¨
            requiredTables: æ‰€éœ€çš„è¡¨ååˆ—è¡¨
        """
        self.tagIds = tagIds
        self.requiredTables = requiredTables
        self.name = f"Group_{len(tagIds)}tags_{len(requiredTables)}tables"
        
        print(f"ðŸ“¦ åˆ›å»ºæ ‡ç­¾ç»„: {self.name}")
        print(f"   ðŸ·ï¸  æ ‡ç­¾: {tagIds}")
        print(f"   ðŸ“Š ä¾èµ–è¡¨: {requiredTables}")
    
    def computeTags(self, hiveMeta, mysqlMeta, rulesDF: DataFrame) -> DataFrame:
        """è®¡ç®—è¯¥ç»„æ‰€æœ‰æ ‡ç­¾
        
        Args:
            hiveMeta: Hiveæ•°æ®æºç®¡ç†å™¨
            mysqlMeta: MySQLæ•°æ®æºç®¡ç†å™¨
            rulesDF: æ ‡ç­¾è§„åˆ™DataFrame
            
        Returns:
            DataFrame: æ ‡ç­¾è®¡ç®—ç»“æžœï¼ŒåŒ…å« user_id, tag_ids_array å­—æ®µ
        """
        print(f"ðŸš€ å¼€å§‹è®¡ç®—æ ‡ç­¾ç»„: {self.name}")
        
        try:
            # 1. è¿‡æ»¤è¯¥ç»„ç›¸å…³çš„æ ‡ç­¾è§„åˆ™
            groupRulesDF = rulesDF.filter(col("tag_id").isin(self.tagIds))
            print(f"   ðŸ“‹ è¯¥ç»„æ ‡ç­¾è§„åˆ™æ•°: {groupRulesDF.count()}")
            
            # 2. åˆ†æžå­—æ®µä¾èµ–
            fieldDependencies = self._analyzeFieldDependencies(groupRulesDF)
            
            # 3. åŠ è½½å¹¶JOINæ‰€éœ€çš„Hiveè¡¨
            joinedDF = hiveMeta.loadAndJoinTables(self.requiredTables, fieldDependencies)
            print(f"   ðŸ”— JOINå®Œæˆï¼Œç”¨æˆ·æ•°: {joinedDF.count()}")
            
            # 4. ä¸ºè¯¥ç»„å¹¶è¡Œè®¡ç®—æ‰€æœ‰æ ‡ç­¾
            tagResultsDF = self._computeAllTagsParallel(joinedDF, groupRulesDF)
            
            # 5. èšåˆç”¨æˆ·æ ‡ç­¾
            userTagsDF = self._aggregateUserTags(tagResultsDF)
            
            print(f"âœ… æ ‡ç­¾ç»„è®¡ç®—å®Œæˆ: {userTagsDF.count()} ä¸ªç”¨æˆ·")
            return userTagsDF
            
        except Exception as e:
            print(f"âŒ æ ‡ç­¾ç»„è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return self._createEmptyResult(hiveMeta.spark)
    
    def _analyzeFieldDependencies(self, rulesDF: DataFrame) -> Dict[str, List[str]]:
        """åˆ†æžè¯¥ç»„æ ‡ç­¾çš„å­—æ®µä¾èµ–å…³ç³»"""
        from ..parser.TagRuleParser import TagRuleParser
        
        parser = TagRuleParser()
        fieldDependencies = parser.analyzeFieldDependencies(rulesDF)
        
        return fieldDependencies
    
    def _computeAllTagsParallel(self, joinedDF: DataFrame, groupRulesDF: DataFrame) -> DataFrame:
        """å¹¶è¡Œè®¡ç®—è¯¥ç»„æ‰€æœ‰æ ‡ç­¾"""
        print(f"   ðŸŽ¯ å¹¶è¡Œè®¡ç®— {len(self.tagIds)} ä¸ªæ ‡ç­¾...")
        
        # å¯¼å…¥UDF
        from ..utils.TagUdfs import tagUdfs
        
        # æ”¶é›†è§„åˆ™åˆ°Driver
        rules = groupRulesDF.select("tag_id", "rule_conditions").collect()
        
        tagResults = []
        
        for row in rules:
            tagId = row['tag_id']
            ruleConditions = row['rule_conditions']
            
            print(f"      ðŸ·ï¸  è®¡ç®—æ ‡ç­¾ {tagId}...")
            
            # ä½¿ç”¨TagRuleParserè§£æžè§„åˆ™ä¸ºSQLæ¡ä»¶
            from ..parser.TagRuleParser import TagRuleParser
            parser = TagRuleParser()
            sqlCondition = parser.parseRuleToSql(ruleConditions)
            
            # åº”ç”¨è§„åˆ™ç­›é€‰ç”¨æˆ·
            try:
                if sqlCondition and sqlCondition.strip() and sqlCondition != "1=0":
                    # ä½¿ç”¨è§£æžåŽçš„SQLæ¡ä»¶ç­›é€‰ç”¨æˆ·
                    tagDF = joinedDF.filter(expr(sqlCondition)) \
                                   .select("user_id") \
                                   .withColumn("tag_id", lit(tagId))
                else:
                    # ç©ºè§„åˆ™æˆ–æ— æ•ˆè§„åˆ™è¿”å›žç©ºç»“æžœ
                    tagDF = joinedDF.select("user_id") \
                                   .withColumn("tag_id", lit(tagId)) \
                                   .limit(0)
                
                tagResults.append(tagDF)
                print(f"         âœ… æ ‡ç­¾ {tagId}: {tagDF.count()} ä¸ªç”¨æˆ·")
                
            except Exception as e:
                print(f"         âŒ æ ‡ç­¾ {tagId} è®¡ç®—å¤±è´¥: {e}")
                # åˆ›å»ºç©ºç»“æžœ
                emptyDF = joinedDF.select("user_id") \
                                 .withColumn("tag_id", lit(tagId)) \
                                 .limit(0)
                tagResults.append(emptyDF)
        
        # åˆå¹¶æ‰€æœ‰æ ‡ç­¾ç»“æžœ
        if tagResults:
            allTagsDF = tagResults[0]
            for tagDF in tagResults[1:]:
                allTagsDF = allTagsDF.union(tagDF)
            return allTagsDF
        else:
            return self._createEmptyTagResult(joinedDF)
    
    def _aggregateUserTags(self, tagResultsDF: DataFrame) -> DataFrame:
        """èšåˆç”¨æˆ·æ ‡ç­¾ï¼ŒåŽ»é‡æŽ’åº"""
        print("   ðŸ”€ èšåˆç”¨æˆ·æ ‡ç­¾...")
        
        # å¯¼å…¥UDF
        from ..utils.TagUdfs import tagUdfs
        
        # æŒ‰ç”¨æˆ·èšåˆæ ‡ç­¾
        userTagsDF = tagResultsDF.groupBy("user_id").agg(
            tagUdfs.mergeUserTags(collect_list("tag_id")).alias("tag_ids_array")
        )
        
        print(f"   âœ… ç”¨æˆ·æ ‡ç­¾èšåˆå®Œæˆ: {userTagsDF.count()} ä¸ªç”¨æˆ·")
        return userTagsDF
    
    def _createEmptyResult(self, spark) -> DataFrame:
        """åˆ›å»ºç©ºçš„è®¡ç®—ç»“æžœ"""
        from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType
        
        schema = StructType([
            StructField("user_id", StringType(), False),
            StructField("tag_ids_array", ArrayType(IntegerType()), True)
        ])
        
        return spark.createDataFrame([], schema)
    
    def _createEmptyTagResult(self, baseDF: DataFrame) -> DataFrame:
        """åˆ›å»ºç©ºçš„æ ‡ç­¾ç»“æžœDataFrame"""
        return baseDF.select("user_id") \
                    .withColumn("tag_id", lit(0)) \
                    .limit(0)
    
    def getGroupInfo(self) -> Dict[str, any]:
        """èŽ·å–æ ‡ç­¾ç»„ä¿¡æ¯
        
        Returns:
            Dict: æ ‡ç­¾ç»„ä¿¡æ¯
        """
        return {
            "name": self.name,
            "tagIds": self.tagIds,
            "tagCount": len(self.tagIds),
            "requiredTables": self.requiredTables,
            "tableCount": len(self.requiredTables)
        }
    
    def __str__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return f"TagGroup(tags={self.tagIds}, tables={self.requiredTables})"
    
    def __repr__(self) -> str:
        """å¯¹è±¡è¡¨ç¤º"""
        return self.__str__()