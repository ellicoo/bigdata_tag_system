#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ‡ç­¾è®¡ç®—ç³»ç»Ÿå‘½ä»¤è¡Œå…¥å£
æ”¯æŒå¤šç§æ‰§è¡Œæ¨¡å¼å’Œå‚æ•°é…ç½®
"""
import sys
import os
import argparse
from typing import List, Optional, Dict
from pyspark.sql import SparkSession

# åŠ¨æ€æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))  # å‘ä¸Šä¸¤çº§åˆ°é¡¹ç›®æ ¹ç›®å½•
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.tag_engine.engine.TagEngine import TagEngine


def create_spark_session(app_name: str = "TagComputeEngine") -> SparkSession:
    """åˆ›å»ºSparkä¼šè¯
    
    Args:
        app_name: åº”ç”¨ç¨‹åºåç§°
        
    Returns:
        SparkSession: Sparkä¼šè¯
    """
    print(f"ğŸš€ åˆ›å»ºSparkä¼šè¯: {app_name}")
    
    spark = SparkSession.builder \
        .appName(app_name) \
        .enableHiveSupport() \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.sql.adaptive.skewJoin.enabled", "true") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    spark.sparkContext.setLogLevel("WARN")
    
    print(f"âœ… Sparkä¼šè¯åˆ›å»ºå®Œæˆï¼Œç‰ˆæœ¬: {spark.version}")
    return spark


def load_mysql_config() -> Dict[str, str]:
    """åŠ è½½MySQLé…ç½®
    
    Returns:
        Dict: MySQLé…ç½®å­—å…¸
    """
    # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶åŠ è½½
    # æµ·è±šè°ƒåº¦å™¨ç¯å¢ƒä½¿ç”¨ç»Ÿä¸€é…ç½®
    import os
    
    return {
        "host": os.getenv("MYSQL_HOST", "cex-mysql-test.c5mgk4qm8m2z.ap-southeast-1.rds.amazonaws.com"),
        "port": int(os.getenv("MYSQL_PORT", "3358")),
        "database": os.getenv("MYSQL_DATABASE", "biz_statistics"),
        "user": os.getenv("MYSQL_USER", "root"),
        "password": os.getenv("MYSQL_PASSWORD", "ayjUzzH8b7gcQYRh"),
        "charset": "utf8mb4"
    }


def parse_tag_ids(tag_ids_str: Optional[str]) -> Optional[List[int]]:
    """è§£ææ ‡ç­¾IDå­—ç¬¦ä¸²
    
    Args:
        tag_ids_str: é€—å·åˆ†éš”çš„æ ‡ç­¾IDå­—ç¬¦ä¸²
        
    Returns:
        List[int]: æ ‡ç­¾IDåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰æ ‡ç­¾
    """
    if not tag_ids_str:
        return None
    
    try:
        tag_ids = [int(tag_id.strip()) for tag_id in tag_ids_str.split(",")]
        return tag_ids
    except ValueError as e:
        print(f"âŒ æ ‡ç­¾IDè§£æå¤±è´¥: {e}")
        return None


def generate_comprehensive_test_data(spark) -> bool:
    """ç”Ÿæˆå®Œæ•´çš„æµ‹è¯•æ•°æ®ï¼ŒåŒ¹é…json_demo.txtä¸­çš„æ‰€æœ‰å­—æ®µ
    
    Args:
        spark: SparkSession
        
    Returns:
        bool: ç”Ÿæˆæ˜¯å¦æˆåŠŸ
    """
    try:
        print("ğŸ—ï¸ åˆ›å»ºè¡¨ç»“æ„...")
        
        # 1. åˆ›å»º user_basic_info è¡¨
        spark.sql("""
            CREATE TABLE IF NOT EXISTS tag_system.user_basic_info (
                user_id STRING,
                age INT,
                user_level STRING,
                registration_date DATE,
                birthday DATE,
                first_name STRING,
                last_name STRING,
                middle_name STRING,
                phone_number STRING,
                email STRING,
                is_vip BOOLEAN,
                is_banned BOOLEAN,
                kyc_status STRING,
                account_status STRING,
                primary_status STRING,
                secondary_status STRING
            ) USING HIVE
            STORED AS PARQUET
        """)
        
        # 2. åˆ›å»º user_asset_summary è¡¨
        spark.sql("""
            CREATE TABLE IF NOT EXISTS tag_system.user_asset_summary (
                user_id STRING,
                total_asset_value DECIMAL(18,2),
                cash_balance DECIMAL(18,2),
                debt_amount DECIMAL(18,2)
            ) USING HIVE
            STORED AS PARQUET
        """)
        
        # 3. åˆ›å»º user_activity_summary è¡¨
        spark.sql("""
            CREATE TABLE IF NOT EXISTS tag_system.user_activity_summary (
                user_id STRING,
                trade_count_30d INT,
                last_login_date DATE,
                last_trade_date DATE
            ) USING HIVE
            STORED AS PARQUET
        """)
        
        # 4. åˆ›å»º user_risk_profile è¡¨
        spark.sql("""
            CREATE TABLE IF NOT EXISTS tag_system.user_risk_profile (
                user_id STRING,
                risk_score INT
            ) USING HIVE
            STORED AS PARQUET
        """)
        
        # 5. åˆ›å»º user_preferences è¡¨
        spark.sql("""
            CREATE TABLE IF NOT EXISTS tag_system.user_preferences (
                user_id STRING,
                interested_products ARRAY<STRING>,
                owned_products ARRAY<STRING>,
                blacklisted_products ARRAY<STRING>,
                active_products ARRAY<STRING>,
                expired_products ARRAY<STRING>,
                optional_services ARRAY<STRING>,
                required_services ARRAY<STRING>
            ) USING HIVE
            STORED AS PARQUET
        """)
        
        print("âœ… è¡¨ç»“æ„åˆ›å»ºå®Œæˆ")
        
        # ç”Ÿæˆå¤šæ ·åŒ–æµ‹è¯•æ•°æ®
        print("ğŸ“Š ç”Ÿæˆå¤šæ ·åŒ–æµ‹è¯•æ•°æ®...")
        
        import random
        from datetime import datetime, timedelta
        
        # ç”Ÿæˆ1000ä¸ªç”¨æˆ·çš„åŸºç¡€ä¿¡æ¯ï¼Œç¡®ä¿æ•°æ®å¤šæ ·æ€§
        basic_info_data = []
        for i in range(1000):
            user_id = f"user_{i+1:05d}"
            age = random.randint(18, 80)
            user_level = random.choice(["VIP1", "VIP2", "VIP3", "VIP4", "VIP5", "NORMAL"])
            reg_date = datetime(2020, 1, 1) + timedelta(days=random.randint(0, 1500))
            birthday = datetime(1943 + age, random.randint(1, 12), random.randint(1, 28))
            
            first_name = random.choice(["John", "Jane", "Mike", "Lisa", "Tom", "Alice", "Bob", "Carol", None])
            last_name = random.choice(["Smith", "Johnson", "Williams", "Brown", "Jones", "Garcia", None])
            middle_name = random.choice(["A", "B", "C", None, None, None])  # å¤šæ•°ä¸ºç©º
            
            phone_prefix = random.choice(["+86", "+1", "+44"])
            phone_number = f"{phone_prefix}{random.choice(['138', '139', '186'])}{random.randint(10000000, 99999999)}"
            
            email_domain = random.choice(["gmail.com", "yahoo.com", "hotmail.com", "temp.com"])
            email = f"{user_id}@{email_domain}"
            
            is_vip = random.choice([True, False])
            is_banned = random.choice([True, False, False, False])  # å¤§éƒ¨åˆ†ä¸è¢«å°ç¦
            
            kyc_status = random.choice(["verified", "pending", "rejected"])
            account_status = random.choice(["active", "suspended", "banned", "normal"])
            primary_status = random.choice(["gold", "silver", "bronze", None])
            secondary_status = random.choice(["premium", "standard", None, None])  # å¤šæ•°ä¸ºç©º
            
            basic_info_data.append((
                user_id, age, user_level, reg_date.date(), birthday.date(),
                first_name, last_name, middle_name, phone_number, email,
                is_vip, is_banned, kyc_status, account_status, primary_status, secondary_status
            ))
        
        basic_info_df = spark.createDataFrame(basic_info_data, [
            "user_id", "age", "user_level", "registration_date", "birthday",
            "first_name", "last_name", "middle_name", "phone_number", "email",
            "is_vip", "is_banned", "kyc_status", "account_status", "primary_status", "secondary_status"
        ])
        
        # ç”Ÿæˆèµ„äº§æ•°æ®
        asset_data = []
        for i in range(1000):
            user_id = f"user_{i+1:05d}"
            total_asset = random.choice([0, 100000, 50000, 200000, 1000, 500000])  # åŒ…å«ç²¾ç¡®åŒ¹é…å€¼
            cash_balance = random.choice([0, 50000, 25000, 75000, 100000])  # åŒ…å«ç²¾ç¡®åŒ¹é…å€¼
            debt_amount = random.choice([None, 0, 10000, 5000]) if random.random() > 0.3 else None
            
            asset_data.append((user_id, float(total_asset), float(cash_balance), 
                             float(debt_amount) if debt_amount is not None else None))
        
        asset_df = spark.createDataFrame(asset_data, ["user_id", "total_asset_value", "cash_balance", "debt_amount"])
        
        # ç”Ÿæˆæ´»åŠ¨æ•°æ®
        activity_data = []
        for i in range(1000):
            user_id = f"user_{i+1:05d}"
            trade_count = random.choice([0, 5, 10, 15, 1, 2])  # åŒ…å«ç²¾ç¡®åŒ¹é…å€¼
            
            # ç¡®ä¿æœ‰ç”¨æˆ·åŒ¹é…ç‰¹å®šæ—¥æœŸæ¡ä»¶
            if i < 50:  # å‰50ä¸ªç”¨æˆ·æœ‰ç‰¹å®šæ—¥æœŸ
                last_login = datetime(2025, 1, 1)
            elif i < 100:
                last_login = datetime(2025, 7, 15)
            else:
                last_login = datetime(2024, 1, 1) + timedelta(days=random.randint(0, 500))
            
            # last_trade_date éƒ¨åˆ†ä¸ºç©º
            last_trade = None if random.random() > 0.7 else last_login - timedelta(days=random.randint(0, 30))
            
            activity_data.append((user_id, trade_count, last_login.date(), 
                                last_trade.date() if last_trade else None))
        
        activity_df = spark.createDataFrame(activity_data, ["user_id", "trade_count_30d", "last_login_date", "last_trade_date"])
        
        # ç”Ÿæˆé£é™©æ¡£æ¡ˆæ•°æ®
        risk_data = []
        for i in range(1000):
            user_id = f"user_{i+1:05d}"
            risk_score = random.choice([10, 20, 30, 40, 50])  # åŒ…å« <= 30 çš„å€¼
            risk_data.append((user_id, risk_score))
        
        risk_df = spark.createDataFrame(risk_data, ["user_id", "risk_score"])
        
        # ç”Ÿæˆåå¥½æ•°æ®
        preferences_data = []
        product_options = ["stocks", "bonds", "forex", "savings", "checking", "premium", "gold", "platinum", "high_risk", "speculative"]
        service_options = ["advisory", "trading", "research", "premium_support"]
        
        for i in range(1000):
            user_id = f"user_{i+1:05d}"
            
            # ç¡®ä¿ä¸åŒçš„åˆ—è¡¨ç»„åˆç”¨äºæµ‹è¯•ä¸åŒæ“ä½œç¬¦
            interested = random.sample(product_options[:4], random.randint(1, 3))
            owned = random.sample(["savings", "checking", "premium"], random.randint(0, 2))
            blacklisted = ["forex"] if random.random() > 0.8 else []
            active = random.sample(["premium", "gold", "silver"], random.randint(0, 2))
            expired = random.sample(["premium", "platinum"], random.randint(0, 1))
            optional = [] if random.random() > 0.6 else random.sample(service_options, 1)
            required = random.sample(service_options, random.randint(1, 2))
            
            preferences_data.append((user_id, interested, owned, blacklisted, active, expired, optional, required))
        
        preferences_df = spark.createDataFrame(preferences_data, [
            "user_id", "interested_products", "owned_products", "blacklisted_products",
            "active_products", "expired_products", "optional_services", "required_services"
        ])
        
        # æ’å…¥æ•°æ®
        print("ğŸ’¾ æ’å…¥æµ‹è¯•æ•°æ®...")
        basic_info_df.write.mode("overwrite").insertInto("tag_system.user_basic_info")
        asset_df.write.mode("overwrite").insertInto("tag_system.user_asset_summary")
        activity_df.write.mode("overwrite").insertInto("tag_system.user_activity_summary")
        risk_df.write.mode("overwrite").insertInto("tag_system.user_risk_profile")
        preferences_df.write.mode("overwrite").insertInto("tag_system.user_preferences")
        
        # éªŒè¯æ•°æ®
        print("ğŸ” éªŒè¯ç”Ÿæˆçš„æµ‹è¯•æ•°æ®...")
        tables = ["user_basic_info", "user_asset_summary", "user_activity_summary", "user_risk_profile", "user_preferences"]
        for table in tables:
            count = spark.sql(f"SELECT COUNT(*) as cnt FROM tag_system.{table}").collect()[0]['cnt']
            print(f"   ğŸ“Š tag_system.{table}: {count} æ¡è®°å½•")
        
        print("ğŸ¯ æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆï¼Œå·²ç¡®ä¿å¤šæ ·æ€§åŒ¹é…æ‰€æœ‰æ ‡ç­¾æ¡ä»¶")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ•°æ®ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="æ ‡ç­¾è®¡ç®—ç³»ç»Ÿ")
    parser.add_argument(
        "--mode", 
        choices=["health", "full", "specific", "task-all", "task-tags", "generate-test-data", "list-tasks"],
        default="health",
        help="æ‰§è¡Œæ¨¡å¼ï¼šhealth(å¥åº·æ£€æŸ¥)ã€full/task-all(å…¨é‡è®¡ç®—)ã€specific/task-tags(æŒ‡å®šæ ‡ç­¾)ã€generate-test-data(ç”Ÿæˆæµ‹è¯•æ•°æ®)ã€list-tasks(åˆ—å‡ºä»»åŠ¡)"
    )
    parser.add_argument(
        "--tag-ids",
        type=str,
        help="æŒ‡å®šæ ‡ç­¾IDåˆ—è¡¨ï¼Œé€—å·åˆ†éš”ï¼Œå¦‚: 1,2,3"
    )
    parser.add_argument(
        "--app-name",
        type=str,
        default="TagComputeEngine",
        help="Sparkåº”ç”¨ç¨‹åºåç§°"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="è¯¦ç»†è¾“å‡ºæ¨¡å¼"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®è¯¦ç»†è¾“å‡º
    if args.verbose:
        import logging
        logging.basicConfig(level=logging.INFO)
    
    print("=" * 60)
    print("ğŸ·ï¸  å¤§æ•°æ®æ ‡ç­¾è®¡ç®—ç³»ç»Ÿ")
    print("=" * 60)
    print(f"æ‰§è¡Œæ¨¡å¼: {args.mode}")
    print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"è„šæœ¬ç›®å½•: {current_dir}")
    print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"Pythonè·¯å¾„å‰3é¡¹: {sys.path[:3]}")
    
    if args.tag_ids:
        print(f"æŒ‡å®šæ ‡ç­¾: {args.tag_ids}")
    
    spark = None
    tag_engine = None
    
    try:
        # 1. åˆ›å»ºSparkä¼šè¯
        spark = create_spark_session(args.app_name)
        
        # 2. åŠ è½½é…ç½®
        mysql_config = load_mysql_config()
        print(f"MySQLé…ç½®: {mysql_config['host']}:{mysql_config['port']}/{mysql_config['database']}")
        
        # 3. åˆ›å»ºæ ‡ç­¾å¼•æ“
        tag_engine = TagEngine(spark, mysqlConfig=mysql_config)
        
        # 4. æ ¹æ®æ¨¡å¼æ‰§è¡Œç›¸åº”æ“ä½œ
        success = False
        
        if args.mode == "health":
            print("\nğŸ” æ‰§è¡Œå¥åº·æ£€æŸ¥...")
            success = tag_engine.healthCheck()
            
        elif args.mode in ["full", "task-all"]:
            print("\nğŸš€ æ‰§è¡Œå…¨é‡æ ‡ç­¾è®¡ç®—...")
            success = tag_engine.computeTags(mode="task-all")
            
        elif args.mode in ["specific", "task-tags"]:
            tag_ids = parse_tag_ids(args.tag_ids)
            if tag_ids is None:
                print("âŒ æŒ‡å®šæ ‡ç­¾æ¨¡å¼éœ€è¦æä¾› --tag-ids å‚æ•°")
                sys.exit(1)
            
            print(f"\nğŸ¯ æ‰§è¡ŒæŒ‡å®šæ ‡ç­¾è®¡ç®—: {tag_ids}")
            success = tag_engine.computeTags(mode="task-tags", tagIds=tag_ids)
            
        elif args.mode == "generate-test-data":
            print("\nğŸ§ª ç”Ÿæˆæµ‹è¯•æ•°æ®...")
            # å…ˆåˆ›å»ºæ•°æ®åº“
            spark.sql("CREATE DATABASE IF NOT EXISTS tag_system")
            print("âœ… æ•°æ®åº“ tag_system åˆ›å»ºæˆåŠŸ")
            
            # ä½¿ç”¨éƒ¨ç½²åŒ…ä¸­çš„æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨
            try:
                # å°è¯•å¯¼å…¥éƒ¨ç½²åŒ…ä¸­çš„æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨ï¼ˆæµ·è±šè°ƒåº¦å™¨ç¯å¢ƒï¼‰
                from generate_test_data import generate_test_data
                generate_test_data(spark)
                success = True
                print("âœ… æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆ")
            except ImportError:
                # å¦‚æœæ‰¾ä¸åˆ°éƒ¨ç½²åŒ…çš„ç”Ÿæˆå™¨ï¼Œä½¿ç”¨å†…ç½®ç”Ÿæˆå™¨
                print("ğŸ”„ ä½¿ç”¨å†…ç½®æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨...")
                success = generate_comprehensive_test_data(spark)
                if success:
                    print("âœ… å†…ç½®æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆ")
                else:
                    print("âŒ å†…ç½®æµ‹è¯•æ•°æ®ç”Ÿæˆå¤±è´¥")
            
        elif args.mode == "list-tasks":
            print("\nğŸ“‹ åˆ—å‡ºå¯ç”¨æ ‡ç­¾ä»»åŠ¡...")
            from src.tag_engine.meta.MysqlMeta import MysqlMeta
            mysql_meta = MysqlMeta(spark, mysql_config)
            
            try:
                tags = mysql_meta.loadTagRules()
                print("å¯ç”¨æ ‡ç­¾ä»»åŠ¡:")
                for tag in tags.collect():
                    print(f"  {tag.tag_id}: {tag.tag_name if hasattr(tag, 'tag_name') else 'æœªçŸ¥æ ‡ç­¾'}")
                success = True
            except Exception as e:
                print(f"âŒ è·å–æ ‡ç­¾åˆ—è¡¨å¤±è´¥: {e}")
                success = False
        
        # 5. è¾“å‡ºç»“æœ
        print("\n" + "=" * 60)
        if success:
            print("âœ… ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ")
            exit_code = 0
        else:
            print("âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
            exit_code = 1
        
        print("=" * 60)
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ä»»åŠ¡è¢«ç”¨æˆ·ä¸­æ–­")
        exit_code = 2
        
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿå¼‚å¸¸: {e}")
        import traceback
        if args.verbose:
            traceback.print_exc()
        exit_code = 3
        
    finally:
        # æ¸…ç†èµ„æº
        if tag_engine:
            tag_engine.cleanup()
        
        if spark:
            print("ğŸ§¹ å…³é—­Sparkä¼šè¯...")
            spark.stop()
        
        print("ğŸ‘‹ ç¨‹åºé€€å‡º")
    
    sys.exit(exit_code)


if __name__ == "__main__":
    main()