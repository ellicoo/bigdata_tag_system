#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ‡ç­¾è®¡ç®—ç³»ç»Ÿå‘½ä»¤è¡Œå…¥å£
æ”¯æŒå¤šç§æ‰§è¡Œæ¨¡å¼å’Œå‚æ•°é…ç½®
"""
import sys
import os
import argparse
from typing import List, Optional, Dict
from pyspark.sql import SparkSession

# åŠ¨æ€æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))  # å‘ä¸Šä¸¤çº§åˆ°é¡¹ç›®æ ¹ç›®å½•
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.tag_engine.engine.TagEngine import TagEngine


def create_spark_session(app_name: str = "TagComputeEngine") -> SparkSession:
    """åˆ›å»ºSparkä¼šè¯
    
    Args:
        app_name: åº”ç”¨ç¨‹åºåç§°
        
    Returns:
        SparkSession: Sparkä¼šè¯
    """
    print(f"ğŸš€ åˆ›å»ºSparkä¼šè¯: {app_name}")
    
    spark = SparkSession.builder \
        .appName(app_name) \
        .enableHiveSupport() \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.sql.adaptive.skewJoin.enabled", "true") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    spark.sparkContext.setLogLevel("WARN")
    
    print(f"âœ… Sparkä¼šè¯åˆ›å»ºå®Œæˆï¼Œç‰ˆæœ¬: {spark.version}")
    return spark


def load_mysql_config() -> Dict[str, str]:
    """åŠ è½½MySQLé…ç½®
    
    Returns:
        Dict: MySQLé…ç½®å­—å…¸
    """
    # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶åŠ è½½
    # æµ·è±šè°ƒåº¦å™¨ç¯å¢ƒä½¿ç”¨ç»Ÿä¸€é…ç½®
    import os

    # return {
    #     "host": os.getenv("MYSQL_HOST",
    #                       "rm-3ns765y13i6wf0hp3.mysql.rds.aliyuncs.com"),
    #     "port": int(os.getenv("MYSQL_PORT", "3358")),
    #     "database": os.getenv("MYSQL_DATABASE", "biz_user"),
    #     "user": os.getenv("MYSQL_USER", "dev_rw"),
    #     "password": os.getenv("MYSQL_PASSWORD", "nLjE49a20!h6vhHF"),
    #     "charset": "utf8mb4"
    # }

    return {
        "host": os.getenv("MYSQL_HOST",
                          "cex-mysql-ex-test-cluster.cluster-c5mgk4qm8m2z.ap-southeast-1.rds.amazonaws.com"),
        "port": int(os.getenv("MYSQL_PORT", "3358")),
        "database": os.getenv("MYSQL_DATABASE", "biz_statistics"),
        "user": os.getenv("MYSQL_USER", "ex_test_rw"),
        "password": os.getenv("MYSQL_PASSWORD", "NqaBacRMzCKRRqfEWb"),
        "charset": "utf8mb4"
    }


def parse_tag_ids(tag_ids_str: Optional[str]) -> Optional[List[int]]:
    """è§£ææ ‡ç­¾IDå­—ç¬¦ä¸²
    
    Args:
        tag_ids_str: é€—å·åˆ†éš”çš„æ ‡ç­¾IDå­—ç¬¦ä¸²
        
    Returns:
        List[int]: æ ‡ç­¾IDåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰æ ‡ç­¾
    """
    if not tag_ids_str:
        return None
    
    try:
        tag_ids = [int(tag_id.strip()) for tag_id in tag_ids_str.split(",")]
        return tag_ids
    except ValueError as e:
        print(f"âŒ æ ‡ç­¾IDè§£æå¤±è´¥: {e}")
        return None


def generate_comprehensive_test_data(spark) -> bool:
    """ç”Ÿæˆå®Œæ•´çš„æµ‹è¯•æ•°æ®ï¼ŒåŒ¹é…æ–°çš„DWSå±‚è¡¨ç»“æ„
    
    Args:
        spark: SparkSession
        
    Returns:
        bool: ç”Ÿæˆæ˜¯å¦æˆåŠŸ
    """
    try:
        # åˆ›å»ºDWSæ•°æ®åº“
        spark.sql("CREATE DATABASE IF NOT EXISTS dws_user")
        print("âœ… æ•°æ®åº“ dws_user åˆ›å»ºæˆåŠŸ")
        
        print("ğŸ—ï¸ åˆ›å»ºDWSå±‚è¡¨ç»“æ„...")
        
        # 1. ç”¨æˆ·åŸºç¡€ç”»åƒè¡¨
        spark.sql("""
            CREATE TABLE IF NOT EXISTS dws_user.dws_user_profile_df (
                user_id STRING,
                register_time STRING,
                register_source_channel STRING,
                register_method STRING,
                register_country STRING,
                is_kyc_completed STRING,
                kyc_country STRING,
                is_2fa_enabled STRING,
                user_level STRING,
                is_agent STRING
            ) USING HIVE
            STORED AS PARQUET
        """)
        
        # 2. ç”¨æˆ·èµ„äº§è´¢åŠ¡è¡¨
        spark.sql("""
            CREATE TABLE IF NOT EXISTS dws_user.dws_user_asset_df (
                user_id STRING,
                total_deposit_amount STRING,
                total_withdraw_amount STRING,
                net_deposit_amount STRING,
                last_deposit_time STRING,
                last_withdraw_time STRING,
                withdraw_count_30d STRING,
                deposit_fail_count STRING,
                spot_position_value STRING,
                contract_position_value STRING,
                finance_position_value STRING,
                onchain_position_value STRING,
                current_total_position_value STRING,
                spot_available_balance STRING,
                contract_available_balance STRING,
                onchain_available_balance STRING,
                available_balance STRING,
                spot_locked_amount STRING,
                contract_locked_amount STRING,
                finance_locked_amount STRING,
                onchain_locked_amount STRING,
                locked_amount STRING
            ) USING HIVE
            STORED AS PARQUET
        """)
        
        # 3. ç”¨æˆ·äº¤æ˜“è¡Œä¸ºè¡¨
        spark.sql("""
            CREATE TABLE IF NOT EXISTS dws_user.dws_user_trading_df (
                user_id STRING,
                spot_trading_volume STRING,
                contract_trading_volume STRING,
                spot_recent_30d_volume STRING,
                contract_recent_30d_volume STRING,
                spot_trade_count STRING,
                contract_trade_count STRING,
                finance_trade_count STRING,
                onchain_trade_count STRING,
                first_trade_time STRING,
                last_trade_time STRING,
                has_contract_trading STRING,
                contract_trading_style STRING,
                has_finance_management STRING,
                has_pending_orders STRING
            ) USING HIVE
            STORED AS PARQUET
        """)
        
        # 4. ç”¨æˆ·æ´»è·ƒè¡Œä¸ºè¡¨
        spark.sql("""
            CREATE TABLE IF NOT EXISTS dws_user.dws_user_activity_df (
                user_id STRING,
                days_since_register STRING,
                days_since_last_login STRING,
                last_login_time STRING,
                last_activity_time STRING,
                login_count_7d STRING,
                login_ip_address STRING,
                country_region_code STRING,
                email_suffix STRING,
                operating_system STRING
            ) USING HIVE
            STORED AS PARQUET
        """)
        
        # 5. ç”¨æˆ·é£é™©é£æ§è¡¨
        spark.sql("""
            CREATE TABLE IF NOT EXISTS dws_user.dws_user_risk_df (
                user_id STRING,
                is_blacklist_user STRING,
                is_high_risk_ip STRING,
                channel_source STRING
            ) USING HIVE
            STORED AS PARQUET
        """)
        
        # 6. ç”¨æˆ·è¥é”€æ¿€åŠ±è¡¨
        spark.sql("""
            CREATE TABLE IF NOT EXISTS dws_user.dws_user_marketing_df (
                user_id STRING,
                red_packet_count STRING,
                successful_invites_count STRING,
                commission_rate STRING,
                total_commission_amount STRING
            ) USING HIVE
            STORED AS PARQUET
        """)
        
        # 7. ç”¨æˆ·è¡Œä¸ºåå¥½è¡¨
        spark.sql("""
            CREATE TABLE IF NOT EXISTS dws_user.dws_user_behavior_df (
                user_id STRING,
                current_holding_coins ARRAY<STRING>,
                traded_coins_list ARRAY<STRING>,
                device_fingerprint_list ARRAY<STRING>,
                participated_activity_ids ARRAY<STRING>,
                reward_claim_history ARRAY<STRING>,
                used_coupon_types ARRAY<STRING>
            ) USING HIVE
            STORED AS PARQUET
        """)
        
        print("âœ… DWSå±‚è¡¨ç»“æ„åˆ›å»ºå®Œæˆ")
        
        # ç”ŸæˆDWSå±‚æµ‹è¯•æ•°æ®
        print("ğŸ“Š ç”ŸæˆDWSå±‚æµ‹è¯•æ•°æ®...")
        
        import random
        from datetime import datetime, timedelta
        
        # ç”Ÿæˆ1000ä¸ªç”¨æˆ·çš„æ•°æ®
        user_count = 1000
        
        # 1. ç”¨æˆ·åŸºç¡€ç”»åƒæ•°æ®
        profile_data = []
        for i in range(user_count):
            user_id = f"user_{i+1:05d}"
            register_time = (datetime(2020, 1, 1) + timedelta(days=random.randint(0, 1500))).strftime("%Y-%m-%d %H:%M:%S")
            
            profile_data.append((
                user_id,
                register_time,
                random.choice(["å®˜ç½‘", "APP", "æ¨è", "å¹¿å‘Š"]),
                random.choice(["é‚®ç®±", "æ‰‹æœº"]),
                random.choice(["CN", "US", "SG", "JP"]),
                random.choice(["true", "false"]),
                random.choice(["CN", "US", "SG", "JP"]),
                random.choice(["true", "false"]),
                random.choice(["VIP1", "VIP2", "VIP3", "VIP4", "NORMAL"]),
                random.choice(["true", "false"])
            ))
        
        profile_df = spark.createDataFrame(profile_data, [
            "user_id", "register_time", "register_source_channel", "register_method", "register_country",
            "is_kyc_completed", "kyc_country", "is_2fa_enabled", "user_level", "is_agent"
        ])
        
        # 2. ç”¨æˆ·èµ„äº§æ•°æ®
        asset_data = []
        for i in range(user_count):
            user_id = f"user_{i+1:05d}"
            
            # ç”Ÿæˆèµ„äº§æ•°æ®ï¼Œä½¿ç”¨å­—ç¬¦ä¸²ç±»å‹
            spot_position = str(random.choice([0, 5000, 25000, 50000, 100000]))
            contract_position = str(random.choice([0, 10000, 50000, 100000, 200000]))
            finance_position = str(random.choice([0, 20000, 50000, 100000, 300000]))
            onchain_position = str(random.choice([0, 5000, 10000, 25000, 50000]))
            
            total_position = str(int(spot_position) + int(contract_position) + int(finance_position) + int(onchain_position))
            
            asset_data.append((
                user_id,
                str(random.randint(0, 1000000)),  # total_deposit_amount
                str(random.randint(0, 500000)),   # total_withdraw_amount
                str(random.randint(-100000, 500000)),  # net_deposit_amount
                (datetime.now() - timedelta(days=random.randint(1, 365))).strftime("%Y-%m-%d %H:%M:%S"),
                (datetime.now() - timedelta(days=random.randint(1, 180))).strftime("%Y-%m-%d %H:%M:%S"),
                str(random.randint(0, 10)),       # withdraw_count_30d
                str(random.randint(0, 5)),        # deposit_fail_count
                spot_position,
                contract_position,
                finance_position,
                onchain_position,
                total_position,
                str(random.randint(0, 50000)),    # spot_available_balance
                str(random.randint(0, 100000)),   # contract_available_balance
                str(random.randint(0, 25000)),    # onchain_available_balance
                str(random.randint(0, 175000)),   # available_balance
                str(random.randint(0, 10000)),    # spot_locked_amount
                str(random.randint(0, 50000)),    # contract_locked_amount
                str(random.randint(0, 100000)),   # finance_locked_amount
                str(random.randint(0, 25000)),    # onchain_locked_amount
                str(random.randint(0, 185000))    # locked_amount
            ))
        
        asset_df = spark.createDataFrame(asset_data, [
            "user_id", "total_deposit_amount", "total_withdraw_amount", "net_deposit_amount",
            "last_deposit_time", "last_withdraw_time", "withdraw_count_30d", "deposit_fail_count",
            "spot_position_value", "contract_position_value", "finance_position_value", "onchain_position_value", "current_total_position_value",
            "spot_available_balance", "contract_available_balance", "onchain_available_balance", "available_balance",
            "spot_locked_amount", "contract_locked_amount", "finance_locked_amount", "onchain_locked_amount", "locked_amount"
        ])
        
        # 3. ç”¨æˆ·äº¤æ˜“è¡Œä¸ºæ•°æ®
        trading_data = []
        for i in range(user_count):
            user_id = f"user_{i+1:05d}"
            
            trading_data.append((
                user_id,
                str(random.randint(0, 500000)),   # spot_trading_volume
                str(random.randint(0, 1000000)),  # contract_trading_volume
                str(random.randint(0, 50000)),    # spot_recent_30d_volume
                str(random.randint(0, 100000)),   # contract_recent_30d_volume
                str(random.randint(0, 100)),      # spot_trade_count
                str(random.randint(0, 200)),      # contract_trade_count
                str(random.randint(0, 50)),       # finance_trade_count
                str(random.randint(0, 20)),       # onchain_trade_count
                (datetime.now() - timedelta(days=random.randint(30, 1000))).strftime("%Y-%m-%d %H:%M:%S"),
                (datetime.now() - timedelta(days=random.randint(1, 30))).strftime("%Y-%m-%d %H:%M:%S"),
                random.choice(["true", "false"]),
                random.choice(["æ¿€è¿›", "ç¨³å¥", "ä¿å®ˆ"]),
                random.choice(["true", "false"]),
                random.choice(["true", "false"])
            ))
        
        trading_df = spark.createDataFrame(trading_data, [
            "user_id", "spot_trading_volume", "contract_trading_volume", "spot_recent_30d_volume", "contract_recent_30d_volume",
            "spot_trade_count", "contract_trade_count", "finance_trade_count", "onchain_trade_count",
            "first_trade_time", "last_trade_time", "has_contract_trading", "contract_trading_style", "has_finance_management", "has_pending_orders"
        ])
        
        # 4. ç”¨æˆ·æ´»è·ƒè¡Œä¸ºæ•°æ®
        activity_data = []
        for i in range(user_count):
            user_id = f"user_{i+1:05d}"
            
            activity_data.append((
                user_id,
                str(random.randint(1, 1500)),     # days_since_register
                str(random.randint(0, 30)),       # days_since_last_login
                (datetime.now() - timedelta(days=random.randint(0, 30))).strftime("%Y-%m-%d %H:%M:%S"),
                (datetime.now() - timedelta(hours=random.randint(1, 72))).strftime("%Y-%m-%d %H:%M:%S"),
                str(random.randint(0, 10)),       # login_count_7d
                f"192.168.{random.randint(1, 255)}.{random.randint(1, 255)}",
                random.choice(["CN", "US", "SG", "JP", "UK"]),
                random.choice(["gmail.com", "yahoo.com", "qq.com", "163.com"]),
                random.choice(["Windows", "macOS", "iOS", "Android", "Linux"])
            ))
        
        activity_df = spark.createDataFrame(activity_data, [
            "user_id", "days_since_register", "days_since_last_login", "last_login_time", "last_activity_time",
            "login_count_7d", "login_ip_address", "country_region_code", "email_suffix", "operating_system"
        ])
        
        # 5. ç”¨æˆ·é£é™©æ•°æ®
        risk_data = []
        for i in range(user_count):
            user_id = f"user_{i+1:05d}"
            
            risk_data.append((
                user_id,
                random.choice(["true", "false", "false", "false"]),  # å¤§éƒ¨åˆ†ä¸æ˜¯é»‘åå•
                random.choice(["true", "false", "false"]),           # å°‘æ•°é«˜é£é™©IP
                random.choice(["å®˜ç½‘", "æ¨è", "å¹¿å‘Š", "åˆä½œä¼™ä¼´"])
            ))
        
        risk_df = spark.createDataFrame(risk_data, [
            "user_id", "is_blacklist_user", "is_high_risk_ip", "channel_source"
        ])
        
        # 6. ç”¨æˆ·è¥é”€æ•°æ®
        marketing_data = []
        for i in range(user_count):
            user_id = f"user_{i+1:05d}"
            
            marketing_data.append((
                user_id,
                str(random.randint(0, 20)),       # red_packet_count
                str(random.randint(0, 10)),       # successful_invites_count
                str(random.choice(["0.01", "0.02", "0.05", "0.1"])),  # commission_rate
                str(random.randint(0, 10000))     # total_commission_amount
            ))
        
        marketing_df = spark.createDataFrame(marketing_data, [
            "user_id", "red_packet_count", "successful_invites_count", "commission_rate", "total_commission_amount"
        ])
        
        # 7. ç”¨æˆ·è¡Œä¸ºåå¥½æ•°æ®
        behavior_data = []
        coins = ["BTC", "ETH", "BNB", "USDT", "ADA", "DOT", "LINK", "UNI"]
        activities = ["æ–°äººæ´»åŠ¨", "äº¤æ˜“èµ›", "ç†è´¢æ´»åŠ¨", "æ¨èæ´»åŠ¨"]
        rewards = ["ç°é‡‘", "ä»£å¸", "æ‰‹ç»­è´¹å‡å…", "VIPæƒç›Š"]
        coupons = ["äº¤æ˜“åˆ¸", "ç†è´¢åˆ¸", "æ‰‹ç»­è´¹åˆ¸"]
        
        for i in range(user_count):
            user_id = f"user_{i+1:05d}"
            
            behavior_data.append((
                user_id,
                random.sample(coins, random.randint(1, 4)),
                random.sample(coins, random.randint(2, 6)),
                [f"device_{random.randint(1000, 9999)}" for _ in range(random.randint(1, 3))],
                random.sample(activities, random.randint(0, 2)),
                random.sample(rewards, random.randint(0, 3)),
                random.sample(coupons, random.randint(0, 2))
            ))
        
        behavior_df = spark.createDataFrame(behavior_data, [
            "user_id", "current_holding_coins", "traded_coins_list", "device_fingerprint_list",
            "participated_activity_ids", "reward_claim_history", "used_coupon_types"
        ])
        
        # æ’å…¥æ•°æ®
        print("ğŸ’¾ æ’å…¥DWSå±‚æµ‹è¯•æ•°æ®...")
        profile_df.write.mode("overwrite").insertInto("dws_user.dws_user_profile_df")
        asset_df.write.mode("overwrite").insertInto("dws_user.dws_user_asset_df")
        trading_df.write.mode("overwrite").insertInto("dws_user.dws_user_trading_df")
        activity_df.write.mode("overwrite").insertInto("dws_user.dws_user_activity_df")
        risk_df.write.mode("overwrite").insertInto("dws_user.dws_user_risk_df")
        marketing_df.write.mode("overwrite").insertInto("dws_user.dws_user_marketing_df")
        behavior_df.write.mode("overwrite").insertInto("dws_user.dws_user_behavior_df")
        
        # éªŒè¯æ•°æ®
        print("ğŸ” éªŒè¯ç”Ÿæˆçš„DWSå±‚æµ‹è¯•æ•°æ®...")
        dws_tables = ["dws_user_profile_df", "dws_user_asset_df", "dws_user_trading_df", 
                      "dws_user_activity_df", "dws_user_risk_df", "dws_user_marketing_df", "dws_user_behavior_df"]
        for table in dws_tables:
            count = spark.sql(f"SELECT COUNT(*) as cnt FROM dws_user.{table}").collect()[0]['cnt']
            print(f"   ğŸ“Š dws_user.{table}: {count} æ¡è®°å½•")
        
        print("ğŸ¯ DWSå±‚æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆï¼Œå·²ç¡®ä¿å¤šæ ·æ€§åŒ¹é…æ‰€æœ‰æ ‡ç­¾æ¡ä»¶")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ•°æ®ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="æ ‡ç­¾è®¡ç®—ç³»ç»Ÿ")
    parser.add_argument(
        "--mode", 
        choices=["health", "full", "specific", "task-all", "task-tags", "generate-test-data", "list-tasks"],
        default="health",
        help="æ‰§è¡Œæ¨¡å¼ï¼šhealth(å¥åº·æ£€æŸ¥)ã€full/task-all(å…¨é‡è®¡ç®—)ã€specific/task-tags(æŒ‡å®šæ ‡ç­¾)ã€generate-test-data(ç”Ÿæˆæµ‹è¯•æ•°æ®)ã€list-tasks(åˆ—å‡ºä»»åŠ¡)"
    )
    parser.add_argument(
        "--tag-ids",
        type=str,
        help="æŒ‡å®šæ ‡ç­¾IDåˆ—è¡¨ï¼Œé€—å·åˆ†éš”ï¼Œå¦‚: 1,2,3"
    )
    parser.add_argument(
        "--app-name",
        type=str,
        default="TagComputeEngine",
        help="Sparkåº”ç”¨ç¨‹åºåç§°"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="è¯¦ç»†è¾“å‡ºæ¨¡å¼"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®è¯¦ç»†è¾“å‡º
    if args.verbose:
        import logging
        logging.basicConfig(level=logging.INFO)
    
    print("=" * 60)
    print("ğŸ·ï¸  å¤§æ•°æ®æ ‡ç­¾è®¡ç®—ç³»ç»Ÿ")
    print("=" * 60)
    print(f"æ‰§è¡Œæ¨¡å¼: {args.mode}")
    print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"è„šæœ¬ç›®å½•: {current_dir}")
    print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"Pythonè·¯å¾„å‰3é¡¹: {sys.path[:3]}")
    
    if args.tag_ids:
        print(f"æŒ‡å®šæ ‡ç­¾: {args.tag_ids}")
    
    spark = None
    tag_engine = None
    
    try:
        # 1. åˆ›å»ºSparkä¼šè¯
        spark = create_spark_session(args.app_name)
        
        # 2. åŠ è½½é…ç½®
        mysql_config = load_mysql_config()
        print(f"MySQLé…ç½®: {mysql_config['host']}:{mysql_config['port']}/{mysql_config['database']}")
        
        # 3. åˆ›å»ºæ ‡ç­¾å¼•æ“ï¼ˆHiveMetaå†…éƒ¨è‡ªåŠ¨å¤„ç†å½“å¤©åˆ†åŒºï¼‰
        tag_engine = TagEngine(spark, mysqlConfig=mysql_config)
        
        # 4. æ ¹æ®æ¨¡å¼æ‰§è¡Œç›¸åº”æ“ä½œ
        success = False
        
        if args.mode == "health":
            print("\nğŸ” æ‰§è¡Œå¥åº·æ£€æŸ¥...")
            success = tag_engine.healthCheck()
            
        elif args.mode in ["full", "task-all"]:
            print("\nğŸš€ æ‰§è¡Œå…¨é‡æ ‡ç­¾è®¡ç®—...")
            success, failed_tag_ids = tag_engine.computeTags(mode="task-all")
            if failed_tag_ids:
                print(f"âš ï¸  {len(failed_tag_ids)} ä¸ªæ ‡ç­¾å› è¡¨åŠ è½½å¤±è´¥è€Œè·³è¿‡: {failed_tag_ids}")
            else:
                print("âœ… æ‰€æœ‰æ ‡ç­¾è®¡ç®—æˆåŠŸ")
            
        elif args.mode in ["specific", "task-tags"]:
            tag_ids = parse_tag_ids(args.tag_ids)
            if tag_ids is None:
                print("âŒ æŒ‡å®šæ ‡ç­¾æ¨¡å¼éœ€è¦æä¾› --tag-ids å‚æ•°")
                sys.exit(1)
            
            print(f"\nğŸ¯ æ‰§è¡ŒæŒ‡å®šæ ‡ç­¾è®¡ç®—: {tag_ids}")
            success, failed_tag_ids = tag_engine.computeTags(mode="task-tags", tagIds=tag_ids)
            if failed_tag_ids:
                print(f"âš ï¸  {len(failed_tag_ids)} ä¸ªæ ‡ç­¾å› è¡¨åŠ è½½å¤±è´¥è€Œè·³è¿‡: {failed_tag_ids}")
            else:
                print("âœ… æ‰€æœ‰æŒ‡å®šæ ‡ç­¾è®¡ç®—æˆåŠŸ")
            
        elif args.mode == "generate-test-data":
            print("\nğŸ§ª ç”Ÿæˆæµ‹è¯•æ•°æ®...")
            # å…ˆåˆ›å»ºæ•°æ®åº“
            spark.sql("CREATE DATABASE IF NOT EXISTS tag_system")
            spark.sql("CREATE DATABASE IF NOT EXISTS dws_user")
            print("âœ… æ•°æ®åº“ tag_system å’Œ dws_user åˆ›å»ºæˆåŠŸ")
            
            # ä½¿ç”¨éƒ¨ç½²åŒ…ä¸­çš„æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨
            try:
                # å°è¯•å¯¼å…¥éƒ¨ç½²åŒ…ä¸­çš„æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨ï¼ˆæµ·è±šè°ƒåº¦å™¨ç¯å¢ƒï¼‰
                from generate_test_data import generate_test_data
                generate_test_data(spark)
                success = True
                print("âœ… æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆ")
            except ImportError:
                # å¦‚æœæ‰¾ä¸åˆ°éƒ¨ç½²åŒ…çš„ç”Ÿæˆå™¨ï¼Œä½¿ç”¨å†…ç½®ç”Ÿæˆå™¨
                print("ğŸ”„ ä½¿ç”¨å†…ç½®æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨...")
                success = generate_comprehensive_test_data(spark)
                if success:
                    print("âœ… å†…ç½®æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆ")
                else:
                    print("âŒ å†…ç½®æµ‹è¯•æ•°æ®ç”Ÿæˆå¤±è´¥")
            
        elif args.mode == "list-tasks":
            print("\nğŸ“‹ åˆ—å‡ºå¯ç”¨æ ‡ç­¾ä»»åŠ¡...")
            from src.tag_engine.meta.MysqlMeta import MysqlMeta
            mysql_meta = MysqlMeta(spark, mysql_config)
            
            try:
                tags = mysql_meta.loadTagRules()
                print("å¯ç”¨æ ‡ç­¾ä»»åŠ¡:")
                for tag in tags.collect():
                    print(f"  {tag.tag_id}: {tag.tag_name if hasattr(tag, 'tag_name') else 'æœªçŸ¥æ ‡ç­¾'}")
                success = True
            except Exception as e:
                print(f"âŒ è·å–æ ‡ç­¾åˆ—è¡¨å¤±è´¥: {e}")
                success = False
        
        # 5. è¾“å‡ºç»“æœ
        print("\n" + "=" * 60)
        if success:
            print("âœ… ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ")
            exit_code = 0
        else:
            print("âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
            exit_code = 1
        
        print("=" * 60)
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ä»»åŠ¡è¢«ç”¨æˆ·ä¸­æ–­")
        exit_code = 2
        
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿå¼‚å¸¸: {e}")
        import traceback
        if args.verbose:
            traceback.print_exc()
        exit_code = 3
        
    finally:
        # æ¸…ç†èµ„æº
        if tag_engine:
            tag_engine.cleanup()
        
        if spark:
            print("ğŸ§¹ å…³é—­Sparkä¼šè¯...")
            spark.stop()
        
        print("ğŸ‘‹ ç¨‹åºé€€å‡º")
    
    sys.exit(exit_code)


if __name__ == "__main__":
    main()