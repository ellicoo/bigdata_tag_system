#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Hiveæ•°æ®æºç®¡ç†ç±»
å‚è€ƒTFECUserPortraitçš„EsMetaè®¾è®¡æ¨¡å¼ï¼Œè´Ÿè´£Hiveè¡¨çš„è¯»å–ã€ç¼“å­˜å’ŒJOINæ“ä½œ
"""
from typing import List, Dict, Optional, Tuple
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import *
from datetime import datetime


class HiveMeta:
    """Hiveæ•°æ®æºç®¡ç†å™¨
    
    èŒè´£ï¼š
    1. ç®¡ç†Hiveè¡¨çš„è¯»å–å’Œç¼“å­˜
    2. æ™ºèƒ½JOINå¤šä¸ªè¡¨
    3. å­—æ®µé€‰æ‹©å’Œæ€§èƒ½ä¼˜åŒ–
    """
    
    def __init__(self, spark: SparkSession):
        """åˆå§‹åŒ–Hiveæ•°æ®æºç®¡ç†å™¨
        
        Args:
            spark: Sparkä¼šè¯
        """
        self.spark = spark
        self.cachedTables: Dict[str, DataFrame] = {}
        self.failed_tables: Dict[str, str] = {}  # è®°å½•åŠ è½½å¤±è´¥çš„è¡¨
        
        # è®¾ç½®åˆ†åŒºæ—¥æœŸä¸ºå½“å¤©
        self.partition_date = datetime.now().strftime("%Y-%m-%d")
        
        print(f"ğŸ—„ï¸  HiveMetaåˆå§‹åŒ–å®Œæˆï¼Œå°†ä½¿ç”¨åˆ†åŒº: {self.partition_date}")
    
    def loadAndJoinTablesWithFailureTracking(self, tableNames: List[str], 
                                             tagTableMapping: Dict[int, List[str]],
                                             fieldMapping: Optional[Dict[str, List[str]]] = None,
                                             joinKey: str = "user_id") -> Tuple[DataFrame, List[int]]:
        """åŠ è½½å¹¶JOINå¤šä¸ªè¡¨ï¼Œè·Ÿè¸ªå¤±è´¥çš„æ ‡ç­¾ID
        
        Args:
            tableNames: éœ€è¦JOINçš„è¡¨ååˆ—è¡¨
            tagTableMapping: æ ‡ç­¾IDåˆ°ä¾èµ–è¡¨çš„æ˜ å°„ {tag_id: [table1, table2]}
            fieldMapping: è¡¨ååˆ°å­—æ®µåˆ—è¡¨çš„æ˜ å°„
            joinKey: JOINçš„é”®ï¼Œé»˜è®¤ä¸ºuser_id
            
        Returns:
            Tuple[DataFrame, List[int]]: (JOINåçš„DataFrame, å¤±è´¥çš„æ ‡ç­¾IDåˆ—è¡¨)
        """
        print(f"ğŸ”— å¼€å§‹å®¹é”™åŠ è½½è¡¨: {tableNames}")
        
        # æ¸…ç©ºä¹‹å‰çš„å¤±è´¥è®°å½•
        self.failed_tables.clear()
        
        successful_tables = []
        failed_tag_ids = set()
        
        # é€ä¸ªå°è¯•åŠ è½½è¡¨
        for tableName in tableNames:
            try:
                selectFields = fieldMapping.get(tableName) if fieldMapping else None
                
                # å»ºç«‹åŸºç¡€é“¾å¼è°ƒç”¨ï¼šè¡¨ -> åˆ†åŒºè¿‡æ»¤
                df = self.spark.table(tableName).filter(col('dt') == self.partition_date)
                print(f"   ğŸ“… åº”ç”¨åˆ†åŒºè¿‡æ»¤: dt='{self.partition_date}'")
                
                # å­—æ®µé€‰æ‹©å’ŒéªŒè¯
                if selectFields:
                    # éªŒè¯å­—æ®µæ˜¯å¦å­˜åœ¨ï¼ˆè¿™é‡Œä¼šè§¦å‘æ‡’åŠ è½½æ‰§è¡Œï¼‰
                    available_columns = df.columns
                    missing_fields = [field for field in selectFields if field not in available_columns]
                    
                    if missing_fields:
                        error_msg = f"å­—æ®µä¸å­˜åœ¨: {missing_fields}"
                        raise Exception(error_msg)
                    
                    # ç¡®ä¿user_idåœ¨é€‰æ‹©å­—æ®µä¸­
                    if "user_id" not in selectFields:
                        selectFields = ["user_id"] + selectFields
                    
                    # ç»§ç»­é“¾å¼è°ƒç”¨ï¼šæ·»åŠ å­—æ®µé€‰æ‹©
                    df = df.select(*selectFields)

                successful_tables.append((tableName, df))
                print(f"âœ… è¡¨ {tableName} åŠ è½½æˆåŠŸ")
                
            except Exception as e:
                error_msg = str(e)
                self.failed_tables[tableName] = error_msg
                print(f"âŒ è¡¨ {tableName} åŠ è½½å¤±è´¥: {error_msg}")
                
                # æ‰¾å‡ºä¾èµ–æ­¤è¡¨çš„æ ‡ç­¾ID
                for tag_id, dependent_tables in tagTableMapping.items():
                    if tableName in dependent_tables:
                        failed_tag_ids.add(tag_id)
                        print(f"   ğŸ“Œ æ ‡ç­¾ {tag_id} å—å½±å“")
        
        
        # è®°å½•æˆåŠŸåŠ è½½çš„è¡¨ååˆ—è¡¨
        successful_table_names = [tableName for tableName, df in successful_tables]
        self._last_successful_tables = successful_table_names
        
        # å¦‚æœæ²¡æœ‰æˆåŠŸåŠ è½½çš„è¡¨ï¼Œè¿”å›ç©ºDataFrame
        if not successful_tables:
            print("âŒ æ‰€æœ‰è¡¨åŠ è½½å¤±è´¥ï¼Œè¿”å›ç©ºDataFrame")
            self._last_successful_tables = []
            return self._createEmptyDataFrame(), list(failed_tag_ids)
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªæˆåŠŸçš„è¡¨ï¼Œç›´æ¥è¿”å›
        if len(successful_tables) == 1:
            tableName, resultDF = successful_tables[0]
            aliasName = tableName.split('.')[-1]
            resultDF = resultDF.alias(aliasName)
            print(f"âœ… å•è¡¨åŠ è½½å®Œæˆ: {tableName}")
            return resultDF, list(failed_tag_ids)
        
        # å¤šè¡¨JOIN
        print(f"ğŸ”— å¼€å§‹JOIN {len(successful_tables)} ä¸ªæˆåŠŸåŠ è½½çš„è¡¨")
        resultDF = None
        
        for i, (tableName, currentDF) in enumerate(successful_tables):
            aliasName = tableName.split('.')[-1]
            
            if resultDF is None:
                # ç¬¬ä¸€ä¸ªè¡¨ä½œä¸ºåŸºç¡€
                resultDF = currentDF.alias(aliasName)
                print(f"   ğŸ“‹ åŸºç¡€è¡¨: {tableName}")
            else:
                # LEFT JOINåç»­è¡¨
                resultDF = resultDF.join(
                    currentDF.alias(aliasName),
                    joinKey,
                    "left"
                )
                print(f"   ğŸ”— LEFT JOIN: {tableName}")
        
        print(f"âœ… å®¹é”™JOINå®Œæˆï¼ŒæˆåŠŸ: {len(successful_tables)} è¡¨ï¼Œå¤±è´¥æ ‡ç­¾: {list(failed_tag_ids)}")
        return resultDF, list(failed_tag_ids)
    
    def clearCache(self):
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜çš„è¡¨"""
        for tableName, df in self.cachedTables.items():
            try:
                df.unpersist()
            except:
                pass
        
        self.cachedTables.clear()
        print("ğŸ§¹ HiveMetaç¼“å­˜å·²æ¸…ç†")
    
    def clearGroupCache(self, groupTables: List[str]):
        """æ¸…ç†ç‰¹å®šæ ‡ç­¾ç»„çš„è¡¨ç¼“å­˜ï¼ˆæ™ºèƒ½ç¼“å­˜ç®¡ç†ï¼‰
        
        Args:
            groupTables: è¯¥æ ‡ç­¾ç»„ä¾èµ–çš„è¡¨ååˆ—è¡¨
        """
        if not groupTables:
            return
        
        clearedTables = []
        
        # éå†æ‰€æœ‰ç¼“å­˜çš„è¡¨ï¼Œæ£€æŸ¥æ˜¯å¦å±äºè¯¥ç»„
        cachesToRemove = []
        
        for cacheKey, df in self.cachedTables.items():
            # æ£€æŸ¥ç¼“å­˜keyæ˜¯å¦å±äºè¯¥ç»„
            shouldClear = False
            cacheDescription = cacheKey  # ç”¨äºæ˜¾ç¤ºçš„æè¿°
            
            if cacheKey.startswith("JOIN:"):
                # JOINç¼“å­˜æ ¼å¼: "JOIN:table1,table2:fieldKey:joinKey"
                joinTables = cacheKey.split(":")[1].split(",")
                # å¦‚æœJOINä¸­åŒ…å«è¯¥ç»„çš„ä»»ä½•è¡¨ï¼Œåˆ™æ¸…ç†
                if any(table in groupTables for table in joinTables):
                    shouldClear = True
                    cacheDescription = f"JOIN({','.join(joinTables)})"
            else:
                # å•è¡¨ç¼“å­˜æ ¼å¼: "tableName:fields" æˆ– "tableName:ALL"  
                tableName = cacheKey.split(":")[0]
                if tableName in groupTables:
                    shouldClear = True
                    cacheDescription = tableName
            
            if shouldClear:
                try:
                    df.unpersist()
                    cachesToRemove.append(cacheKey)
                    clearedTables.append(cacheDescription)
                except Exception as e:
                    print(f"   âš ï¸  æ¸…ç†ç¼“å­˜ {cacheDescription} å¤±è´¥: {e}")
        
        # ä»ç¼“å­˜å­—å…¸ä¸­ç§»é™¤
        for cacheKey in cachesToRemove:
            del self.cachedTables[cacheKey]
        
        if clearedTables:
            # å»é‡æ˜¾ç¤ºæ¸…ç†çš„è¡¨
            uniqueTables = list(set(clearedTables))
            print(f"   ğŸ§¹ å·²æ¸…ç† {len(uniqueTables)} ä¸ªè¡¨çš„ç¼“å­˜: {uniqueTables}")
        else:
            print(f"   â„¹ï¸  ç»„ {groupTables} æ— éœ€æ¸…ç†ç¼“å­˜")
    
    def getFailureSummary(self) -> str:
        """è·å–å¤±è´¥æ‘˜è¦ä¿¡æ¯"""
        if not self.failed_tables:
            return "æ— è¡¨åŠ è½½å¤±è´¥"
        
        summary_parts = []
        for table, error in self.failed_tables.items():
            summary_parts.append(f"{table}: {error}")
        
        return f"å¤±è´¥è¡¨({len(self.failed_tables)}ä¸ª): {'; '.join(summary_parts)}"
    
    def getSuccessfulTables(self) -> List[str]:
        """è·å–æˆåŠŸåŠ è½½çš„è¡¨ååˆ—è¡¨"""
        # ä»æœ€è¿‘ä¸€æ¬¡å®¹é”™åŠ è½½ä¸­è·å–æˆåŠŸçš„è¡¨
        # è¿™ä¸ªéœ€è¦åœ¨loadAndJoinTablesWithFailureTrackingä¸­è®°å½•
        if hasattr(self, '_last_successful_tables'):
            return self._last_successful_tables
        return []

    def _createEmptyDataFrame(self) -> DataFrame:
        """åˆ›å»ºç©ºçš„DataFrameï¼ˆåŒ…å«user_idå­—æ®µï¼‰"""
        from pyspark.sql.types import StructType, StructField, StringType
        
        schema = StructType([
            StructField("user_id", StringType(), True)
        ])
        
        return self.spark.createDataFrame([], schema)
    
    def setPartitionDate(self, partition_date: str):
        """è®¾ç½®åˆ†åŒºæ—¥æœŸï¼ˆä»…åœ¨éœ€è¦æ—¶ä½¿ç”¨ï¼‰
        
        Args:
            partition_date: åˆ†åŒºæ—¥æœŸï¼Œæ ¼å¼ä¸º'YYYY-MM-DD'
        """
        self.partition_date = partition_date
        print(f"ğŸ“… åˆ†åŒºæ—¥æœŸå·²æ›´æ–°ä¸º: {self.partition_date}")
        
        # æ¸…ç†ç¼“å­˜ï¼Œå› ä¸ºåˆ†åŒºæ—¥æœŸå˜äº†
        self.clearCache()
    
    def getPartitionDate(self) -> str:
        """è·å–å½“å‰è®¾ç½®çš„åˆ†åŒºæ—¥æœŸ"""
        return self.partition_date