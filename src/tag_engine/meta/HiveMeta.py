#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Hiveæ•°æ®æºç®¡ç†ç±»
å‚è€ƒTFECUserPortraitçš„EsMetaè®¾è®¡æ¨¡å¼ï¼Œè´Ÿè´£Hiveè¡¨çš„è¯»å–ã€ç¼“å­˜å’ŒJOINæ“ä½œ
"""
from typing import List, Dict, Optional
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import *


class HiveMeta:
    """Hiveæ•°æ®æºç®¡ç†å™¨
    
    èŒè´£ï¼š
    1. ç®¡ç†Hiveè¡¨çš„è¯»å–å’Œç¼“å­˜
    2. æ™ºèƒ½JOINå¤šä¸ªè¡¨
    3. å­—æ®µé€‰æ‹©å’Œæ€§èƒ½ä¼˜åŒ–
    """
    
    def __init__(self, spark: SparkSession):
        """åˆå§‹åŒ–Hiveæ•°æ®æºç®¡ç†å™¨
        
        Args:
            spark: Sparkä¼šè¯
        """
        self.spark = spark
        self.cachedTables: Dict[str, DataFrame] = {}
        
        print("ğŸ—„ï¸  HiveMetaåˆå§‹åŒ–å®Œæˆ")
    
    def loadTable(self, tableName: str, selectFields: Optional[List[str]] = None) -> DataFrame:
        """åŠ è½½å•ä¸ªHiveè¡¨
        
        Args:
            tableName: è¡¨å
            selectFields: éœ€è¦é€‰æ‹©çš„å­—æ®µåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºé€‰æ‹©æ‰€æœ‰å­—æ®µ
            
        Returns:
            DataFrame: åŠ è½½çš„è¡¨DataFrame
        """
        cacheKey = f"{tableName}:{','.join(selectFields) if selectFields else 'ALL'}"
        
        if cacheKey not in self.cachedTables:
            print(f"ğŸ“– åŠ è½½Hiveè¡¨: {tableName}")
            
            try:
                df = self.spark.table(tableName)
                
                # å­—æ®µé€‰æ‹©ä¼˜åŒ–
                if selectFields:
                    # ç¡®ä¿user_idåœ¨é€‰æ‹©å­—æ®µä¸­ï¼ˆJOINéœ€è¦ï¼‰
                    if "user_id" not in selectFields:
                        selectFields = ["user_id"] + selectFields
                    df = df.select(*selectFields)
                
                # ç¼“å­˜DataFrame
                df.cache()
                self.cachedTables[cacheKey] = df
                
                print(f"âœ… è¡¨ {tableName} åŠ è½½å®Œæˆï¼Œå­—æ®µ: {selectFields or 'ALL'}")
                
            except Exception as e:
                print(f"âŒ åŠ è½½è¡¨ {tableName} å¤±è´¥: {e}")
                # è¿”å›ç©ºDataFrameï¼Œé¿å…ç¨‹åºå´©æºƒ
                return self._createEmptyDataFrame()
        
        return self.cachedTables[cacheKey]
    
    def loadTables(self, tableNames: List[str], fieldMapping: Optional[Dict[str, List[str]]] = None) -> Dict[str, DataFrame]:
        """æ‰¹é‡åŠ è½½å¤šä¸ªHiveè¡¨
        
        Args:
            tableNames: è¡¨ååˆ—è¡¨
            fieldMapping: è¡¨ååˆ°å­—æ®µåˆ—è¡¨çš„æ˜ å°„ï¼Œæ ¼å¼: {"table1": ["field1", "field2"]}
            
        Returns:
            Dict[str, DataFrame]: è¡¨ååˆ°DataFrameçš„æ˜ å°„
        """
        result = {}
        
        for tableName in tableNames:
            selectFields = fieldMapping.get(tableName) if fieldMapping else None
            result[tableName] = self.loadTable(tableName, selectFields)
        
        print(f"ğŸ“š æ‰¹é‡åŠ è½½å®Œæˆ: {len(result)} ä¸ªè¡¨")
        return result
    
    def loadAndJoinTables(self, tableNames: List[str], 
                         fieldMapping: Optional[Dict[str, List[str]]] = None,
                         joinKey: str = "user_id") -> DataFrame:
        """åŠ è½½å¹¶JOINå¤šä¸ªè¡¨
        
        Args:
            tableNames: éœ€è¦JOINçš„è¡¨ååˆ—è¡¨
            fieldMapping: è¡¨ååˆ°å­—æ®µåˆ—è¡¨çš„æ˜ å°„
            joinKey: JOINçš„é”®ï¼Œé»˜è®¤ä¸ºuser_id
            
        Returns:
            DataFrame: JOINåçš„ç»“æœDataFrame
        """
        if not tableNames:
            return self._createEmptyDataFrame()
        
        if len(tableNames) == 1:
            # åªæœ‰ä¸€ä¸ªè¡¨ï¼Œç›´æ¥è¿”å›
            return self.loadTable(tableNames[0], fieldMapping.get(tableNames[0]) if fieldMapping else None)
        
        print(f"ğŸ”— å¼€å§‹JOINè¡¨: {tableNames}")
        
        # åŠ è½½æ‰€æœ‰è¡¨
        tables = self.loadTables(tableNames, fieldMapping)
        
        # å¼€å§‹JOINæ“ä½œ
        resultDF = None
        
        for i, tableName in enumerate(tableNames):
            currentDF = tables[tableName]
            
            if resultDF is None:
                # ç¬¬ä¸€ä¸ªè¡¨ä½œä¸ºåŸºç¡€
                resultDF = currentDF.alias(tableName)
                print(f"   ğŸ“‹ åŸºç¡€è¡¨: {tableName}")
            else:
                # LEFT JOINåç»­è¡¨
                resultDF = resultDF.join(
                    currentDF.alias(tableName),
                    joinKey,
                    "left"
                )
                print(f"   ğŸ”— LEFT JOIN: {tableName}")
        
        print(f"âœ… JOINå®Œæˆï¼Œæ¶‰åŠ {len(tableNames)} ä¸ªè¡¨")
        return resultDF
    
    def getTableInfo(self, tableName: str) -> Dict[str, str]:
        """è·å–è¡¨ä¿¡æ¯
        
        Args:
            tableName: è¡¨å
            
        Returns:
            Dict: è¡¨ä¿¡æ¯å­—å…¸
        """
        try:
            df = self.spark.table(tableName)
            
            return {
                "tableName": tableName,
                "columnCount": len(df.columns),
                "columns": df.columns,
                "rowCount": df.count()
            }
        except Exception as e:
            print(f"âŒ è·å–è¡¨ {tableName} ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def clearCache(self):
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜çš„è¡¨"""
        for tableName, df in self.cachedTables.items():
            try:
                df.unpersist()
            except:
                pass
        
        self.cachedTables.clear()
        print("ğŸ§¹ HiveMetaç¼“å­˜å·²æ¸…ç†")
    
    def getCacheStats(self) -> Dict[str, Dict]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict: ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {}
        
        for cacheKey, df in self.cachedTables.items():
            try:
                stats[cacheKey] = {
                    "columns": len(df.columns),
                    "isCached": df.is_cached,
                    "storageLevel": str(df.storageLevel) if hasattr(df, 'storageLevel') else "UNKNOWN"
                }
            except Exception as e:
                stats[cacheKey] = {"error": str(e)}
        
        return stats
    
    def _createEmptyDataFrame(self) -> DataFrame:
        """åˆ›å»ºç©ºçš„DataFrameï¼ˆåŒ…å«user_idå­—æ®µï¼‰"""
        from pyspark.sql.types import StructType, StructField, StringType
        
        schema = StructType([
            StructField("user_id", StringType(), True)
        ])
        
        return self.spark.createDataFrame([], schema)
    
    def optimizeForTagComputation(self, df: DataFrame) -> DataFrame:
        """ä¸ºæ ‡ç­¾è®¡ç®—ä¼˜åŒ–DataFrame
        
        Args:
            df: è¾“å…¥DataFrame
            
        Returns:
            DataFrame: ä¼˜åŒ–åçš„DataFrame
        """
        # å»é‡ï¼ˆç¡®ä¿æ¯ä¸ªç”¨æˆ·åªæœ‰ä¸€æ¡è®°å½•ï¼‰
        df = df.dropDuplicates(["user_id"])
        
        # ç¼“å­˜ï¼ˆæ ‡ç­¾è®¡ç®—ä¼šå¤šæ¬¡ä½¿ç”¨ï¼‰
        df.cache()
        
        print(f"ğŸš€ DataFrameä¼˜åŒ–å®Œæˆï¼Œç”¨æˆ·æ•°: {df.count()}")
        return df