#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Hiveæ•°æ®æºç®¡ç†ç±»
å‚è€ƒTFECUserPortraitçš„EsMetaè®¾è®¡æ¨¡å¼ï¼Œè´Ÿè´£Hiveè¡¨çš„è¯»å–ã€ç¼“å­˜å’ŒJOINæ“ä½œ
"""
from typing import List, Dict, Optional
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import *


class HiveMeta:
    """Hiveæ•°æ®æºç®¡ç†å™¨
    
    èŒè´£ï¼š
    1. ç®¡ç†Hiveè¡¨çš„è¯»å–å’Œç¼“å­˜
    2. æ™ºèƒ½JOINå¤šä¸ªè¡¨
    3. å­—æ®µé€‰æ‹©å’Œæ€§èƒ½ä¼˜åŒ–
    """
    
    def __init__(self, spark: SparkSession):
        """åˆå§‹åŒ–Hiveæ•°æ®æºç®¡ç†å™¨
        
        Args:
            spark: Sparkä¼šè¯
        """
        self.spark = spark
        self.cachedTables: Dict[str, DataFrame] = {}
        
        print("ğŸ—„ï¸  HiveMetaåˆå§‹åŒ–å®Œæˆ")
    
    def loadTable(self, tableName: str, selectFields: Optional[List[str]] = None) -> DataFrame:
        """çº¯ç²¹åŠ è½½å•ä¸ªHiveè¡¨ï¼Œä¸ç¼“å­˜
        
        Args:
            tableName: è¡¨å
            selectFields: éœ€è¦é€‰æ‹©çš„å­—æ®µåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºé€‰æ‹©æ‰€æœ‰å­—æ®µ
            
        Returns:
            DataFrame: åŠ è½½çš„è¡¨DataFrameï¼ˆæœªç¼“å­˜ï¼‰
        """
        print(f"ğŸ“– åŠ è½½Hiveè¡¨: {tableName}")
        
        try:
            df = self.spark.table(tableName)
            
            # å­—æ®µé€‰æ‹©ä¼˜åŒ–
            if selectFields:
                # ç¡®ä¿user_idåœ¨é€‰æ‹©å­—æ®µä¸­ï¼ˆJOINéœ€è¦ï¼‰
                if "user_id" not in selectFields:
                    selectFields = ["user_id"] + selectFields
                df = df.select(*selectFields)
            
            print(f"âœ… è¡¨ {tableName} åŠ è½½å®Œæˆï¼Œå­—æ®µ: {selectFields or 'ALL'}")
            return df
            
        except Exception as e:
            print(f"âŒ åŠ è½½è¡¨ {tableName} å¤±è´¥: {e}")
            # è¿”å›ç©ºDataFrameï¼Œé¿å…ç¨‹åºå´©æºƒ
            return self._createEmptyDataFrame()
    
    def loadTables(self, tableNames: List[str], fieldMapping: Optional[Dict[str, List[str]]] = None) -> Dict[str, DataFrame]:
        """æ‰¹é‡åŠ è½½å¤šä¸ªHiveè¡¨
        
        Args:
            tableNames: è¡¨ååˆ—è¡¨
            fieldMapping: è¡¨ååˆ°å­—æ®µåˆ—è¡¨çš„æ˜ å°„ï¼Œæ ¼å¼: {"table1": ["field1", "field2"]}
            
        Returns:
            Dict[str, DataFrame]: è¡¨ååˆ°DataFrameçš„æ˜ å°„
        """
        result = {}
        
        for tableName in tableNames:
            selectFields = fieldMapping.get(tableName) if fieldMapping else None
            result[tableName] = self.loadTable(tableName, selectFields)
        
        print(f"ğŸ“š æ‰¹é‡åŠ è½½å®Œæˆ: {len(result)} ä¸ªè¡¨")
        return result
    
    def loadAndJoinTables(self, tableNames: List[str], 
                         fieldMapping: Optional[Dict[str, List[str]]] = None,
                         joinKey: str = "user_id") -> DataFrame:
        """åŠ è½½å¹¶JOINå¤šä¸ªè¡¨ï¼Œç»Ÿä¸€æ§åˆ¶ç¼“å­˜ç­–ç•¥
        
        ç­–ç•¥ï¼š
        - å•è¡¨æƒ…å†µï¼šç¼“å­˜å•è¡¨
        - å¤šè¡¨æƒ…å†µï¼šç¼“å­˜JOINç»“æœ
        
        Args:
            tableNames: éœ€è¦JOINçš„è¡¨ååˆ—è¡¨
            fieldMapping: è¡¨ååˆ°å­—æ®µåˆ—è¡¨çš„æ˜ å°„
            joinKey: JOINçš„é”®ï¼Œé»˜è®¤ä¸ºuser_id
            
        Returns:
            DataFrame: JOINåçš„ç»“æœDataFrameï¼ˆå·²ç¼“å­˜ï¼‰
        """
        if not tableNames:
            return self._createEmptyDataFrame()
        
        if len(tableNames) == 1:
            # ğŸš€ å•è¡¨æƒ…å†µï¼šç¼“å­˜å•è¡¨
            tableName = tableNames[0]
            selectFields = fieldMapping.get(tableName) if fieldMapping else None
            
            # ç”Ÿæˆå•è¡¨ç¼“å­˜key
            fieldKey = str(sorted(selectFields)) if selectFields else "ALL"
            singleTableCacheKey = f"{tableName}:{fieldKey}"
            
            # æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
            if singleTableCacheKey in self.cachedTables:
                print(f"ğŸš€ å¤ç”¨å•è¡¨ç¼“å­˜: {tableName}")
                return self.cachedTables[singleTableCacheKey]
            
            # åŠ è½½å¹¶ç¼“å­˜å•è¡¨
            resultDF = self.loadTable(tableName, selectFields)
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ç®€åŒ–çš„è¡¨åä½œä¸ºaliasï¼Œé¿å…ç‚¹å·å¯¼è‡´çš„åå¼•å·é—®é¢˜
            # ä¾‹å¦‚ï¼štag_system.user_asset_summary -> user_asset_summary
            aliasName = tableName.split('.')[-1]
            resultDF = resultDF.alias(aliasName)
            
            from pyspark import StorageLevel
            resultDF.persist(StorageLevel.MEMORY_AND_DISK)
            self.cachedTables[singleTableCacheKey] = resultDF
            
            print(f"âœ… å•è¡¨åŠ è½½å¹¶ç¼“å­˜å®Œæˆ: {tableName} (alias: {aliasName})")
            return resultDF
        
        # ğŸš€ å¤šè¡¨æƒ…å†µï¼šç¼“å­˜JOINç»“æœ
        # ä¸ºJOINç»“æœç”Ÿæˆç¼“å­˜keyï¼Œç¡®ä¿ç›¸åŒçš„JOINæ“ä½œå¤ç”¨ç»“æœ
        sortedTables = sorted(tableNames)
        fieldKey = ""
        if fieldMapping:
            fieldItems = sorted([(t, sorted(fields)) for t, fields in fieldMapping.items() if t in tableNames])
            fieldKey = str(fieldItems)
        joinCacheKey = f"JOIN:{','.join(sortedTables)}:{fieldKey}:{joinKey}"
        
        # æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜JOINç»“æœ
        if joinCacheKey in self.cachedTables:
            print(f"ğŸš€ å¤ç”¨JOINç¼“å­˜: {tableNames}")
            return self.cachedTables[joinCacheKey]
        
        print(f"ğŸ”— å¼€å§‹JOINè¡¨: {tableNames}")
        
        # ğŸš€ å…³é”®ä¼˜åŒ–ï¼šå¤šè¡¨æƒ…å†µç›´æ¥JOINï¼Œä¸ç¼“å­˜ä¸­é—´å­è¡¨ï¼Œåªç¼“å­˜æœ€ç»ˆJOINç»“æœ
        resultDF = None
        
        for i, tableName in enumerate(tableNames):
            # ç›´æ¥åŠ è½½è¡¨ï¼Œä¸ç¼“å­˜å­è¡¨
            selectFields = fieldMapping.get(tableName) if fieldMapping else None
            currentDF = self.loadTable(tableName, selectFields)
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ç®€åŒ–çš„è¡¨åä½œä¸ºaliasï¼Œé¿å…ç‚¹å·å¯¼è‡´çš„åå¼•å·é—®é¢˜
            # ä¾‹å¦‚ï¼štag_system.user_asset_summary -> user_asset_summary
            aliasName = tableName.split('.')[-1]
            
            if resultDF is None:
                # ç¬¬ä¸€ä¸ªè¡¨ä½œä¸ºåŸºç¡€
                resultDF = currentDF.alias(aliasName)
                print(f"   ğŸ“‹ åŸºç¡€è¡¨: {tableName} (alias: {aliasName})")
            else:
                # LEFT JOINåç»­è¡¨
                resultDF = resultDF.join(
                    currentDF.alias(aliasName),
                    joinKey,
                    "left"
                )
                print(f"   ğŸ”— LEFT JOIN: {tableName} (alias: {aliasName})")
        
        # ğŸš€ å…³é”®ç­–ç•¥ï¼šåªç¼“å­˜æœ€ç»ˆJOINç»“æœï¼Œä¸ç¼“å­˜ä¸­é—´å­è¡¨
        from pyspark import StorageLevel
        resultDF.persist(StorageLevel.MEMORY_AND_DISK)
        self.cachedTables[joinCacheKey] = resultDF
        
        print(f"âœ… JOINå®Œæˆå¹¶ç¼“å­˜æœ€ç»ˆç»“æœï¼Œæ¶‰åŠ {len(tableNames)} ä¸ªè¡¨")
        return resultDF
    
    def clearCache(self):
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜çš„è¡¨"""
        for tableName, df in self.cachedTables.items():
            try:
                df.unpersist()
            except:
                pass
        
        self.cachedTables.clear()
        print("ğŸ§¹ HiveMetaç¼“å­˜å·²æ¸…ç†")
    
    def clearGroupCache(self, groupTables: List[str]):
        """æ¸…ç†ç‰¹å®šæ ‡ç­¾ç»„çš„è¡¨ç¼“å­˜ï¼ˆæ™ºèƒ½ç¼“å­˜ç®¡ç†ï¼‰
        
        Args:
            groupTables: è¯¥æ ‡ç­¾ç»„ä¾èµ–çš„è¡¨ååˆ—è¡¨
        """
        if not groupTables:
            return
        
        clearedTables = []
        
        # éå†æ‰€æœ‰ç¼“å­˜çš„è¡¨ï¼Œæ£€æŸ¥æ˜¯å¦å±äºè¯¥ç»„
        cachesToRemove = []
        
        for cacheKey, df in self.cachedTables.items():
            # æ£€æŸ¥ç¼“å­˜keyæ˜¯å¦å±äºè¯¥ç»„
            shouldClear = False
            cacheDescription = cacheKey  # ç”¨äºæ˜¾ç¤ºçš„æè¿°
            
            if cacheKey.startswith("JOIN:"):
                # JOINç¼“å­˜æ ¼å¼: "JOIN:table1,table2:fieldKey:joinKey"
                joinTables = cacheKey.split(":")[1].split(",")
                # å¦‚æœJOINä¸­åŒ…å«è¯¥ç»„çš„ä»»ä½•è¡¨ï¼Œåˆ™æ¸…ç†
                if any(table in groupTables for table in joinTables):
                    shouldClear = True
                    cacheDescription = f"JOIN({','.join(joinTables)})"
            else:
                # å•è¡¨ç¼“å­˜æ ¼å¼: "tableName:fields" æˆ– "tableName:ALL"  
                tableName = cacheKey.split(":")[0]
                if tableName in groupTables:
                    shouldClear = True
                    cacheDescription = tableName
            
            if shouldClear:
                try:
                    df.unpersist()
                    cachesToRemove.append(cacheKey)
                    clearedTables.append(cacheDescription)
                except Exception as e:
                    print(f"   âš ï¸  æ¸…ç†ç¼“å­˜ {cacheDescription} å¤±è´¥: {e}")
        
        # ä»ç¼“å­˜å­—å…¸ä¸­ç§»é™¤
        for cacheKey in cachesToRemove:
            del self.cachedTables[cacheKey]
        
        if clearedTables:
            # å»é‡æ˜¾ç¤ºæ¸…ç†çš„è¡¨
            uniqueTables = list(set(clearedTables))
            print(f"   ğŸ§¹ å·²æ¸…ç† {len(uniqueTables)} ä¸ªè¡¨çš„ç¼“å­˜: {uniqueTables}")
        else:
            print(f"   â„¹ï¸  ç»„ {groupTables} æ— éœ€æ¸…ç†ç¼“å­˜")
    
    def _createEmptyDataFrame(self) -> DataFrame:
        """åˆ›å»ºç©ºçš„DataFrameï¼ˆåŒ…å«user_idå­—æ®µï¼‰"""
        from pyspark.sql.types import StructType, StructField, StringType
        
        schema = StructType([
            StructField("user_id", StringType(), True)
        ])
        
        return self.spark.createDataFrame([], schema)