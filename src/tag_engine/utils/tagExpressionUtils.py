#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ‡ç­¾è¡¨è¾¾å¼æž„å»ºå·¥å…·
ä¸“é—¨å¤„ç†å¹¶è¡Œæ ‡ç­¾è®¡ç®—çš„è¡¨è¾¾å¼æž„å»ºé€»è¾‘ - ä½¿ç”¨æ¨¡å—çº§å‡½æ•°é¿å…åºåˆ—åŒ–é—®é¢˜
"""
from pyspark.sql.functions import *


def buildParallelTagExpression(tagConditions):
    """æž„å»ºå¹¶è¡Œæ ‡ç­¾è®¡ç®—è¡¨è¾¾å¼ - TagGroupæ ¸å¿ƒä¼˜åŒ–é€»è¾‘
    
    å°†å¤šä¸ªæ ‡ç­¾æ¡ä»¶ç»„åˆæˆä¸€ä¸ªå¹¶è¡Œè®¡ç®—è¡¨è¾¾å¼ï¼Œä¸€æ¬¡æ€§è¯„ä¼°æ‰€æœ‰æ ‡ç­¾
    åœ¨Driverç«¯æ‰§è¡Œï¼Œè¿”å›žSpark Columnè¡¨è¾¾å¼ï¼Œå®Œå…¨é¿å…Pythonå¯¹è±¡åºåˆ—åŒ–
    
    Args:
        tagConditions: List[Dict] - æ ‡ç­¾æ¡ä»¶åˆ—è¡¨
            [{'tag_id': 1, 'condition': 'age >= 30'}, 
             {'tag_id': 2, 'condition': 'assets >= 10000'}, ...]
            
    Returns:
        Column - å¹¶è¡Œæ ‡ç­¾æ•°ç»„è¡¨è¾¾å¼ï¼Œå¯ç›´æŽ¥ç”¨äºŽDataFrame.withColumn()
        
    Example:
        >>> conditions = [
        ...     {'tag_id': 1, 'condition': 'age >= 30'},
        ...     {'tag_id': 2, 'condition': 'assets >= 10000'}
        ... ]
        >>> expr = buildParallelTagExpression(conditions)
        >>> df.withColumn("tag_ids_array", expr)
    """
    if not tagConditions:
        return array()
    
    # ðŸš€ å…³é”®ä¼˜åŒ–ï¼šæž„å»ºSQLè¡¨è¾¾å¼å­—ç¬¦ä¸²ï¼Œä½¿ç”¨filteré«˜é˜¶å‡½æ•°
    # ç›´æŽ¥æž„å»ºå®Œæ•´çš„SQLè¡¨è¾¾å¼ï¼Œé¿å…Columnå¯¹è±¡çš„å¤æ‚æ€§
    
    # æž„å»ºcase whenè¡¨è¾¾å¼åˆ—è¡¨
    case_expressions = []
    for tagInfo in tagConditions:
        tag_id = tagInfo['tag_id']
        condition = tagInfo['condition']
        case_expressions.append(f"case when {condition} then {tag_id} else null end")
    
    # æž„å»ºå®Œæ•´çš„è¡¨è¾¾å¼ï¼šarray + filter + sort + distinct
    sql_expr = f"""
    array_distinct(
        array_sort(
            filter(
                array({', '.join(case_expressions)}), 
                x -> x is not null
            )
        )
    )
    """.strip().replace('\n', ' ').replace('    ', ' ')
    
    return expr(sql_expr)