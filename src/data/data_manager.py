"""
ç»Ÿä¸€æ•°æ®ç®¡ç†å™¨ - è´Ÿè´£æ‰€æœ‰æ•°æ®çš„è¯»å–ã€ç¼“å­˜å’Œå†™å…¥
è§£å†³é‡å¤æ•°æ®åº“è¿æ¥é—®é¢˜ï¼Œå®ç°èµ„æºå¤ç”¨
"""

import json
import logging
from typing import List, Dict, Any, Optional
from datetime import date
from pyspark.sql import SparkSession, DataFrame
from pyspark import StorageLevel

from src.config.base import MySQLConfig

logger = logging.getLogger(__name__)


class UnifiedDataManager:
    """ç»Ÿä¸€æ•°æ®ç®¡ç†å™¨ - ä¸€æ¬¡è¿æ¥ï¼Œå…¨å±€å¤ç”¨"""
    
    def __init__(self, spark: SparkSession, mysql_config: MySQLConfig):
        self.spark = spark
        self.mysql_config = mysql_config
        
        # ç¼“å­˜æ‰€æœ‰DataFrameï¼Œç›´æ¥æä¾›ç»™å„ç»„ä»¶ä½¿ç”¨
        self._rules_df = None
        self._tag_definitions_df = None
        self._existing_user_tags_df = None
        
        # æ ‡è®°æ˜¯å¦å·²åˆå§‹åŒ–
        self._initialized = False
    
    def initialize(self):
        """ä¸€æ¬¡æ€§åˆå§‹åŒ–æ‰€æœ‰éœ€è¦çš„æ•°æ®"""
        if self._initialized:
            logger.info("æ•°æ®ç®¡ç†å™¨å·²åˆå§‹åŒ–ï¼Œä½¿ç”¨ç¼“å­˜æ•°æ®")
            return
        
        logger.info("ğŸ”„ å¼€å§‹ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰åŸºç¡€æ•°æ®...")
        
        try:
            # 1. åŠ è½½æ ‡ç­¾è§„åˆ™ï¼ˆè”è¡¨æŸ¥è¯¢ï¼Œä¸€æ¬¡æå®šï¼‰
            self._load_tag_rules()
            
            # 2. åŠ è½½æ ‡ç­¾å®šä¹‰ï¼ˆå°è¡¨ï¼ŒæŒä¹…åŒ–åˆ°å†…å­˜+ç£ç›˜ï¼‰
            self._load_tag_definitions()
            
            # 3. åŠ è½½ç°æœ‰ç”¨æˆ·æ ‡ç­¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            self._load_existing_user_tags()
            
            self._initialized = True
            logger.info("âœ… ç»Ÿä¸€æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _load_tag_rules(self):
        """åŠ è½½æ ‡ç­¾è§„åˆ™ï¼ˆä¸€æ¬¡æ€§ï¼Œå¸¦æŒä¹…åŒ–ï¼‰- ç›´æ¥ä¿æŒDataFrameæ ¼å¼"""
        logger.info("ğŸ“– åŠ è½½æ ‡ç­¾è§„åˆ™...")
        
        query = """
        (SELECT 
            tr.rule_id,
            tr.tag_id,
            tr.rule_conditions,
            tr.is_active as rule_active,
            td.tag_name,
            td.tag_category,
            td.description as tag_description,
            td.is_active as tag_active
         FROM tag_rules tr 
         JOIN tag_definition td ON tr.tag_id = td.tag_id 
         WHERE tr.is_active = 1 AND td.is_active = 1) as active_rules
        """
        
        self._rules_df = self.spark.read.jdbc(
            url=self.mysql_config.jdbc_url,
            table=query,
            properties=self.mysql_config.connection_properties
        ).persist(StorageLevel.MEMORY_AND_DISK)  # æŒä¹…åŒ–åˆ°å†…å­˜+ç£ç›˜
        
        # è§¦å‘æŒä¹…åŒ–å¹¶è·å–ç»Ÿè®¡
        rule_count = self._rules_df.count()
        logger.info(f"âœ… æ ‡ç­¾è§„åˆ™DataFrameåŠ è½½å®Œæˆï¼Œå…± {rule_count} æ¡")
    
    def _load_tag_definitions(self):
        """åŠ è½½æ ‡ç­¾å®šä¹‰ï¼ˆå°è¡¨ï¼Œç›´æ¥æŒä¹…åŒ–ï¼‰"""
        logger.info("ğŸ“– åŠ è½½æ ‡ç­¾å®šä¹‰...")
        
        self._tag_definitions_df = self.spark.read.jdbc(
            url=self.mysql_config.jdbc_url,
            table="tag_definition",
            properties=self.mysql_config.connection_properties
        ).persist(StorageLevel.MEMORY_AND_DISK)
        
        tag_def_count = self._tag_definitions_df.count()
        logger.info(f"âœ… æ ‡ç­¾å®šä¹‰DataFrameåŠ è½½å®Œæˆï¼Œå…± {tag_def_count} æ¡")
    
    def _load_existing_user_tags(self):
        """åŠ è½½ç°æœ‰ç”¨æˆ·æ ‡ç­¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰- ä¼˜åŒ–è¶…æ—¶å¤„ç†"""
        try:
            logger.info("ğŸ“– åŠ è½½ç°æœ‰ç”¨æˆ·æ ‡ç­¾...")
            
            # ä¼˜åŒ–è¿æ¥å±æ€§ï¼Œæ·»åŠ è¶…æ—¶è®¾ç½®
            timeout_properties = {
                **self.mysql_config.connection_properties,
                "connectTimeout": "5000",      # 5ç§’è¿æ¥è¶…æ—¶
                "socketTimeout": "10000",      # 10ç§’socketè¶…æ—¶
                "queryTimeout": "30"           # 30ç§’æŸ¥è¯¢è¶…æ—¶
            }
            
            existing_df = self.spark.read.jdbc(
                url=self.mysql_config.jdbc_url,
                table="user_tags",
                properties=timeout_properties
            )
            
            # ä½¿ç”¨limit(1)å¿«é€Ÿæ£€æŸ¥è¡¨æ˜¯å¦æœ‰æ•°æ®ï¼Œé¿å…å…¨è¡¨æ‰«æ
            sample_check = existing_df.limit(1)
            sample_count = sample_check.count()
            
            if sample_count > 0:
                # æœ‰æ•°æ®æ‰è¿›è¡Œå®Œæ•´åŠ è½½
                logger.info("æ£€æµ‹åˆ°ç°æœ‰ç”¨æˆ·æ ‡ç­¾ï¼Œå¼€å§‹å®Œæ•´åŠ è½½...")
                
                # è½¬æ¢JSONå¹¶æŒä¹…åŒ–
                from pyspark.sql.functions import from_json, col
                from pyspark.sql.types import ArrayType, IntegerType
                
                self._existing_user_tags_df = existing_df.select(
                    "user_id",
                    from_json(col("tag_ids"), ArrayType(IntegerType())).alias("tag_ids"),
                    "tag_details"
                ).persist(StorageLevel.MEMORY_AND_DISK)
                
                existing_count = self._existing_user_tags_df.count()
                logger.info(f"âœ… ç°æœ‰ç”¨æˆ·æ ‡ç­¾DataFrameåŠ è½½å®Œæˆï¼Œå…± {existing_count} æ¡")
            else:
                logger.info("ğŸ“ user_tagsè¡¨ä¸ºç©ºï¼ˆé¦–æ¬¡è¿è¡Œï¼‰")
                self._existing_user_tags_df = None
                
        except Exception as e:
            logger.info(f"ğŸ“ ç°æœ‰ç”¨æˆ·æ ‡ç­¾è¯»å–å¤±è´¥ï¼ˆå¯èƒ½è¡¨ä¸å­˜åœ¨æˆ–è¿æ¥è¶…æ—¶ï¼‰: {str(e)}")
            self._existing_user_tags_df = None
    
    # ==== å¯¹å¤–æä¾›çš„æ•°æ®è®¿é—®æ¥å£ ====
    
    def get_active_rules_df(self) -> DataFrame:
        """è·å–æ´»è·ƒæ ‡ç­¾è§„åˆ™DataFrame"""
        if not self._initialized:
            self.initialize()
        return self._rules_df
    
    def get_rules_by_category_df(self, category_name: str) -> DataFrame:
        """æŒ‰åˆ†ç±»è·å–æ ‡ç­¾è§„åˆ™DataFrame"""
        rules_df = self.get_active_rules_df()
        return rules_df.filter(rules_df.tag_category == category_name)
    
    def get_tag_definitions_df(self) -> DataFrame:
        """è·å–æ ‡ç­¾å®šä¹‰DataFrame"""
        if not self._initialized:
            self.initialize()
        return self._tag_definitions_df
    
    def get_existing_user_tags_df(self) -> Optional[DataFrame]:
        """è·å–ç°æœ‰ç”¨æˆ·æ ‡ç­¾DataFrame"""
        if not self._initialized:
            self.initialize()
        return self._existing_user_tags_df
    
    def get_rules_for_computation(self):
        """ä¸ºæ ‡ç­¾è®¡ç®—æä¾›è§„åˆ™æ•°æ® - è¿”å›å¯è¿­ä»£çš„è§„åˆ™ä¿¡æ¯"""
        rules_df = self.get_active_rules_df()
        # åªåœ¨éœ€è¦æ—¶è½¬æ¢ä¸ºPythonå¯¹è±¡ï¼ˆç”¨äºè§„åˆ™è§£æï¼‰
        return rules_df.collect()
    
    def write_user_tags(self, result_df: DataFrame, mode: str = "overwrite") -> bool:
        """ç»Ÿä¸€çš„æ ‡ç­¾å†™å…¥æ¥å£ - å¸¦é‡è¯•æœºåˆ¶"""
        import time
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                logger.info(f"ğŸ”„ å¼€å§‹å†™å…¥ç”¨æˆ·æ ‡ç­¾ï¼Œæ¨¡å¼: {mode}ï¼Œå°è¯•: {attempt + 1}/{max_retries}")
                
                # æ•°æ®é¢„å¤„ç†
                from pyspark.sql.functions import to_json, col, when
                mysql_ready_df = result_df.select(
                    col("user_id"),
                    when(col("tag_ids").isNotNull(), to_json(col("tag_ids")))
                    .otherwise("[]").alias("tag_ids"),
                    col("tag_details"),
                    col("computed_date")
                )
                
                # æ˜¾ç¤ºæ•°æ®æ ·ä¾‹ï¼ˆä»…ç¬¬ä¸€æ¬¡å°è¯•æ—¶ï¼‰
                if attempt == 0:
                    logger.info("å†™å…¥æ•°æ®æ ·ä¾‹:")
                    mysql_ready_df.show(3, truncate=False)
                
                # å•è¿æ¥å¤šæ‰¹æ¬¡å†™å…¥å‚æ•°ä¼˜åŒ–
                write_properties = {
                    **self.mysql_config.connection_properties,
                    "batchsize": "5000",           # å•è¿æ¥å†…æ¯5000æ¡ä¸€ä¸ªæ‰¹æ¬¡æäº¤
                    "numPartitions": "1",          # å¼ºåˆ¶å•åˆ†åŒºï¼ˆé…åˆcoalesceç¡®ä¿å•è¿æ¥ï¼‰
                    "connectTimeout": "60000",     # 60ç§’è¿æ¥è¶…æ—¶
                    "socketTimeout": "300000",     # 5åˆ†é’Ÿsocketè¶…æ—¶ï¼Œç»™æ‰¹é‡æäº¤è¶³å¤Ÿæ—¶é—´
                    "queryTimeout": "300",         # 5åˆ†é’ŸæŸ¥è¯¢è¶…æ—¶
                    "autoReconnect": "true",       # è‡ªåŠ¨é‡è¿
                    "rewriteBatchedStatements": "true",  # å¯ç”¨æ‰¹é‡SQLé‡å†™ä¼˜åŒ–
                    "useCompression": "true",      # å¯ç”¨å‹ç¼©å‡å°‘ç½‘ç»œä¼ è¾“
                    "cachePrepStmts": "true",      # ç¼“å­˜é¢„å¤„ç†è¯­å¥
                    "prepStmtCacheSize": "250",    # é¢„å¤„ç†è¯­å¥ç¼“å­˜å¤§å°
                    "prepStmtCacheSqlLimit": "2048" # é¢„å¤„ç†è¯­å¥SQLé•¿åº¦é™åˆ¶
                }
                
                total_count = mysql_ready_df.count()
                logger.info(f"å‡†å¤‡å†™å…¥ {total_count} æ¡ç”¨æˆ·æ ‡ç­¾æ•°æ®")
                
                # é‡‡ç”¨è‡ªå®šä¹‰æ‰¹é‡å†™å…¥ç­–ç•¥ï¼šforeachPartition + JDBCæ‰¹å¤„ç†
                logger.info("é‡‡ç”¨ foreachPartition + JDBCæ‰¹å¤„ç† ç­–ç•¥")
                
                # æ¢å¤å¤šåˆ†åŒºæµ‹è¯•ï¼šä½¿ç”¨DELETEæ”¯æŒå¹¶å‘
                target_rows_per_partition = 8000
                optimal_partitions = max(1, min(8, total_count // target_rows_per_partition))
                logger.info(f"å¤šåˆ†åŒºæ¨¡å¼ï¼š{optimal_partitions}ä¸ªåˆ†åŒºï¼Œæ¯åˆ†åŒºçº¦{total_count//optimal_partitions if optimal_partitions > 0 else total_count}æ¡æ•°æ®")
                
                repartitioned_df = mysql_ready_df.repartition(optimal_partitions)
                
                # å¤„ç†overwriteæ¨¡å¼ï¼šåˆ é™¤å½“å¤©æ•°æ®
                if mode == "overwrite":
                    from datetime import date
                    today = date.today()
                    logger.info(f"overwriteæ¨¡å¼ï¼šåˆ é™¤ {today} çš„ç”¨æˆ·æ ‡ç­¾æ•°æ®")
                    self._delete_user_tags_for_date(today)
                
                # ä½¿ç”¨è‡ªå®šä¹‰æ‰¹é‡å†™å…¥å‡½æ•°
                self._write_with_custom_batch(repartitioned_df, "append")
                
                logger.info(f"âœ… ç”¨æˆ·æ ‡ç­¾å†™å…¥æˆåŠŸï¼Œå…± {result_df.count()} æ¡è®°å½•")
                return True
                
            except Exception as e:
                logger.error(f"âŒ ç¬¬ {attempt + 1} æ¬¡å†™å…¥å¤±è´¥: {str(e)}")
                
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 10  # é€’å¢ç­‰å¾…æ—¶é—´
                    logger.info(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"âŒ ç”¨æˆ·æ ‡ç­¾å†™å…¥å½»åº•å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡")
                    return False
        
        return False
    
    def _write_with_custom_batch(self, df: DataFrame, mode: str):
        """è‡ªå®šä¹‰æ‰¹é‡å†™å…¥ï¼šforeachPartition + JDBCæ‰¹å¤„ç†"""
        import pymysql
        from urllib.parse import urlparse, parse_qs
        
        # æå–é…ç½®å‚æ•°é¿å…é—­åŒ…åºåˆ—åŒ–é—®é¢˜
        host = self.mysql_config.host
        port = self.mysql_config.port
        username = self.mysql_config.username
        password = self.mysql_config.password
        database = self.mysql_config.database
        
        def write_partition_to_mysql(partition_data):
            """æ¯ä¸ªåˆ†åŒºçš„å†™å…¥é€»è¾‘"""
            import pymysql
            import json
            from datetime import date
            
            # è½¬æ¢è¿­ä»£å™¨ä¸ºåˆ—è¡¨
            rows = list(partition_data)
            if not rows:
                print("åˆ†åŒºä¸ºç©ºï¼Œè·³è¿‡")
                return
                
            partition_size = len(rows)
            batch_size = 2000  # æ¯æ‰¹å¤„ç†2000æ¡
            print(f"å¼€å§‹å¤„ç†åˆ†åŒºï¼š{partition_size}æ¡æ•°æ®")
            
            # å»ºç«‹MySQLè¿æ¥
            connection = None
            try:
                print(f"è¿æ¥MySQL: {host}:{port}")
                connection = pymysql.connect(
                    host=host,
                    port=port,
                    user=username,
                    password=password,
                    database=database,
                    charset='utf8mb4',
                    autocommit=False,  # æ‰‹åŠ¨æ§åˆ¶äº‹åŠ¡
                    connect_timeout=30,  # 30ç§’è¿æ¥è¶…æ—¶
                    read_timeout=60,     # 60ç§’è¯»å–è¶…æ—¶
                    write_timeout=60     # 60ç§’å†™å…¥è¶…æ—¶
                )
                print("MySQLè¿æ¥æˆåŠŸ")
                
                cursor = connection.cursor()
                
                # æ ¹æ®æ¨¡å¼å¤„ç†
                if mode == "overwrite":
                    # åªåœ¨ç¬¬ä¸€ä¸ªåˆ†åŒºæ‰§è¡ŒTRUNCATEï¼ˆéœ€è¦åè°ƒæœºåˆ¶ï¼‰
                    pass  # æš‚æ—¶è·³è¿‡ï¼Œé¿å…å¤šåˆ†åŒºå†²çª
                
                # å‡†å¤‡æ‰¹é‡æ’å…¥SQLï¼ˆç®€åŒ–ç‰ˆï¼Œé¿å…é”å†²çªï¼‰
                insert_sql = """
                INSERT INTO user_tags (user_id, tag_ids, tag_details, computed_date) 
                VALUES (%s, %s, %s, %s)
                """
                
                # åˆ†æ‰¹å¤„ç†æ•°æ®
                for i in range(0, partition_size, batch_size):
                    batch_rows = rows[i:i + batch_size]
                    batch_data = []
                    
                    print(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}ï¼Œæ•°æ®è¡Œæ•°ï¼š{len(batch_rows)}")
                    
                    try:
                        for j, row in enumerate(batch_rows):
                            # å¤„ç†Spark Rowå¯¹è±¡æ•°æ®æ ¼å¼
                            user_id = str(row.user_id)
                            tag_ids = str(row.tag_ids) if row.tag_ids else '[]'
                            tag_details = str(row.tag_details) if row.tag_details else '{}'
                            computed_date = row.computed_date
                            
                            batch_data.append((user_id, tag_ids, tag_details, computed_date))
                            
                            if j < 2:  # åªæ‰“å°å‰2æ¡è°ƒè¯•
                                print(f"æ•°æ®æ ·ä¾‹ {j}: user_id={user_id}, tag_ids={tag_ids[:50]}")
                    
                        print(f"å¼€å§‹æ‰§è¡Œæ‰¹é‡æ’å…¥ï¼Œæ•°æ®é‡ï¼š{len(batch_data)}")
                        cursor.executemany(insert_sql, batch_data)
                        print(f"æ‰¹é‡æ’å…¥SQLæ‰§è¡Œå®Œæˆ")
                        
                        print(f"åˆ†åŒºæ‰¹æ¬¡å®Œæˆï¼š{len(batch_data)}æ¡æ•°æ®")
                        
                    except Exception as batch_e:
                        print(f"æ‰¹æ¬¡å¤„ç†å¤±è´¥ï¼š{str(batch_e)}")
                        raise
                
                # æäº¤äº‹åŠ¡
                connection.commit()
                print(f"åˆ†åŒºå†™å…¥å®Œæˆï¼šæ€»è®¡{partition_size}æ¡æ•°æ®")
                
            except Exception as e:
                if connection:
                    connection.rollback()
                print(f"åˆ†åŒºå†™å…¥å¤±è´¥ï¼š{str(e)}")
                raise
            finally:
                if connection:
                    connection.close()
        
        # æ‰§è¡Œåˆ†åŒºå†™å…¥
        try:
            logger.info("å¼€å§‹æ‰§è¡Œ foreachPartition è‡ªå®šä¹‰æ‰¹é‡å†™å…¥")
            df.foreachPartition(write_partition_to_mysql)
            logger.info("âœ… foreachPartition æ‰¹é‡å†™å…¥å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ è‡ªå®šä¹‰æ‰¹é‡å†™å…¥å¤±è´¥: {str(e)}")
            raise
    
    def _delete_user_tags_for_date(self, computed_date):
        """åˆ é™¤æŒ‡å®šæ—¥æœŸçš„ç”¨æˆ·æ ‡ç­¾ - ä½¿ç”¨è¡Œçº§é”ï¼Œæ”¯æŒå¹¶å‘"""
        import pymysql
        from datetime import date
        
        connection = None
        try:
            connection = pymysql.connect(
                host=self.mysql_config.host,
                port=self.mysql_config.port,
                user=self.mysql_config.username,
                password=self.mysql_config.password,
                database=self.mysql_config.database,
                charset='utf8mb4',
                autocommit=True
            )
            
            cursor = connection.cursor()
            # ä½¿ç”¨DELETEä»£æ›¿TRUNCATEï¼Œæ”¯æŒå¹¶å‘
            delete_sql = "DELETE FROM user_tags WHERE computed_date = %s"
            cursor.execute(delete_sql, (computed_date,))
            deleted_count = cursor.rowcount
            logger.info(f"âœ… åˆ é™¤ {computed_date} çš„ç”¨æˆ·æ ‡ç­¾æ•°æ®ï¼Œå…± {deleted_count} æ¡")
            
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤ç”¨æˆ·æ ‡ç­¾æ•°æ®å¤±è´¥: {str(e)}")
            raise
        finally:
            if connection:
                connection.close()
    
    def get_all_required_fields_from_rules(self) -> str:
        """ä»è§„åˆ™DataFrameä¸­è·å–æ‰€æœ‰éœ€è¦çš„å­—æ®µ"""
        fields_set = set(['user_id'])
        
        # ä»DataFrameä¸­è·å–è§„åˆ™æ¡ä»¶ï¼Œè§£æå­—æ®µ
        rules_list = self.get_rules_for_computation()
        for rule in rules_list:
            rule_dict = rule.asDict()
            try:
                rule_conditions = json.loads(rule_dict['rule_conditions'])
                conditions = rule_conditions['conditions']
                for condition in conditions:
                    if 'field' in condition:
                        fields_set.add(condition['field'])
            except (json.JSONDecodeError, KeyError, TypeError):
                continue
        
        return ','.join(sorted(fields_set))
    
    def cleanup(self):
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜ï¼Œé‡Šæ”¾èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†ç»Ÿä¸€æ•°æ®ç®¡ç†å™¨ç¼“å­˜...")
        
        try:
            # é‡Šæ”¾æ‰€æœ‰persistçš„DataFrame
            if self._rules_df is not None:
                logger.info("ğŸ§¹ é‡Šæ”¾è§„åˆ™DataFrame persistç¼“å­˜")
                self._rules_df.unpersist()
                
            if self._tag_definitions_df is not None:
                logger.info("ğŸ§¹ é‡Šæ”¾æ ‡ç­¾å®šä¹‰DataFrame persistç¼“å­˜")
                self._tag_definitions_df.unpersist()
            
            if self._existing_user_tags_df is not None:
                logger.info("ğŸ§¹ é‡Šæ”¾ç°æœ‰ç”¨æˆ·æ ‡ç­¾DataFrame persistç¼“å­˜")
                self._existing_user_tags_df.unpersist()
                
            # æ¸…ç©ºæ‰€æœ‰DataFrameå¼•ç”¨
            self._rules_df = None
            self._tag_definitions_df = None
            self._existing_user_tags_df = None
            self._initialized = False
            
            logger.info("âœ… æ•°æ®ç®¡ç†å™¨ç¼“å­˜æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç¼“å­˜æ¸…ç†å¼‚å¸¸: {str(e)}")
    
    def get_statistics(self) -> dict:
        """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_rules": self._rules_df.count() if self._rules_df else 0,
            "total_tag_definitions": self._tag_definitions_df.count() if self._tag_definitions_df else 0,
            "existing_user_tags": self._existing_user_tags_df.count() if self._existing_user_tags_df else 0,
            "initialized": self._initialized
        }